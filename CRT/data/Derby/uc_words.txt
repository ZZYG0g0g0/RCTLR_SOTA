Currently the stream writing methods in network server and client require a length parameter . This means that we have to get the length of the stream before sending it . For example in network server in EXTDTAInputStream we have to use getString and getbytes ( ) instead of getCharacterStream and getBinaryStream so that we can get the length . SQLAM Level 7 provides for the enhanced LOB processing to allow streaming without indicating the length , so , the writeScalarStream methods in network server DDMWriter.java and network client Request.java can be changed to not require a length . Code inspection of these methods seems to indicate that while the length is never written it is used heavily in generating the DSS . One strange thing is that it appears on error , the stream is padded out to full length with zeros , but an actual exception is never sent . Basically I think perhaps these methods need to be rewritten from scratch based on the spec requirements for lobs . After the writeScalarStream methods have been changed , then EXTDAInputStream can be changed to properly stream LOBS . See TODO tags in this file for more info . I am guessing similar optimizations available in the client as well , but am not sure where that code is . 
If not unlimited than we should at least make them 80 ( just to pick a abitrary number ) 
It would be nice if Derby supported the NATURAL JOIN syntax ( feature F401-01 in SQL:2003 ) . 
Provide an implementation of the ROLLUP form of multi-dimensional grouping according to the SQL standard . See for some more detailed information about this aspect of the SQL standard . 
Cloudscape used to expose Virtual Table Interfaces , by which any class which implemented ResultSet could be included in a query s FROM list . Derby still exposes a number of these VTIs as diagnostic tools . However , Derby now prevents customers from declaring their own VTIs . The parser raises an error if a VTI s package isn t one of the Derby diagnostic packages . This is a very powerful feature which customers can use to solve many problems . We should discuss the reasons that it was disabled and come up with a plan for putting this power back into our customers hands . 
Would like to see support added for sequences . This would permit a select against the sequence to always obtain a ever increasing/decreasing value . The identity column works fine but there are times for applications where the application needs to obtain the sequence number and use it prior to the database write . Subsequent calls to the table/column would result in a new number on each call . SQL such as the following : SELECT NEXT VALUE FOR sequence_name FROM sometable ; would result in a next value . 
& gt ; Shouldn t DistinctScalarAggregateRS implement a close or a finish method & gt ; & gt ; ( not sure what the difference is ) and close the scan controller there . The close ( ) and finish ( ) methods are actually explained in their javadoc in the language org.apache.derby.iapi.sql.ResultSet class . & # 91 ; note this is not a JDBC java.sql.ResultSet object & # 93 ; close ( ) - Tells the system that there will be no more calls to getNextRow ( ) ( until the next open ( ) call ) finish ( ) - Tells the system that there will be no more access to any database information via this result set So close means the ResultSet may be opened again for more access , while finish means it will not be used again . However , their use in the code always doesn t match that , and that does cause confusion , at least to me . Language ResultSets ( not JDBC ones ) can be and are opened multiple times , for example when scanning a table multiple times within a join . An Activation , which represents the internal state of java.sql.PreparedStatement object & amp ; has the lifetime of the java.sql.PreparedStatement , contains a top-level language ResultSet . This top-level language ResultSet provides the execution of the SQL statement , DML , DDL or a query . The top-level ResultSet may contain other ResultSets and could be seen as a tree structure . For the simple case of a primary key lookup query like : select name from customer where id = ? The activation would contain this : top result set ProjectRestrictRS & lt ; & lt ; IndexRowToBaseRowRS & lt ; & lt ; TableScanRS Now for some reason , even though the api of ResultSet say they can be re-used , and in some cases they are , this result set tree is thrown away after each execution . That is , the top result set has its finish ( ) method called and then the activation removes its reference to it . Then on the next execution a new ( identical ) tree is set up . There is potential for a huge performance gain if this top level result set and its tree are re-used and have the same lifetime as the Activation . The saving comes in two forms , not having to create many objects on each execution , and not creating short-lived objects for the garbage collector to handle . I made a simple fix , it s a couple of lines of code , just calling close & amp ; finish at the correct times , and for the above simple primary key lookup query , the performance went from 17,300 to 24,000 selects per second ( cached data , single user ) . I ll post a patch shortly as an indication of the direction , once I can separate it from other changes in my client . However , I m running the Derby tests and there are some ( maybe 25-30 ) failures , I think because not all the language ResultSet implementations are correctly written to be re-opened . Interestingly , the first failure I saw was in an aggregrate test , which goes back to the issue Manish saw . Even if derbyall passed I would be nervous about submitting this patch for real , because I don t think there s a lot of testing using repeat executions of PreparedStatements in the tests . The ij tests mainly use Statement , this is a single use of an activation so this change would not affect them . Thus such a patch could regress Derby by making it more likely existing bugs would be exposed . Given the performance gains , I think we need to start re-using ResultSets from Activation , and devise a way to ensure the testing covers the re-use . The main issue is there is a large number of ResultSet implementations to cover . 
Currently , the client driver does not pre-fetch data when executeQuery ( ) is called , but it does on the first call to ResultSet.next ( ) . Pre-fetching data on executeQuery ( ) would reduce network traffic and improve performance . The DRDA protocol supports this . From the description of OPNQRY ( open query ) : The qryrowset parameter specifies whether a rowset of rows is to be returned with the command . This is only honored for non-dynamic scrollable cursors ( QRYATTSNS not equal to QRYSNSDYN ) and for non-scrollable cursors conforming to the limited block query protocol . The target server fetches no more than the requested number of rows . It may fetch fewer rows if it is restricted by extra query block limits , or if a fetch operation results in a negative SQLSTATE or an SQLSTATE of 02000 . 
The JDBC 2.0 API introduced the ability to update/delete/insert rows from a resultset using methods in the Java programming language rather than having to send an SQL command . This Jira entry is to track the insert rows functionality using JDBC 2.0 apis . 
Implement layer B streaming for new methods defined in JDBC4.0 JDBC 4.0 introduced new methods which take parameters for object to be sent to sever without length information . For those methods , Layer B streaming is best way to implement sending object to server . This issue is representation of in Network Client . 
API between the network server and engine is not well defined , leading to inconsistent & amp ; multiple ways of handling the different objects returned , such as reflection , explicit casting etc . This in turn has lead to bugs such as . , and , and access to underlying objects by the application that should be hidden . Define interfaces , such as EngineConnection , that both EmbedConnection and BrokeredConnection implement . Thus the network server can rely on the fact that any connection it obtains will implement EngineConnection , and call the required methods through that interface . Most likely will need EngineConnection , EnginePreparedStatement and EngineResultSet .. These interfaces would be internal to derby and not exposed to applications . 
If a database is created with logDevice pointing to a directory containing old log files , one might experience errors during recovery . ERROR XSDB4 : Page Page ( 0 , Container ( 0 , 768 ) ) is at version 2,657 , the log file contains change version 2,715 , either there are log records of this page missing , or this page did not get written out to disk properly . To avoid this problem , database creation should fail if logDevice is pointing to an existing directory . 
Change the internal implemetation of SYSCS_INPLACE_COMPRESS_TABLE to share existing alter table code . One suggested approach is to use the same mechanism as SYSCS_COMPRESS_TABLE . Such an implementation would involve : o change the parser for alter table to accecpt some internal only syntax for SYSCS_INPLACE_COMPRESS_TABLE o change the alter table constant action to get the required information for inplace compress o move most of the inplace compress driving code to a routine in alter table execution , following the same coding pattern as existing alter table compress . I believe doing this will have multiple benefits : 1 ) shares existing alter table code , so things like error checking , security management , ... is all done by one piece of code 2 ) by sharing the code I believe the following outstanding JIRA issues will be addressed : , - 3 ) future changes addressing ddl like grant/revoke will automatically work . 
Add to the jdbc40 suite a test which verifies that our JDBC4 classes satisfy the expected class/interface signatures . 
Currently , you can encrypt an unencrypted database and you can change the encryption key on an already encrypted database . However , Derby does not expose a way to turn off ( unencrypt ) an already encrypted database . 
encrypted database to be re-encrypted with a new password . Here are some ideas for an initial implementation . The easiest way to do this is to make sure we have exclusive access to the data and that no log is required in the new copy of the db . I want to avoid the log as it also is encrypted . Here is my VERY high level plan : 1 ) Force exclusive access by putting all the work in the low level store , offline boot method . We will do redo recovery as usual , but at the end there will be an entry point to do the copy/encrypt operation . copy/encrypt process : 0 ) The request to encrypt/re-encrypt the db will be handled with a new set of url flags passed into store at boot time . The new flags will provide the same inputs as the current encrypt flags . So at high level the request will be connect db old_encrypt_url_flags ; new_encrypt_url_flags . TODO - provide exact new flag syntax . 1 ) Open a transaction do all logged work to do the encryption . All logging will be done with existing encryption . 2 ) Copy and encrypt every db file in the database . The target files will be in the data directory . There will be a new suffix to track the new files , similar to the current process used for handling drop table in a transaction consistent manner without logging the entire table to the log . Entire encrypted destination file is guaranteed synced to disk before transaction commits . I don t think this part needs to be logged . Files will be read from the cache using existing mechanism and written directly into new encrypted files ( new encrypted data does not end up in the cache ) . 3 ) Switch encrypted files for old files . Do this under a new log operation so the process can be correctly rolled back if the encrypt db operation transaction fails . Rollback will do file at a time switches , no reading of encrypted data is necessary . 4 ) log a change encryption of db log record , but do not update system.properties with the change . 5 ) commit transaction . 6 ) update system.properties and sync changes . 7 ) TODO - need someway to handle crash between steps 5 and 6 . 6 ) checkpoint all data , at this point guaranteed that there is no outstanding transaction , so after checkpoint is done there is no need for the log . ISSUES : o there probably should be something that catches a request to encrypt to whatever db was already encrypted with . 
If you run the VerifySignatures test , you will see a lot of JDBC4 methods which don t appear in our implementation yet . We need to add vacuous implementations for these methods which raise SQLFeatureNotSupportedExceptions . Most of these methods won t need any more attention because they refer to features ( like xml and national strings ) which we don t support . We will open additional JIRAs for the methods which need non-vacuous implementations . 
ij should accept a -- help option to print command line syntax for reference and friendliness . 
The next rev of the JDBC4 spec will clarify that databases should raise SQLException when an application calls methods on closed sql objects : ResultSet , Preparedstatement , CallableStatement , Connection , and Statement . We should verify that we conform . 
An upcoming release of jdk1.6 will move isPoolable ( ) and setPoolable ( ) from PreparedStatement to Statement . We should conform . 
Handle interrupt received while waiting for database lock Subtask of : this issue tracks the changes needed to handle interrupt received while a thread is waiting for a database lock . Let query stop execution if an interrupt is seen , at same time as we check the query timeout Subtask of . In BasicNoPutResultSetImpl # checkCancellationFlag we currently check whether a statement has been canceled or it timed out at certain times during query execution . We would like an interrupt seen during execution hitherto to also throw at this point . Cf the way we also stop execution of batches if a batch element sees an interrupt . 
Implement Connection.createClob ( ) and Connection.createBlob ( ) . Right now these methods are implemented in the network client but not the embedded client . 
Currently the client tracing can be enabled by setting attributes on the client url , setXXX methods on the DataSource or calling DriverManager.setLogWriter ( ) , but it often can not be enabled in a deployed client application because all of these API s require modification of the application or its configuration files . It would be good to have a global way to turn on client tracing . A system property pointing to a property file is one possibility but probably not ideal because of the impact in class loader contexts . I am not sure what the other possiblities are , 
Detectability of own changes is implemented in the client using warnings cf the write-up for . When a row has been deleted and/or updated , a warning will be sent to the client to indicate that fact . Presently , only one warning can be sent each time a data row is sent from to the client , that means that some warnings may be lost . Using extended diagnostic allows us to send several warnings for each data row . I propose to use extended diagnostics to send ROW_UPDATED and ROW_DELETED warnings when necessary . This may later be extended for other warnings , but I do not plan to do it as a part of the work in this issue . 
Each derby table or index is stored in a separate file . Space from deleted rows is eventually reclaimed within the file as is used for subsequent inserts into the same file . That space is not returned to the OS unless the user calls the SYSCS_UTIL.SYSCS_COMPRESS_TABLE system procedure . That procedure will return the unused space in the tables and indexes to the OS . It gets an exclusive lock on the table , copies all rows in the indexes and the base table into new compressed files and delete the old files . Prior to jdk 1.4 this was the only way to return space from a file to the OS . In jdk 1.4 RandomAccessFile was enhanced to allow the truncation of a file , which would return the space at the end of the file back to the OS . In order to take advantage of this new feature a new compress feature is needed in derby . The assumption is that this work will be used in future work which will automatically schedule this job and others in background , with no interaction needed from the dba . The 1st phase of this work will simply build a procedure that will do the work . The 2nd phase will be to look into scheduling the procedure automatically as part of the current background post commit processing . Longer term it would be best if this fit into a new background task monitor , which could schedule larger background tasks balanced against the other priorities of the system . These tasks might include : this new online compress , automatic statistics gathering , more proactive deleted row reclamation , .... The proposed feature will reorganize base tables and indexes , moving empty pages to the end . It will release space back to the operating system when it has created a chunk of empty pages at the end of the file . It will be designed to run in background , and will lock resources of the table for as short a time as possible so that it can iteratively process the table . To reclaim space in the heap , it will scan the heap in page reverse order . It will get a short term table lock , process all the rows on a page , and then commit that transaction releasing the lock . The commit will be optimized like other internal transactions , and will not need to wait for a synchonized write . Each row moved , will require all the index entries for that row to also be updated . While doing the processing it will also take care of processing committed deleted rows . When space is free at the end of the table it will be freed back to the operating system , using the RandomAccessFile.setLength ( ) interface . To reclaim space in the btree , data on pages will be moved rather than rows . Data from pages at the end of the file will be moved to free smaller numbered pages . Again short term table locks will be required , and the operation will look similar to the current btree merge operations already implemented . Again when a chunk of pages is free at the end of the file , they will be returned to the OS using the same mechanism as the heap . 
The antiGC thread was originally created to avoid the DriverManager class being garbage collected when no refrences existed to it and it had loaded the embedded JDBC driver ( and hence shutting down the engine ) . This was an issue with JDK 1.1 . Since Derby does not support jdk1.1 and garbage collection of classes is clearly defined , it is possible the thread serves no useful purpose . 
The task involves Adding and modifying the following methods , which were recently added by the JDBC4 expert group and which appeared in Mustang build 86 : add PreparedStatement.setAsciiStream ( int columnIndex , InputStream x , long length ) add PreparedStatement.setBinaryStream ( int columnIndex , InputStream x , long length add PreparedStatement.setCharacterStream ( int columnIndex , Reader x , long length ) add CallableStatement.setAsciiStream ( String columnName , InputStream x , long length ) add CallableStatement.setBinaryStream ( String columnName , InputStream x , long length ) add CallableStatement.setCharacterStream ( String columnName , Reader x , long length ) add ResultSet.updateAsciiStream ( int columnIndex , InputStream x , long length ) add ResultSet.updateBinaryStream ( int columnIndex , InputStream x , long length ) add ResultSet.updateCharacterStream ( int columnIndex , Reader x , long length ) add ResultSet.updateAsciiStream ( String columnName , InputStream x , long length ) add ResultSet.updateBinaryStream ( String columnName , InputStream x , long length ) add ResultSet.updateCharacterStream ( String columnName , Reader x , long length ) add ResultSet.updateBlob ( int columnIndex , InputStream x , long length ) add ResultSet.updateBlob ( String columnName , InputStream x , long length ) add ResultSet.updateClob ( int columnIndex , Reader x , long length ) add ResultSet.updateClob ( String columnName , Reader x , long length ) add ResultSet.updateNClob ( int columnIndex , Reader x , long length ) add ResultSet.updateNClob ( String columnName , Reader x , long length ) modify ResultSet.updateNCharacterStream ( int columnIndex , Reader x , int length ) to ResultSet.updateNCharacterStream ( int columnIndex , Reader x , long length ) modify ResultSet.updateNCharacterStream ( String columnName , Reader x , int length ) to ResultSet.updateNCharacterStream ( String columnName , Reader x , long length ) 
When streaming data to Derby , the application stream will be wrapped in a Derby-specific stream to convert the data to the correct representation . At a minimum , this consists of getting the data to the on-disk format used by Derby . The wrapping stream can be extended to provide the following features at an earlier stage : a ) Data cut-off when the maximum allowed size is exceeded b ) Truncation of trailing blanks where allowed Both features can reduce the amount of data needed to be kept in memory on insertion . Implementing this will require additional column/data type information in the streaming class ( es ) . The implementation must be able to handle streams for which the length is specified and also streams with unknown/unspecified length . 
Currently four no-argument diagnostic tables exist that provide information about the running state of Derby , or its error messages . These tables are invoked using an awkward , non-standard syntax . As an example : SELECT * FROM NEW org.apache.derby.diag.LockTable ( ) as LOCK_TABLE The improvement will provide an internal mapping from a regular table name in the SYSCS_DIAG schema to the runtime virtual table code . Thus the above example would be replaced by : SELECT * FROM SYSCS_DIAG.LOCK_TABLE These diagnostic table expressions are regular table expressions ( as is the NEW VTI construct ) and can be used wherever a normal table can . Any DDL , INSERT/UPDATE/DELETE , compression procedure etc . that references a diagnostic table will result in an exception . The old style syntax will remain in place for 10.2 , but become deprecated . The tables to be implemented in this change are : SYSCS_DIAG.LOCK_TABLE replaces org.apache.derby.diag.LockTable SYSCS_DIAG.STATEMENT_CACHE replaces org.apache.derby.diag.StatementCache SYSCS_DIAG.TRANSACTION_TABLE replaces org.apache.derby.diag.TransactionTable SYSCS_DIAG.ERROR_MESSAGES replaces org.apache.derby.diag.ErrorMessages Adding such a table will be table driven , thus easy for others to provide additional diagnostics . Information about these diagnostic tables will not appear in the system catalogs or JDBC DatabaseMetaData . The ResultSetMetaData for the any query involving a diagnostic table will be valid . This is a first step in a progression towards supporing a fully application/user defined virtual table . These steps are not part of this jira issue , but added for information purposes . second step - supporting diagnostic tables with parameters , e.g . SELECT * FROM SYSCS_DIAG.SPACE_TABLE ( sales , orders ) ; third step - providing a create virtual table statement ( most databases support some form of virtual table , or wrappers ) . The DDL would be non-standard but the data access would be standard . & # 91 ; need to check table functions in part 13 of SQL standard & # 93 ; E.g . syntax yet to be defined , but to give the general idea CREATE VIRTUAL TABLE ( TICKER VARCHAR ( 10 ) , START TIMESTAMP , END TIMESTAMP ) LANGUAGE JAVA PARAMETER STYLE JAVA EXTERNAL NAME com.acme.stocks.historyFromYahooFinance ; 
Provide a way to drop a column from an existing table . Possible syntax would be : ALTER TABLE tablename DROP COLUMN columnname CASCADE / RESTRICT ; Feature should properly handle columns which are used in constraints , views , triggers , indexes , etc . 
EmbedPreparedStatement and EmbedResultSet have many occurrences of switch ( ... ) { case Types.CHAR : case Types.VARCHAR : case Types.LONGVARCHAR : case Types.CLOB : break ; default : throw dataTypeConversion ( ... ) ; } and switch ( ... ) { case Types.BINARY : case Types.VARBINARY : case Types.LONGVARBINARY : case Types.BLOB : break ; default : throw dataTypeConversion ( ... ) ; } Instead of having many copies of this code , it would be good to factor it out into methods that can be shared . For instance , one could add these methods to DataTypeDescriptor : public static boolean isAsciiStreamCompatible ( int jdbcType ) public static boolean isBinaryStreamCompatible ( int jdbcType ) public static boolean isCharacterStreamCompatible ( int jdbcType ) 
The JDBC4 expert group has made the following changes to their spec : DatabaseMetaData.getFunctionParameters ( ) is changing name to getFunctionColumns ( ) The DatabaseMetaData.functionParameter * constants are changing name to functionColumn * 
getDatabaseProductVersion and getDriverVersion ( ) report only the four digit Derby version number and not the svn build number . It would be useful to return the full version including the build number as sysinfo does : e.g . 10.1.2.4 - ( 392472 ) , That way it will be clear from application logs that collect this information exactly what revision level they are running if they are using rolled up fixes on the maintenance branch between releases . There may be risk in doing this however if applications are parsing the version information , but hopefully they will use getDatabaseMajorVersion ( ) , getDatbaseMinorVersion , getDriverMajorVersion , and getDriverMinorVersion for such proccessing . 
The heavy use of static state information in the existing Derby ( non-Junit ) harness was a major mistake , it should not be repeated in the Junit model . It should be changed before too many tests are written . A static configuration will limit the ability for the tests to be flexible and have different instances running with different configurations , a instance based scheme will be more flexible . This change will just change the api , the current implementation of a single static configuration will remain but be hidden within the base-classes . 
Idea is to drop objects based upon input from DatabaseMetaData or Derby system catalogs where the meta data is insufficient . Would allow Junit tests to share databases . Can be incrementally developed and used , e.g . first step drop all views . Then tables etc . etc . 
From work on & amp ; it was discovered the DependencyMangerImpl loaded the same Provier multiple times from the DataDictionary while loading the stored dependecies and converting them to an in-memory form . There is no need to load the Provider from the DataDictionary because it is passed into the DependencyManger.invalidateFor call that results in the loading of the stored dependencies . 
Derby currently doesn t allow CALL statement to be used in a trigger body . It would be great to allow java stored procedure invocation inside a trigger . Since Derby doesn t have SQL procedure language , triggers can only execute a single SQL statement . If we allow stored procedures in triggers , it would be possible to write a trigger that involves more than just one SQL statement . Functions are currently allowed , but they are read-only . I believe it is fairly easy to support this enhancement . Need good amount of testing though . 
Useful for running ij SQL scripts as part of JUnit tests and for applications to use instead of the awkward way to use ij from a program today . / * * Run a SQL script from an input stream and write the resulting output to the provided OutputStream . @ param conn Connection to be used as the script s default connection . @ param sqlIn InputStream for the script . @ param inputEncoding Encoding of the script . @ param sqlOut OutputStream for the script s output @ param outputEncoding Output encoding to use . @ return Number of SQLExceptions thrown during the execution , -1 if not known . @ throws UnsupportedEncodingException * / public static int runScript ( Connection conn , InputStream sqlIn , String inputEncoding , PrintStream sqlOut , String outputEncoding ) throws UnsupportedEncodingException 
TestConfiguration uses the org.apache.derby.iapi.services.info.JVMInfo to determine if JDBC 4 is supported . Ideally test code should not be calling engine private api code . Suggest two changes : 1 ) Determine JDBC level support from ability to load classes : E.g . JDBC2 - java.sql.Driver JDBC3 - java.sql.Driver AND java.sql.ParameterMetaData JDBC4 - java.sql.Driver AND java.sql.SQLXML JSR169 - java.sql.ParameterMetaData AND NOT java.sql.Driver 2 ) Move the supportsJDBC4 ( ) method from TestConfiguraiton to JDBC , since if the VM supports JDBC4 or not is not an attribute of a test configuration ( and add other supportsJDBCX methods ) . 
As part of ACCSECRD , if the server does not accept the security mechanism sent by the client , the server will send a list of security mechanism that it supports . Currently even when the server is running with sun jvm , it will still send EUSRIDPWD as a sec mec that it supports , which is incorrect . The server should test if it can support EUSRIDPWD dynamically and if it does , only then send EURRIDPWD as an option that it supports . see DRDAConnThread.writeACCSECRD ( int ) 
A UNION node will generate byte code to call this method : NoPutResultSet getUnionResultSet ( NoPutResultSet source1 , NoPutResultSet source2 , Activation activation , int resultSetNumber , double optimizerEstimatedRowCount , double optimizerEstimatedCost , GeneratedMethod closeCleanup ) The closeCleanup method is passed in as null for all result sets in a tree except the top one . Instead of passing it on each new result set , it would be much more efficient to have a setCloseMethod ( ) that is only called once for the top result set . Could also look at having an closeCleanup method in the interface and call it directly , rather than through reflection . This applies to any node that takes a closeCleanup method . Split out from 
You currently can t use a correlation ID in an UPDATE/DELETE statement . This makes it cumbersome to do the following : UPDATE EMPLOYEE_BONUS SET BONUS = ( SELECT SUM ( BONUSES.BONUS ) FROM BONUSES WHERE EMPLOYEE_BONUS.EMPL_ID = BONUSES.EMPL_ID ) ; The use of a correlation ID makes this easier to code . UPDATE EMPLOYEE E SET BONUS = ( SELECT SUM ( B.BONUS ) FROM BONUSES B WHERE B.EMPL_ID = E.EMPL_ID ) ; This is particularly important if you get carried away with long SCHEMA and TABLE names ! 
By default MySQL is case insensitive in its string comparisons , as you can see from the MySQL docs shown below . Similar functionality is available in Sybase iAnywhere and in SQLServer . I d like the same to be true for Derby . What , I wonder , are chances of that ? I am aware that functions could be used to force comparisons in upper case but that subverts the indexes and makes searches unacceptably long . If you were to ask people you might find that this is a feature whose abscence is causing many to look elsewhere . thanks for all the great work , Terry The MySQL Docs say : -- -- -- -- start quote By default , MySQL searches are not case sensitive ( although there are some character sets that are never case insensitive , such as czech ) . This means that if you search with col_name LIKE a % , you get all column values that start with A or a . If you want to make this search case sensitive , make sure that one of the operands has a case sensitive or binary collation . For example , if you are comparing a column and a string that both have the latin1 character set , you can use the COLLATE operator to cause either operand to have the latin1_general_cs or latin1_bin collation . For example : col_name COLLATE latin1_general_cs LIKE a % col_name LIKE a % COLLATE latin1_general_cs col_name COLLATE latin1_bin LIKE a % col_name LIKE a % COLLATE latin1_bin If you want a column always to be treated in case-sensitive fashion , declare it with a case sensitive or binary collation . See Section 13.1.5 , CREATE TABLE Syntax . By default , the search is performed in case-insensitive fashion . In MySQL 4.1 and up , you can make a full-text search by using a binary collation for the indexed columns . For example , a column that has a character set of latin1 can be assigned a collation of latin1_bin to make it case sensitive for full-text searches . -- -- -- -- -- -- -- - end quote 
A revoke execute ... restrict should fail if there are dependent objects on the execute privilege As per the functional spec attached to , a revoke execute ... restrict should fail if there are dependent objects on the execute privilege In order to implement this , when revoke execute is executed , a special invalidation action should be sent and dependent objects will need to catch that invalidation and throw an exception . If there are no dependents , then no exception will be thrown and revoke execute will succeed . I am just creating a new jira entry here so it is easier to track sub items of . Will link this Jira entry to . See the functional spec attached to A view should be dropped when a privilege required by the view is revoked . A view tracks its privileges requirements using Derby s Dependency Manager . If any one of those required privileges are revoked , the view should be dropped automatically . I am just creating a new jira entry here so it is easier to track sub items of . Will link this Jira entry to . See the functional spec attached to Upgrade test needs to be enhanced to test grant revoke Grant Revoke is one of the features targeted for 10.2 Release . The upgrade test should be modified to test this feature with various upgrade scenarios to make sure everything works fine . add lang/grantrevoke.java to derbynetclientmats I noticed running lang./grantrevoke.java with client fails with : & gt ; Unexpected exception & gt ; 08004 : Connection authentication failure occurred . Reason : userid or password invalid . & gt ; java.sql.SQLException : Connection authentication failure occurred . Reason : userid or password invalid . & gt ; Caused by : org.apache.derby.client.am.SqlException : Connection authentication failure occurred . Reason : userid or password invalid . & gt ; ... 8 more & gt ; FAILED . 1 error Test Failed . End : grantRevoke jdk1.4.2_07 DerbyNetClient 2006-06-01 12:17:25 * * * Perhaps just a test configuration problem of some sort , but It would be good to have client testing for grant revoke . Remove SYS.SYSREQUIREDPERM from Derby 10.2 . This was added for Grant Revoke functionality With the Grant Revoke functionality . Derby engine needs to keep track of view/constraint/trigger s dependencies on various privileges . SYS.SYSREQUIREDPERM table was added for this purpose . But these depdencies can be mantained using the existing Dependency Manager . I have done quite a bit of work using Dependency Manager for Grant Revoke and do not see a need for SYS.SYSREQUIREDPERM . Before 10.2 release , we should drop SYS.SYSREQUIREDPERM from the Derby code and update the Grant/Revoke functional spec accordingly . A trigger should be dropped when a privilege required by the trigger is revoked . A trigger tracks its privileges requirements using Derby s Dependency Manager . If any one of those required privileges are revoked , the trigger should be dropped automatically . I am just creating a new jira entry here so it is easier to track sub items of . Will link this Jira entry to . See the functional spec attached to A constraint should be dropped when a privilege required by the constraint is revoked . A constraint tracks its privileges requirements using Derby s Dependency Manager . If any one of those required privileges are revoked , the constraint should be dropped automatically . I am just creating a new jira entry here so it is easier to track sub items of . Will link this Jira entry to . See the functional spec attached to 
Grant revoke functionality was added in Derby 10.2 The comments that went into the grant revoke code , in some places refer to database owner as dba . They are not the same thing . In the grant revoke world , dba is a role . We haven t added roles into Derby yet but current use of dba in comments might make it confusing when we do start working on roles including dba . 
Adds the following functions : SIGN - 0 , 1 or -1 for zero , positive or negative arguments SQRT - Square root RAND - Random number RANDOM - as above COSH - Hyperbolic Cosine SINH - Hyperbolic Sine TANH - Hyperbolic Tangent . 
We are currently developing a system where we load between 1000 and 5000 objects in one go . The user can load different chunks of objects at any time as he/she is navigating . The system consist of a java application which accesses derby via hibernate . During profiling we discovered that the org.apache.derby.iapi.util.StringUtil is the biggest bottleneck in the system . The method SQLEqualsIgnoreCase ( String s1 , String s2 ) is doing upperCase on both s1 and s2 , all the time . By putting the uppcase value into a Hashtable and using the input-string as key we increates the performance with about 40 % . Our test-users report that the system now seems to run at double speed . The class calling the StringUtil.SQLEqualsIgnoreCase in this case is org.apache.derby.impl.jdbc.EmbedResultSet This class should also be checked as it seems to do a lot of looping . It might be a canditate for hashing , as it is stated in the code : // REVISIT : we might want to cache our own info ... Here is a diff agains the 10.1.3.1 source for org.apache.derby.iapi.util.StringUtil 22a23 & gt ; import java.util.Hashtable ; 319c320,326 & lt ; return s1.toUpperCase ( Locale.ENGLISH ) .equals ( s2.toUpperCase ( Locale.ENGLISH ) ) ; & # 8212 ; & gt ; { & gt ; String s1Up = ( String ) uppercaseMap.get ( s1 ) ; & gt ; if ( s1Up == null ) & gt ; { & gt ; s1Up = s1.toUpperCase ( Locale.ENGLISH ) ; & gt ; uppercaseMap.put ( s1 , s1Up ) ; & gt ; } 320a328,332 & gt ; String s2Up = ( String ) uppercaseMap.get ( s2 ) ; & gt ; if ( s2Up == null ) & gt ; { & gt ; s2Up = s2.toUpperCase ( Locale.ENGLISH ) ; & gt ; uppercaseMap.put ( s2 , s2Up ) ; 321a334 & gt ; return s1Up.equals ( s2Up ) ; 322a336,339 & gt ; //return s1.toUpperCase ( Locale.ENGLISH ) .equals ( s2.toUpperCase ( Locale.ENGLISH ) ) ; & gt ; } & gt ; } & gt ; private static Hashtable uppercaseMap = new Hashtable ( ) ; 
JPA is requiring databases to support this ANSI feature esp the ability to chose the trimmed character TRIM ( FROM ] str ) 
EmbedResultSet creates and holds references to information that logically is at the plan level , the same for all ResultSet s that use the same prepared plan . Holding this information at the EmbedResultSet hurts performance and memory usage as ResultSet objects are short lived . Saving the ResultSetMetaData object in the ResultDescription object means a single creation for the lifetime of the plan ( shared across connections ) , rather than once per ResultSet object as needed . Saving the column name to position mapping added in in the ResultDescription has a similar benefit , the map is set up once per prepared plan , not once per executeQuery ( ) . With test changes ( will attach patch soon ) , the performance of the derby1862 test in improves by around 15 % 
The documentation will need to be updated after is committed . The reference manual will need to describe how to use the new ALTER TABLE DROP COLUMN feature to drop a column from a table . The documentation for the ALTER TABLE command is becoming somewhat unwieldy , so perhaps there is a way to restructure the page to make it easier and more approachable . In the documentation , it will be important to clearly describe the RESTRICT and CASCADE behaviors , as users may be confused by what things cause RESTRICT to refuse to drop a column . The comments in AlterTableConstantAction.java may help . Specifically , the documentation should note these possibly unexpected behaviors : If a column is present in one or more indexes , these indexes by themselves do not cause RESTRICT to refuse to drop a column . Instead , the column will simply be dropped from the index , and if that was the last column in that index , the entire index will be dropped . Explicitly named CHECK constraints will cause RESTRICT to refuse to drop a column , as will PRIMARY KEY , FOREIGN KEY , and UNIQUE constrants . However , an unnamed simple NOT NULL constraint on a column will NOT cause RESTRICT to refuse to drop it . 
Derby presently does not implement support for the method PreparedStatement.setObject ( and similarly for CallableStatement.setObject ) when the supplied value is null , unless a type argument ( 3rd arg ) is also present . That is , in : void setObject ( int parameterIndex , Object x ) throws SQLException x can not be null . Derby will presently throw an SQLException ( client : XJ021 , embedded : 22005 ) if x is null when calling this method on a preparedStatement . Porting some applications may be made easier if this restriction is lifted . See also discussion in . 
Statement and ParameterMetaData contain flags called escapedProcedureCallWithResult_ . These flags might have had a function before , but now they are always false . They are used in many if-statements and make the code harder to read . Removing the flags and the code that is only executed when the flags are true , would simplify the code and make it more maintainable . 
Every method that calls setUpSocket should have a call to a new closeSocket ( ) method . 
Derby currently supports the following JDBC methods for auto-generated keys : // Indicate that we want to retrieve auto-generated key values . Connection.prepareStatement ( String sql , int autoGeneratedKeys ) ; Statement.execute ( String sql , int autoGeneratedKeys ) ; Statement.executeUpdate ( String sql , int autoGeneratedKeys ) ; // Retrieve the auto-generated values ( only applies to INSERT statements ) . ResultSet rs = Statement.getGeneratedKeys ( ) ; The current implementation of getGeneratedKeys ( ) internally maps to the IDENTITY_VAL_LOCAL ( ) method , which means that Derby s implementation only returns generated keys for autoincrement columns ( no other default columns are supported ) . Further : 1 . The generated key result set only ever has a single column . This is because Derby only allows one autoincrement column per table . 2 . The type of the single column in the result set will be DECIMAL ( 31,0 ) . This is defined by IDENTITY_VAL_LOCAL ( ) . 3 . The generated key result set will only ever have a single row . This is because IDENTITY_VAL_LOCAL ( ) only returns values that were assigned as the result of a single row INSERT statement using a VALUES clause . For a single row INSERT statement , at most one autoincrement value will be generated . All of that said , JDBC 3.0 also defines the following methods , which allow the user to explicitly indicate , via column position or column name , the columns for which the auto-generated keys should be made available : Connection.prepareStatement ( String sql , String columnNames ) ; Connection.prepareStatement ( String sql , int columnIndexes ) ; Statement.execute ( String sql , String columNames ) ; Statement.execute ( String sql , int columIndexes ) ; Statement.executeUpdate ( String sql , String columnNames ) ; Statement.executeUpdate ( String sql , int columnIndexes ) ; Derby currently throws a Feature not supported error for all of these methods . However , it seems like the above methods could be mapped onto the existing Derby behavior with relatively little effort ( in embedded mode ) . Most of the required code is already in place . Doing so would make it easier for applications that rely on the columnNames and/or columnIndexes APIs to work with Derby ( assuming the app just wants generated keys for identity ( autoincrement ) columns ) . Note that this Jira does not entail removing any of the restrictions nor changing any of the behavior outlined above . All of that will remain exactly as it is . This Jira simply exposes the existing functionality ( restrictions and all ) through additional ( standard ) API methods . In particular this means that any column specified by index ( position ) or name must be an auto-increment column for the INSERT table ; otherwise Derby should throw an error . Or put differently , a user who specifies a column name/position will get- in the absence of errors -the exact same results as s/he would get from invoking the ( String sql , int autoGeneratedKeys ) method . Note also : This Jira is specifically for embedded mode . I think it would be harder to support these methods for Derby Client and so do not plan to address that . 
Rewrite the test in JUnit using the utility methods added in . 
My session at Apachecon 2006 ( Austin ) included a demo , showing how to use user-coded Java functions and procedures inside the Derby engine . I would like to check this demo into the Derby demo directory . 
Need to work through the current failures when running NistScripts , which are due to blank lines and generated constraint names in the output . 
For writing the transaction log to disk Derby uses a RandomAccessFile . If it is supported by the JVM , the log files are opened in rws mode making the file system take care of syncing writes to disk . rws mode will ensure that both the data and the file meta-data is updated for every write to the file . On some operating systems ( e.g . Solaris ) this leads to two write operation to the disk for every write issued by Derby . This is limiting the throughput of update intensive applications . If we could change the file mode to rwd this could reduce the number of updates to the disk . I have run some simple tests where I have changed mode from rws to rwd for the Derby log file . When running a small numbers of concurrent client threads the throughput is almost doubled and the response time is almost halved . I will attach some graphs that show this when running a given number of concurrent tpc-b like clients . These graphs show the throughput when running with rws and rwd mode when the disk s write cache has been enabled and disabled . I am creating this Jira to have a place where we can collect information about issues both for and against changing the default mode for writing to log files . 
Make client driver treat BOOLEAN columns the same way as the embedded driver does . The only operation currently allowed on BOOLEAN columns is to select them from the system tables . For instance , the following query selects BOOLEAN values : select systemalias from sys.sysaliases ; There are discrepancies in how the embedded and client drivers handle these BOOLEAN columns . The embedded behavior is correct and the client behavior should conform . I will attach a spec describing what needs to be done . Allow functions to return BOOLEAN values As part of our expanding support for the BOOLEAN datatype , it would be nice to allow functions to return BOOLEAN values . Allow explicit casts of string values to BOOLEAN The SQL Standard allows strings to be explicitly cast to BOOLEAN values . Strings are the only type ( other than BOOLEAN itself ) which can be cast to BOOLEAN . As part of our expanding support for the BOOLEAN datatype , we should allow these casts . Casting string types to boolean is defined by part 2 , section 6.12 ( & lt ; cast specification & gt ; ) , general rule 20 : a ) Trim whitespace off the string b ) Then apply the rules in section 5.3 ( & lt ; literal & gt ; ) . This means that the trimmed string must be TRUE , FALSE , or UNKNOWN , regardless of case . c ) Otherwise , raise an exception . Allow routines to take arguments of BOOLEAN type We should allow routines to take BOOLEAN arguments as part of our expanding support for the BOOLEAN datatype . Make it possible for table functions to return BOOLEAN columns Right now a function can not return a table with a BOOLEAN column . Enable BOOLEAN typed columns Allow tables to have boolean columns . 
The methods of the PageKey class are called frequently both from the lock manager and the cache manager . Since no other classes extend it , it should be declared final to help the run-time compiler optimize the method calls . 
Since this is compilinga statement returning the specific node makes more sense and will lead to a little more clarity in the code . Some of the methods defined on QueryTreeNode such as generate ( ) and needsSavepoint ( ) can move down to StatementNode and thus make the code more readable ( which generate is called for a value node ? ) and remove implementations of these methods that can never be called . 
Latching of pages could be done more efficiently locally in store than in the lock manager . See the discussion here : 
DataDictionaryContext adds complexity to getting a reference to the data dictionary and is a hang-over from old code that used to push multiple dictionaries . That code no longer exists . 
In org.apache.derby.impl.store.access.RAMTransaction , synchronized collection objects could be replaced with unsynchronized ones . See discussion here : 
Convert org.apache.derbyTesting.functionTests.tests.lang.holdCursorIJ.sql to junit . Convert org.apache.derbyTesting.functionTests.tests.lang.holdCursorIJ.sql to junit . Convert org.apache.derbyTesting.functionTests.tests.store.holdCursorJDBC30.sql to junit . Convert org.apache.derbyTesting.functionTests.tests.store.holdCursorExternalSortJDBC30.sql to junit . Convert derbynet/testProtocol.java to JUnit testProtocol.java executes DRDA commands from a file written in a special-purpose language . The statements are very much like assertions , so it should be fairly easy to convert the test to JUnit . Suggested approach : Change the interpreter ( TestProto.java ) so that is uses Assert.fail ( ) instead of System.err.println ( ) and System.exit ( ) , and BaseTestCase.println ( ) instead of System.out.println ( ) . It should also use TestConfiguration to get the host name and port number . Convert derbynet/runtimeinfo to JUnit Convert derbynet/DerbyNetAutoStart to JUnit With derby client setTransactionIsolation executes and commits even if isolation has not changed With in EmbedConnection.setIsolation ( ) we have a check to see if the isolation level is the same and if so just return without doing a commit : public void setTransactionIsolation ( int level ) throws SQLException { if ( level == getTransactionIsolation ( ) ) return ; with org.apache.derby.client.am.Connection we have no such check . It would be good if the client driver acted like embedded . Finish client info api for JDBC4 . Compliance task . The following java.sql.Connection methods need to be finished : getClientInfo ( ) getClientInfo ( java.lang.String ) setClientInfo ( java.lang.String , java.lang.String ) Implement not implemented Embedded methods Blob.getBinaryStream ( long pos , long length ) and Clob.getCharacterStream ( long pos , long length ) The following methods were introduced in the java.sql.Clob and java.sql.Blob interface as part of JDBC 4.0 and need to be implemented . Clob -- -- -- getCharacterStream ( long pos , long length ) Blob -- -- -- getBinaryStream ( long pos , long length ) The implementation on the Network Client is already done as part of Derby-2444 Expose existing auto-generated key functionality through more JDBC APIs in Derby Client . See for details . Desired functionality is the same as for , except that this issue is specifically for Derby Client ( only addressed embedded mode ) . Detailed prompt for Error XCL16 is different between Client and Embed For the sql script below : create table t1 ( c11 int , c12 int ) ; & # 8211 ; insert data into tables insert into t1 values ( 1,1 ) ; insert into t1 values ( 2,2 ) ; & # 8211 ; set autocommit off autocommit off ; get with nohold cursor jdk1 as SELECT * FROM t1 ; & # 8211 ; do fetches from the cursor next jdk1 ; -- commit commit ; & # 8211 ; now try the fetch on cursor again after commit & # 8211 ; cursors jdk1 will give Error XCL16 next jdk1 ; & # 8211 ; clean up . close jdk1 ; for the line next jdk1 ; , an Error XCL16 will be thrown . However , detailed prompt for Error XCL16 is different between Client and Embed . In client mode , we get ERROR XCL16 : ResultSet not open . Verify that autocommit is OFF . While , in embed mode , we get ERROR XCL16 : ResultSet not open . Operation next not permitted . Verify that autocommit is OFF . Different behavior in Client and Embedded modes when update on an invalid cursor Consider the sql snippet below : create table test ( c1 int , c2 int ) ; insert into test values ( 1,1 ) ; update test set c1=2 where current of jdk4 ; for the update line , we get ERROR XJ202 : Invalid cursor name JDK4 . in Client mode , while ERROR 42X30 : Cursor JDK4 not found . Verify that autocommit is OFF . in Embed mode . Change sysinfo to print out more specific JVM information It would be nice if sysinfo printed out more specific jvm information that is provided with java -version . At least with the IBM jvm the system properties java.runtime.version and java.fullversion give some more ( but not all ) information . More research is needed across multiple jvms to tie it down . Alternatively sysinfo could dump all the system property information , but that could make the output pretty big and show irrelavant information in some contexts . Behaviour of setTypeMap ( ) differs between embedded and client On the embedded driver , Connection.setTypeMap ( ) behaves like this ( when the connection is not closed ) : if the map argument is null , throw an SQLException with SQLState XJ081 if the map is not null and not empty , throw an SQLException with SQLState 0A000 if the map is not null and empty , do nothing The behaviour on the client driver is this : always throw an SQLException with SQLState 0A000 We should try to make the two drivers behave the same way when setTypeMap ( ) is called . ( This would also allow us to simplify some of the tests in J2EEDataSourceTest ) . SQLBinary.writeBlob is inefficient , reading one byte at a time from the source BLOB SQLBinary.writeBlob is inefficient , since it is only reading one byte at the time from the source BLOB . It would be better if a transfer buffer was used to facilitate the write . Release system resources in CanonTestCase thoroughly Now , in the method of compareCanon ( String canon ) in CanonTestcase , two BufferedReaders -- -- cannonReader and testOutput are closed in try catch block , however if some exception occurs above , the two BufferReaders won t be closed , and related system resources won t be released . Besides , a releative problem is about the ByteArrayOutputStream rawBytes in CanonTestcase , it s created in getOutputStream ( ) and closed in compareCanon ( String canon ) . It s pointed out by Knut in Derby-2708 , ByteArrayOutputStream s javadoc says that closing it has no effect , so tearDown ( ) method has been added to CanonTestCase and sets rawBytes to null . It certainly can work . However , is it also OK to null out rawBytes in compareCanon ( String canon ) ? If so , it seems cleaner than to null it out in tearDown ( ) method . 
Derby allows a user to change the interval between consecutive values of the identity column using ALTER TABLE . But there is no way to change the next value to be generated for an identity column . Such a support in Derby will be very handy for tables with identity column defined as GENERATED BY DEFAULT and with a unique key defined on them . Column defined with GENERATED BY DEFAULT allows system to generate values for them or allows the user to manually supply the value for them . A column defined this way is very useful when the user might want to import some data into the generated column manually . But this can create problems when the system generated values conflict with manually inserted values . eg autocommit on ; create table tauto ( i int generated by default as identity , k int ) ; create unique index tautoInd on tauto ; insert into tauto ( k ) values 1,2 ; & # 8211 ; let system generate values for the identity column & # 8211 ; now do few manual inserts into identity column insert into tauto values ( 3,3 ) ; insert into tauto values ( 4,4 ) ; insert into tauto values ( 5,5 ) ; insert into tauto values ( 6,6 ) ; insert into tauto values ( 7,7 ) ; insert into tauto values ( 8,8 ) ; & # 8211 ; notice that identity column at this point has used 1 through 8 & # 8211 ; now if the user wants to let the system generate a value , system will generate 3 but that is already used and hence & # 8211 ; insert will throw unique key failure exception . System has consumed 3 at this point . insert into tauto ( k ) values 9 ; & # 8211 ; the insert above will continue to fail with the unique key failure exceptions until system has consumed all the values till 8 & # 8211 ; If we add ALTER TABLE syntax to allow changing the next value to be generated , then user can simply use that to change & # 8211 ; next value to be generated to 9 after the manual inserts above and then insert into tauto ( k ) values 9 will not fail SQL standard syntax for changing the next generated value ALTER TABLE & lt ; tablename & gt ; ALTER & lt ; columnName & gt ; RESTART WITH integer-constant 
In org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext , it is probably safe to replace some of the synchronized collections with unsynchronized ones . This should be investigated , and the unnecessary synchronization should be removed . See discussion here : 
Expand the work of to support the remaining diagnostic tables that take parameters . Syntax would use the table constructor , like ( not sure if an AS clause will be required : select * from TABLE ( SYSCS_DIAG.SPACE_TABLE ( ? , ? ) ) Diagnostic VTIs that could be handled this way are : ErrorLogReader ( String log file name ) SpaceTable ( String tableName ) SpaceTable ( String schemaName , String tableName ) StatementDuration ( String inputFileName ) This is the second stage mentioned in 
Can be used of the base for other projects such as a standalone SQL syntax checker discussed here : 
A timeout is set on the session socket ( ClientThread ) but the SocketTimeoutException is not taken care of . Connections is therefore closed down if derby.drda.timeSlice is set and the client idles longer then the timeslice . See for more details . 
Some of the code for locking a row while holding a latch doesn t seem to be used outside the unit tests for store . This code should be removed in order to reduce the overall complexity . See the discussion in for details . 
There is a simple performance junit framework that has been added as part of derby-1970 . There are a number of improvements suggested for derby-1970 , please see . This jira is to add new performance tests to derby so that these tests are easily available for anyone to run , report issues , fix issues etc .. 
1 ) It s good to perform testing with different jvm classpaths , included those typically used , such as an embedded environment . 2 ) Running tests with platofrms such as J2ME/CDC/Foundation should be supported with the only jars that the platform supports in the class path . Currently running suites.All with just derby.jar hits this error : Failed to invoke suite ( ) : java.lang.NoClassDefFoundError : org.apache.derby.client.am.SqlException 
The class SPSDescriptor is kind of hard to understand , and doesn t quite follow the pattern used by other tuple descriptors . Parts of the code don t agree with the documentation ( i.e . SPS_TYPE_TRIGGER marked as not implemented ) This issue tracks work to make the class easier to understand and to modify . The critical part that may need to be changed is the use of synchronized ( this ) . The problem is that database locks are obtained within the critical regions , and in some special cases this causes deadlocks . I m not yet certain this can be fixed at this level ( only ) , but I ll continue investigation . 
I would like to enable the old test harness to support the new version of IBM s j2ME implementation , which is based on j2ME jdk spec version 1.1 . This is available with a product named Websphere Everyplace Micro Edition 6.1. from IBM . We already support j9_foundation , which matches to j2ME jdk spec 1.0 . I d like to add j9_foundation11 , which then matches to j2ME jdk spec 1.1 . I m proposing to switch my automated tests over to the newer version going forward , and to minimize complexity of the change , I d like to make the canons reflect behavior of the new version . The differences are minimal . However , I want to be able to still run with the old ( except where the results differ , failures would occur with the old version ) . One of the reasons for moving to the new version is that there is a bug with the older version in regards to security manager , preventing a smooth run of the junit tests , and I d like to run all short-running tests ( suites.All and derbyall ) with at least one of the versions . Another reason is that the j2ME spec 1.0 is really old . 
Create a test for DatabaseMetaData that matches the testing of metadata.java . Should also handle the odbc_metadata variant . Once created can be used by other tests to ensure database meta data works in other situations , such as upgrade and read-only databases . 
This JIRA separates out the database-owner powers from the system privileges in the master security JIRA . Restrict the following powers to the database owner for the moment : shutdown , upgrade , and encryption . 
To get an updatable result set , the JDBC 3.0 spec , section 14.2.4 Modifying ResultSet Objects states : ResultSet objects with concurrency CONCUR_UPDATABLE can be updated using ResultSet objects . In addition , Derby requires the SQL SELECT statement to have a FOR UPDATE clause for updates to be allowed . This may be a usability issue , as many examples , e.g . in JDBC API tutorial and reference and reference book and the JDBC 3.0 Specification ( 14.2.4.1 ) do not include a FOR UPDATE clause in the SQL SELECT . Mamta Satoor says : Derby implements the JDBC updatable resultset by using the existing updatable cursor implementation . And in order to do that , it requires that the SELECT statement should include the FOR UPDATE clause . One can change the Derby implementation so that it does not require FOR UPDATE clause to piggyback on updatable cursor implementation . Dan DeBrunner says : Technically I wonder if this is covered by the JDBC standard , I see nothing in the JDBC 3.0 that states any requirements for the SQL statement for an updateable result set . I know the JDBC tutorial book has some guidelines as to what will typically work , but isn t it up to the database engine to define what works here ? Having said that I think that not requiring the FOR UPDATE would be a useful improvement . 
Disallowing routines from accessing Derby code directly stops the potential of remote code exploiting any security holes in Derby . Derby code can be seen as a special case since it is known that the Derby code will be on the classpath . Disallowing such routines makes security analysis easier and safer rather than trying to guarantee every public static method in Derby can not expose secured information . Routines in existing applications ( in upgraded databases ) that map to such Derby methods will fail at execute time . 
Implement DETERMINISTIC keyword for procedures and functions We will need DETERMINISTIC functions when we implement generated columns ( ) . The syntax and behavior of the DETERMINISTIC keyword are defined in the functional spec attached to . Add tests of generated columns when sql authorization is turned on Add tests of generated columns under sql authorization . For instance , we need to verify the current_role can not appear in generation clauses . 
Since Derby is open source and ( obviously ) contains the code to read database files and is modular the potential exists that routines could utilize code on the classpath to read/modify database information directly , bypassing SQL level security . Derby is a special case here as it is known that Derby code will be on the classpath and that it will have the correct permissions to read/write database files . Existing routines from upgraded databases will fail at execute time when they try to resolve such classes . 
Default SSL behaviour is to require serer authentication . For a database application this is not as important as it is for web browsers and also creates som extra hassle for the user/application programmer . Since the main objective for SSL in Derby is encryption on the wire , server authentication should be optional ( the same way client authentication is ) . This also creates some symmetry which can be exploited to simplify the user interfce somewhat . This improvement to is described in the attached functional specification . See the attachment for details . 
When trying to learn more about the access layer , it was discovered that some code improvements could easily be made to increase the readability of the code . Patches attached to this issue will be cleanup patches only , and no functionality should be changed . Changes the will/may be made : remove unused imports remove unused methods fix JavaDoc errors tighter encapsulation and removal of unused variables where appropriate 
The shutdown command takes at least 1.5 seconds , broken into two parts : 0.5 seconds is the first sleep after the shutdown request and before the first ping to see if the server is shutdown . ~1 second is the time trying to obtain a socket in order to perform the ping to see if the server is shutdown . This socket creation fails ( correctly ) but takes time to do so . The socket creation time can be avoided by re-using the socket that was used to send the shutdown request . The sleep time could be reduced by sleeping on a finer granularity , 0.5secs is a long time for a modern processor . 
This jira entry is one of the tasks involved in implementing . The existing SQLChar datatype has the Derby s default collation which is UCS_BASIC defined on them . With Derby 10.3 , we want to support an additional collation for char datatypes which will be based on the territory . This jira issue is the placeholder for creating subclass of SQLChar which will use the passed Collator to do the collation . The current use of this class in Derby 10.3 will be for territory based collation but this class can be used in future for other kinds of collations like case-insensitive etc . 
Add an MBean that an application can register to change the state of Derby s JMX management JMX in Derby was originally proposed as a mechanism to configure Derby replacing or enhancing the system properties which tend to be static in nature . Thus it is somewhat ironic that jmx is enabled with a static system property derby.system.jmx . I propose to add a public mbean that allows the state Derby s JMX management to be changed . This bean is not automatically registered by Derby if derby.system.jmx is false , but instead can be registered by an application . I believe this could occur at any time so that JMX could be enabled on a running application , possibly by a remote client . This standard Mbean ( o.a.d.mbeans.Management & amp ; ManagementMBean ) would have these operations & amp ; attribute : public boolean isManagementActive ( ) ; public void startManagement ( ) ; public void stopManagement ( ) ; If Derby is not booted within the jvm then the operations would be no-ops . If derby.system.jmx is true then Derby will itself register an mbean that implements ManagementMBean to allow dynamic control of the visibility of Derby s mbeans . 
The following methods were introduced in the java.sql.Clob and java.sql.Blob interface as part of JDBC 4.0 and need to be implemented . Clob -- -- -- getCharacterStream ( long pos , long length ) Blob -- -- -- getBinaryStream ( long pos , long length ) 
Seen intermittently on Linux since 2007-02-27 . Seen on Solaris 2007-03-12 . & lt ; signature & gt ; ejbql ( org.apache.derbyTesting.functionTests.tests.lang.LangScripts ) junit.framework.ComparisonFailure : Output at line 454 expected : & lt ; & # 91 ; 0.0 & # 93 ; & gt ; but was : & lt ; & # 91 ; -0.0 & # 93 ; & gt ; at junit.extensions.TestDecorator.basicRun ( TestDecorator.java:24 ) at junit.extensions.TestSetup $ 1.protect ( TestSetup.java:21 ) at junit.extensions.TestSetup.run ( TestSetup.java:25 ) & lt ; /signature & gt ; & lt ; signature & gt ; floattypes ( org.apache.derbyTesting.functionTests.tests.lang.LangScripts ) junit.framework.ComparisonFailure : Output at line 1823 expected : & lt ; & # 91 ; 0.0 & # 93 ; & gt ; but was : & lt ; & # 91 ; -0.0 & # 93 ; & gt ; at junit.extensions.TestDecorator.basicRun ( TestDecorator.java:24 ) at junit.extensions.TestSetup $ 1.protect ( TestSetup.java:21 ) at junit.extensions.TestSetup.run ( TestSetup.java:25 ) & lt ; /signature & gt ; 
As Derby has moved to J2SE 1.4 , Throwable.initCause ( ) can now be used to improve error reporting . As stated in the initial commit ( that was backed out , see below ) : Set Throwable.initCause ( ) in a couple of locations at the JDBC level where we setup nested SQLExceptions . This means that automatically the stack trace for a database failed to start includes the reason for the failure , rather than just see next exception . This is a great help when running JUnit tests and getting failures to start a database . The initial commit was backed out because it broke numerous tests under JDK 1.6 , and the author indicated he had no time to investigate . The patch caused no failures when running the tests with Java SE 5.0 . 
BackingStoreHashtable uses a Vector and a Hashtable , but doesn t need the synchronization provided by these classes ( I think ) . Replacing them with ArrayList and HashMap could improve performance for some kinds of operations . 
In the process of converting this test , I realized we needed an way to run ij tests that would work with multiple connections . So , I made an IjTestCase that runs a script through ij s main method , redirects the output to a file , and then compares the file to a canon . Attaching a patch for this issue which does that . The test creates several databases during its run . Should the JUnit wrappers for ij tests be cleaning these up , or will these get cleaned up later automatically ? 
This the one of the ground works for getting different kinds of collations working for character string types . More information on this project can be found at . Basically , all the types in Derby have a DTD associated with them . For character string types , these DTDs should have valid values for collation derivation and collation type . For other data types , these 2 fields do not apply and should be ignored . SQL spec talks about character string types having collation type and collation derivation associated with them ( SQL spec Section 4.2.2 Comparison of character strings ) . If collation derivation says explicit or implicit , then it means that there is a valid collation type associated with the charcter string type . If the collation derivation is none , then it means that collation type can t be established for the character string type . 1 ) Collation derivation will be explicit if COLLATE clause has been used for character string type ( this is not a possibility for Derby 10.3 , because we are not planning to support SQL COLLATE clause in this release ) . 2 ) Collation derivation will be implicit if the collation can be determined w/o the COLLATE clause eg CREATE TABLE t1 ( c11 char ( 4 ) ) then c11 will have collation of USER character set . Another eg , TRIM ( c11 ) then the result character string of TRIM operation will have collation of the operand , c11 . 3 ) Collation derivation will be none if the aggregate methods are dealing with character strings with different collations ( Section 9.3 Data types of results of aggregations Syntax Rule 3aii ) . 
Persistent user defined character columns get added through CREATE TABLE and ALTER TABLE statements , These 2 statements should set the collation type of character columns to the same value as the collation type of the schema they belong to . This collation type will get saved in SYSCOLUMNS and will also be saved in the metadata maintained by Store . 
In Derby 10.3 , the collation of char datatypes can be different depending on what kind of collation is requested by the user at the database create time through the optional JDBC url attribute COLLATION . The collation type associated with the DTD will determine which kind of DVD needs to be generated . ( Note that , irrespective of what collation is used , the format id of the char datatypes remain same . ) In order to support this behavior of generating the base DVD or the collation sensitive DVD for character datatypes , we need to add a new api to StringDataValue which will look as follows / * * Gets either SQLChar/SQLVarchar/SQLLongvarchar/SQLClob ( base classes ) or CollatorSQLChar/CollatorSQLVarchar/CollatorSQLLongvarch/CollatorSQLClob ( subclasses ) . Whether this method returns the base class or the subclass depends on the value of the RuleBasedCollator . If RuleBasedCollator is null , then the object returned would be baseclass otherwise it would be subcalss . * / public StringDataValue getValue ( RuleBasedCollator collatorForComparison ) ; 
Add a new api on DVF which will make Locale object available to DVF . This new api on DVF will get called by the boot method of BasicDatabase after BasicDatabase has finished booting DVF . This Locale will be either the Locale obtained from the territory attribute supplied by the user on the JDBC url at database create time or if user didn t provide the territory attribute at database create time , then it will be set to the default JVM locale . This Locale object will be used by DVF to construct the Collator object if user has requested territory based collation . The new api will look like following void setLocale ( Locale localeOfTheDatabase ) ; 
Provide a new interface , called getInstanceUsingFormatIdAndCollationType ( formatId , collationType ) on DVF . Store will call this interface to create empty DVD objects and then load these DVD objects using readExternal . For performance efficiency , when Store needs more of the same base DVD , then on subsequent needs , it will call DVD.getNewNull rather than calling DVF.getInstanceUsingFormatIdAndCollationType again . This work is required for . 
The performance of reading modified Clobs is poor , which is demonstrated by running a test program selecting a 10 MB Clob and then getting the contents using getSubString : unmodified Clob ( StoreStreamClob ) : ~1 300 ms modified Clob ( TemporaryClob ) : ~156 000 ms In this case , the Clob was modified by changing the first character . A number of subtasks will be created to handle the various issues , which will be related to both performance and code cleanup . For a brief overview , see 
Currently , when Derby needs to check if 2 DTDs are comparable or not , it calls TypeCompiler.comparable method . This method now has to move to DTD because we should consider collation information while deciding if 2 DTDs are comparable or not . That collation information is not available to TypeCompiler but it is available at DTD level . More information on this can be found at . Some of the important info from the thread is copied below & gt ; I think what you are suggesting is to move comparable method out from & gt ; the TypeCompiler and into DataTypeDescriptor altogether . So , the & gt ; existing code , where we use TypeCompiler to decide if 2 types can be & gt ; compared or not should now call a method on DTD to determine & gt ; comparability . This might be cleaner than stuffing collation information & gt ; in CharTypeCompiler but I am just wondering why was comparable not & gt ; defined on DTD at the very start . Why do we go through TypeCompiler and & gt ; what functionality does TypeCompiler provide that DTD does not ? In other & gt ; words , I don t understand the connection between TypeCompiler and DTD & gt ; and how they fit together . It s not that TypeCompiler provides functionality that DTD does not , but instead DTD has functionality/information that TypeCompiler does not . Ignoring the compiler aspect for the moment there are two components to a DataTypeDescriptor , the underlying SQL type ( INTEGER , CHAR , VARCHAR , XML etc . ) represented as TypeId and attributes of the descriptor ( nullablity , length , precision , scale and now collation ) . Thus DTD = TypeId + { attributes } Some functionality is applicable to a type regardless of a specific DTD s attributes , thus methods for that functionality can be declared on TypeId instead of DTD . Some functionality on the other hand needs the attribute information as well , say the display length of a type is a function of its length/precision & amp ; scale and its underlying SQL type . The collation changes have moved the comparable check from being only reliant on the SQL type ( TypeId ) to being dependent on the type s attributes ( collation type and implicit/explicit ) . Thus the original location for the comparable method made sense , but now does not . The TypeCompiler/TypeId split was due to an early plan to have a execute-only version of the technology , this never happened as there was no demand for it . One of the benefits of a SQL engine is the ability to execute arbitrary queries , which would not be available in an execute only version . Code cleanup could be done here which probably would decrease the footrprint of derby . HTH , Dan . 
Currently , AssertFailure implements its own chaining of exceptions . It should use the chaining mechanism provided by the JDK . 
Some places in the code where SanityManager.THROWASSERT is used to wrap unexpected exceptions , the underlying exception is not linked to the assert exception . Therefore , the original stack trace , and possibly the message text , are lost . THROWASSERT ( Throwable ) or THROWASSERT ( String , THROWABLE ) should be used instead in these cases . 
Currently , in Derby 10.2 , we generate SQLChar/SQLVarchar/SQLLongvarchar/SQLClob for various SQL character types . All these DVDs have the default collation of UCS_BASIC . Starting 10.3 , we should look at the collation type of the DTD before deciding what kind of DVD gets generated for SQL character types . If the collation type of the character type is terriotry based collation , then we should generate CollatorSQLChar/CollatorSQLVarchar/CollatorSQLLongvarchar/CollatorSQLClob rather than SQLChar/SQLVarchar/SQLLongvarchar/SQLClob . The CollatorSQLxxx DVDs will have the territory based collator associated with them and hence the collation order of these DVDs will be different than the DVDs with default collation of UCS_BASIC . 
It would be good if when derby dumped an error to derby.log it printed the parameters for the failed statement . Currently the default behaviour is that only the statement text will print . Users have to set derby.language.logStatementText=true if they want to see the parameters . It would be useful if any errors included the parameters as well as the statement text . To reproduce put derby.stream.error.logSeverityLevel=0 in your derby.properties and run this script : connect jdbc : derby : wombat ; create=true ; create table t ( i int ) ; prepare p as insert into t values ; execute p using values ( 1 ) ; execute p using values ( 10000000000000000000000000000000000000000000 ) ; 
In Derby 10.3 , we are adding a new feature ( ) which will allow character string types to have a territory based collation ( except for persistent character string type columns from system schema ) . When 2 character types are compared , they should have the same collation type associated with them . If not then Derby 10.3 will throw an exception . Currently , in Derby 10.3 codeline , the exception thrown is something like ERROR 42818 : Comparisons between VARCHAR and CHAR are not supported . but this error message is misleading . The error is really for collation mismatch . We should fix following error so it indicates the real reason behind the exception . eg sql ij & gt ; connect nodb ; create=true ; territory=no ; collation=TERRITORY_BASED ; ij & gt ; select * from sys.systables where tablename = T1 ; ERROR 42818 : Comparisons between VARCHAR and CHAR are not supported . 
Performance problems are being reported that can be resolved by updating the cardinality statistics used by the optimizer . Currently the only time the statistics are guaranteed to be an up-to-date is when the index is first created on a fully populated table . This is most easily accomplished on an existing table by using the command : alter table & lt ; table-name & gt ; compress & # 91 ; sequential & # 93 ; Compress table is an I/O intensive task . A better way to achieve this would be to re-enable parser support for the update statistics command or re-implement the update in some other fashion . 
Derby still has some untested , unused code relating to a non-standard implementation of a Nationa Char type . The current code can be removed . I believe the interesting functionality associated with this is now provided by ( territory based collation ) . If Derby ever implements a National Char type it should do so differently than the existing code , collation should not be tied to the National Char type . I believe a future National char type might have to maintain a separate type id for compatibility with jdbc interface , but actual implmentation should be the same code as the char types . Collating of the the national char type should be supported in exactly same way as regular char types . If anyone is really intested in the national char code , it s history will always be available in svn , and a consistent version is available by looking at 10.0 , 10.1 , and 10.2 codelines . I would propose any removal of code only take place in trunk and not be backported to a released codeline . 
Add procedure & amp ; function to set ( and get ) a user s connection authorization to full access , read-only or not defined . Based upon the existing UserUtility class but only exposing a single procedure that sets the permission . This is to avoid any appearance that the procedure adds a user . SYSCS_UTIL.SYSCS_SET_USER_ACCESS ( VARCHAR ( 128 ) USERNAME , VARCHAR ( 8 ) CONNECTION_PERMISSION ) Valid values for CONNECTION_PERMISSION FULL - Add the user to the list of full access users for the database , i.e . the database property derby.database.fullAccessUsers ) READONLY - - Add the user to the list of read-only users for the database , i.e . the database property derby.database.readOnlyAccessUsers ) NULL - remove the user from the list of permissions , reverting it to the default permission . SYSCS_UTIL.SYSCS_GET_USER_ACCESS ( VARCHAR ( 128 ) USERNAME ) RETURNS VARCHAR ( 8 ) Gets the current connection access permissions for the user , factors in the default connection mode . Return either FULL , READONLY , NO or NULL . ( NO means connection attempt by user will be denied by the user not having an entry in derby.database.fullAccessUsers or derby.database.readOnlyAccessUsers and derby.database.defaultConnectionMode is set to noAccess ) The names of the connection permissions match the existing names in use by Derby . 
Sun s Java 1.5.0 and higher includes Xalan , but Derby doesn t find it because it has been moved to a non-standard package . Derby should be able to detect and use these classes if it can not find Xalan in the standard package on the classpath . This would make it easier for many users to start using Derby s XML features . See also the discussion in this thread : & lt ; URL : & gt ; 
Emptying the statement cache is a potentially useful diagnostic capabaility and is tested by StatementPlanCacheTest . However the current functionality is only useable by creating a user procedure that maps directly into Derby s code which will be disabled by . In addition such functionality should be under the control of the database owner which will become automatic ( in SQL Authorization mode ) once it is a system procedure . 
When the fix for is in place , the current implementation of ClobUpdateableReader will fail . This patch is a preparation for the mentioned issue , and I have also removed some internal ( package private ) methods from EmbedClob and replaced it with a single one . Instead of writing methods that forwards calls to the underlying InternalClob , the method getInternalClob will return the internal clob itself . This seems more in place , since the method getByteLength does not belong in EmbedClob . It also comes with a warning that using the reference to the internal clob object requires the caller to take more care . Second , isWritable ( ) does not belong to EmbedClob either , as a Clob is always writable ( unless the database/connection itself is read/only ) . However , the internal clob representation may be read-only , in case EmbedClob must clone the content and create a writable representation . The fix leaves more of the handling of implementation details to the InternalClob itself . 
The spec attached to describes how to allow the policy file to be dynamically reloaded while a server is running : We add a getPolicy permission to the Basic policy and we add a DBA-owned system procedure , SYSCS_UTIL.SYSCS_REFRESH_SECURITY_POLICY ( ) , which reloads the policy file . This JIRA tracks that work . 
Currently StoreStreamClob reads the whole Clob stream , including decoding it , to find the length of it . It also does this the second time the length is asked for . StoreStreamClob is the internal Clob representation for Clobs that are read-only . As soon as the user updates the Clob , it is transferred to a modifiable Clob representation . It should be determined if it is safe to cache the length ( both in bytes and in characters ) of the store stream to improve the performance and reduce the load on Derby for certain Clob operations . To do this , the locking mechanism used for Clobs must be analyzed . If you have obtained a Clob object , is there a lock in place that stops others from changing the content ? For all isolation levels ? What about scrollable result sets ? Can the streamed content from store be changed under us , and thus invalidate a cached length ? 
I plan to do the following changes to UTF8Reader : Improve the error reporting when hitting a UTF8 decoding error ( currently an UTFDataFormatException with no message ) . This might also lead to deleting one helper method for generating an exception ( the one with no message ) . Improve error reporting for trying to use the reader after it has been closed ( currently an IOException with no message ) . Remove trailing spaces , and add a few newlines here and there . Replace tabs in the file with spaces . Now , the last point can be discussed , but here are my arguments for doing it : The file now has a mix of tabs and spaces ( but still more tabs ) . Spaces are the preferred/required method of indentation . I want to get it fixed before the branch is cut , which makes it easier to port fixes from trunk/10.3-NEXT to 10.3 . If I don t make it for 10.3 , I won t do it . I don t see it as very likely that we will back-port major fixes to this class on the 10.2 branch . If we have to , I will volunteer Since so much else of the Clob infrastructure has changed recently , this seems like a good time to do the clean-up . Please raise your concern as soon as possible if you want to veto these changes . I do plan to commit them tomorrow . 
Rename UpdateableBlobStream to UpdatableBlobStream . Also rename the corresponding test , and all classes affected by the rename . 
If Network Client can not connect to the database to retrieve an error message , it will print only the message tokens , the non-ascii token separators , and derby log location . It would be good if at least the message could be formatted to present a better message to the user without the non-ascii characters . To reproduce try a database shutdown . Because the database is shutdown , the client can not retrieve the actual message from the server . It therefore just prints the tokens . Start network server java org.apache.derby.drda.NetworkServerControl start $ java org.apache.derby.tools.ij ij version 10.1 ij & gt ; connect jdbc : derby : //localhost:1527/wombat ; create=true ; ij & gt ; connect jdbc : derby : //localhost:1527/wombat ; shutdown=true ; ERROR 08006 : DERBY SQL error : SQLCODE : -1 , SQLSTATE : 08006 , SQLERRMC : wombat08006.DDatabase wombat shutdown . ( server log : derby.log ) ij & gt ; Note : The actual offending characters have been replaced in the output in this bug by . This is because they break Jira XML retrieval ! 
Each time a leaf node in a B-tree is visited in an index scan , a scan protection row is locked and unlocked . Both the lock operation and the unlock operation will allocate a new RecordId object representing the scan protection row ( the unlock operation additionally allocates a PageKey object for the RecordId ) . Since the scan protection handle created will be identical ( seen from equals ( ) ) each time it is created for a page , it would make sense to cache it in BasePage . Then we only need to allocate the protection handle for a page once for as long as it stays in the page cache . This would save three object allocations per single-record lookup via index . 
Each time a transaction is committed or aborted , GenericLanguageConnectionContext.resetSavepoints ( ) goes through the stack of statement contexts and invokes resetSavePoint ( ) on them . Instead of traversing the list in the CtxStack object directly , it fetches a read-only view of the list . Since the lifetime of a CtxStack object and the list within it can span multiple transactions , the read-only view only needs to be created the first time CtxStack.getUnmodifiableList ( ) is called on an object . 
There are indications that the buffer manager is a bottleneck for some types of multi-user load . For instance , Anders Morken wrote this in a comment on : With a separate table and index for each thread ( to remove latch contention and lock waits from the equation ) we ( ... ) found that org.apache.derby.impl.services.cache.Clock.find ( ) /release ( ) caused about 5 times more contention than the synchronization in LockSet.lockObject ( ) and LockSet.unlock ( ) . That might be an indicator of where to apply the next push . It would be interesting to see the scalability and performance of a buffer manager which exploits the concurrency utilities added in Java SE 5 . 
Most of the implementations of DataValueDescriptor.readExternalFromArray ( ArrayInputStream ) are identical to their corresponding Externalizable.readExternal ( ObjectInput ) methods . Since ArrayInputStream implements ObjectInput , readExternalFromArray ( ) could in those cases just have forwarded calls to readExternal ( ) instead of duplicating the code . A default forwarding implementation of readExternalFromArray ( ) could be placed in org.apache.derby.iapi.types.DataType , and all the existing implementations , except those with optimizations for ArrayInputStream , could be removed . 
Derby s query optimizer usually makes the best choice of join order and access path . The default join strategy ususally works the best too . However , there are some cases in which user may want to override the optimizer or the default values . Providing support for optimizer overrides will allow users to hand-tune the optimizer for queries . 
org.apache.derby.impl.drda.DDMWriter uses a byte array as a buffer . Wrapping the array in a java.nio.ByteBuffer has some advantages , for instance : utility methods for encoding primitive types into the byte array could be used instead of manually encoding the values it allows us to encode strings directly into the buffer ( using a CharsetEncoder ) without doing an expensive String.getBytes ( String encoding ) in an intermediate step By using a utility class , the code becomes easier to maintain . Also , ByteBuffer allows us to access the backing byte array without going through the ByteBuffer interface , so we still have the possibility to modify the byte array directly in cases where that s more convenient . 
To investigate whether there was anything in the SQL execution layer that prevented scaling on a multi-CPU machine , I wrote a multi-threaded test which continuously executed VALUES 1 using a PreparedStatement . I ran the test on a machine with 8 CPUs and expected the throughput to be proportional to the number of concurrent clients up to 8 clients ( the same as the number of CPUs ) . However , the throughput only had a small increase from 1 to 2 clients , and adding more clients did not increase the throughput . Looking at the test in a profiler , it seems like the threads are spending a lot of time waiting to enter synchronization blocks in GenericPreparedStatement.upToDate ( ) and BaseActivation.checkStatementValidity ( ) ( both of which are synchronized on the a GenericPreparedStatement object ) . I then changed the test slightly , appending a comment with a unique thread id to the VALUES 1 statement . That means the threads still did the same work , but each thread got its own plan ( GenericPreparedStatement object ) since the statement cache didn t regard the SQL text strings as identical . When I made that change , the test scaled more or less perfectly up to 8 concurrent threads . We should try to find a way to make the scalability the same regardless of whether or not the threads share the same plan . 
( Language ) ResultSet.getResultDescription ( ) returns the ResultDescription of the top-level tree ( ie . of the results that will be passed to the application ) and thus is an attribute of the plan or activation . The ResultDescription is already available through the plan and activation . Having this method of the ResultSet can lead to the assumption that it is the ResultDescription for that specific ResultSet , e.g . the shape of the results before any joins or projection . Note the main implementation of this method just calls activation.getResultDescription . 
Varargs were added in Java 5 . It would be nice if Derby let you invoke a function bound to a method with a variable length argument list . The Reference Guide states a small number of restrictions for methods which can be invoked as Derby functions : They must be public , static , and not have arguments which are long datatypes . I see no reason that Derby shouldn t be able to resolve and invoke functions which are bound to methods which don t suffer these limitations but which have variable argument lists . 
In BaseActivation.java there is the following code : protected final DataValueDescriptor getColumnFromRow ( int rsNumber , int colId ) throws StandardException { if ( row & # 91 ; rsNumber & # 93 ; == null ) { / * This actually happens . NoPutResultSetImpl.clearOrderableCache attempts to prefetch invariant values * into a cache . This fails in some deeply nested joins . See Beetle 4736 and 4880 . * / return null ; } return row & # 91 ; rsNumber & # 93 ; .getColumn ( colId ) ; } During the investigation of , I came to the conclusion that this if statement is no longer necessary , and in fact is counter-productive , for it makes diagnosing other problems harder by delaying the point at which data structure problems are exposed as errors in the code . This JIRA issue requests that this code be evaluated , to determine whether or not it truly is necessary , and , if it is not necessary , suggests that it should be removed , to result in simpler , clearer code . 
Derby s page cache often has a memory footprint that is much larger than pageSize * pageCacheSize . One large contributor to the footprint is the array of StoredPageHeader objects in BasePage . The memory consumed by these objects can be as large as , and sometimes even larger than , the byte arrays containing the raw page data . ( See for instance . ) Reducing the size of the StoredPageHeader objects could therefore reduce Derby s memory footprint significantly , especially if the page cache is large and contains many pages from tables with small records or from indices . 
The LIKE clause for territory based characters was implemented correctly based on SQL standards in but the object ( String and CollationElementIterator ) creations introduced in can be cut down by following Knut s simple solution . I am copying that solution from . We should implement that solution to improve the performance of LIKE for territory based characters . * * * * * * * * * copied from * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Another simple way to cut down the string allocations ... I think you could express iapi.types.Like : checkEquality ( ) like this : if ( val & # 91 ; vLoc & # 93 ; == pat & # 91 ; pLoc & # 93 ; ) { // same character , so two strings consisting of this // single character must be equal regardless of territory return true ; } else if ( collator == null ) { // not same character , must be unequal in UCS_BASIC return false ; } String s1 = new String ( val , vLoc , 1 ) ; String s1 = new String ( pat , pLoc , 1 ) ; return collator.compare ( s1 , s2 ) == 0 ; This would only allocate new objects if the characters are not equal . 
Proposal to enhance Derby s Built-In DDL User Management . ( See proposal spec attached to the JIRA ) . Abstract : This feature aims at improving the way BUILT-IN users are managed in Derby by providing a more intuitive and familiar DDL interface . Currently ( in 10.1.2.1 ) , Built-In users can be defined at the system and/or database level . Users created at the system level can be defined via JVM or/and Derby system properties in the derby.properties file . Built-in users created at the database level are defined via a call to a Derby system procedure ( SYSCS_UTIL.SYSCS_SET_DATABASE_PROPERTY ) which sets a database property . Defining a user at the system level is very convenient and practical during the development phase ( EOD ) of an application - However , the user s password is not encrypted and consequently appears in clear in the derby.properties file . Hence , for an application going into production , whether it is embedded or not , it is preferable to create users at the database level where the password is encrypted . There is no real ANSI SQL standard for managing users in SQL but by providing a more intuitive and known interface , it will ease Built-In User management at the database level as well as Derby s adoption . 
While working on roles , I notice that there is a max size of 30 on user ids in derby ( authorization identifiers ) , e.g . the check being performed in the parser : private void checkAuthorizationLength ( String authorization ) : checkIdentifierLengthLimit ( authorization , Limits.DB2_MAX_USERID_LENGTH ) ; : where Limits.DB2_MAX_USERID_LENGTH == 30 . I have checked , and I don t think there are any fundamental reasons why Derby can t lift this DB2 restriction : Then authorization identifiers would have the same max limit as other identifiers : 128 ( Limits.MAX_IDENTIFIER_LENGTH ) . Currently , this limit of 30 is enforced for GRANT/REVOKE , i.e . for the grantees . However , in the CREATE SCHEMA statement , the clause AUTHORIZATION & lt ; authorization identifier & gt ; which allows specifying a schema s owner , is not subject to this restriction . This is also reflected in the reference documentation for system tables : SYS.SYSCHEMAS : Column Name Type Length Nullability Contents -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - AUTHORIZATIONID VARCHAR 128 false the authorization identifier of the owner of the schema SYS.SYSTABLEPERMS : Column Name Type Length Nullability Contents -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - GRANTEE VARCHAR 30 False The authorization ID of the user to whom the privilege is granted . Furthermore , the limit is enforced in the authorizer code ( AuthorizationServiceBase # authenticate ) . It is also reflected in the metadata : EmbedDatabaseMetaData # getMaxUserNameLength . I think it would be good to harmonize these two different limits for authorization identifier and change the limit to 128 ( Limits.MAX_IDENTIFIER_LENGTH ) . 
These type classes save a copy of the value when it is converted to a String ( e.g . through a ResultSet.getString ( ) ) . This complicates the code & amp ; increases memory use for little value , in most cases the cached value will never be used . E.g . for any type of scan the String value will be discarded when moving to the next row . In most cases applications do not call getString ( ) twice on a column . The code has some historical basis in the fact that these types used to be represented by a java.sql.Time/Date/Timestamp object and its conversion to String was slow . Now the conversion of all these types to a String is simple . In addition I think the getString ( ) will sometimes return a non-normalized form , if the value is set by a non-standard format then the cached String is set to the non-standard format , not the standard format , I believe this is incorrect . 
The reason for doing this is to avoid a rather substantial performance hit observed when the client driver is used together with an appserver that uses connection pooling . There are two problems : 1 ) The connection pool will compare the isolation level it has stored for the connection with the value returned from Connection.getTransactionIsolation ( ) each and every time someone requests a new connection from the pool . 2 ) The users of the connection pool ( ab ) use it to avoid having to keep track of their current connection . So each time a query needs to be executed a call to the connection pool s getConnection ( ) method is made . Getting a connection from the connection pool like this also means that a new PreparedStatement must be prepared each time . The net result is that each query results in the following sequence : getConnection ( ) getTransactionIsolation ( ) -- & gt ; roundtrip + lookup in server s statement cache prepareStatment ( ) -- & gt ; roundtrip + lookup in server s statement cache executeQuery ( ) -- & gt ; roundtrip Arguably this is a user error but when suggesting this I m kindly informed that this works just fine with other datbases ( such as PostgreSQL and ORACLE ) . The reason why it works is that these databases do statement caching in the driver . I ve tried to implement a very ( too ) simple statement cache in Derby s client driver and to re-enable caching of the isolation level ( see ) . With these changes I observe a marked performance improvement when running with appserver load . A proper statment cache can not be implemented without knowing what the current schema is . If the current schema has changed since the statement was prepared , it is no longer valid and must be evicted from the cache . The problem with caching both the isolation level and the current schema in the driver is that both can change on the server without the client detecting it ( through SQL and XA and possibly stored procedures ) . I think this problem can be overcome if we piggy-back the information we would like to cache on messages going back to the client . This can be done by utilizing the EXCSQLSET DRDA command . According to the DRDA spec ( v4 , volume 3 , page 359-360 ) it is possible to add one or more SQLSTT objects after SQLCARD in the reply , I think it would be possible to cache additional session information when this becomes relevant . It would also be possible to use EXCSQLSET to batch session state changes going from the client to the server . 
The explicit mechanisms to start the network server make no mention of if they install a default security manager and policy or not . These are at least : NetworkServerControl javadoc summary - start command on command line main ( ) method start ( ) method derby.drda.startNetworkServerProperty Each start mechanism should clearly state if they install or do not install the default security manager and policy . ( Other starts commands are ) java -jar derbynet.jar java -jar derbyrun.jar server start I didn t look to see if these document the default policy or not , not sure where they are documented . 
As part of researching , I came to the conclusion that the trim ( ) method in SQLChar.java is no longer used , and could be removed . It appears that all calls to this method have been switched to call ansiTrim ( ) instead . Hopefully , this just requires removing the code from SQLChar.java and StringDataValue.java , and running all the tests to verify that nothing is broken . 
I m trying to get suites.All to run on the phoneME advanced platform . 
Add remaining JDBC 4.1 bits which did not appear in the Java 7 javadoc . In addition to the JDBC 4.1 bits which were visible in the Java 7 javadoc , a couple other items appear in the JDBC 4.1 Maintenance Review spec . This spec has been published on the JCP website at . I will attach a functional spec for the remaining bits . 
The hash values returned by RecordId.hashCode ( ) are constructed by performing bitwise xor on the 32 least significant bits of the record number , the page number , the container id and the segment id . Since all of these values tend to be relatively small positive integers , the hash values also tend to be clustered in a very small range . This leads to a higher frequency of hash collisions in the lock table , which makes the hash tables work less efficiently and thereby reduces the performance . As an example , the simple join load in the test client attached to uses two tables , TENKTUP and ONEKTUP , with 10000 rows and 1000 rows , respectively . The RecordIds for these 11000 rows map to less than 900 distinct hash codes . 
Minor issue but CancelQueryTask.forgetContext ( ) has this code ( in GenericStatementContext.java ) public void forgetContext ( ) { boolean mayStillRun = ! cancel ( ) ; if ( mayStillRun ) { synchronized ( this ) { statementContext = null ; } } } The mayStillRun = ! cancel ( ) is somewhat confusing . I can t see from the javadoc of TimerTask.cancel ( ) how its return value could indicate the task may still run . Less confusing code could be : public void forgetContext ( ) { synchronized ( this ) { statementContext = null ; } cancel ( ) ; } 
The asynchronous replication functionality writes information to the derby log . It would be good to improve this in the following ways : 1 : startSlave and stopSlave stack traces are written twice to the log - one is obviously enough 2 : It should be possible to configure if replication messages written to the log should be followed by a stack trace of the cause . 3 : logged messages should have a timestamp 
CompilerContext saves the isolation level of the LanguageConnectionContext ( LCC ) when it is pushed and restores it when it is popped . However there is no mechanism for the isolation level to change during compilation . Isolation level can only be changed by a Connection.setTransactionIsolation ( ) or the SET ISOLATION SQL statement . 
Derby doesn t support the entire range of valid ( finite ) values of the Java primitive types double/float in columns with SQL type DOUBLE or REAL . This appears to be a limitation that was introduced for compatibility with DB2 . There have been some requests on derby-user that we lift this restriction . The restriction is enforced by the methods normalizeREAL ( ) and normalizeDOUBLE ( ) in org.apache.derby.iapi.types.NumberDataType . 
The client driver has code for caching connect bytes , but it is not used . This unused code clutters the rest of the code and makes it harder to understand . Secondly , the variable used to determine if the cached connect bytes can be used is public in am.Connection . If someone has historic information about this feature , it would be nice if they could add the information to this Jira . 
The replication source code is currently in package org.apache.derby . { impl|iapi } .services.replication , but the services package is intended for low-level self-contained modules . Since the replication code is highly integrated with the store modules , the replication code should be moved to org.apache.derby . { impl|iapi } .store.replication . See discussion in mail-thread : 
Stop running jdbc40 tests in the old test framework Tests in the old jdbc40 test suite are run in the derbyall test suite even if all the tests have been converted to Junit and is run as part of the Junit All test suite . Rewrite derbynet/SuicideOfStreaming to a JUnit test The test derbynet/SuicideOfStreaming should be rewritten to a JUnit test more in line with our newly created test system . It is one of the last tests still being run from the deprecated tests/junitTests/ directory , through a wrapper class . Investigate if largeData/LobLimits.java can be run for client When investigating , I noticed that the largeData suite does not run for client . The comment in suites/derbyNetClient.exclude says : excluding TestErrorStreamTarget.java since it s not relevant for clients excluding largedata/LobLimits.java to run with the network server because curr ently lobs are materialized and this test tests for 2G lobs . see and issues # largedata/LobLimits.java Perhaps now that these issues are fixed the test can be run in client . Perhaps it should be a separate suite as just the embedded test takes over 10 hours to run . Convert store/streamingColumn to JUnit Convert org.apache.derbyTesting.functionTests.tests.store.streamingColumn to a JUnit testcase . Convert derbynet/OutBufferedStream to JUnit 
One side effect is that the toString ( ) of such a warning would change format from starting with SQLWarning : to java.sql.SQLWarning : , which should match any warnings created on the network client side . See comments in as well for possible correct package of SQLWarningFactory . Note : generateCsSQLWarning ( ) can be removed as it doesn t seem to be used . Note : Also SQLWarningFactory would not extend java.sql.SQLWarning . 
With the additional system permissions proposed in I wonder if it makes sense to change the style of names & amp ; actions in SystemPermission . Today a shutdown name is proposed and potential for future shutdownEngine and shutdownServer with no actions . is proposing names of jmxControl , serverControl , engineControl etc also with no actions . Looking at the standard Permission class it seems the name is meant to represent an object that the permission applies to and action represent actions on that object . Thus it would seem to make more sense and be consistent with other Permissions to have : name=server action=control | monitor | shutdown name=engine action=control | monitor | shutdown name=jmx action=control 
In Dan suggested ... Setting up a NormalizeResultSetNode is spread over three locations , the class itself ( very little , it s almost acting like a C struct ) , the genNormalizeResultSetNode method and then copyLengthsAndTypesToSource . A good O-O implementation would have the logic to create a NormalizeResultSetNode self-contained in NormalizeResultSetNode . Since the ResultColumnList of the original ResultSetNode correctly describes the desired outcome , it s not clear to me why NormalizeResultSetNode can t just refer to the same list and use it for its processing . They may be some chance that this would cause recursion at some point , where a NormalizeResultSetNode would think it needed to be wrapped in a NormalizeResultSetNode since the types of its columns and expression don t match ( i.e . when it is handled as a regular ResultSetNode ) . I think moving the setup of a NormalizeResultSetNode into the class itself , so that its inputs are just the ResultSetNode to wrap would help clear up the code , especially if comments were added indicating why certain actions were being taken . I am separating this task out into a separate issue , so that it can be worked on independently of . 
Jdk 1.5 introduced Thread.getAllStackTraces ( ) which can be used to print a thread dump programatically . The test stress.multi kills off its testers if it reaches a deadlock or the testers can t complete on their own . It would be helpful in this case to get a thread dump automatically . The code could only be enabled for jdk 1.5 and higher . 
The DrdaStreamOutBufferSize attribute of org.apache.derby.mbeans.drda.NetworkServerMBean is currently of type String . The value of this attribute describes the size of the buffer for streaming blob/clob from server to client ( derby.drda.streamOutBufferSize property ) , and is handled as an integer ( int ) in the DDMWriter class of the Network Server . The type was changed to int in the functional specification of & # 91 ; 1 & # 93 ; , since no reason was found for this to be a String . Similar attributes , such as DrdaMaxThreads , have return type int . It may useful for a JMX client to know that the value should be an int instead of just any arbitrary string ; hence this change . & # 91 ; 1 & # 93 ; : 
Currently the network server has support for the following security mechanisms 1 ) USRIDONL ( userid only ) , 2 ) USRIDPWD ( clear text userid and password ) , 3 ) EUSRIDPWD ( encrypted userid and password ) . Thus the # 3 encrypted userid and password security mechanism is secure with respect to the userid/password sent across the wire . Currently there is no way to setup the network server to ensure that it accepts connections coming in at a certain security mechanism . It seems reasonable & amp ; useful to have a server want to accept connections from clients with a particular security mechanism ( e.g lets say encrypted userid/password and reject usridpwd ie clear text userid and password ) This jira will add support for this by adding a property to enable the server to be able to accept connections from clients with a certain security mechanism . -- -- -- -- -- -- -- -- -- -- I actually couldnt find if a rank was given to the security mechanisms in the drda spec . If it were so , then maybe a property for setting the minimum security mechanism accepted by the server would be appropriate . 
Army attached great write-ups to and explaining how NormalizeResultSetNode works . These should be incorporated into NormalizeResultSetNode javadoc 
It would be good to have a stack traces for all threads dump to the derby.log when an assertion occurs with JVM s that support it . 
Knut Anders raised the issue of whether we still need this jar file : I have built Derby without this jar file and run the regression tests under Java 5 and jdk1.4 : they passed cleanly for me in both environments . I believe we can remove this jar file , which was added in revision 395643 in order to compile the xml support under jdk1.3 -- we no longer support that platform . 
It would be good to simplify the Derby build so that the whole product could be built out-of-the-box just from what s checked into the Derby repository . As a step toward this goal , it would be good to be able to build the jsr169 support without having to download proprietary libraries . 
Network Server console messages should print a time stamp . Methods that need to be changed are NetworkServerControlImpl. , consoleMessage , onsolePropertyMessage and consoleExceptionPrintTrace 
The API specification for TableFunctions can not provide information to the implementer of the TableFunction about the details of the query . For example : ( a ) I defined a table function named MyFunction with columns a , b , & amp ; c ( b ) I bind the table function properly using the CREATE FUNCTION SQL . User executes the following SQL : select a , b from table ( MyFunction ( ) ) where c = 123 Without passing the column list and/or where clause as arguments to the table function , my implementation can not know that it only needs two of the three columns , and only rows where c = 123 . For TableFunctions that are built to integrate distant/legacy data , the cost of the query can be prohibitive . It would be better if information regarding the columns in the select and restrictions from the where clause could be passed to the developer . 
A likely cause of this error is some sort of page level corruption which has corrupted the format id field of the page itself . Some more information about the data on the page may help diagnose what is going on - for instance a page dump may give some insight . ERROR XSDB1 : Unknown page format at page Page ( 122 , Container ( 0 , 1248 ) ) at ae.sphere.arena.networkManager.db.dao.SyslogDAO.getAllSyslogSortedByDate ( SyslogDAO.java:40 ) at ae.sphere.arena.networkManager.server.syslog.dataPurging.SyslogHistoryPurgerManager.purge ( SyslogHistoryPurgerManager.java:35 ) at ae.sphere.arena.networkManager.db.DbPurger $ DataPurgerJob.run ( DbPurger.java:41 ) Caused by : ERROR XBM0U : No class was registered for identifier 23364 . ============= begin nested exception , level ( 1 ) =========== ERROR XBM0U : No class was registered for identifier 23364 . at ae.sphere.arena.networkManager.db.dao.SyslogDAO.getAllSyslogSortedByDate ( SyslogDAO.java:40 ) at ae.sphere.arena.networkManager.server.syslog.dataPurging.SyslogHistoryPurgerManager.purge ( SyslogHistoryPurgerManager.java:35 ) at ae.sphere.arena.networkManager.db.DbPurger $ DataPurgerJob.run ( DbPurger.java:41 ) ============= end nested exception , level ( 1 ) =========== 
In this thread on derby-dev , , it was mentioned that RAFContainer4 calls padFile ( ) when creating a container . Since padFile ( ) uses old I/O calls and the rest of RAFContainer4 uses NIO , it could possibly cause similar issues as those seen in . Although we haven t verified that this is a problem , we should try to avoid mixing old I/O and NIO to be on the safe side . 
Improve the text associated with ERROR XSDB3 : Container information can not change once written : was 103 , now 80 . It would be nice to know which database and container at least , and possibly any other info that may have changed . Maybe page/hex dumps to track down overlapping I/O errors on page 0 . Here is a stack trace reported as part of , as encountered by a back ground thread : 2008-06-12 08:53:12.421 GMT Thread & # 91 ; derby.rawStoreDaemon,5 , derby.daemons & # 93 ; Cleanup action starting^M ERROR XSDB3 : Container information can not change once written : was 103 , now 80^M 
I was creating a test database on an external USB drive formatted as FAT32- it contains some tables that have quite large binary objects in : This was in conjunction with Hibernate . I got this rather cryptic error message . Looks rather scary : 18:02:37,550 WARN JDBCExceptionReporter:77 - SQL Error : 40000 , SQLState : 08006 18:02:37,550 ERROR JDBCExceptionReporter:78 - A network protocol error was encountered and the connection has been terminated : the requested command encountered an unarchitected and implementation-specific condition for which there was no architected message 18:02:37,597 ERROR AbstractFlushingEventListener:301 - Could not synchronize database state with session org.hibernate.exception.JDBCConnectionException : could not insert : & # 91 ; proteinChainMoleculeBinaryData & # 93 ; java:2263 ) Initially it didnt even occur to me that this may be due to me using a FAT32 drive , but eventually I figured out that the table s file had got to the operating FAT32 limit : I had a file of 4,194,272 KB . In the derby log , there s a more revealing , but still incorrect , error message : ERROR XSDG1 : Page Page ( 131071 , Container ( 0 , 2384 ) ) could not be written to disk , please check if disk is full . Caused by : java.io.IOException : There is not enough space on the disk at sun.nio.ch.FileDispatcher.pwrite0 ( Native Method ) at sun.nio.ch.FileDispatcher.pwrite ( FileDispatcher.java:51 ) at sun.nio.ch.IOUtil.writeFromNativeBuffer ( IOUtil.java:100 ) at sun.nio.ch.IOUtil.write ( IOUtil.java:75 ) at sun.nio.ch.FileChannelImpl.write ( FileChannelImpl.java:651 ) The error is still strictly speaking incorrect - my disk is far from full , but I have created a file too big for the disk type - but the error is at least closer to the truth and this would be useful information for the derby client to display rather than the rather scary looking message I was getting . 
Currently the SQL length function materializes the entire lob into memory . In SQLBinary.getLength ( ) we have public final int getLength ( ) throws StandardException { if ( stream ! = null ) { if ( streamValueLength ! = -1 ) return streamValueLength ; } return ( getBytes ( ) == null ) ? 0 : getBytes ( ) .length ; } Which actually is doubly bad because we call getBytes twice and materialize it twice . It would be good to read the length from the stream if available and otherwise stream the value to get the length , rather than materializing it into memory . To reproduce , run the attached repro . java -Xmx16M LengthLargeLob It gives an out of memory exception Caused by : java.lang.OutOfMemoryError : Java heap space 
Adding new replication state tests : testReplication_Local_3_p1_StateNegativeTests testReplication_Local_3_p2_StateTests_bigInsert_immediateStopMaster testReplication_Local_3_p2_StateTests_smallInsert_immediateStopMaster_DISABLED ( Due to ) testReplication_Local_3_p2_StateTests_bigInsert_sleepBeforeStopMaster testReplication_Local_3_p2_StateTests_smallInsert_sleepBeforeStopMaster testReplication_Local_3_p3_StateNegativeTests testReplication_Local_3_p4_StateNegativeTests plus refactoring and cleanup . 
The class org.apache.derby.iapi.services.io.CompressedNumber contains a significant amount of test code . This code should be moved into a unit test to reduce the footprint of the product jars and to ensure that the code is tested in the regression tests . The test code is located in the methods checkInt ( ) , checkLong ( ) and main ( ) . 
The docs say that REVOKE EXECUTE ... RESTRICT should fail if there is a dependent constraint : The RESTRICT clause specifies that the EXECUTE privilege can not be revoked if the specified routine is used in a view , trigger , or constraint , and the privilege is being revoked from the owner of the view , trigger , or constraint . Revoking the privilege will be correctly restricted , but possibly for the wrong reason . 
The network server needs SocketPermission listen on the port that it listens to , but this permission is not granted by the basic server policy that s installed by default . This doesn t cause any problems in most cases , since the JVM s default policy grants all code bases SocketPermission listen on a range of ports , and Derby s network server port is within that range . Still , the network server should not rely on this fact . It is possible to run the network server on any port , not only those ports that happen be in the range that s given carte blanche by the platform s default policy . The network server will however not be able to run on those ports with the basic policy currently , only with a custom policy or with the security manager disabled . The default policy should make this permission explicit . 
EmbedBlob.length uses read to process the whole Blob when the length has not been encoded at the head of the stream . Using skip instead of read can lead to better performance . I also plan to make some minor cleanups under this issue ; JavaDoc and to rename a variable . 
Derby has a max length for VARBINARY and VARCHAR , which is 32 672 bytes or characters ( see Limits.DB2_VARCHAR_MAXWIDTH ) . When working with LOBs represented by locators , using a read buffer larger than the max value causes the server to process far more data than necessary . Say the read buffer is 33 000 bytes , and these bytes are requested by the client . This requests ends up in LOBStoredProcedure.BLOBGETBYTES . Assume the stream position is 64 000 , and this is where we want to read from . The following happens : a ) BLOBGETBYTES instructs EmbedBlob to read 33 000 bytes , advancing the stream position to 97 000 . b ) Derby fetches/receives the 33 000 bytes , but can only send 32 672 . The rest of the data ( 328 bytes ) is discarded . c ) The client receives the 32 672 bytes , recalculates the position and length arguments and sends another request . d ) BLOBGETBYTES ( locator , 96672 , 328 ) is executed . EmbedBlob detects that the stream position has advanced too far , so it resets the stream to position zero and skips/reads until position 96 672 has been reached . e ) The remaining 328 bytes are sent to the client . This issue deals with points b ) and d ) , by avoiding the need to reset the stream . Points a ) and e ) are also problematic if a large number of bytes are going to be read , say hundreds of megabytes , but that s another issue . It is unfortunate that using 32 K ( 32 * 1024 ) as the buffer size is almost the worst case ; 32 768 - 32 672 = 96 bytes . 
The contract of InputStream.skip is somewhat difficult , some would even say broken . See ) ) A utility class should be created to ensure that we use the same skip procedure throughout the Derby code base . Suggested functionality : long skipFully ( InputStream ) : skips until EOF , returns number of bytes skipped void skipFully ( InputStream , long ) : skips requested number of bytes , throws EOFException if there is too few bytes in the stream I know of two different approaches , both skipping in a loop : a ) Verify EOF with a read call when skip returns zero . b ) Throw EOFException if skip returns zero before requested number of bytes have been skipped . There s related code in iapi.util.UTF8Util . Maybe this class , say StreamUtil , could be put in the same package ? 
Currently if I have a table that contains clob/blob column , import/export operations on that table throghs unsupported feature exception . set schema iep ; set schema iep ; create table ntype ( a int , ct CLOB ( 1024 ) ) ; create table ntype1 ( bt BLOB ( 1024 ) , a int ) ; call SYSCS_UTIL.SYSCS_EXPORT_TABLE ( iep , ntype , extinout/ntype.dat , null , null , null ) ; ERROR XIE0B : Column CT in the table is of type CLOB , it is not supported by th e import/export feature . 
LOBStreamControl throws three types of exceptions : IOException , SQLException and StandardException . All the SQLException are generated/thrown from the code in LOBStreamControl . At this level of the code , SQLException should not be thrown , as it is more tedious to handle both SQLException and StandardException at higher levels . I propose to replace SQLException with StandardException in LOBStreamControl . The purpose of this change is consistency , and also results it slightly less code at higher levels . 
provided a manual way to update the statisitcs . There was some discussion in that jira entry for possibly optimizing the cases where there is no need to update the statistics . I will enter the related comments from that jira entry here for reference . * * * * * * * * * * * * * * * * * * * * * * * * * * Knut Anders Hatlen - 18/Jul/08 12:39 AM If I have understood correctly , unique indexes always have up to date cardinality statistics because cardinality == row count . If that s the case , one possible optimization is to skip the unique indexes when SYSCS_UPDATE_STATISTICS is called . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Mike Matrigali - 18/Jul/08 09:48 AM is the cardinality of a unique index 1 or is it row count ? It is also more complicated than just skipping unique indexes , it depends on the number of columns in the index because in a multi-column index , multiple cardinalities are calculated . So for instance on an index on columns A , B , C there are actually 3 cardinalities calculated : A A , B A , B , C I agree that the calculation of cardinality of A , B , C could/should be short circuited for a unique index . * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * Knut Anders Hatlen - 18/Jul/08 03:25 PM Mike , It looks to me as if the cardinality is the number of unique values , so I think the cardinality of a unique index is equal to its row count ( for the full key , that is ) . You re right that we can t short circuit it if we have a multi-column index . I don t know if it s worth the extra complexity to short circuit the A , B , C case , since we d have to scan the entire index anyway . For a single-column unique index it sounds like a good idea , though . * * * * * * * * * * * * * * * * * * * * * * * * * * 
The method InternalClob.getBytePosition is only used in one internal Clob implementation , and can be removed to reduce complexity and code volume . The method InternalClob.getByteLength can be removed . 
After the fix for and was committed , these code clean-ups could be performed : remove workaround for in the tests ( search for the string to find them ) remove logic to prevent waiting for table locks in SPSDescriptor.updateSYSSTATEMENTS ( ) since the new mechanism will prevent waiting for all types of locks 
I have sometimes seen in a profiler that an unreasonably high amount of the CPU time is spent in GenericLanguageConnectionContext.lookupCursorActivation ( ) when the network server is running . That method is used to check that there is no active statement in the current transaction with the same cursor name as the statement currently being executed , and it is normally only used if the executing statement has a cursor name . None of the client-side statements had a cursor name when I saw this . The method is always called when the network server executes a statement because the network server assigns a cursor name to each statement even if no cursor name has been set on the client side . If the list of open statements is short , the method is relatively cheap . If one uses ClientConnectionPoolDataSource with the JDBC statement cache , the list of open statements can however be quite long , and lookupCursorActivation ( ) needs to spend a fair amount of time iterating over the list and comparing strings . The time spent looking for duplicate names in lookupCursorActivation ( ) is actually wasted time when it is called from the network server , since the network server assigns unique names to the statements it executes , even when there are duplicate names on the client . It would be good if we could reduce the cost of this operation , or perhaps eliminate it completely when the client doesn t use cursor names . 
Rick suggested it would be nice to have something to allow inspection of enabled roles , cf the ENABLED_ROLES view of the information schema , SQL 2003 , vol 11 , section 5.29 . I enclose patch suggestion for doing this via a new VTI table and syntactic sugar in ij : SYSCS_DIAG.ENABLED_ROLES and and ij command show enabled_roles . Not for commit at this point , we may want to handle to general question of information schemata in another way . What say thee ? 
ALTER TABLE .. ADD COLUMN can not be used to add an identity column . There is code to handle identity columns , but it is disabled in the parser . See this thread on derby-user : The code was disabled for DB2 compatibility . Since DB2 compatibility is not a goal for Derby , we should see if we could re-enable it . 
If a unique constraint is violated by an insert statement , then Derby throws an SQLIntegrityConstraintViolationException . The error message contains , in particular , the constraint name and the table name . To distinguish between cases with various constraints , Derby should instead throw a subclass of SQLIntegrityConstraintViolationException , with methods like getConstraintName ( ) , and getTableName ( ) . See also . 
If slave replication mode is started on an encrypted database , derby fails with a NPE and then hangs . The reason for the hang is that LogToFile # initializeSlaveReplicationMode needs to scan the log to find the end . For encrypted databases , this scan uses RawStoreFactory # decrypt . At this stage , LTF # rawStoreFactory variable has not been set . A solution may be to set this variable in LTF before scanning the log . 
The store should save useful length information for Clobs . This allows the length to be found without decoding the whole data stream . The following thread raised the issue on what information to store , and also contains some background information : The information to store , and the exact format of it , is still to be discussed/determined . Currently two bytes are set aside for length information , which is inadequate . 
SQLBinary.hashCode ( ) and SQLChar.hashCode ( ) use a very simple algorithm that just takes the sum of the values in the array . This gives a poor distribution of hash values because similar values will have a higher probability of mapping to the same hash code , and the higher bits won t be used unless the array is very long . We should change these methods so that they use an algorithm similar to the one used in java.lang.String.hashCode ( ) , described here : & lt ; URL : ) & gt ; . This may have a positive effect on the performance of hash scans as it will reduce the likelihood of collisions in the hash table . 
Inserting from a table function is a lot like importing from a file : 1 ) Derby has limited visibility into the size of the external data source . 2 ) The user is often trying to import a large data set . The import procedures assume that Derby should always apply the bulk-insert optimization when importing from a file . The same assumption seems reasonable whenever a table function appears in the source stream of an INSERT . 
TemporaryClob doesn t save the known length of the Clob in all situations . The following places in the code should be improved ( some easier than others ) : a ) TemporaryClob ( String , ConChild ) b ) copyClobContent ( InternalClob , long ) ( non-static ) c ) copyClobContent ( InternalClob ) ( non-static ) There might be additional places to fix too . 
In order to bundle Derby with popular linux distributions like Debian and Ubuntu , it must be possible to build Derby from sources and existing linux components . By replacing Felix jar with the corresponding sources , we move another step closer to being able to bundle Derby with linux distributions . 
provided a new stored procedure SYSCS_UTIL.SYSCS_UPDATE_STATISTICS to update the statistics . The code to do the work was added in AlterTableConstantAction and the name of new method in that class is updateStatistics ( Activation activation ) . This code was copied from an existing unused class execute/UpdateStatisticsConstantAction.java ( that unused class has been removed from Derby code ) . The copied code in AlterTableConstantAction : updateStatistics gets compile transaction as shown below to do the work of updating the statistics . TransactionController tc = lcc.getTransactionCompile ( ) ; The javadoc for LangaugeConnectionContext for getTransactionCompile says following / * * Get the transaction controller to use with this language connection context at compile time . * / TransactionController getTransactionCompile ( ) ; I think we should be using TransactionController tc = lcc.getTransactionExecute ( ) to do the work since we are not in the compile phase . The rest of the code in AlterTableConstantAction uses lcc.getTransactionExecute to get the transaction controller . I will make the changes in AlterTableConstantAction.updateStatistics to use getTransactionExecute and will run the existing tests to make sure nothing breaks . I ran into this issue of getTransactionExecute vs getTransactionCompile while working on ( provide a zero-admin way of updating the statistics ) . While trying to run the statistics inline duirn the compile phase of a SELECT query , I ran into exception that statistics can t be updated because we are in read-only mode . But if I change the AlterTableConstantAction to use getTransactionExecute , then I do not run into the read-only mode exception . Please let me know if anyone has any comments on this . I will run the existing tests and if no failures , then I will go ahead and commit the change of using getTransactionExecute rather than getTransactionCompile in AlterTableConstantAction : updateStatistics 
Derby objects if you try to declare a function or procedure with an argument whose type is BLOB or CLOB . These kinds of arguments are allowed in the SQL standard and the matching Java types are java.sql.Blob and java.sql.Clob . See the SignatureMatching.html summary attached to . We should lift this restriction and allow functions and procedures to take large object arguments . 
SQL 2008 has added new syntax to support a direct way to limit the returned set of rows in a result set . This allows an application to retrieve only some rows of an otherwise larger result set , similar to the popular LIMIT clauses use in some databases . Up till now , in Derby ( and SQL ) we have had to use the ROW_NUMBER ( ) function in a nested subquery to achieve the effect of the & lt ; fetch first clause & gt ; , cf . , a method which is rather more indirect and still not efficient ( ) , and primarily intended for OLAP functionality , perhaps . There has been no direct way to achieve the effect of the & lt ; result offset clause & gt ; via SQL . Syntax ( cf . SQL 2008 , section 7.13 ) : & lt ; result offset clause & gt ; : := OFFSET & lt ; n & gt ; { ROW | ROWS } & lt ; fetch first clause & gt ; : := FETCH { FIRST | NEXT } & # 91 ; & lt ; n & gt ; & # 93 ; { ROW | ROWS } ONLY where & lt ; n & gt ; is an integer . The two clauses syntactically follow the ORDER BY clause in the grammar . Note that both ORDER BY and the new clauses above are allowed also in subqueries in the new version of the SQL standard ( section 7.13 ) . I only propose to include this at the top level in DERBY for now . ( ORDER BY is presently also not allowed in subqueries in Derby since SQL didn t allow for this until SQL 2008 either ) . 
After the change for the connection toString ( ) output is an integer which correspond to SESSIONID . The output should identify the type and also the meaning of the identifier that it prints . Perhaps a format that appends the default toString output with the sessionid information as it prints in the derby.log would be more informative . org.apache.derby.impl.jdbc.EmbedConnection @ efd552 ( SESSONID = 2 ) Ultimately this could be expanded to included other diagnostic information e.g org.apache.derby.impl.jdbc.EmbedConnection @ efd552 ( XID = 132 ) , ( SESSIONID = 5 ) , ( DATABASE = wombat ) , ( DRDAID = NF000001.H324-940125304405039114 { 7 } ) 
The javadoc for BTreeLockingPolicy still talks about scan locks , although the scan lock is no longer in use after . The javadoc should be updated so that it reflects the current state of the code . 
Network server currently only reports the start and stop of network server to the console output and does not print an associated timestamp . It would be helpful if the start and stop times of network server were recorded in the derby.log , so there would not be a need to check t the console output . 
Currently , the system properties aren t being used by the runScript ( ) method on the org.apache.derby.tools.ij This raises an issue with allowing the port used for the JUnit tests to be customized , therefore blocking . The idea , was to allow properties such as ij.protocol to be set either as system properties , or in a Properties object , then passed to the runScript ( ) method . Please share any thoughts and ideas as to which may be the best approach . 
When a store stream Clob is going to be modified , it will be written out to the temporary area of Derby and represented as a TemporaryClob . The transfer of the data is done in a sub-optimal manner for two reasons ; o for transfer of the complete Clob , the copy method operates on the byte level and we re not able to save the character length . o for transfer of parts of the Clob ( i.e . truncation ) , we have to first decode the UTF-8 encoding to find the byte count and then transfer the same bytes . I intend to do the following two changes ; 1 ) Add a getCharLengthIfKnow-method to InternalClob . 2 ) Add a UTF-8 aware copy method to LOBStreamControl . When a complete Clob is to be copied , code like this will be executed ; cachedCharLength = internalClob.getLengthIfKnown ( ) ; if ( cachedCharLength & gt ; 0 ) // use existing byte-oriented copy method for best performance ( copy until EOF ) else cachedCharLength = control.copyUTF8Data ( ) When parts of a Clob is to be copied , we always use the UTF-8 aware copy method , but we also do a cheap range check . cachedCharLength = internalClob.getLengthIfKnown ( ) ; if ( cachedCharLength & gt ; 0 & amp ; & amp ; requestedLength & gt ; cachedCharLength ) throw EOFException ( ) ; if ( cachedCharLength == requestedLength ) // use existing byte-oriented copy method for best performance ( copy until EOF ) else cachedCharLength = control.copyUTF8Data ( requestedLength ) ; Adding the UTF-8 aware copy method was started under , including comments on the first revision of a patch . 
There are quite a few instances of public static fields which appear to be intended to be constant , but which are not final . The code would be safer if all the constant fields were made final . Patch to follow for some of the problem fields . The following also ought to be fixed : org.apache.derby.iapi.services.property.PropertyUtil.servicePropertyList ( e.g . use an accessor to lookup the values ) org.apache.derby.iapi.types.JSQLType.primitiveNames ( ditto ) 
While working on , found that an exception from Connection.close was not handled properly by the server shutdown code which caused a new instance server startup to hang . Resolved the problem with Connection close but in general , we should 1 ) Make sure an exception during shutdown processing does not prevent the remaining shutdown tasks , like closing the server socket from occurring . 2 ) Make sure any exceptions that occur in shutdown processing are reported to the console . 
The SQL standard defines two ways to specify the join condition in an INNER JOIN or a ( LEFT/RIGHT/FULL ) OUTER JOIN : with an ON clause or with a USING clause . Derby currently only accepts joins with an ON clause . Internally , Derby has code that supports USING . This code should be enabled to ease the migration to Derby . We must also verify that the implementation adheres to the standard before we enable it . Since USING is already a reserved keyword in Derby s parser , enabling the USING syntax should not cause any compatibility issues for existing queries . 
SOME is not allowed in ON-clause : ij & gt ; create table t1 ( i integer ) ; 0 rows inserted/updated/deleted ij & gt ; create table t2 ( i integer ) ; 0 rows inserted/updated/deleted ij & gt ; create table t3 ( i integer ) ; 0 rows inserted/updated/deleted ij & gt ; insert into t1 values ( 1 ) ; 1 row inserted/updated/deleted ij & gt ; insert into t2 values ( 2 ) ; 1 row inserted/updated/deleted ij & gt ; insert into t3 values 2,3,4 ; 3 rows inserted/updated/deleted ij & gt ; select * from t1 where t1.i = some ( select i from t3 ) ; I -- -- -- -- -- - 0 rows selected ij & gt ; select * from t1 inner join t2 on t1.i = some ( select i from t3 ) ; ERROR 42972 : An ON clause associated with a JOIN operator is not valid . ij & gt ; 
SQL 2008 allows ORDER BY to be specified in subqueries . In conjunction with OFFSET/FETCH and/or ROW_NUMBER meaningful subqueries with row ordering may be formulated . Cf . MySQL s LIMIT may be used in subqueries as well . Note that OFFSET/FETCH is currently not allowed in subqueries , either . 
SQL 2008 specifies that OFFSET/FETCH ( & lt ; result offset clause & gt ; , & lt ; fetch first clause & gt ; ) may be used in subqueries . Currently , Derby only allows this at the outer ( cursor ) level . Cf . also on allowing ORDER BY in subqueries for this extension to be portable and useful . SQL features : F856 Nested & lt ; fetch first clause & gt ; in & lt ; query expression & gt ; F857 Top-level & lt ; fetch first clause & gt ; in & lt ; query expression & gt ; ( already present in 10.5 ) F858 & lt ; fetch first clause & gt ; in subqueries F859 Top-level & lt ; fetch first clause & gt ; in views F860 dynamic & lt ; fetch first row count & gt ; in & lt ; fetch first clause & gt ; ( already present in 10.5 ) F861 Top-level & lt ; result offset clause & gt ; in & lt ; query expression & gt ; ( already present in 10.5 ) F862 & lt ; result offset clause & gt ; in subqueries F863 Nested & lt ; result offset clause & gt ; in & lt ; query expression & gt ; F864 Top-level & lt ; result offset clause & gt ; in views F865 dynamic & lt ; offset row count & gt ; in & lt ; result offset clause & gt ; ( already present in 10.5 ) 
If Network Server is started with a PrintWriter specified for console output it will not automatically flush output such as starting the server . This can be confusing as the console output shows no activity . Users currently need to specify the PrintWriter to autoflush e.g . starterWriter = new PrintWriter ( new FileOutputStream ( new File ( SERVER_START_LOG ) ) , true ) ; derbyServer = new NetworkServerControl ( ) ; derbyServer.start ( starterWriter ) ; For repro see : And change the following line in the program to not autoflush as follows : starterWriter = new PrintWriter ( new FileOutputStream ( new File ( SERVER_START_LOG ) ) , false ) ; 
The number of parameters to a Derby routine is arbitrarily limited to 90 . This limit is causing difficulties for users ( see ) . It ought to be safe to lift this limit . Changes need to be made to CreateAliasNode and to the corresponding dbManagerLimits.testMostParametersInStoredProcedures ( ) test . 
It would be nice to be able to tell the compiler to use user-written formatters for pretty-printing Derby s abstract syntax trees . For instance , it would be nice to be able to plug in the AST printers attached to , including the Eclipse-based tree grapher written by Kay Rpke . 
In the lack of the boolean data type , Derby forces you to use expressions like 1=1 and 1 & lt ; & gt ; 1 to express true and false . Generated SQL statements also tend to use such expressions , and so does Derby in its own meta-data queries . Derby has many useful optimizations for boolean true/false . For instance , ProjectRestrictNode and PredicateList are able to eliminate predicates , and in some cases the entire ProjectRestrictNode , if the predicate contains constant true or false values . However , during parsing and compilation , expressions like 1=1 are not rewritten to TRUE , and we don t get any benefit from the boolean optimization code . This leads to more complex , and possibly less efficient , byte code being generated for the statements . Also , boolean constants are assigned a selectivity of 0.0 ( false ) or 1.0 ( true ) , since they will always match no rows when false and all rows when true . The expression 1=1 does however get it s selectivity from the = operator , which means that it ll be 0.1 . The same selectivity is assigned to 1=0 . Other operators have different selectivity , so 2 & lt ; 3 has the selectivity 0.33 , even though the actual selectivity of the expression is the same as 1=1 and TRUE , namely 1.0 . This leads to oddities like the optimizer choosing a different plan when you change 2 & lt ; 3 to 1=1 in a WHERE clause . See for an example of that . If we could go through the query tree and replace occurrences of comparisons between constant values with a boolean constant at bind time , such queries would end up with simpler byte code , and the selectivity passed to the optimizer would be more accurate , possibly resulting in a better plan being chosen . 
Currently , QueryTreeNode.accept ( ) walks the tree top-down , always calling visit ( ) on the parent before it calls visit ( ) on the children . Although this is fine in most cases , there are use cases where visiting the nodes bottom-up would be better . One example is mentioned in . The visitor posted there looks for binary comparison operators and checks whether both operands are constants . If they are , the operator is replaced with a boolean constant . Take this expression as an example : ( 1 & lt ; 2 ) = ( 2 & gt ; 1 ) The query tree looks like this : = / \ / \ & lt ; & gt ; / \ / \ / \ / \ 1 2 2 1 If we walk the tree top-down with the said visitor , the = node doesn t have constant operands when it s visited . The & lt ; and & gt ; operators do have constant operands , and they re both replaced with constant TRUE . This means the expression ( 1 & lt ; 2 ) = ( 2 & gt ; 1 ) is rewritten to TRUE=TRUE , and that s how far the transformation goes . If the tree had been processed bottom-up , we would start with the & lt ; and & gt ; operators , and again replace them with TRUE . The query tree would therefore have been transformed to this intermediate form when the = operator was visited : = / \ / \ TRUE TRUE This is the same as the end result when visiting top-down , but now the = operator hasn t been visited yet . Since both the operands of the = operator are constants , the visitor will perform yet another transformation so the tree is simplified further and ends up as : TRUE 
Dyre Tjeldvoll posted some results on that indicated that some types of load might cause contention on XactFactory.tranId ( a shared long ) , and suggested that it was replaced with a java.util.concurrent.atomic.AtomicLong on the platforms that support java.util.concurrent . * . I m splitting this issue out from so that the two possible improvements reported there can be addressed independently . 
The BUILTIN authentication scheme protects the passwords by hashing them with the SHA-1 algorithm . It would be nice to have way to specify a different algorithm so that users can take advantage of new , stronger algorithms provided by their JCE provider if so desired . This issue tracks our response to security vulnerability CVE-2009-4269 , which Marcell Major identified . See 
TransactionTable.add ( ) first checks if the Hashtable trans contains a transaction with the same id as the one being added . If it does , add ( ) does nothing . If there is no such transaction , a new TransactionTableEntry is created and put into the Hashtable . I believe that TransactionTable.add ( ) is never called on a transaction that has already been added to the table . If this is the case , there s no point in checking the Hashtable first . Instead , we could just create a new TransactionTableEntry and add it unconditionally . This would reduce the number of ( synchronized ) Hashtable calls and could improve the performance in scenarios like the one described in . 
The SYSCS_IMPORT_TABLE ( and SYSCS_IMPORT_DATA ) function allow import of data from external resources . In general , they can process CSV files that created with various tools - with one exception : the header line . While there is no accepted standard , most tools will include a header line in the CSV file with column names . This convention is supported in Excel and many other tools . My Request : extend the SYSCS_IMPORT_TABLe and SYSCS_IMPORT_DATA ( and other related procedures ) to include an extra indicator for the number of header lines to be ignored . As an extra bonus it will be help is the SYSCS_IMPORT_DATA will accept column names ( instead of column indexes ) in the COLUMNINDEXES arguments . E.g. , it should be possible to indicate COLUMNINDEXES of 1,3 , sales,5 , . This feature will make it significantly easier to handle cases where the external input files is extended to include additional columns . 
The usage of the method DataValueDescriptor.setValue ( InputStream stream , int length ) is unclear . The intended use seems to be to pass on the known length of an input stream set from the JDBC-layer ( i.e . setBinaryStream ) . There seems to be two distinct cases : the logical length of the stream is known the logical length of the stream is not known Using -1 when the length is not known seems to be an established pattern . 
With the increased use of streams to represent data values , the cloning facilities needs to be improved . Unless I get pushback , I will proceed by producing patches to reach the following goals : move the functionality provided by CloneableObject into DataValueDescriptor ( all classes implementing CloneableObject also implements DataValueDescriptor ) introduce the cloning methods cloneValue , cloneState and cloneHolder ( all in DataValueDescriptor , see description below ) Note that they all return a usable DVD . I m all ears for better names for the clone methods ( another suggestion mentioned is cloneDeep , cloneHalfDeep , and cloneShallow ) . NOTE : See comment below , the method names changed during the course of development . cloneValue & lt ; deep & gt ; ( new method , functionality was present through combined calls to the DVD public interface ) a DVD obtained through cloneValue is independent of other DVDs and the state of the Derby store the data value will be materialized cloneState & lt ; halfDeep & gt ; ( ~= DataValueDescriptor.getClone ) a DVD obtained through cloneState is independent of other DVDs , but may depend on the state of the Derby store ( due to references to store streams ) the data value will be materialized if the value is represented by a non-cloneable stream or if Derby believes materializing the value is more appropriate than keeping the stream representation cloneHolder & lt ; shallow & gt ; ( ~= CloneableObject.cloneObject ) a DVD obtained through cloneHolder is dependent on the original DVD and its clones made through cloneHolder . If one of the DVDs changes its state , all of them will be affected . Will also be dependent on the state of the Derby store if there are references to store streams . the data value will never be materialized due to cloneHolder being invoked For many of the data types , cloneState and cloneHolder will forward to cloneValue . cloneState will be used the most . cloneValue is currently only required in the sorter . cloneHolder is required ( for performance reasons and maybe to avoid OOME ) when DVDs pass through temporary holders ( BackingStoreHashtable , TemporaryRowHolderImpl ) . I have not gone through all the usages of cloneState to see if any of them can be , or has to be , replaced with another clone-call . The ability to clone store streams will be added by Mike s patch attached to . New method names : It turned out that using only two methods was sufficient : cloneHolder & lt ; shallow & gt ; ( as above ) cloneValue ( boolean forceMaterialization ) & lt ; halfDeep or deep & gt ; Basically , cloneValue ( false ) equals cloneState ( ) above , and cloneValue ( true ) equals cloneValue ( ) above . 
In order for the trigger action to have access to before and after values of the triggering table , the CREATE TRIGGER should use the REFERENCING clause . Without the REFERENCING clause , old and new values of triggering table can t be accessed by the trigger action . Based on this , we can improve Derby memory utilization by not keeping old and new values if REFERENCING clause is missing . It will be good to see if the code already does this optimization and if not , then introducing this optimization will definitely be very useful when the triggering table could have LOB columns . 
A pattern in Derby is to use the following piece of code to determine if the data value has a stream or not : if ( dvd.getStream ( ) ! = null ) ... Since the stream has mutable state , obtaining a reference to it just to check if it is not null is sub-optimal for several reasons : it may throw an exception ( data types not supporting streams ) the stream reference is leaked , which is unfortunately if we are / will be required to guarantee something about the stream state ( for instance that the descriptor is in sync with the stream ) . in cases where we have to investigate the state of the stream , we re doing unnecessary work makes it harder to write debug code ( i.e. , is a stream reference obtained from the data value descriptor more than once ? ) I plan to introduce the method DataValueDescriptor.hasStream , returning a boolean . In addition to the obvious check if the stream variable is non-null , it can also be used to instruct Derby to treat certain data values as non-streams even though the underlying value is currently a stream . One example is CHAR and VARCHAR , whose maximum lengths are so small that they should always be materialized to avoid the added complexity coming with streams ( stream state , isolation levels - extra lock to keep stream stable ? , cloning ) . 
Now that has been fixed , some users upgrading to the 1.6 JVM are seeing an error on dual boot where they did not before ( which is of course is a very good thing ) . They do however at that time need to diagnose where the dual boot is coming from which is sometimes harder than one might think with a complex system and legacy code . It would be helpful to have a property that they could set which would print a stack trace on any boot attempt , even if successful , to help diagnose the application error , maybe : derby.stream.error.logBootTrace=true 
I have found a database in my application that prevents startup due to it being corrupted . The driver reports that the database does not exist , even though it does . Then when my app tries to create the database using ; create=true ; on the URL it fails . I think this happened due to the app being killed in Task Manager while it was creating the database . I have the database saved so that you can reproduce the problem . ( I m not sure if I can attach it yet ) 
CoalesceFunctionNode manually iterates over argumentsList ( a ValueNodeList ) in acceptChildren ( ) , preprocess ( ) , remapColumnReferencesToExpressions ( ) and isEquivalent ( ) . ValueNodeList has helper methods for the first three of those , and those helper methods should be used instead to simplify the code . There is no helper method for isEquivalent ( ) , but since very similar code is also found in the isEquivalent ( ) methods of BinaryListOperatorNode and ConditionalNode , we should add a new helper and use it in those classes too . 
Curretnly there is no way to hide data and database structure in embedded derby from the end user . One way to accomplish the above requirement is as follows : 1 . Create encrypted database so data is protected 2 . Enable authentication and sql authorization in database 3 . Create two users , dbUser and dbOwner 4 . Store application logic as stored procedure in the databse so dbUser does not know what tables are accecced by the application logic , thus hiding table structure 5 . Revoke select permission from dbUser so he can not describe tables thus protecting table structures 6 . Give only Execute permissions on stored procedures to dbUser The above steps will ensure that data and data structure is hidden when application is delivered to end user . The problem is , if user does not have select permission , the stored procedures will not execute . So I am requesting the following enhancement to Derby : If dbOwner has given Execure permission to stored procecure to a dbUser , then allow stored procedure to execute even if the dbUser has no select permission . In otherwords , When dbUser calls stored procedure , database will use dbOwners authorization to execute stored procedure rather than dbUsers . This may be implemented by creating new permission called RunAsDbOwner . DbOwner can then grant permission to dbUser to execute a stored procedure with RunAsDbOwner . If this is implemented , applications can be created which will truely hide the database structure and data from end users . Database will behave as a blackbox with only in/out data exposed in stored procedures . 
SQLBinary.compare ( int , DataValueDescriptor , boolean , boolean ) converts the values to strings in order to check whether any of them are null . The isNull ( ) method should be used instead to prevent the unnecessary conversion to strings . See this thread on derby-user : 
When inserting character values Derby converts from Java char to an on-disk encoding of UTF-8 . To to this , the user stream is read and the resulting bytes after conversion are placed in a translation buffer . The default size of the buffer is 32 KB . When inserting a lot of short values , the pressure on the Java garbage collector is unnecessary high and the allocation/GC also causes a somewhat higher CPU usage . This effect of this issue can easily be reduced by sizing the buffer in the appropriate cases . 
OpenConglomerate.lockPositionForWrite ( ) contains this code twice : if ( ! waitForLock ) { // throw lock timeout error . throw StandardException.newException ( SQLState.LOCK_TIMEOUT ) ; } The second occurrence of this code can never end up throwing an exception , since waitForLock is guaranteed to be true there because of the identical check a couple of lines above . ( Judging by the similar method lockPositionForRead ( ) , it is probably the first check that should be removed , so that the latch on the page is released before the exception is thrown . ) Also , the method is always called with forInsert==false , so the forInsert parameter can be removed . ( I also suspect that the method doesn t work correctly if ever used in an insert operation , since it calls latchPage ( RowPosition ) which will unlatch the page if the row isn t found on the page , and I assume that a row that is about to be inserted does not exist yet . ) 
While looking into derby-4679 , I also looked at the query in derby-2526 to validate that my changes also worked for that variant of the problem . During inspection of the query trees after the bind phase , I found one instance in which the pair ( tablenumber , columnnumber ) of a column reference was wrong . Although it did not seem to impact the query result , I note it here as as something we should probably investiate as it may be a symptom an underlying problem , or a potential for future problems . The query looks like this : select b3 . * from b3 join bvw on ( b3.c8 = bvw.c5 ) join b4 on ( bvw.c1 = b4.c7 ) where b4.c4 = 42 and the underlying DDL is this : create table b2 ( c1 int , c2 int , c3 char ( 1 ) , c4 int , c5 int , c6 int ) ; create table b4 ( c7 int , c4 int , c6 int ) ; create table b3 ( c8 int , c9 int , c5 int , c6 int ) ; create table b ( c1 int , c2 int , c3 char ( 1 ) , c4 int , c5 int , c6 int ) ; create view bvw ( c5 , c1 , c2 , c3 , c4 ) as select c5 , c1 , c2 , c3 , c4 from b2 union select c5 , c1 , c2 , c3 , c4 from b ; create view bvw2 ( c1 , c2 , c3 , c4 , c5 ) as After the bind phase , the join clause bvw.c1 = b4.c7 has the following entry for the column reference bvw.C1 : tableNumber : 1 columnNumber : 6 The problem is that the node with tablenumber 1 is bvw , which is the view with the subquery for the union , which has only 5 resulting columns , so 6 must be wrong . Although both the view participant tables ( b , b2 ) both have six column , the view does not . In any case , C1 is column 2 in the view and column 2 in the two union selects from both b and b2 . C1 is however , column 6 of the join node resulting from select b3 . * from b3 join bvw on ( b3.c8 = bvw.c5 ) , but the correct table number for that would be 5 , not 1 . So , it would seem the table number has been bound to the bvw view s result set , but the column number has been bound to the innermost join node s result set . This looks worrying to me . See derby.log attached for the full dump of the query tree after the bind phase . sourceResultSet : org.apache.derby.impl.sql.compile.FromSubquery @ 12789d2 correlation Name : BVW null tableNumber 1 & lt ; -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Note ! level 0 resultSetNumber : 0 referencedTableMap : null statementResultSet : false resultColumns : org.apache.derby.impl.sql.compile.ResultColumnList @ c943d1 indexRow : false orderBySelect : 0 & # 91 ; 0 & # 93 ; : org.apache.derby.impl.sql.compile.ResultColumn @ d3c6a3 * * truncated * * & # 91 ; 1 & # 93 ; : org.apache.derby.impl.sql.compile.ResultColumn @ 18352d8 exposedName : C1 name : C1 tableName : null isDefaultColumn : false wasDefaultColumn : false isNameGenerated : false sourceTableName : B2 type : INTEGER columnDescriptor : null isGenerated : false isGeneratedForUnmatchedColumnInInsert : false isGroupingColumn : false isReferenced : true isRedundant : false virtualColumnId : 2 resultSetNumber : -1 dataTypeServices : INTEGER expression : org.apache.derby.impl.sql.compile.VirtualColumnNode @ b40ec4 dataTypeServices : null sourceColumn : org.apache.derby.impl.sql.compile.ResultColumn @ 1d95da8 * * truncated * * sourceResultSet : org.apache.derby.impl.sql.compile.UnionNode @ 14d7745 * * truncated * * & # 91 ; 2 & # 93 ; : org.apache.derby.impl.sql.compile.ResultColumn @ 13576a2 exposedName : C2 name : C2 tableName : null isDefaultColumn : false wasDefaultColumn : false isNameGenerated : false sourceTableName : B2 type : INTEGER columnDescriptor : null isGenerated : false isGeneratedForUnmatchedColumnInInsert : false isGroupingColumn : false isReferenced : true isRedundant : false virtualColumnId : 3 resultSetNumber : -1 dataTypeServices : INTEGER expression : org.apache.derby.impl.sql.compile.VirtualColumnNode @ ff8c74 dataTypeServices : null sourceColumn : org.apache.derby.impl.sql.compile.ResultColumn @ 61736e * * truncated * * sourceResultSet : org.apache.derby.impl.sql.compile.UnionNode @ 14d7745 * * truncated * * & # 91 ; 3 & # 93 ; : org.apache.derby.impl.sql.compile.ResultColumn @ 15e2ccd exposedName : C3 name : C3 tableName : null isDefaultColumn : false wasDefaultColumn : false isNameGenerated : false sourceTableName : B2 type : CHAR ( 1 ) columnDescriptor : null isGenerated : false isGeneratedForUnmatchedColumnInInsert : false isGroupingColumn : false isReferenced : true isRedundant : false virtualColumnId : 4 resultSetNumber : -1 dataTypeServices : CHAR ( 1 ) expression : org.apache.derby.impl.sql.compile.VirtualColumnNode @ 1cf7491 dataTypeServices : null sourceColumn : org.apache.derby.impl.sql.compile.ResultColumn @ 11946c2 * * truncated * * sourceResultSet : org.apache.derby.impl.sql.compile.UnionNode @ 14d7745 * * truncated * * & # 91 ; 4 & # 93 ; : & lt ; -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - highest column number is 5 ( index is zero-based ) org.apache.derby.impl.sql.compile.ResultColumn @ edf730 exposedName : C4 name : C4 tableName : null isDefaultColumn : false wasDefaultColumn : false isNameGenerated : false sourceTableName : B2 type : INTEGER columnDescriptor : null isGenerated : false isGeneratedForUnmatchedColumnInInsert : false isGroupingColumn : false isReferenced : true isRedundant : false virtualColumnId : 5 resultSetNumber : -1 dataTypeServices : INTEGER expression : org.apache.derby.impl.sql.compile.VirtualColumnNode @ ff94b1 dataTypeServices : null sourceColumn : org.apache.derby.impl.sql.compile.ResultColumn @ 17a4989 * * truncated * * sourceResultSet : org.apache.derby.impl.sql.compile.UnionNode @ 14d7745 * * truncated * * subquery : org.apache.derby.impl.sql.compile.UnionNode @ 14d7745 * * truncated * * 
Add a mechanism for system functions to be easily added . Resolution of functions will check SYSFUN. & lt ; name & gt ; for a function call in SQL when the function is not qualified by a schema . If the current schema does not have a function matching the name , then an additional resolution is made using SYSFUN. & lt ; name & gt ; . Add a table driven mechanism for simple single argument functions ( could be expanded in the future ) . Add these functions / * SYSFUN functions * & # 91 ; 0 & # 93 ; = FUNCTION name * & # 91 ; 1 & # 93 ; = RETURNS type * & # 91 ; 2 & # 93 ; = Java class * & # 91 ; 3 & # 93 ; = method name * & # 91 ; 4 & # 93 ; = parameter type ( single parameter ) * * / private static final String SYSFUN_FUNCTIONS = { { ACOS , DOUBLE , java.lang.Math , acos , DOUBLE } , { ASIN , DOUBLE , java.lang.Math , asin , DOUBLE } , { ATAN , DOUBLE , java.lang.Math , atan , DOUBLE } , { COS , DOUBLE , java.lang.Math , cos , DOUBLE } , { SIN , DOUBLE , java.lang.Math , sin , DOUBLE } , { TAN , DOUBLE , java.lang.Math , tan , DOUBLE } , { DEGREES , DOUBLE , java.lang.Math , toDegrees , DOUBLE } , { RADIANS , DOUBLE , java.lang.Math , toRadians , DOUBLE } , { LN , DOUBLE , java.lang.Math , log , DOUBLE } , { EXP , DOUBLE , java.lang.Math , exp , DOUBLE } , { CEIL , DOUBLE , java.lang.Math , ceil , DOUBLE } , { CEILING , DOUBLE , java.lang.Math , ceil , DOUBLE } , { FLOOR , DOUBLE , java.lang.Math , floor , DOUBLE } , } ; 
While queries of the form & lt ; column & gt ; LIKE & lt ; string-constant & gt ; % are optimized into index scans , the equivalent expression using a concatenation & lt ; column & gt ; LIKE & lt ; string-constant & gt ; || % would result in a table scan . Queries of the form & lt ; column & gt ; LIKE ? are optimizable using an internally generated parameter , so it doesn t seem far-fetched to generate such a parameter for a concatenation of strings ( or other string expressions ) as well , once its value has been calculated . This is of course limited to cases where the result of the string expression can be calculated once , i.e . it is independent of columns in the query . It is sometimes possible to work around this by manually adding the x & gt ; = & lt ; string-constant & gt ; AND x & lt ; & lt ; string-constant & gt ; \uffff ... condition . 
It is possible to have open connections to several databases while running ij , but it is not currently possible to copy data from one DB to an other one . Not only such a feature would allow to copy data between Derby databases . But , ij being mostly DB agnostic , if will ease import/export from any JDBC compliant data source . See 
Currently , whenever the client driver is used , there is a limit of 255 bytes for the database name . This is defined by the DRDA spec and there has been a discussion on the list & # 91 ; 1 & # 93 ; / & # 91 ; 2 & # 93 ; as to whether this limit should be raised due to the introduction of the new ACR that allows for UTF-8 characters . UTF-8 characters can take up to four bytes and this reduces the limit in characters dramatically . This should be an easy change as there is a codepoint that defines this limit . & # 91 ; 1 & # 93 ; did not work but & # 91 ; 2 & # 93 ; did & # 91 ; 1 & # 93 ; - & # 91 ; 2 & # 93 ; http : //apache-database.10148.n7.nabble.com/Database-name-length-td33182.html 
For the preliminary platform testing & # 91 ; 1 & # 93 ; Kathey , Dag and Lily have seen SysinfoCPCheckTest fail on different Windows platforms ( XP and Windows 7 ) when run as part of suitesAll . & # 91 ; 1 & # 93 ; Thanks to Myrna points out that it could relate to . 
Often when a Derby issue occurs , the derby.log is the one thing that does get retrieved , but often gets passed through many layers before it reaches the people supporting Derby . It would be helpful if derby.log also included the following to help instruct customers how to change derby.properties or look again at the log . 1 ) derby.properties location . 2 ) derby.log location or indicate how error log redirection is defined . 
Somewhat simplified , the release manager ( RM ) must currently perform the following manual steps to feed the release note generate the data it needs : a ) Create manual JIRA filter to select issues addressed by the release . b ) Save the filter result to disk as XML . c ) Write/modify the XML parser to be able to parse the report . d ) Determine and record all JIRA release note attachment ids for the issues requiring a release note . By using the current version of the SOAP API ( 3.13.5 ) , steps ( b ) to ( d ) can be removed . 
JVMInfo contains code to switch between using Types.BIT and Types.BOOLEAN based on the JVM version . This was needed when we supported Java 1.3 , since Types.BOOLEAN was introduced in Java 1.4 and so Types.BIT had to be used instead on old platforms . Currently , all supported JDBC versions ( including JSR-169 for small devices ) have Types.BOOLEAN , so the code is no longer needed . 
The current mechanism for deleting in-memory databases isn t good enough , and a proper one must be added . It is also important to be able to delete in-memory databases , since they occupy valuable main memory that should be discarded when the database is no longer needed . I intend to implement the mechanism by using the JDBC connection URL : jdbc : derby : memory : myDatabase ; delete=true & # 91 ; ; user=X ; password=Y & # 93 ; The connection attempt will throw an exception in any case , either because the request failed or because it succeeded . Reasons for a failure can be invalid user and/or password , lacking encryption attributes , or conflicting attributes . For the time being , only the database owner will be allowed to delete databases ( note : do we have a way to control/limit in-memory database creation ? ) 
These warnings I only see if java15compile.classpath points to Java 6 class libraries ( either set explicitly in ant.properties , or implicitly by the property setter ) : & # 91 ; javac & # 93 ; /code/derby/trunk2/java/engine/org/apache/derby/impl/services/jmx/JMXManagementService.java:215 : warning : & # 91 ; unchecked & # 93 ; unchecked conversion & # 91 ; javac & # 93 ; found : java.lang.Class & # 91 ; javac & # 93 ; required : java.lang.Class & lt ; java.lang.Object & gt ; & # 91 ; javac & # 93 ; new StandardMBean ( bean , beanInterface ) { & # 91 ; javac & # 93 ; ^ & # 91 ; javac & # 93 ; /code/derby/trunk2/java/engine/org/apache/derby/impl/services/jmx/JMXManagementService.java:215 : warning : & # 91 ; unchecked & # 93 ; unchecked method invocation : & lt ; T & gt ; StandardMBean ( T , java.lang.Class & lt ; T & gt ; ) in javax.management.StandardMBean is applied to ( java.lang.Object , java.lang.Class ) & # 91 ; javac & # 93 ; new StandardMBean ( bean , beanInterface ) { & # 91 ; javac & # 93 ; ^ & # 91 ; javac & # 93 ; 2 warnings & # 91 ; javac & # 93 ; /code/derby/trunk2/java/demo/vtis/java/org/apache/derbyDemo/vtis/core/QueryVTIHelper.java:185 : warning : & # 91 ; unchecked & # 93 ; unchecked call to getMethod ( java.lang.String , java.lang.Class & lt ; ? & gt ; ... ) as a member of the raw type java.lang.Class & # 91 ; javac & # 93 ; ( methodName , new Class { String.class } ) ; & # 91 ; javac & # 93 ; ^ & # 91 ; javac & # 93 ; /code/derby/trunk2/java/demo/vtis/java/org/apache/derbyDemo/vtis/core/XmlVTI.java:253 : warning : & # 91 ; unchecked & # 93 ; unchecked call to getMethod ( java.lang.String , java.lang.Class & lt ; ? & gt ; ... ) as a member of the raw type java.lang.Class & # 91 ; javac & # 93 ; ( methodName , new Class { String.class } ) ; & # 91 ; javac & # 93 ; ^ & # 91 ; javac & # 93 ; /code/derby/trunk2/java/demo/vtis/java/org/apache/derbyDemo/vtis/core/XmlVTI.java:260 : warning : & # 91 ; unchecked & # 93 ; unchecked call to getConstructor ( java.lang.Class & lt ; ? & gt ; ... ) as a member of the raw type java.lang.Class & # 91 ; javac & # 93 ; ( new Class { String.class , String.class , String.class } ) ; & # 91 ; javac & # 93 ; ^ & # 91 ; javac & # 93 ; /code/derby/trunk2/java/demo/vtis/java/org/apache/derbyDemo/vtis/snapshot/Subscription.java:278 : warning : & # 91 ; unchecked & # 93 ; unchecked call to getMethod ( java.lang.String , java.lang.Class & lt ; ? & gt ; ... ) as a member of the raw type java.lang.Class & # 91 ; javac & # 93 ; ( methodName , new Class { } ) ; & # 91 ; javac & # 93 ; ^ & # 91 ; javac & # 93 ; 4 warnings 
Many of the private methods in impl.sql.execute.AlterTableConstantAction take the activation ( and sometimes a number of references obtained through it ) as an argument . This seems unnecessary , and it clutters the code with argument passing , boilerplate code and variable hiding . 
Currently , the Embedded Driver supports Statement.setQueryTimeout ( ) , but the Client Driver does not . The Client Driver should be enhanced and match the Embedded Driver . For this , we need to transfer the timeout value from the client to the server , preferably without a separate round-trip . I have some loose thoughts on how to do this : If the client has set a timeout value for a statement , prepend the ( DRDA ) EXCSQLSTT command with an EXCSQLSET command which contains the timeout value ; conceptually a SET STATEMENT TIMEOUT & lt ; seconds & gt ; ( this does not mean that we need to extend the Derby grammar ; only the Network Server needs to understand this DRDA EXCSQLSET command ) . In DRDAConnThread.parseEXCSQLSETobjects ( ) on the server side , recognize the SET STATEMENT TIMEOUT text , parse the timeout value and remember it for the coming EXCSQLSTT command . Do NOT invoke executeUpdate ( ) with the SET statement & # 91 ; see note below & # 93 ; . In DRDAConnThread.parseEXCSQLSTT ( ) , check if a timeout value has been set ; if so , use it ( by setting the timeout value on the server-side Statement object before calling execute/executeQuery ) . 
made it possible to use the JIRA SOAP API to fetch a list of issues fixed in a release . However , the release manager have to manually create a JIRA filter and feed the filter id to the tool . Now that the Apache JIRA instance has been updated to 4.x , we can use the JIRA Query Language ( JQL ) to avoid that manual step . 
The automatic index statistics daemon will by default log what it s doing . That means messages like these ones will be written to derby.log during normal operation : Wed Mar 02 13:36:22 CET 2011 Thread & # 91 ; main,5 , main & # 93 ; { istat } APP . T : update scheduled - 23ce809c-012e-7691-6c29-000003480128 reason= & # 91 ; no stats , row-estimate=36167 & # 93 ; ( queueSize=1 ) Wed Mar 02 13:36:22 CET 2011 Thread & # 91 ; index-stat-thread,5 , main & # 93 ; { istat } APP . T : generating index statistics Wed Mar 02 13:36:23 CET 2011 Thread & # 91 ; index-stat-thread,5 , main & # 93 ; { istat } APP . T : generation complete ( 509 ms ) Wed Mar 02 13:36:23 CET 2011 Thread & # 91 ; main,5 , main & # 93 ; { istat } stopping daemon , active=false , work/age=510/14736 & # 91 ; q/p/s=0/1/1 , err : k/u/c=0/0/0 , rej : f/d/o=0/0/0 & # 93 ; Although these messages may be helpful in diagnosing bugs , they may also be a source of confusion since users probably only expect errors/problems to be logged by default . Also , for some workloads , large quantities of log may be produced and take up disk space . Therefore , I think we should disable the logging before the 10.8 release , and instead instruct users who experience problems to enable logging manually with the derby.storage.indexStats.log property . 
For the sake of observability and debugging in the wild , the tracing and/or logging of runtime exceptions raised in the istat thread should be improved . 
The Derby web site describes the layout of the log control file , log.ctrl , here : This description does not agree with the format actually produced by LogToFile.writeControlFile ( ) . That format can be examined using the ControlFileReader tool attached to . The description on the web site should be corrected . 
We should see if we could use a java.nio.ByteBuffer instead of a byte array in org.apache.derby.client.net.Request , similar to what we did for DDMWriter in . ByteBuffer provides some helper methods that allows us to simplify the code that puts multi-byte values into the buffer ( like ByteBuffer.putShort ( ) , putInt ( ) , putLong ( ) ) . It may also be a first step on the way to using a CharsetEncoder to encode strings without going via an intermediate throw-away byte array ( see for details ) . 
Users often on bad advice or desperation touch or delete files in the log or seg0 directories ( mostly log ) . I think it would be good for new databases and on upgrade that a file be created in these directories with a name like : TOUCHING_FILES_HERE_WILL_CORRUPT_DB_README.txt or some such to warn of the perils of doing this and the corrupting effects and how it can eliminate any possibility of salvage . It should also encourage users to make a zip backup of the database directory with no jvm currently accessing it before trying to do anything with the database if it appears to be already corrupt . Also point to backup/restore documentation and encourage restore of a good backup into an empty directory if the database is corrupt . I m not sure if it would help but it couldn t hurt . 
The Derby Network Server appears to look up the Derby Embedded Driver from the java.sql.DriverManager in a couple of places in order to get the connection to the actual database . This means the network server can not operate in environments where for whatever reason it is not OK to access the embedded driver from the DriverManager . Just in general this behavior is unexpected as it should be possible to directly load the embedded driver . I ve observed this issue in 10.6.1.0 and checked that the relevant usages are unchanged in trunk . 
Most critical information from sysinfo is now available in the Derby boot message in derby.log , except for the operating system information . e.g . OS name : Windows XP OS architecture : x86 OS version : 5.1 build 2600 Service Pack 3 It would be useful to log the operating system info as well in the derby.log on boot . I ll close out after filing this issue as this is the last piece of info that needs to be added to the error log . 
ConcatenationOperatorNode generates bytecode that ensures the result object is not null before calling the method that implements the operator . This breaks the pattern used by other operators ( which ensures that the result object is not null inside the method that implements the operator , not in the generated bytecode ) , and it unnecessarily complicates the code in BinaryOperatorNode . The comments indicate that the current approach was chosen to prevent the null check from happening at execution time , but the generated bytecode does perform the null check at execution time , so generating byte code for it shouldn t have any real benefit over writing Java code for it . In general , implementing as much as possible of the execution time code as Java code is preferred to implementing it directly as bytecode because it s easier to read and debug Java code , and because the generated bytecode can not be shared between execution plans and take more memory . 
The error reporting in AssertFailure is somewhat imprecise , and it may also fail with an NPE : testAssertFailureNoThreadDump ( org.apache.derbyTesting.unitTests.junit.AssertFailureTest ) ERROR : java.lang.NullPointerException at sun.reflect.NativeMethodAccessorImpl.invoke0 ( Native Method ) at sun.reflect.NativeMethodAccessorImpl.invoke ( NativeMethodAccessorImpl.java:57 ) at sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) at junit.framework.TestCase.runTest ( TestCase.java:164 ) at junit.framework.TestCase.runBare ( TestCase.java:130 ) at junit.framework.TestResult $ 1.protect ( TestResult.java:106 ) at junit.framework.TestResult.runProtected ( TestResult.java:124 ) at junit.framework.TestResult.run ( TestResult.java:109 ) at junit.framework.TestCase.run ( TestCase.java:120 ) at junit.extensions.TestDecorator.basicRun ( TestDecorator.java:24 ) at junit.extensions.TestSetup $ 1.protect ( TestSetup.java:21 ) at junit.framework.TestResult.runProtected ( TestResult.java:124 ) at junit.extensions.TestSetup.run ( TestSetup.java:25 ) at junit.framework.TestSuite.runTest ( TestSuite.java:230 ) at junit.framework.TestSuite.run ( TestSuite.java:225 ) at junit.framework.TestSuite.runTest ( TestSuite.java:230 ) at junit.framework.TestSuite.run ( TestSuite.java:225 ) at kah.TestRunner.main ( TestRunner.java:77 ) 
This variant of the method is never called with replace == true , so the code to handle replacement can be removed . 
This JIRA will add support for ( DRDA ) Strong User ID and Password Substitute Authentication ( USRSSBPWD ) scheme in the network client/server driver layers . Current Derby DRDA network client driver supports encrypted userid/password ( EUSRIDPWD ) via the use of DH key-agreement protocol - however current Open Group DRDA specifications imposes small prime and base generator values ( 256 bits ) that prevents other JCE s to be used as java cryptography providers - typical minimum security requirements is usually of 1024 bits ( 512-bit absolute minimum ) when using DH key-agreement protocol to generate a session key . Strong User ID and Password Substitute Authentication ( USRSSBPWD ) is part of DRDA specifications as another alternative to provide ciphered passwords across the wire . Support of USRSSBPWD authentication scheme will enable additional JCE s to be used when encrypted passwords are required across the wire . USRSSBPWD authentication scheme will be specified by a Derby network client user via the securityMechanism property on the connection UR - A new property value such as ENCRYPTED_PASSWORD_SECURITY will be defined in order to support this new ( DRDA ) authentication scheme . 
DataDictionaryImpl and CreateTriggerNode contain implementations of bubble sort that could be replaced by calls to Collections.sort ( ) . This isn t performance critical code ( it s only used for sorting vectors of column references or table nodes at compile time ) so there shouldn t be much to gain by hand-coding the sorting . Using Collections.sort ( ) has the benefits that it results in less code and that the implementation has probably gone through more testing than our hand-coded one . 
The error reporting in the replication test when deleting files to initialize the master/slave is insufficient . A message is printed to the console , but the test continues to run . This can cause the test to fail for what appears to be a different reason the left-over files . 
Currently Derby allows users to perfoms online backups using SYSCS_UTIL.SYSCS_BACKUP_DATABASE ( ) procedure , but while the backup is in progress , update operations are temporarily blocked , but read operations can still proceed . Blocking update operations can be real issue specifically in client server environments , because user requests will be blocked for a long time if a backup is in the progress on the server . 
I tried running Derby on Oracle Java ME Embedded Client 1.0 , and booting the engine failed : Caused by : java.lang.NoClassDefFoundError : java.nio.channels.OverlappingFileLockException This seems to happen because Derby recognizes it as a Java 1.4 platform , whereas it s actually a CDC/FP 1.1.2 platform . 
The classes Cursor and NetCursor in the client driver contain many unused methods , most of them for managing the internal byte buffer . These methods should be removed . 
Any of these RDBMS support the SQL standard statistics functions STDDEV_POP , STDDEV_SAMP , VAR_POP , VAR_SAMP : DB2 ( only STDDEV , VARIANE ) H2 HSQLDB Ingres MySQL Oracle Postgres SQL Server ( named STDEVP , STDEV , VARP , VAR ) Sybase ASE Sybase SQL Anywhere These don t : Derby SQLite This would be a useful addition for Derby , I think . An even larger example list of possible statistics aggregate functions is listed in the Postgres documentation : 
In various support situations I have seen problems with JDBC metadata stored prepared statements or trigger stored prepared statements that need to be invalidated . It would be nice to have a way to do this in the field . For 10.9 a stored procedure would make most sense , but it would be good to have something available in the release branches too . 
Since there have been no other network clients besides Derby Network Client tested or supported with Derby since 10.1 and since any protocol based client needs to understand Derby s DRDA extensions , deviations , and stored procedure usage . I think it would be a good idea in 10.9 for Network Server to outright reject any network clients that are not Derby Network Client . This would eliminate confusion up front for those that might not be aware that the DB2 Universal JDBC Driver and DB2 Runtime Client are not supported . They would get a clean reasonable error instead of hitting various protocol errors . Also it would mean if someone does want to add support for some network client in the future they would at least need to add the one or two lines of code in AppRequester to identify it , which I think would be a good thing . I think the code change would not be hard but the biggest impact might be anyone who still runs tests with JCC on trunk would need to disable those tests . There is a separate issue that Jayaram is working on to complete remove the JCC related code from the tests and test infrastructure . 
There are two issues , already seen in : the boot issue : there is a 1/2 * * 16 chance that a wrong bootPassword will allow boot to proceed ( but since its decoded key is wrong the boot will fail ) . the password change issue : similarly , there is a chance that the wrong bootPassword will be accepted trying to change it via SYSCS_UTIL.SYSCS_SET_DATABASE_PROPERTY ( bootPassword , ... ) at least for algorithms that do not check IV ( initialization vector ) in addition to the digest , e.g . DES/ECB/NoPadding The latter case may lead to data corruption , cf . discussion . I think the risk is fairly low , though : One would need to have execution permission to change the property if SQL authorization is used , and in most scenarios the supplied existing password would be correct . But since the results can be bad , it would be good to reduce or eliminate the risk . 
When opening a large number of databases in parallel , many threads get stuck for a long time in FileMonitor , as reported in this thread on derby-user : The synchronization in FileMonitor is only needed because the monitor instance is also used as a shared PrivilegedExceptionAction instance . Since all databases share one FileMonitor instance , threads that access any of these methods are serialized . If each method created its own PrivilegedAction or PrivilegedExceptionAction instance instead of using the monitor s run ( ) method , these methods wouldn t need synchronization . 
BaseDataFileFactory has some unused methods , they can be removed : syncSideLog , pageToDirty and getTempDirectory In addition I suggest the following cleanups : o remove unused imports o remove unnecessary return statement o make synchronization object freezeSemaphore final o remove unused instance variable backupPath o remove unused local variable ( and exception instantiation ) multipleJBMSWarning o replace new Boolean with Boolean.valueOf 
If you log on to a database ( other than the credentials db ) and your password is about to expire , you ll be advised to change your password using the SYSCS_UTIL.SYSCS_MODIFY_PASSWORD procedure . However , the warning message does not say you need to log on to the credentials db to change your password . This may lead the user to modify the password in the current database instead of the credentials database , thinking everything is well . ij ( CONNECTION1 ) & gt ; connect jdbc : derby : otherdb ; user=test ; password=abc ; WARNING 01J15 : Your password will expire in 0 day ( s ) . Please use the SYSCS_UTIL.SYSCS_MODIFY_PASSWORD procedure to change your password . ij ( CONNECTION2 ) & gt ; CALL SYSCS_UTIL.SYSCS_MODIFY_PASSWORD ( new-password ) ; 0 rows inserted/updated/deleted ij ( CONNECTION2 ) & gt ; connect jdbc : derby : otherdb ; user=test ; password=new-password ; ERROR 08004 : Connection authentication failure occurred . Reason : Invalid authentication .. Even though SYSCS_MODIFY_PASSWORD succeeds , the password has not been updated in the credentials db . 
The error message XCY05 is rather complex : Invalid change of the derby.authentication.provider property . Once set to NATIVE authentication , this property can not be changed . NATIVE : :LOCAL is the only NATIVE value accepted by derby.authentication.provider . This property can not be set to NATIVE : :LOCAL unless credentials for the database owner have been stored in the database using the syscs_util.syscs_create_user procedure . I d be in favor of simplifying this error message if possible , especially raising a more specific message for the error condition when the DBO hasn t been added as a user . Further , it was not clear to me that the information about derby.authentication.provider applies when setting it as a database property only . Is this message trying to describe three problem scenarios ? a ) Turning off NATIVE authentication is forbidden . b ) Specifying an invalid value for derby.authentication.provider when set as a database property . c ) Missing credentials for the DBO . 
Background : Sun 1.4 jvms bundle the Crimson XML parser , but do not have Xerces . IBM jvms bundle the Xerces XML parser , but do not have Crimson . As of J2SE 1.5 , Sun JVMs bundle Xerces instead of Crimson ( according to , # 10 ( JAXP ) ) I don t know anything about what other JVMs use for XML parsing ... Issue : As of Derby has a very basic level of support for an XML datatype that uses the Apache Xerces 2 parser . The parser name is hard-coded into Derby and thus anyone wishing to use Derby XML must include Xerces in his/her classpath . Up to now , the basic XML functionality used by Derby is available in both the Crimson and the Xerces parsers . Thus , it d be nice if we could improve Derby to support both of these parsers ( and perhaps others ) dynamically , thereby allowing XML to work regardless of whether the JVM is Sun or IBM or something else ( assuming the JVM is 1.4 or later ) . For example , it d be useful if we could perhaps have Derby choose a parser based on the JVM in use and/or a user-specified database property . Making this change would allow people to use XML in Derby within environments of their own choosing , instead of forcing them to download a specific parser that they don t otherwise want/need . Someone using a Sun 1.4 JVM could use XML in Derby without having to download Xerces ; someone using an IBM JVM could use XML in Derby without having to download Crimson ; and someone using a Sun 1.4 JVM could , if desired , download Xerces and then use that parser with the Sun 1.4 JVM ; etc . Basically , we d just be increasing the flexibility of Derby XML and thus making it more valuable to users . I haven t yet looked in detail at how to best make this improvement , but I do think it s worth filing for ( near- ) future development ... 
SQL roles : add parser support SQL roles : add upgrade support Derby roles introduce a new system catalog ( SYSROLES ) . When a database is hard upgraded , this catalog needs to be added . SQL roles : Implement authorization stack ( and SQL session context to hold it ) The current LanguageConnectionContext keeps the user authorization identifier for an SQL session . The lcc is shared context also for nested connections ( opened from stored procedures ) . So far , for roles , the current role has been stored in the lcc also . However , SQL requires that authorization identifers be pushed on a authorization stack when calling a stored procedure , cf . SQL 2003 , vol 2 , section 4.34.1.1 and 4.27.3 and 10.4 GR 5h and i . This allows a caller to keep its current role after a call even if changed by the stored procedure . This issue will implement the current role name part ( cell ) of the authorization stack . The authorization stack will be implemented as part of the SQL session context . The patch will also implement the pushing of the current unqualified schema name part of the SQL session context , cf . 10.4 GR 5a ( ) . 
DataDictionaryImpl.getSetAutoincrementValue contains an unused local variable , and while looking at removing it I saw that some of the statements in the method could be reordered to reduce object creation and improve readability . 
The constructor LOBStreamControl ( EmbedConnection , byte ) always makes the buffer size equal to the LOB size , effectively creating an extra , fully materialized copy of the LOB in memory . I think the assumption here is that a LOB that s already materialized is a small one . That is , LOBs that are smaller than 32 KB and fit in a single page are typically materialized when read from store . However , we sometimes materialize LOBs that are a lot bigger than 32 KB . For example , triggers that access LOBs may materialize them regardless of size ( see comment in DMLWriteResultSet s constructor for details ) . For these large LOBs , it sounds unreasonable to allocate a buffer of the same size as the LOB itself . I d suggest that we change the constructor so that it never allocates a buffer larger than 32KB . That would mean that the behaviour is preserved for all LOBs fetched directly from store ( only LOBs that don t fit in a single page will cause temporary files to be created ) , whereas we ll prevent large LOBs accessed by triggers from being duplicated in memory by overflowing to temporary files . 
Right now if you want to connect with a lowercase authorization id , you need to double-quote it : connect jdbc : derby : db ; user= dbo ; password=dbo_password ; But you don t use double-quotes when creating NATIVE credentials for that user : call syscs_util.syscs_create_user ( dbo , dbo_password ) ; I will attach a proof-of-concept patch which causes the NATIVE procedures to normalize USERNAME arguments before using them to key into SYS.SYSUSERS . This preserves the following feature of the current implementation : 1 ) Only one set of NATIVE credentials can be stored for a given authorization id . Note that this differs from the behavior of other authentication schemes . The other authentication schemes let you store a set of credentials for every upper/lower-case permutation of the authorization id . To me , this seems like a big security hole in those other authentication schemes . In addition , the proof-of-concept patch has the following behavior : 2 ) You connect with the same username string which you use when calling syscs_util.syscs_create_user . If this seems like the right casing behavior , I will write some tests and check this in . 
The subset of tests from DatabaseMetaDataTest being run as part of the upgrade tests is sensitive to changes in the database made by other tests . For instance , adding tables with foreign keys will make the test fail due to extra rows in system tables . Usually this could be solved by using a single-use db wrapper of some sort , but in the upgrade tests the database will be booted several times with different versions of Derby and the data needs to be preserved between some of these boots . 
As a result of the spec change introduced by derby-866-22-aa-dboFirst.diff , it is no longer possible to directly set derby.authentication.provider to the value NATIVE : :LOCAL . That setting only happens when the DBO stores her credentials . We should remove confusing references to NATIVE : :LOCAL from Derby error messages . 
The product jars carry version stamps in property files stored in the org/apache/derby/info directory . It would be good for the test jar to carry a version stamp too . This can be useful in verifying that the tests are at the right revision level tor the product being tested . 
Currently the invalidation of statements accessing a table is issued before dropping the old statistics and writing new ones . While we can not guarantee that all compilations will see the new statistics ( we can currently only drop all statistics for an index , so we can not add the new ones first and then drop the old ones ) , we can do the invalidation after the work has been done to reduce the chances that queries are compiled without statistics at all . 
Currently istat logs the affected table twice when dropping disposable statistics entries , but it doesn t log which index is affected at all . This information is only useful when the user wants to understand what happened , i.e . was there an orphaned entry in the system or was the entry dropped because it was computed for a single-column primary key ? 
The replication tests invoke Runtime.exec ( ) directly in order to spawn processes . The sub-processes that run on the same host as the main test process , should instead be started with the helper method BaseTestCase.execJavaCmd ( ) . Having all the tests use the helper method would make it easier if we for example want to pass specific flags to all sub-processes created in a test run . Note that the replication tests also have code for starting processes on a remote host via ssh . BaseTestCase.execJavaCmd ( ) can not do that , so only local processes can be started with the helper method . When the replication tests run as part of suites.All , all the spawned processes run locally . 
Support use of the WHEN clause in CREATE TRIGGER statements . The clause is described in the SQL standard ( 2003 ) in chapter 11.39 & lt ; trigger definition & gt ; under & lt ; triggered action & gt ; . There are traces in the code that suggests some work has been done on this earlier . If anyone knows something about this , please add a comment to this issue . 
When I open DRDAConnThread in NetBeans , I see 49 warnings . Most of them are harmless ( like static fields accessed via an instance , suggestions about using StringBuilder instead of StringBuffer , or using System.arraycopy ( ) instead of for loops ) . Others indicate real problems , like the use of ! = to compare SQL states in writeSQLDIAGGRP ( ) . We should clean up the warnings so that it s easier to notice new warnings about potential problems . 
Many of the methods in DDMReader and DDMWriter are not used anymore . Those methods could be removed . 
The client classes that implement java.sql.Wrapper , implement the interface in the leaf classes instead of the base classes . This is because unwrap ( ) has a generic signature , so it could not be in the base classes as long as they had to be compatible with Java 1.4 . For example , in the statement class hierarchy , we implement unwrap ( ) in Statement40 , PreparedStatement40 and CallableStatement40 . Now that we compile the client with source level 1.5 , we can move the unwrap ( ) method to the Statement class and eliminate the duplication . 
Code coverage of logical statements can be improved by invoking them in existing statement tests . This could be done by having a decorator that makes use of pooled connections . 
Java 5 introduced some new static factory methods ( called valueOf ( ) ) in the Number sub-classes , to be used in preference of the constructors unless a fresh instance is required . The valueOf ( ) methods are allowed to cache and reuse objects , possibly reducing the allocation/gc cost and the memory footprint . For example , Integer.valueOf ( int ) uses a pre-allocated cache for values between -128 and +127 by default ( the cache size can be tuned with JVM flags ) . Now that the server and client code is compiled against Java 5 libraries , we should use the valueOf ( ) methods to get those benefits . Note also that Java 5 auto-boxing of numbers uses these methods implicitly , so in many cases we can just remove the calls to the constructor and the compiler will automatically insert the calls for us . 
The JSR-169 interface is a subset of JDBC 3.0 , but still the JDBC 3.0 implementation classes do not extend the JSR-169 implementation classes . Instead , the JSR-169 and JDBC 3.0 implementation classes extend a common base class . The reason for this structure , is that the JSR-169 compile targets used to be optional , so the JDBC 3.0 classes could not depend on them . For example , the class javadoc comment for EmbedResultSet169 says : ResultSet implementation for JSR169 . Adds no functionality to its ( abstract ) parent class . If Derby could be compiled against JSR169 that the parent class could be the concrete class for the environment . Just like for the JDBC 2.0 specific classes . Until that is possible ( ie . easily downloadable J2ME/CDC/Foundation/JSR169 jar files , this class is required and is only compiled by an optional target . Since the JSR-169 code is no longer optional , we should do as the comment suggests , and use the base class directly instead . This would allow us to simplify the class tree . 
Change all the network server code under java/drda to use spaces instead of tabs . Having mixed tabs and spaces can be confusing , irritating and time consuming for developers especially in the network code where client is all spaces and the server mostly tabs . With the -x -b options on svn merge and svn diff , merges should be reasonable even if we change all the tabs to 4 spaces in the files under java/drda with a script something like ( not yet tried ) : # ! /bin/bash files= $ @ for file in $ files { echo $ file mv $ file $ file.orig sed -e s/\t/ /g & lt ; $ file.orig & gt ; $ file rm $ file.orig } Are there any objections to this change ? If not does anyone have any pending DRDA changes they would like to get in before I make the change ? 
Since the functions added in overlap with the numeric scaler functions defined in section C.1 of JDBC 3.0 it would be benfiical to expand the table to indicate if the function can be used as a JDBC escaped scaler as well . Plus adding all the code to find them and call them etc . I ll add a comment later to show which JDBC escape functions this would add . 
The reflection service has some code for creating factory classes for each class generated for SQL queries , but the code is disabled due to memory concerns . We should remove the code to reduce code complexity and engine jar footprint . If someone wants to enable this code later , they can always revive it from the svn history . 
There s some code that s added to all classes generated by Derby s query compiler . For example , there are three static fields that contain statistics used to check if the plan is stale , and there are getter and setter methods for each of the three fields . The fields and their accessor methods take up 468 bytes in every generated class . We should see if we can factor out some of this code so that there is a single shared copy in BaseActivation . Advantages would be : less complicated byte-code generation , less memory occupied by generated classes in the statement cache , smaller disk footprint for stored prepared statements . 
While waiting for a Java module system ( aka project Jigsaw ) , it has been decided to define a few subsets of the Java SE Platform Specification , cf JEP 161 ( ) . A quote from the JEP : More broadly , this feature is intended to enable the migration of applications currently built on top of the Java ME Connected Device Configuration ( CDC ) to appropriate Profiles of the Java SE Platform , part of the long-term effort to converge CDC with Java SE . It would be good if we make Derby to run on such limited profiles . The current proposal places JDBC in Compact Profile 2 ( cf . link above ) , while other libraries used by Derby , e.g . javax.naming ( JNDI ) are in Profile 3 ( larger ) . It would be good if Derby could run on the smallest posible platform , i.e . Profile 2 , but that will probably involve some changes to make Derby gracefully limit functionality when some libraries are missing . 
When looking at a test coverage report , I noticed that these methods in the SQLBoolean class are not tested : falseTruthValue trueTruthValue greaterOrEquals greaterThan lessOrEquals lessThan is isNot readExternal hashCode falseTruthValue ( ) and trueTruthValue ( ) are never called anywhere in the code and could be removed . is ( ) and isNot ( ) also have no callers . However , they implement the boolean IS operator which is part of the SQL standard , so it might make more sense to file a JIRA to add the syntax to the grammar rather than removing the methods . 
For all result set nodes that have predicate lists , Derby s compiler lays out byte code for fields to hold qualifiers , and also code to reinitialize the values in those fields on every execution . It does this even if there are no qualifiers . See PredicateList.generateQualifiers ( ) . That method does actually check whether there are qualifiers , and it skips some of the code generation if there are none . It should be changed so that it skips all the code generation if there are no qualifiers . For an example , see the decompiled generated class for a natural join attached to : That class contains three Qualifier fields : private Qualifier e1 ; private Qualifier e3 ; private Qualifier e4 ; Only one of them ( e4 ) is ever set to a non-null value . Still , the reinit ( ) method , which is called on every execution , has code for each of the fields : protected void reinit ( ) throws StandardException { BaseActivation.reinitializeQualifiers ( e1 ) ; BaseActivation.reinitializeQualifiers ( e3 ) ; BaseActivation.reinitializeQualifiers ( e4 ) ; } 
Open JDK 8 will include maintenance rev 4.2 of JDBC . The public discussion of JDBC 4.2 will take place here : . We will want to build Derby support for JDBC 4.2 after a public spec appears . At this time , it is unclear what Derby release will carry this support . 
The constructors for many of the result set classes take GeneratedMethod parameters that create row templates ( an ExecRow of a certain size and column types , each column initialized to an SQL null value ) . As an alternative , the compiler could produce an ExecRow instance and put it into the savedObjects field of GenericPreparedStatement , and the constructors could take parameter that points to the object in savedObjects . Where the result sets currently invoke the generated method to produce a fresh template , they could instead clone the saved object . Advantages with the suggested approach would be : Reduce the size of the code generator , which should reduce total code complexity . Reduce the amount of generated code , which makes it easier for tools ( profilers , static code analyzers , IDEs ) to map executable code to source code . Reduce the actual number of generated methods , which makes it less likely that queries need to use reflection to invoke the remaining generated methods ( there s a switchover from DirectCall to ReflectCall when the number of generated methods exceeds 10 ) . 
The new OFFSET/FETCH syntax does not support Parameters . From ij against the toursdb I would like to do the following : ij & gt ; prepare foo as select * from cities offset ? rows fetch first 10 rows only ; but results in the following syntax error : ERROR 42X01 : Syntax error : Encountered ? at line 1 , column 29 . Parameterized OFFSET/FETCH is important for performance ( can be prepared ) and security ( SQL strings not created on the fly ) . 
Now that vararg routines have been added to Derby ( see ) , I would like to add a new vararg system procedure for registering and unregistering optional packages of Derby tools . For starters , these would be tools which aren t checked into the Derby codeline but are just attached to various JIRAs . These tools are : o DBMDWrapper ( and ) - This tool creates functions and table functions for all of the DatabaseMetaData methods so that you can write complicated queries which join and filter JDBC metadata . o ForeignTableVTI ( ) - This tool creates views against foreign databases so that you can bulk-import foreign data into Derby without indirecting through csv files . It also may be possible to use this approach to expose the log and data file reading tools attached to and . The new system procedure would look like this : create procedure syscs_util.syscs_register_tool ( toolName varchar ( 32672 ) , boolean register , optionalArgs varchar ( 32672 ) ... ) language java parameter style derby modifies sql data external name willFigureOutWhereToPutThis ; The arguments would have these meanings : o toolName - A name specific to the tool . o register - True means register the tool and false means unregister the tool . o optionalArgs - Each tool could have its own variable set of additional configuration parameters . By default , only the DBO could run this procedure . The DBO could grant execute permission to other users . The known tool names and their optional parameters would be documented in the Derby Reference Manual in the section on syscs_util.syscs_register_tool . I am thinking that we should put the optional tools themselves in derbytools.jar . We might want to document all of the optional tools in the Tools Guide , although I can see arguments for documenting some tools in the Admin Guide . I would appreciate other people s thoughts about this proposal . Thanks , -Rick 
It would be useful to have a tool for dumping the data in a corrupted database . This could start out as a custom tool . After we debug the tool and get some experience with it , we can consider promoting it to be a ( possibly undocumented ) optional tool which we ship with the product . I think the tool should have the following behavior : 1 ) The tool should not subvert the security of the corrupted database . If the corrupted database is password-protected , then you would need to present its DBO s credentials in order to use the tool . Naturally , an encryption key would have to be presented in order to decode an encrypted database . 2 ) The tool should not stop reading a table when it hits a corrupt record . Instead , the tool should soldier on and collect a list of warnings on bad records . Such a tool would be useful in situations where some part of a heap table is corrupt but the following heap conglomerates are intact : i ) SYSSCHEMAS ii ) SYSTABLES iii ) SYSCONGLOMERATES iv ) SYSCOLUMNS v ) property conglomerate Such a tool would be useful for some situations where data can t be dumped even after you delete the log files in order to short-circuit recovery . 
When investigating , I found some code that could be simplified/cleaned up . Will make those changes here . 
org.apache.derby.impl.store.raw.data.PageBasicOperation and its subclasses are public . They are however never used outside of the package in which they live , so package access should be sufficient for most of their methods . Reducing the visibility of the classes and/or their methods silences many warnings in my IDE ( NetBeans ) . Typically , the warnings are about public methods that have parameters of non-public types . Note : These classes implement Formatable , which requires that the classes are public and have public no-arg constructors . We can still reduce the visibility of the abstract classes in the hierarchy , as well as the specialized constructors that take arguments , since they are not used by the serialization logic . 
RegisteredFormatIds and StoredFormatIds contain identifiers for classes that implement the Formatable interfaces and are used when serializing/deserializing instances of those classes . Many of the classes referenced there do however not implement Formatable . The references to them could be removed . I ve seen that many classes in the impl.sql.execute.rts package are there , none of which implement Formatable . There may be more . 
In new JDBC versions colums can be added to DatabaseMetaData . Tests should just check expectedColumns and ignore extra columns to prevent failures with compatibility testing . This check occurs in assertColumnNames and assertColumnTypes . 
I have seen indications of severe monitor contention in SinglePool ( the current lock manager ) when multiple threads access a Derby database concurrently . When a thread wants to lock an object , it needs to obtain the monitor for both SinglePool and LockSet ( both of them are global synchronization points ) . This leads to poor scalability . We should investigate how to allow more concurrency in the lock manager , and either extend SinglePool or implement a new manager . 
Right now the trace logic in the optimizer is hard-coded to produce a stream of diagnostics . It would be good to be able to plug alternative trace logic into the optimizer . This would make the following possible : 1 ) Plug in trace logic which produces formats which are easier to study and which can be analyzed mechanically . E.g. , xml formatted output . 2 ) Plug in trace logic which can be used during unit testing to verify that the optimizer has picked the right plan . Over time this might make it easier to migrate canon-based tests to assertion-based tests . 
Many methods in the client need handling of UnsupportedEncodingException , typically because they call String methods that take the name of the encoding as argument . Most of these methods have overloads that take a Charset instead of a String to describe the encoding , and those methods don t throw UnsupportedEncodingException as the encoding is known to be supported once we have a Charset instance for it . We should use the methods that take a Charset and simplify the exception handling . 
ConcurrentXactFactory extends XactFactory and replaces XactFactory s Hashtable with a ConcurrentHashMap for better scalability . Now that ConcurrentHashMap is available on all platforms supported on trunk , we could make XactFactory use a ConcurrentHashMap and remove ConcurrentXactFactory . 
There are two ClassFactory implementations : ReflectClassesJava2 for Java 1.4 and lower ( including CDC/FP 1.1 ) and Java5ClassFactory for Java 5 and higher . Now that the base level on trunk is Java 6 , it is sufficient with one ClassFactory . 
Both the client driver and the embedded driver have factories that produce SQLExceptions of the correct sub-type . We don t need the factories that produce JDBC 3.0 exceptions now that the base level is JDBC 4.0 . 
StandardException has many factory methods for creating exceptions and warnings . For example , there are newException ( ) methods for 0 to 8 message arguments . A single method taking a vararg argument could replace those nine methods . 
ERROR : Subquery is only allowed to return a single column . PROBLEM : When using derby with hibernate ( or JPA ) queries are created by the JPA engine per JPA spec . For tables with multi-column PK , subqueries are created with two columns in select clause ( see select colofassig6_.activityID , colofassig6_.assigneeID from Assignment in the query below ) . Without this support , I can not use Derby with JPA . Hibernate : select distinct activitybe0_.activityID as activityID69_ , activitybe0_.createdBy as createdBy69_ , activitybe0_.createdOn as createdOn69_ , activitybe0_.lastModifiedBy as lastModi4_69_ , activitybe0_.lastModifiedOn as lastModi5_69_ , activitybe0_.activityDate as activity6_69_ , activitybe0_.activityTypeHierarchyID as activity7_69_ , activitybe0_.activityTypeID as activity8_69_ , activitybe0_.campaignID as campaignID69_ , activitybe0_.comments as comments69_ , activitybe0_.description as descrip11_69_ , activitybe0_.inputID as inputID69_ , activitybe0_.inputTypeID as inputTy13_69_ , activitybe0_.name as name69_ , activitybe0_.notes as notes69_ , activitybe0_.organizationID as organiz16_69_ , activitybe0_.parentActivityTypeID as parentA17_69_ from Activity activitybe0_ , Activity activitybe1_ inner join ActivitySchedule colofactiv2_ on activitybe1_.activityID=colofactiv2_.activityID inner join ActivityScheduleStatus colofactiv3_ on colofactiv2_.activityScheduleID=colofactiv3_.activityScheduleID inner join ActivityScheduleStatusType activitysc4_ on colofactiv3_.activityScheduleStatusTypeID=activitysc4_.activityScheduleStatusTypeID , ActivityTypeHierarchy activityty5_ where activitybe0_.activityTypeHierarchyID=activityty5_.activityTypeHierarchyID and activityty5_.activityTypeHierarchyID= ? and not ( exists ( select colofassig6_.activityID , colofassig6_.assigneeID from Assignment colofassig6_ where activitybe0_.activityID=colofassig6_.activityID ) ) and ( activitybe0_.activityID & lt ; & gt ; activitybe1_.activityID or activitysc4_.name & lt ; & gt ; Route ) 2011-11-14 11:41:13,413 ERROR & # 91 ; org.hibernate.util.JDBCExceptionReporter & # 93 ; ( EJB-Timer-1321288405420 & # 91 ; target=jboss.j2ee : ear=oecrm1.6.3RC1-derby-jboss.ear , jar=builder-ejb.jar , name=WorkflowActivatorBean , service=EJB3 & # 93 ; ) Subquery is only allowed to return a single column . 
Right now there are 2 optimizer implementations , only one of which is ever loaded . The other implementation was disabled by the following comment in modules.properties : use level1 optimizer for small configurations # can t do this in the codeline because with 2 implementations , it is entirely by chance which get picked . So we may be running with different modules depending on which jdk # to be resolve by Siuling and Dan # # derby.module.optimizerSmall=org.apache.derby.impl.sql.compile.OptimizerFactoryImpl # cloudscape.config.optimizerSmall=cloud , cloudtarget Since we have deprecated support for the small CDC configuration , I don t think that we need further resolution by Siuling and Dan . Collapsing the two optimizers together should slightly reduce our static footprint . 
I ve found that many methods that generate messages could be collapsed into a single method using varargs . Right now , many of them exist in multiple variants , typically for zero up to three or four message arguments . Examples : Monitor.logTextMessage ( ) MessageService.getTextMessage ( ) Constructors in SqlException and SqlWarning 
Add a method for printing an org.apache.derby.iapi.sql.ResultSet as xml . The idea is to get a quick snapshot of a plan shape without all of the overhead of PlanExporter and runtimestatistics . This method could be used by our tests to verify plan shapes . 
It would be nice to be able to override the optimizer s choice and specify a complete query plan using the compact summary syntax output by XMLOptTrace . Given how the optimizer handles a statement , this would require binding a query plan at the query block level . Two obvious candidates for such a feature are : 1 ) Extend the use of DERBY-PROPERTIES in the comments of a query . 2 ) Add an extra clause to query blocks . The clause would have to be a clearly marked Derby extension . ( 1 ) might look like this ( here we add a new fullQueryPlan property ) : select tablename from sys.systables t , sys.syscolumns c , sys.sysaliases a where t.tablename = c.columnname and c.columnname = a.alias & # 8211 ; DERBY-PROPERTIES fullQueryPlan = ( SYSCOLUMNS_HEAP # SYSALIASES_INDEX1 ) # SYSTABLES_INDEX1 union all select tablename from sys.systables t , sys.syscolumns c , sys.sysaliases a , sys.syssequences s where t.tablename = c.columnname and c.columnname = a.alias and a.alias = s.sequencename & # 8211 ; DERBY-PROPERTIES fullQueryPlan = ( ( SYSCOLUMNS_HEAP # SYSTABLES_INDEX1 ) # SYSALIASES_INDEX1 ) # SYSSEQUENCES_INDEX2 ; ( 2 ) might look like this ( here we add a new using derby join order clause ) : select tablename from sys.systables t , sys.syscolumns c , sys.sysaliases a where t.tablename = c.columnname and c.columnname = a.alias using derby join order ( SYSCOLUMNS_HEAP # SYSALIASES_INDEX1 ) # SYSTABLES_INDEX1 union all select tablename from sys.systables t , sys.syscolumns c , sys.sysaliases a , sys.syssequences s where t.tablename = c.columnname and c.columnname = a.alias and a.alias = s.sequencename using derby join order ( ( SYSCOLUMNS_HEAP # SYSTABLES_INDEX1 ) # SYSALIASES_INDEX1 ) # SYSSEQUENCES_INDEX2 ; Here s a comparison of these approaches : ( 1 ) + Portability : the same query text can be used against different RDBMSes . Parsing of DERBY-PROPERTIES happens outside the grammer . ( 2 ) + Parsing happens in the parser . Not portable . I slightly prefer approach ( 1 ) . If I pursue that approach , I would like to see if I can move the parsing into the parser . I am interested in other opinions about how to address this feature . Thanks . 
The FileUtil class uses reflection to call the following java.io.File methods : setWritable ( boolean , boolean ) setReadable ( boolean , boolean ) setExecutable ( boolean , boolean ) Reflection was used because the methods were introduced in Java 6 , and the code had to run on older platforms . Now Java 6 is the lowest supported platform , so we can call the methods directly . 
Many of the methods in the Predicate class have no callers . Those methods should be removed . 
ERROR nospc : nospc.U should only be an internal error that is caught and handled . If returned to the user , it is likely a bug . It would be useful if this occurs to have a page dump printed to derby.log . Care should be taken to only do this if the error is returned to the user and not for the internal variety . 
This code in SortBufferScan makes NetBeans ( 7.4beta ) issue a warning : return ( super.current ! = null ) ; Incompatible types : boolean is not a functional interface & lt ; identifier expected & gt ; ; expected 
When opening a connection with a network url : jdbc : derby : //localhost:4000/data The netagent catchs the PrivilegedActionException but doesn t report the checked exception in it . Changing these lines try { socket_ = ( java.net.Socket ) java.security.AccessController.doPrivileged ( new OpenSocketAction ( server , port ) ) ; } catch ( java.security.PrivilegedActionException e ) { throw new DisconnectException ( this , e.getClass ( ) .getName ( ) + : Error opening socket to server + server + on port + port + with message : + e.getMessage ( ) ) ; } To This returns the check exception information : try { socket_ = ( java.net.Socket ) java.security.AccessController.doPrivileged ( new OpenSocketAction ( server , port ) ) ; } catch ( java.security.PrivilegedActionException e ) { throw new DisconnectException ( this , e.getException ( ) .getClass ( ) .getName ( ) + : Error opening socket to server + server + on port + port + with message : + e.getException ( ) .getMessage ( ) ) ; } It currently reports messages like this : Caused by : java.sql.SQLException : org.apache.derby.client.am.SqlException : java.security.PrivilegedActionException : Error opening socket to server localhost on port 1527 with message : null With this fix it reports : org.apache.derby.client.am.DisconnectException : java.net.ConnectException : Error opening socket to server localhost on port 1527 with message : Connection refused : connect 
By default , derby.log grows without bounds if the derby.infolog.append property is set to true . Setting this to true helps in a hands off production environment to ensure that if Derby restarts , the derby.log which might contain important information is not lost . On the other hand , when set the true the derby.log grows without bounds . This is problematic in a long running system . What is really needed is the ability to have a rolling derby.log file support where the maximum file size and maximum number of files can be specified . Derby has the ability to configure the location of the log file ( ie . derby.stream.error.file ) and also two methods of redirecting the error stream ( .ie derby.stream.error.method and derby.stream.error.field ) . There is no standard implementation that supports a rolling derby.log however . This facility should be part of the core Derby system so that it works in both embedded and network server models . 
I suggest to implement a SQL statement to create and fill a table with a query , without having to write the columns definition . e.g . : CREATE TABLE new_table AS SELECT ... ; or : SELECT ... INTO new_table FROM ... ; 
I tried running SELECT MAX on an indexed column in a big ( 8 GB ) table . It took 12 minutes , which is about 12 minutes more than I expected . After a bit of investigation , I found out that a full index scan was performed because all the rows referenced from the rightmost B-tree node were actually deleted . Possible improvements : 1 . Implement backwards scan in the B-trees ( this is also suggested in the comments in BTreeMaxScan ) . 2 . Go up to the parent node and down to the next leaf node on the left side , and continue until a valid max value is found . In Derby , traversing up in a B-tree is not allowed , but would it be ok to go up if the latches were kept on the higher-level nodes in the tree ? Of course , this would have negative impact on concurrency . 3 . Right-to-left traversal on the leaf level is possible ( using ControlRow.getLeftSibling ( ) ) , but will throw an exception if the page can not be latched without waiting . It is therefore possible to try a right-to-left search for the max value , and only fall back to the left-to-right approach if a conflict arises . 
ij version 10.10 ij & gt ; connect jdbc : derby : memory : db ; create=true ; ij & gt ; create type typ external name java.util.ArrayList language java ; 0 rows inserted/updated/deleted ij & gt ; create table ttt ( x int , check ( cast ( null as typ ) is null ) ) ; 0 rows inserted/updated/deleted ij & gt ; insert into ttt values 1,2,3 ; ERROR XJ001 : Java exception : : java.lang.NullPointerException . or with debug build : ij & gt ; insert into ttt values 1,2,3 ; ERROR XJ001 : Java exception : ASSERT FAILED no current dependent for compilation : org.apache.derby.shared.common.sanity.AssertFailure . 
Most of the functions in the SYSFUN schema use methods in java.lang.StrictMath directly . A few of them use methods in SystemProcedures instead . Java 1.5 added more methods to StrictMath , so it is now possible to make more of the functions use StrictMath and drop the implementation in SystemProcedures : LOG10 could use StrictMath.log10 ( ) COSH could use StrictMath.cosh ( ) SINH could use StrictMath.sinh ( ) TANH could use StrictMath.tanh ( ) 
I identified some unused code ; a flag that is always false , and code for handling triggers in bulk insert which is unused ( by inspection and corroborated by JaCoCo results for our regression tests ) : we disable bulk insert if there are triggers present , cf . DMLModStatementNode # getAllRelevantTriggers - & gt ; adjustDeferredFlag ( true ) InsertResultSet # bulkInsert : bulkInsert = verifyBulkInsert ( ) - & gt ; : // bulk insert is disabled for deferred mode inserts if ( constants.deferred ) return false ; 
These classes have a lot of package visible fields . This makes it hard to reason about how they are being used . This issue tracks some improvements to the encapsulation of these classes . 
I suggest to implement the null ordering option for the ORDER BY clause : According to the SQL standard , 10.10 & lt ; sort specification list & gt ; : & lt ; null ordering & gt ; : := NULLS FIRST NULLS LAST 
When the embedded driver raises an SQLException , it is linked to a iapi.error.StandardException whose toString ( ) method adds the SQLState to the error message . An SQLException raised by the client driver is linked to a client.am.SqlException , which does not add the SQLState . Because of this , printStackTrace ( ) gives more information when called on an embedded exception . You have to call getSQLState ( ) on the client exception in order to see the SQLState . We should change client.am.SqlException s toString ( ) method to work the same way as StandardException.toString ( ) . See also the discussion in . 
EmbedSQLException used to be the top-level exception raised on error in the JDBC 3 version of the embedded driver . The primary purpose of the EmbedSQLException class is gone now that JDBC 4.0 is the minimum JDBC level , and all top-level exceptions are vanilla java.sql.SQLExceptions or one of its specialized subtypes . The top-level SQLException still links to an EmbedSQLException in order to provide some extra information ( such as the message id ) that the network server needs when encoding the exception for transport over the wire . I think it should be possible for the network server to get this information from the StandardException which is typically also in the exception chain . When embedded Derby raises an exception currently , it is typically a java.sql.SQLException ( or subclass ) that s linked to an EmbedSQLException that s linked to a StandardException . If we could find a way to eliminate the EmbedSQLException from the exception chain , the stack traces would be easier to read , and the structure of the exception chains would be more consistent with the client exceptions . 
When Derby encounters an exception in a method that s called via reflection , the actual problem may be well hidden at the end of a long exception chain . For example : ij & gt ; create table t ( x int ) ; 0 rows inserted/updated/deleted ij & gt ; call syscs_util.syscs_import_table ( null , T , null , null , null , null , 0 ) ; ERROR 38000 : The exception java.sql.SQLException : The exception java.lang.reflect.InvocationTargetException was thrown while evaluating an expression . was thrown while evaluating an expression . ERROR 38000 : The exception java.lang.reflect.InvocationTargetException was thrown while evaluating an expression . ERROR XJ001 : Java exception : : java.lang.reflect.InvocationTargetException . ERROR XIE05 : Data file can not be null . Only the last exception provides any useful information to the user in this case . I think it would be good to remove the InvocationTargetException from the chain so that it s easier to spot the actual problem . 
The iapi.services.io.FileUtil class contains many methods that are no longer in use . We should remove those methods . 
Another script based test I converted into JUnit . 
Implement SSL/TLS communication between client and server 
In there was some discussion about changing how errors are handled when Derby fails to restrict the file permissions . There seemed to be consensus that Derby should raise an exception if the user had explicitly requested ( by setting derby.storage.useDefaultFilePermissions=false ) that it should try to restrict file permissions . Currently , it only raises an error on non-posix file systems that support access control lists . In the case were the user has not explicitly requested restriction of file permissions , two options have been suggested : 1 ) Raise an exception 2 ) Don t raise an exception , possibly print a warning in derby.log Option 1 is the more secure one , since it forces the user to make a decision on how to handle a possible security problem ( either by addressing the underlying cause of the failure , so that permissions can be successfully restricted by Derby , or by disabling the file restriction functionality ) . Option 2 is the more backward compatible one , since it gracefully falls back to the pre-10.10/pre-Java 7 behaviour if it can not restrict the file permissions . 
This is another attempt to improve the concurrency of identity columns . The previous attempt was tracked by . This new attempt will try out Mike s last suggestion : use system-created sequences managed by SYS.SYSSEQUENCES . This should reduce the contention on the core catalogs . I m hopeful about this approach because of the experiments tracked by . There we are not seeing any problems related to sequence generators but we are seeing lots of identity-related lock timeouts . Here is the general shape of this approach : 1 ) When adding an identity column to a table , Derby will create a sequence generator for the column . The sequence generator will live in the SYS schema and its name will be the table s UUID . 2 ) DROP SEQUENCE will not operate on system-created sequences . System-created sequences will be dropped/modified by DROP/ALTER TABLE commands . 3 ) We will add a new system function for inspecting the current , in-memory value of an identity generator without getting a lock on SYS.SYSSEQUENCES : SYSCS_UTIL.SYSCS_PEEK_AT_IDENTITY ( tableSchemaName , tableName ) . 4 ) Derby will create a sequence for every legacy identity column after hard-upgrade . 5 ) These changes will take place only after hard-upgrade . Soft-upgrade will not change the behavior of identity columns . Comments on this proposal are welcome . Thanks . 
The parser translates both CASE and NULLIF expressions into ConditionalNodes , but it represents untyped NULLs differently in the two cases . In a CASE expression , any branch that is an untyped NULL , is translated into an UntypedNullConstantNode that s wrapped in a CastNode that casts the value to CHAR ( 1 ) . The CastNode is replaced with a cast to the correct type during the bind phase . A NULLIF expression is turned into a CASE expression that has a THEN NULL clause . The parser simply creates an UntypedNullConstantNode for that clause , without wrapping it in a CastNode . A CastNode is instead added during the bind phase . This slight difference in how NULLs are represented by the parser in the two cases , means that ConditionalNode needs to handle the two cases differently during the bind phase . It would be better if the parser generated NULLs in the same way for the two cases , so that ConditionalNode didn t need to know if it was generated for a CASE expression or a NULLIF expression . 
Since JDK 7 , when we use the standard decorator for JUnit tests , e.g . return TestConfiguration.defaultSuite ( AggBuiltinTest.class ) ; the order in which the test fixtures get run is indeterministic . This is undesirable because it doesn t add much coverage to the product and makes the tests more brittle than they need to be . 
Add import/export support for UDTs . Import/export should work on UDTs . Add dblook support for UDTs . The dblook tool should support UDTs and the USAGE privileges granted on them . 
If I repeatedly insert into a clob table and rollback the the transaction the space is not reclaimed and the number of allocated pages continues to grow . I will add a test case to ClobReclamationTest and reference this bug . may be a special case of this bug , but I thought I would file a bug for the general issue . 
Currently , Derby uses the interfaces in the org.w3c.dom.xpath package to implement XPath support . That package is not part of the Java SE specification . Even though the package is included as an internal API in some of the major Java runtimes , using the standard javax.xml.xpath package instead would be better . It would be more portable , and less likely to break in the future if JVM vendors remove the internal API . 
Derby allows users to specify names of classes to use for various pluggable modules . In some cases , it verifies that the class implements the expected interface before it creates an instance of the class . For example in SpecificAuthenticactionServiceImpl : Class sasClass = Class .forName ( specificAuthenticationScheme ) ; if ( ! UserAuthenticator.class.isAssignableFrom ( sasClass ) ) { throw StandardException.newException ( SQLState.AUTHENTICATION_NOT_IMPLEMENTED , specificAuthenticationScheme , org.apache.derby.authentication.UserAuthenticator ) ; } UserAuthenticator aScheme = ( UserAuthenticator ) sasClass.newInstance ( ) ; In other cases , it creates an instance without checking , and instead fails with a ClassCastException or some other exception when trying to use the instance of the incorrect type . Examples : Java5SystemProcedures SYSCS_REGISTER_TOOL ( ) , JCECipherFactory , SequenceUpdater.makePreallocator ( ) . I think it would be good to have similar checks in these other cases too . That ll give clearer error messages which explain what the problem is , and it will be safer because it limits which constructors the users can force the Derby engine to invoke . 
Often , when researching intermittent regression test errors , an assertion on a result set fails , but the error report only shows which column in which row first fails to meet expectations . It would be good if the assertion methods could dump the actual result set in such cases so we d have more information to go on when trying to understand the cause of the problem . 
The code contains backup methods both for specifying the backup directory both as a String and as a File parameter . Only the String versions are currently used . The File versions should be removed to avoid duplication of code etc . Examples of such methods are : BasicDatabase.backup BasicDatabase.backupAndEnableLogArchiveMode RAMAccessManager.backup RAMAccessManager.backupAndEnableLogArchiveMode RawStore.backupAndEnableLogArchiveMode plus corresponding interfaces . 
The getNextRowCore ( ) method of ValidateCheckConstraintResultSet is an almost exact cut-and-paste copy of the method it overrides in its superclass . This code should be re-factored to reduce the cut-and-paste brittleness . This was discovered while investigating . 
Currently Derby space reclamation is initiated after all the rows on a MAIN page are delted . When blob/clob s larger than a page are involved the row on the main page only keeps a pointer to a page chain , so the main page rows can be very small and thus may take a lot of rows to be deleted before we clean up and reuse space associated with blob/clob . So in an extreme case of a table with only a int key and a 1 blob column with N bytes , and a 32k page derby probably stores more than 1000 rows . If the app simply does insert/delete of a single row it will grow to 1000 * N bytes for an app that to the user should only be on the order of N big . It would seem reasonable to queue a post commit for any delete which includes a blob/clob that has been chained . This is in keeping with the current policy to queue the work when it is possible we can reclaim an entire page . The problem is that there would be an extra cost at delete time to determine if the row being deleted has a blob/clob page chain . The actual information is stored in the field header of that particular column so currently the only way to check would be to check every field header of every column in the deleted row . From the store s point of view every column can be long with a page chain & # 8211 ; currently it doesn t know that only blob/clob datatypes can cause this behavior . Some options include : 1 at table create time ask for input from language to say if one of these is at all possible , so that check is never done if not necessary . 2 Maintain a bit in the container header with some sort of indication if any long row exists , may simply 1/0 or a reference count . Note information is easily available at insert time . 3 maintain a bit in the page indicating if any long rows exist 4 maintain a bit in the record header if any long columns exist , note the existing bit is only if the whole record is overflowed , not if a single column is overflowed . options 1-3 would then be used to only perform the slow check at delete time if necessary . I don t really like option 1 unless we change the storage interface to actually check/guarantee the behavior . I lean toward option 4 , but it is sort of a row format change . Given that the system has room saved for this bit I believe we can use it without any sort of upgrade-time work necessary - though I believe it can only be set on a hard upgrade as there may be old code which does not expect it to be set . Soft upgrades won t get the benefit and existing data won t get the benefit . Any other ideas out there ? 
This is one of the tasks tracked by master JIRA . 
Got this request in private conversation with a user . Other databases provide this functionality . Seems straightforward to add . 
is providing SQL support to update identity columns using DEFAULT keyword . This jira will look into collecting generated keys if Statement.RETURN_GENERATED_KEYS flag is supplied to the JDBC call for an UPDATE sql . 
This happens in a static initialization block at the start of AutoloadedDriver . Is this dead code ? 
is the result of poodle security alert in SSLv3 and SSLv2Hello protocols . For future , it will be good to know the protocols that are enabled on the server side so it is easier to know if the product may be impacted by a security issue with any specific protocol . To achieve this , at the server boot up time , we should list these enabled protocols in derby.log 
Other database are adding support for the popular JSON data interchange format defined by . We could add a VTI to Derby which would unpack a JSON document into a set of rows . We could also add a reverse transformation , which turns a query result into a JSON document . 
Currently when an IDENTITY column reaches its maximum value it will produce an error . For tables that are used as transaction logs or event logs it often makes sense to let the table automatically start over with the first identity value again when the max is reached . This would be similar to the CYCLE option on Oracle s SEQUENCE and as defined in SQL:2003 . And Derby is probably used quite often for this purpose , I guess , perhaps even more than other RDBMSs . At the moment every developer have to program their own logic for this . I propose to introduce the CYCLE option . The idea of CYCLE is based on the assumption that there s been a prior cleanup in the table rows so that it will be possible to re-use ids that have been used previously . If that is not the case - and a rollover happens - then a duplicate value error will occur . In this sense it can be argued that the CYCLE option will trade a certain error for a potential error . Most Derby users would possibly gladly accept such a bargain . In other words : This option will greatly enhance the usability of IDENTITY columns . The current implementation of IDENTITY columns SQL grammar in Derby is a subset of the SQL:2003 standard which is the first of the SQL standards to define IDENTITY columns . Interestingly the standard also defines the CYCLE option but this was never implemented in Derby . Also see ( scroll to T174 ) . In other words : The proposal is simply to implement CYCLE as defined in SQL:2003 . 
As of , Derby has some very basic support for XML that consists of an XML datatype and three operators ( XMLPARSE , XMLSERIALIZE , and XMLEXISTS ) . I would like to enhance this existing functionality and , by doing so , help to move Derby incrementally toward a more usable and more complete XPath/XQuery solution ( with emphasis on incrementally ) . I have attached to this issue a document describing the particular changes that I am looking to make . At a high level , they consist of : 1 ) Making it easier to use the XML operators and datatype from within JDBC ( ex . by implicit parsing/serialization of XML values ) . 2 ) Adding a new operator , XMLQUERY , to allow a user to retrieve the results of an XPath expression ( instead of just determining whether or not the expression evaluates to an empty sequence , which is what XMLEXISTS does ) . 3 ) Making changes to the existing operators to line them up with the SQL/XML 2005 specification , and also to take steps toward my eventual hope of having support for XQuery ( as opposed to just XPath ) in Derby . If anyone has time and interest enough to look at the document and provide feedback , that d be great ... 
I m trying to import data from another Derby database with foreignViews tool because system tables got corrupted somehow ( see ) . Tables contain generated ID columns ( created as GENERATED ALWAYS ) . Importing data and allowing Derby to generate new IDs in generated columns will break relationships between tables ( old tables have counter gaps there due to deletes - IDs won t match ) . For a clean import without breaking DDL information in DB version control I would like to be able to switch between generated types as follows : ALTER TABLE table1 ALTER COLUMN col1 SET GENERATED BY DEFAULT ALTER TABLE table1 ALTER COLUMN col1 SET GENERATED ALWAYS 
ReuseFactory used to help reduce object allocations when converting numbers/booleans from primitive types to object types . After and , the ReuseFactory methods are just wrappers around standard library methods such as Integer.valueOf ( ) and Long.valueOf ( ) . Callers could just as easily call the corresponding valueOf ( ) method directly , or rely on auto-boxing . Both ways use the same cache as ReuseFactory currently does , so ReuseFactory has no purpose anymore . One exception : ReuseFactory.getZeroLenByteArray ( ) is still used and provides value , as it avoids the allocation cost when an empty byte array is needed . The ArrayUtil class is probably just as good a home for it , so I propose we move it there and remove the ReuseFactory class . 
Add an MBean that allows users to monitor Derby s CacheFactory instances , as discussed in . 
This JIRA issue was used to track the miscellaneous work I did while producing the 10.13.1. release . 
When Derby is completely disk bound , threads might be starved in RAFContainer.readPage ( ) . This is a real problem when multiple clients are repeatedly accessing one or a small number of large tables . In cases like this , I have observed very high maximum response times ( several minutes in the worst cases ) on simple transactions . The average response time is not affected by this . The starvation is caused by a synchronized block in RAFContainer.readPage ( ) : synchronized ( this ) { fileData.seek ( pageOffset ) ; fileData.readFully ( pageData , 0 , pageSize ) ; } If many threads want to read pages from the same file , there will be a long queue of threads waiting for this monitor . Since the Java specification does not guarantee that threads waiting for monitors are treated fairly , some threads might have to wait for a long time before they get the monitor . ( Usually , a couple of threads get full throughput while the others have to wait . ) 
There must be an entry in the SYSSTATISTICS table in order for the cardinality statistics in SYSSTATISTICS to be created with SYSCS_UTIL.SYSCS_COMPRESS_TABLE SYSCS_UTIL.SYSCS_COMPRESS_TABLE should create statistics if they don t exist . The only way to create them if the index was created on an empty table , seems to be to drop and recreate the index after the table has been loaded . The documentation will also need updating if this change is made . 
DRDAConnThread : :doneData currently creates a new SQLException object that is passed to the writeSQLCAGRP ( ) method . Profiling shows that the creation of these Exception objects takes a lot of CPU because the Throwable constructor will call fillInStackTrace ( ) , which is expensive . This is unnecessary since the exception is not being thrown , and the stack trace is never used . It would be sufficient to keep a static SQLException instance in DRDAConnThread that could be reused each time doneData ( ) is called ( this is also suggested by the following comment in the code : // sqlstate 02000 for end of data . // RESOLVE : Need statics for sqlcodes . Assigning to Knut-Anders , since he has the patch ready in his sandbox . 
Derby s handling of union subqueries in from list can be improved by materializing invariant resultsets once , rather than creating them many times . For example : create view V1 as select i , j from T1 union select i , j from T2 ; create view V2 as select a , b from T3 union select a , b from T4 ; insert into T1 values ( 1,1 ) , ( 2,2 ) , ( 3,3 ) , ( 4,4 ) , ( 5,5 ) ; For a query like select * from V1 , V2 where V1.j = V2.b and V1.i in ( 1,2,3,4,5 ) , it is possible the resultset for V2 is created 5 times . ( assuming V2 is choosen as the the inner table ) This can be very costly if the underlying selects can take long time and also may perform union many times . Enhance materialization logic in setOperatorNode.java . It currently returns FALSE always . public boolean performMaterialization ( JBitSet outerTables ) throws StandardException { // RESOLVE - just say no to materialization right now - should be a cost based decision return false ; / * Actual materialization , if appropriate , will be placed by our parent PRN . This is because PRN might have a join condition to apply . ( Materialization can only occur before that . * / //return true ; } 
Derby currently serializes accesses to a data file . For example , the implementation of RAFContainer.readPage is as follows : synchronized ( this ) { // this is a FileContainer , i.e . a file object fileData.seek ( pageOffset ) ; // fileData is a RandomAccessFile fileData.readFully ( pageData , 0 , pageSize ) ; } I have experiemented with a patch where I have introduced several file descriptors ( RandomAccessFile objects ) per RAFContainer . These are used for reading . The principle is that when all readers are busy , a readPage request will create a new reader . ( There is a maximum number of readers . ) With this patch , throughput was improved by 50 % on linux . For more discussion on this , see The challenge with the suggested approach is to make a mechanism to limit the number of open file descpriptors . Mike Matrigali has suggested to use the existing CacheManager infrastructure for this purpose . For a discussion on that , see : 
See the new RowPosition in GenericConglomerateController.delete and replace . The fetch methods also have new RowPosition calls but in general these calls are not made for scans . Can a single RowPosition object be re-used across the life of the query ? In general Derby should try to avoid creating objects per row in any statement as the gc overhead when operating on a large number of rows can be high . 
The fields columnGotUpdated and copyOfDatabaseRow are allocated and maintained in EmbedResultSet for every ResultSet but they are only required if the result set is updateable . I saw a 5 % improvement in execution rate on a simple VALUES clause when fixing this . 
Forward-only result sets that are exhausted should be implicitly closed on the server . This way , ResultSet.close ( ) does not have to send an explicit close message generating unnecessary network traffic . The DRDA protocol supports this . From the description of OPNQRY ( open query ) : The qryclsimp parameter controls whether the target server implicitly closes a non-scrollable query upon end of data ( SQLSTATE 02000 ) in association with the cursor type . 
A similar change to setBinaryStreamInternal is being handled as part of . 
Instead of copying the entire source tree to generate the javadoc , use Ant s nested filesets in the javadoc target to specify the source files . 
This select would return an error syntax on finding ( after month if group by clause : select idissue , month ( creation ) , year ( creation ) , count ( distinct idissue ) where group by idissue , month ( creation ) , year ( creation ) 
Improve performance of page allocation by no longer requiring sync of the allocated pages . The reason for the sync currently is so that during redo recovery we did not have to handle the case of redoing the allocate and finding no space available on the disk . During tasks like single user load this sync allocation is a significant performance issue - in the case where one does 1000 insert per transaction the total time aproaches very close to durability=test where no syncing is done in the whole system . For instance in a test of loading a 150 meg db with a number of tables and indexes , on a 1.7 ghz laptop with one ide disk , sun jdk1.4.2 , windows XP i got the following types of results : default system : 6 minutes no alloc sync : 1 minute , 43 seconds no syncing : 1 minute , 1 second Once syncing is disabled then recovery must be enhanced to handle the possible out of space condition . Recovery already handles out of space during undo , so this will just handle the error the same as that . It will stop recovery and produce an error indicating that user must add disk space and rerun recovery before system can be brought online . I actually think on most JVM s/OS s this won t happen as the system will still be asking the JVM/OS for the space , just not syncing the values in those pages to disk . Most OS s at this point will reserve the space in the file tables and not let others grab that space . This project will make sure that out of space recovery path has tests . It will also make sure that redo recovery encountering garbage in a newly allocated page during redo of an create page is tested . 
ContextManagers are used extensively , particularly to get access to ExcutionContexts and StatementContexts . Optimizing this access should improve overall performance . 
As described in the JDBC 4 spec , sections 21 and 3.1 . 
As described in the JDBC 4 spec , sections 11.2 , 11.7 , and 3.1 . These are the methods which let app servers listen for connection and statement closure and invalidation events . Section 11.2 of the JDBC 4 spec explains connection events : Connection pool managers which implement the ConnectionEventListener interface can register themselves to listen for connectionClosed and fatal connectionErrorOccurred events . App servers can use these events to help them manage the recycling of connections back to the connection pool . Section 11.7 of the JDBC 4 spec explains statement events : Statement pools which implement StatementEventListener can register themselves to listen for statementClosed and statementErrorOccurred events . Again , this helps statement pools manage the recycling of statements back to the pool . 
Removed deprecated jdk16 variable from the machinery which builds javadoc The jdk16 variable is no longer needed for Derby compilation and has been deprecated as part of . We no longer need the jdk14 variable for building javadoc either . 
Currently , the Network Server and Network Client use a 32K blocksize when returning database data in QRYDTA blocks . I came across the following statement in the DRDA spec ( Volume 1 , page 12 ) : Blocking applies only to the QRYDTA reply objects . Each query block is a QRYDTA DSS . The maximum query block size value allowed in the qryblksz parameter is increased from 32K to 10M , thus accomodating the larger data volumes required by modern , more data-intensive applications . The importance of larger block sizes depends strong on the application profile .. For example , many applications perform single-record selects , and they are not influenced by the block size . But for some applications , it seems like the ability to use a larger block size could be quite valuable . 
Incomplete log record write that occurs because of an out of order partial writes gets recognized as complete during recovery if the first sector and last sector happens to get written . Current system recognizes incompletely written log records by checking the length of the record that is stored in the beginning and end . Format the log records are written to disk is : -- -- -- -- - -- -- -- -- -- - -- -- -- -- -- -- -- -- - length LOG RECORD length -- -- -- -- - -- -- -- -- -- - -- -- -- -- -- -- -- -- - This mechanism works fine if sectors are written in sequential manner or log record size is less than 2 sectors . I believe on SCSI types disks order is not necessarily sequential , SCSI disk drives may sometimes do a reordering of the sectors to optimize the performance . If a log record that spans multiple disk sectors is being written to SCISI type of devices , it is possible that first and last sector written before the crash ; If this occurs recovery system will incorrectly interpret the log records was completely written and replay the record . This could lead to recovery errors or data corruption . - This problem also will not occur if a disk drive has write cache with a battery backup which will make sure I/O request will complete . 
Currently in the client , if userid and password are set in the connection url , the default security mechanism is upgraded to USRIDPWD ( which is clear text userid and password ) . This seems to be a security hole here . Current client driver supports encrypted userid/password ( EUSRIDPWD ) via the use of DH key-agreement protocol - however current Open Group DRDA specifications imposes small prime and base generator values ( 256 bits ) that prevents other JCE s ( apt from ibm jce ) to be used as java cryptography providers . Some thoughts : & # 8211 ; client can make a check to see if it the jvm it is running in supports the encryption necessary for EUSRIDPWD . If it supports , then the client can upgrade to EUSRIDPWD . & # 8211 ; if the jvm the client is running is , doesnt support encryption requirements for EUSRIDPWD , then the security mechanism will be set to USRIDPWD . & # 8211 ; will add support for strong userid and password which is another option to send encrypted passwords across the wire . When this gets added , maybe this can be considered as one of the upgrade options after EUSRIDPWD . 
Dan Debrunner posted a fix to allow for relaxed durability changes in 1 ) Need to add this option in Derby maybe as some property 2 ) Also from discussions on the list , Mike suggested that that the logging system be changed to somehow record that the database has operated in this manner , so that if the database goes corrupt we don t waste effort trying to figure out what when wrong . Probably need some way to mark the log records , the log control file and write a message to the user error log file . 
In reviewing the Network Server Code and profiling there were several areas that showed potential for providing performance improvement . Functions that need optimizing to prevent unneeded object creation and excessive decoding parsePKGNAMCSN ( ) parseSQLDTA_work ( ) buildDB2CursorName ( ) In DDMWriter and DDMReader , use System Routines in java.util.Arrays and System.arrayCopy instead of writing code to do functions like copy arrays and pad strings . 
Improve testing of network server by increasing tests to cover 100 % of the classes and increase the method level coverage . The current code coverage ( class/method ) for network server based on svn revision 208786 are : org.apache.derby.drda.NetworkServercontrol.java - 100/72 org.apache.derby.impl.drda - 97/78 Details of code coverage from EMMA tool are available from Derby Wiki . Links are : 
If a query contains a GROUP BY or HAVING clause , the parser rewrites the abstract syntax tree , putting aggregates into a subselect and treating the HAVING clause as the WHERE clause of a fabricated outer select from the subquery . This allows the compiler to re-use some machinery since the HAVING clause operates on the grouped result the way that the WHERE clause operates on the from list . Unfortunately , this rewriting creates an explosion of special cases in the compiler after parsing is done . The rewriting is not systematically handled later on in the compiler . This gives rise to defects like bug 280 . We need to eliminate this special rewriting and handle the HAVING clause in a straightforward way . This is not a small bugfix but is a medium sized project . 
To allow creation and modification of databases in-memory without requiring disk access or space to store the database . 
Nicolas Dufour in an email thread titled functions and list started on November 2 , 2005 requests the ability to create user defined aggregates . This functionality used to be in Cloudscape . It was disabled presumably because it was considered non-standard . However , most of the machinery needed for this feature is still in the code . We should re-enable user defined aggregates after we agree on acceptable syntax . 
Before Java 6 , files created by Derby would have the default permissions of the operating system context . Under Unix , this would depend on the effective umask of the process that started the Java VM . In Java 6 and 7 , there are methods available that allows tightening up this ( File.setReadable , setWritable ) , making it less likely that somebody would accidentally run Derby with a too lenient default . I suggest we take advantage of this , and let Derby by default ( in Java 6 and higher ) limit the visibility to the OS user that starts the VM , e.g . on Unix this would be equivalent to running with umask 0077 . More secure by default is good , I think . We could have a flag , e.g . derby.storage.useDefaultFilePermissions that when set to true , would give the old behavior . 
This enhancement extends Derby with EXPLAIN functions . Users want to have more feedback than they ` re getting with the current RuntimeStatistics facility . This extension is based on the RuntimeStatistics/ResultSetStatistics functions / classes . 
Set correct collation type and derivation for result character string types from CHAR , VARCHAR and XMLSERIALIZE functions As per the wiki page , assign correct collation type for results of CHAR , VARCHAR and XMLSERIALIZE functions . The rule as copied from the wiki page is as follows 6 ) CHAR , VARCHAR and XMLSERIALIZE functions do not look like they are defined in the SQL spec . Their behavior can be defined as similar to CAST ie , the result character string of CHAR/VARCHAR/XMLSERIALIZE will have the same collation as current schema s character set . The collation derivation will be implicit . Set correct collation type and derivation for result from user defined functions that return character string type . As per the wiki page , assign correct collation type for results of user defined functions . The rule as copied from the wiki page is as follows 7 ) For user defined functions that return character string type , the return type s collation will have the same collation as of the character set of the schema that the function is defined in . The collation derivation will be implicit . Set correct collation type and derivation for result character string types from CURRENT ISOLATION , CURRENT_USER , SESSION_USER , SYSTEM_USER , CURRENT SCHEMA and CURRENT SQLID . As per the wiki page , assign correct collation type for esult character string types from CURRENT ISOLATION , CURRENT_USER , SESSION_USER , SYSTEM_USER , CURRENT SCHEMA and CURRENT SQLID . The rule as copied from the wiki page is as follows 9 ) For CURRENT_USER , SESSION_USER , SYSTEM_USER , SQL spec Section 6.4 Syntax Rule 4 says that their collation type is the collation of character set SQL_IDENTIFIER . In Derby s case , that will mean , the collation of these functions will be UCS_BASIC . The collation derivation will be implicit . 10 ) CURRENT ISOLATION , CURRENT SCHEMA and CURRENT SQLID seem to be Derby specific functions , I didn t find them in the SQL spec . But in order to match their behavior with the other functions covered in 9 ) above , their return character string type s collation will be the collation of character set SQL_IDENTIFIER . The collation derivation will be implicit . 
Adding support for truncate table command will aid to portability 
Hi , I m on the Apache Roller team and we use database migration scripts to update databases between Roller releases . ( We have a common template ( ) that is run through Velocity to create specific scripts for the several databases that we support . ) One handicap with Derby that we re not seeing with other databases is its inability to rename tables that have FK s on them . Renaming one of our tables returns this error from Derby : rename table website to weblog ; Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT WP_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 30000 Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT WE_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 99999 Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT WC_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 99999 Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT FO_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 99999 Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT MF_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 99999 Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT NF_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 99999 Error : Operation RENAME can not be performed on object SQL140718163851800 because CONSTRAINT AP_WEBSITEID_FK is dependent on that object . SQLState : X0Y25 ErrorCode : 99999 This results in the migration scripts needing to be messy , first dropping all constraints before recreating them , for the one RDBMS that requires it . It would be great if a future release of Derby could be coded to support table renames regardless of the constraints defined on it . Thanks ! 
Running scalability tests with the client and buffer manager from shows that access to the TransactionTable.trans ( a Hashtable ) and XactFactory.tranId ( a shared long ) are the next major sources of contention . 
Network client : Add support for scrollable , updatable , insensitive result sets This is a part of the effort . 
The Open Web Application Security Project has some suggestions on how to make it harder for an attacker to crack hashed passwords : The builtin authentication service doesn t follow all the suggestions . In particular , it doesn t add a random salt , and it only performs the hash operation once . I propose that we add two new properties that makes it possible to configure builtin to use a random salt and run multiple iterations of the hash operation : derby.authentication.builtin.saltLength - the length of the random salt to add ( in bytes ) derby.authentication.builtin.iterations - the number of times to perform the hash operation I d also suggest that we set the defaults so that random salt and multiple iterations are used by default . The OWASP page mentions 64 bits of salt ( 8 bytes ) and a minimum of 1000 iterations . I consulted a security expert who thought that these recommendations sounded OK , but he believed the recommended salt length was likely to be revised and suggested 16 bytes instead . The only price we pay by going from 8 to 16 bytes , is that we ll need to store 8 bytes extra per user in the database , so I don t see any reason not to set the default for derby.authentication.builtin.saltLength as high as 16 . Setting the default for derby.authentication.builtin.iterations to 1000 will make authentication of a user somewhat slower ( which is the point , really ) , but experiments on my machine suggest that running our default hash function ( SHA-256 ) 1000 times takes around 1 ms . Since authentication only happens when establishing a new connection to the database , that would be a negligible cost , I think . If saltLength is set to 0 and iterations is set to 1 , the hashing will be done in the exact same way as in previous versions . Both of the properties should only be respected when the data dictionary version is 10.9 or higher , so that users in soft-upgraded databases can still log in after a downgrade . 
Allow unique constraint over keys which include one or more nullable fields . Prior to this change Derby only supported unique constraints on keys that included no nullable columns . The new constraint will allow unlimited inserts of any key with one more null columns , but will limit insert of keys with no null columns to 1 unique value per table . There is no change to existing or newly created unique indexes on null columns ( as opposed to unique constraints on null columns ) . Also there is no change to existing or newly created constraints on keys with no nullable columns . 
The ISO/IEC 9075-2:1999 SQL standard describes two kinds of CASE expressions : simple case and searched case . The current Derby version supports searched case but not simple case . The syntax for simple case is : CASE Expression WHEN Expression THEN Expression ELSE ElseExpression END Example : VALUES CASE 4 WHEN 1 THEN one WHEN 2 THEN two WHEN 3 THEN three ELSE many END 
Currently , Derby doesn t allow ORDER BY nested in a set operand , e.g . in the following construct : ( select i from t1 order by j offset 1 row ) union ( select i from t2 order by j desc offset 2 rows ) This is allowed by the standard , as far as I can understand , cf . this quote from section 7.12 in SQL 2011 : & lt ; query expression body & gt ; : := & lt ; query term & gt ; & lt ; query expression body & gt ; UNION & lt ; query term & gt ; & lt ; query expression body & gt ; EXCEPT & lt ; query term & gt ; & lt ; query term & gt ; : := & lt ; query primary & gt ; & lt ; query term & gt ; INTERSECT & lt ; query primary & gt ; & lt ; query primary & gt ; : := & lt ; simple table & gt ; & lt ; left paren & gt ; & lt ; query expression body & gt ; & lt ; right paren & gt ; I.e . the left paren chooses the second alternative in the production for & lt ; query primary & gt ; . 
The XAResource interface provides function setTransactionTimeout which is currently not supported in derby . When client application uses client driver to connect to derby database and the application crashes outside the unit of work of XA transaction and the transaction is not committed or rolled back yet the locks held by the transaction will not be released . 
Extend the existing functionality for adding Java functions to SQL to support functions with multiple arguments . 
Consider a simple case of - A table tbl has 10000 rows , there is a primary key index on i1 and the query in question is select * from tbl where i1 in ( -1,100000 ) derby does a table scan of the entire table even though the IN list has only two values and the comparison is on a field that has an index . Briefly looking at the code , it seems like we insert a between and use the IN list to get the start and stop values for the scan . Thus the range of the values in the IN list here plays an important role . Thus if the query was changed to select * from tbl where i1 in ( -1 , 1 ) , an index scan would be chosen . It would be nice if we could do something clever in this case where there is clearly an index on the field and the number of values in the IN list is known . Maybe use the rowcount estimate and the IN list size to do some optimizations . consider the length of the IN list to do searches on the table . ie use the IN list values to do index key searches on the table , -or try to convert it to a join . Use the IN list values to create a temporary table and do a join . It is most likely that the optimizer will choose the table with IN list here as the outer table in the join and thus will do key searches on the larger table . -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - some query plans that I logged using derby.language.logQueryPlan=true for some similar queries : Table has ascending values from 0 - 9999 for i1 . primary key index on i1 . GMT Thread & # 91 ; UT0,5 , main & # 93 ; ( XID = 19941 ) , ( SESSIONID = 0 ) , select * from scanfixed where i1 in ( -1,9999,9998,9997,9996,9995,9994,9993,9992,9991,9990 ) * * * * * * * Project-Restrict ResultSet ( 2 ) : Number of opens = 1 Rows seen = 10000 Rows filtered = 9990 restriction = true projection = false constructor time ( milliseconds ) = 0 open time ( milliseconds ) = 0 next time ( milliseconds ) = 0 close time ( milliseconds ) = 0 restriction time ( milliseconds ) = 0 projection time ( milliseconds ) = 0 optimizer estimated row count : 750.38 optimizer estimated cost : 8579.46 Source result set : Table Scan ResultSet for SCANFIXED at read committed isolation level using instantaneous share row locking chosen by the optimizer Number of opens = 1 Rows seen = 10000 Rows filtered = 0 Fetch Size = 16 constructor time ( milliseconds ) = 0 open time ( milliseconds ) = 0 next time ( milliseconds ) = 0 close time ( milliseconds ) = 0 next time in milliseconds/row = 0 scan information : Bit set of columns fetched=All Number of columns fetched=9 Number of pages visited=417 Number of rows qualified=10000 Number of rows visited=10000 Scan type=heap start position : null stop position : null qualifiers : Column & # 91 ; 0 & # 93 ; & # 91 ; 0 & # 93 ; Id : 0 Operator : & lt ; = Ordered nulls : false Unknown return value : false Negate comparison result : false Column & # 91 ; 0 & # 93 ; & # 91 ; 1 & # 93 ; Id : 0 Operator : & lt ; Ordered nulls : false Unknown return value : true Negate comparison result : true optimizer estimated row count : 750.38 optimizer estimated cost : 8579.46 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - l 2004-10-14 18:59:47.577 GMT Thread & # 91 ; UT0,5 , main & # 93 ; ( XID = 19216 ) , ( SESSIONID = 0 ) , select * from scanfixed where i1 in ( 9999,9998,9997,9996,9995,9994,9993,9992,9991,9990 ) * * * * * * * Project-Restrict ResultSet ( 3 ) : Number of opens = 1 Rows seen = 10 Rows filtered = 0 restriction = true projection = true constructor time ( milliseconds ) = 0 open time ( milliseconds ) = 0 next time ( milliseconds ) = 0 close time ( milliseconds ) = 0 restriction time ( milliseconds ) = 0 projection time ( milliseconds ) = 0 optimizer estimated row count : 4.80 optimizer estimated cost : 39.53 Source result set : Index Row to Base Row ResultSet for SCANFIXED : Number of opens = 1 Rows seen = 10 Columns accessed from heap = { 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 } constructor time ( milliseconds ) = 0 open time ( milliseconds ) = 0 next time ( milliseconds ) = 0 close time ( milliseconds ) = 0 optimizer estimated row count : 4.80 optimizer estimated cost : 39.53 Index Scan ResultSet for SCANFIXED using index SCANFIXEDX at read committed isolation level using instantaneous share row locking chosen by the optimizer Number of opens = 1 Rows seen = 10 Rows filtered = 0 Fetch Size = 16 constructor time ( milliseconds ) = 0 open time ( milliseconds ) = 0 next time ( milliseconds ) = 0 close time ( milliseconds ) = 0 next time in milliseconds/row = 0 scan information : Bit set of columns fetched=All Number of columns fetched=2 Number of deleted rows visited=0 Number of pages visited=2 Number of rows qualified=10 Number of rows visited=10 Scan type=btree Tree height=2 start position : & gt ; = on first 1 column ( s ) . Ordered null semantics on the following columns : stop position : & gt ; on first 1 column ( s ) . Ordered null semantics on the following columns : qualifiers : None optimizer estimated row count : 4.80 optimizer estimated cost : 39.53 
Implement new JDBC 4 metadata API getFunctionParameters ( ) FI am currently implementing this to return an empty result set so at least we re compliant , but we should be able to provide real metadata here . 
As described in the JDBC 4 spec , sections 13.1 and 3.1 . This adds support for new Statement methods added by JDBC4 and not addressed by other JIRAs : isClosed ( ) and getResultSetHoldability ( ) . 
It would be good for Derby to have built in Language based ordering based on locale specific Collator . Language based ordering is an important feature for international deployment . offers one implementation option for this but according to the discussion in that issue National Character Types carry a fair amount of baggage with them especially in the form of concerns about conversion to and from datetime and number types . Rick mentioned SQL language for collations as an option for language based ordering . There may be other options too , but I thought it worthwhile to add an issue for the high level functional concern , so the best choice can be made for implementation without assuming that National Character Types is the only solution . For possible 10.1 workaround and examples see : If a user specifies a create database url like following jdbc : derby : abcDB ; create=true ; collation=TERRITORY_BASED we should ignore the collation attribute because no territory attribute has been specified . The current Derby 10.3 code does not do that . Once this is fixed , we should add a test case for it in CollationTest.java 
A statement created or prepared in a local transaction using the default holdability settting can not be used in a global transaction . This is because the default holdability is HOLD_CURSORS_OVER_COMMIT and Derby does not support held cursors in global transactions . The change would be to allow those statements to be used in global transactions , returning ResultSets that are CLOSE_CURSORS_AT_COMMIT . This would allow such statements to be cached by application servers . JDBC 3.0 section 12.6 does allow holdability to be handled in an implementation specific way for global transactions . I believe this change is forward compatible if Derby ever does start supporting held cursors in global transactions ( pretty unlikely ) since the functionality displayed is a ub-set of the holdable functionality . 
RAMTransaction.addColumnToConglomerate ( ) contains this piece of code : // remove old entry in the Conglomerate directory , and add new one if ( tempCongloms ! = null ) tempCongloms.remove ( new Long ( conglomId ) ) ; tempCongloms.put ( new Long ( conglomId ) , conglom ) ; 1 . According to the code coverage report ( ) these lines are not tested . If possible , a test that covers them should be added to the regression suite . 2 . The null check looks either unnecessary ( seems to be the case after a brief inspection of the code ) , or incomplete since the last line will throw a NullPointerException regardless of the check if tempCongloms is null . 3 . The call to remove ( ) before put ( ) is redundant , since HashMap.put ( ) will remove the old mapping implicitly . 4 . It seems to me that the object that is put into the HashMap always is the same as the one that is removed , so perhaps all these lines could be deleted . 
Get rid of the finalizer in EmbedResultSet Finalizers make object allocation and garbage collection more expensive and should be avoided for object types that are frequently allocated . EmbedResultSet has a finalize ( ) method and is frequently allocated for many types of load . The following comment was posted on : By commenting out the finalize ( ) method in EmbedResultSet , the time to run the test on my laptop is reduced from ~3 seconds to ~1.3 seconds ( OpenSolaris , Sun Java SE 6 ) . Since the finalizer is a no-op if the activation isn t single-use , I think we should investigate whether there are ways to avoid this cost for ResultSets that don t need the finalizer . 
From an e-mail discussion : think in this case system/database owners are trusting the database system to ensure that their system can not be attacked . So maybe if Derby is booted as a standalone server with no security manager involved , it should install one with a default security policy . Thus allowing Derby to use Java security manager to manage system privileges but not requiring everyone to become familiar with them . I imagine such a policy would allow any access to databases under derby.system.home and/or user.home . By standalone I mean the network server was started though the main ( ) method ( command line ) . 
We should write a test to verify that the metadata is correct for each release for all hard-upgrade trajectories which terminate in that release . The test should examine all system tables . Note that if there are N releases , then there will ( 2 & lt ; sup & gt ; N & lt ; /sup & gt ; - N ) - 1 trajectories to examine . 
Now that has been resolved , users can update statistics , but once they do , they are committed to using and maintaining the statistics , even if it doesn t improve performance or they have difficulty maintaining the statistics on a regular basis . It would be good to have a way to drop statistics information so that users could revert to the prior behavior if needed . 
