Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop accumulo import java io Exception import java math Big Decimal import java math Big Integer import java util Collection import java util Collections import java util Hash Map import java util Iterator import java util Linked List import java util List import java util Map import java util Map Entry import java util Properties import java util concurrent Time Unit import org apache accumulo core client Accumulo Security Exception import org apache accumulo core client Batch Writer Config import org apache accumulo core client mapreduce Accumulo Input Format import org apache accumulo core client mapreduce Accumulo Output Format import org apache accumulo core client security tokens Password Token import org apache accumulo core data Key import org apache accumulo core data Mutation import org apache accumulo core data Range import org apache accumulo core data Value import org apache accumulo core security Authorizations import org apache accumulo core util Pair import org apache commons cli Command Line import org apache commons cli Parse Exception import org apache commons lang String Utils import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache hadoop io Text import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache pig Load Caster import org apache pig Load Func import org apache pig Load Store Caster import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Store Func Interface import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig backend hadoop hbase Base Binary Converter import org apache pig builtin Utf Storage Converter import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl util Object Serializer import org apache pig impl util Context import org joda time Date Time Load Store Func for retrieving data from and storing data to Accumulo Key Val pair will be returned as tuples key colfam colqual colvis timestamp value All fields except timestamp are Data Byte Array timestamp is a long Tuples can be written in forms key colfam colqual colvis value key colfam colqual value public abstract class Abstract Accumulo Storage extends Load Func implements Store Func Interface private static final Log log Log Factory get Log Abstract Accumulo Storage class protected static final char protected static final String private static final String Accumulo Input Format class get Simple Name private static final String Accumulo Output Format class get Simple Name private final static String Storage Converter private final static String Accumulo Binary Converter private final static String pig accumulo caster protected final Accumulo Storage Options storage Options protected final Command Line command Line private Record Reader Key Value reader private Record Writer Text Mutation writer protected String inst protected String zookeepers protected String user protected String password protected String table protected Text table Name protected Authorizations authorizations protected List Column columns protected String start null protected String end null Defaults from Batch Writer Config protected int max Write Threads protected long max Mutation Buffer Size l protected long max Latency Long protected String column Separator protected boolean ignore Whitespace true protected Load Store Caster caster protected Resource Schema schema protected String context Signature null public Abstract Accumulo Storage String columns String args throws Parse Exception Exception storage Options new Accumulo Storage Options command Line storage Options get Command Line args Extract any command line args extract Args command Line storage Options Split out the user provided columns parse Columns columns Initializes link column Defs and splits columns on link param columns of columns private void parse Columns String column Str columns new Linked List Column if ignore Whitespace column Str String Utils strip column Str if column Str is Empty for String column String Utils split column Str column Separator columns add new Column ignore Whitespace String Utils strip column column else Preserve original functionality for empty columns to fetch all data in a map columns add new Column Extract arguments passed into the constructor to avoid the param cli param opts protected void extract Args Command Line cli Accumulo Storage Options opts throws Exception if opts has Authorizations cli authorizations opts get Authorizations cli this start cli get Option Value Accumulo Storage Options get Opt null this end cli get Option Value Accumulo Storage Options get Opt null if cli has Option Accumulo Storage Options get Opt this max Latency opts get Int cli Accumulo Storage Options if cli has Option Accumulo Storage Options get Opt this max Write Threads opts get Int cli Accumulo Storage Options if cli has Option Accumulo Storage Options get Opt this max Mutation Buffer Size opts get Long cli Accumulo Storage Options Properties client System Props Context get Context get Client System Props String default Caster if null client System Props default Caster client System Props get Property default Caster String caster Option cli get Option Value caster default Caster if equals Ignore Case caster Option caster new Utf Storage Converter else if equals Ignore Case caster Option caster new Base Binary Converter else try caster Load Store Caster Pig Context instantiate Func From Spec caster Option catch Class Cast Exception e log error Configured caster does not implement Load Caster interface throw new Exception e catch Runtime Exception e log error Configured caster class not found e throw new Exception e log debug Using caster caster get Class if cli has Option Accumulo Storage Options get Opt column Separator cli get Option Value Accumulo Storage Options get Opt if cli has Option Accumulo Storage Options get Opt String value cli get Option Value Accumulo Storage Options get Opt if false equals Ignore Case value ignore Whitespace false else if true equals Ignore Case value ignore Whitespace true else log warn Ignoring unknown value for Accumulo Storage Options get Opt value protected Command Line get Command Line return command Line protected Map String String get Input Format Entries Configuration conf return get Entries conf protected Map String String get Output Format Entries Configuration conf return get Entries conf Removes the given values from the configuration accounting for changes in the Configuration given the version of Hadoop being used param conf param entries To Unset protected void unset Entries From Configuration Configuration conf Map String String entries To Unset boolean configuration Has Unset true try conf get Class get Method unset String class catch No Such Method Exception e configuration Has Unset false catch Security Exception e configuration Has Unset false Only Hadoop and actually contains the method Configuration unset if configuration Has Unset simple Unset conf entries To Unset else If we re running on something else we have to remove everything and re add it clear Unset conf entries To Unset Unsets elements in the Configuration using the unset method param conf param entries To Unset protected void simple Unset Configuration conf Map String String entries To Unset for String key entries To Unset key Set conf unset key Replaces the given entries in the configuration by clearing the Configuration and re adding the elements that are n t in the Map of entries to unset param conf param entries To Unset protected void clear Unset Configuration conf Map String String entries To Unset Gets a copy of the entries Iterator Entry String String original Entries conf iterator conf clear while original Entries has Next Entry String String original Entry original Entries next Only re set the pairs that are n t in our collection of keys to unset if entries To Unset contains Key original Entry get Key conf set original Entry get Key original Entry get Value Override public Tuple get Next throws Exception try load the next pair if reader next Key Value return null Key key Key reader get Current Key Value value Value reader get Current Value assert key null value null return get Tuple key value catch Interrupted Exception e throw new Exception e get Message protected abstract Tuple get Tuple Key key Value value throws Exception Override Suppress Warnings rawtypes public Input Format get Input Format return new Accumulo Input Format Override Suppress Warnings unchecked rawtypes public void prepare To Read Record Reader reader Pig Split split this reader reader private void set Location From Uri String location throws Exception ex accumulo table instance myinstance user root password secret zookeepers auths fetch columns col cq col cq start abc end z String columns auths try if location starts With accumulo throw new Exception Bad scheme String url Parts location split if url Parts length for String param url Parts split String pair param split if pair equals instance inst pair else if pair equals user user pair else if pair equals password password pair else if pair equals zookeepers zookeepers pair else if pair equals auths auths pair else if pair equals fetch columns columns pair else if pair equals start start pair else if pair equals end end pair else if pair equals write buffer size bytes max Mutation Buffer Size Long parse Long pair else if pair equals write threads max Write Threads Integer parse Int pair else if pair equals write latency ms max Latency Long parse Long pair String parts url Parts split table parts table Name new Text table if null authorizations auths null authorizations new Authorizations else authorizations new Authorizations String Utils split auths if String Utils is Empty columns parse Columns columns catch Exception e throw new Exception Expected accumulo table instance instance Name user user password password zookeepers zookeepers auths authorizations start start Row end end Row fetch columns cf cq cf cq write buffer size bytes write threads write latency ms e get Message protected Record Writer Text Mutation get Writer return writer Extract elements from the Configuration whose keys match the given prefix param conf param prefix return protected Map String String get Entries Configuration conf String prefix Map String String entries new Hash Map String String for Entry String String entry conf String key entry get Key if key starts With prefix entries put key entry get Value return entries Override public void set Location String location Job job throws Exception set Location From Uri location load Dependent Jars job get Configuration Map String String entries get Input Format Entries job get Configuration unset Entries From Configuration job get Configuration entries try Accumulo Input Format set Connector Info job user new Password Token password catch Accumulo Security Exception e throw new Exception e Accumulo Input Format set Input Table Name job table Accumulo Input Format set Scan Authorizations job authorizations Accumulo Input Format set Zoo Keeper Instance job inst zookeepers List Pair Text Text input Format Columns new Linked List Pair Text Text int colfam Prefix for Column c columns switch c get Type case Pull the colf colq individually input Format Columns add make Pair c get Column Family c get Column Qualifier break case Some colfams colfam Prefix break case Some colquals in a given colfam input Format Columns add make Pair c get Column Family null break default log info Ignoring unhandled column type break If we have colfam prefixes we have to pull all columns and filter on client side Create an iterator that lets us push down all of the filter logic if colfam Prefix input Format Columns is Empty Accumulo Input Format fetch Columns job input Format Columns Collection Range ranges Collections singleton new Range start end log info Scanning Accumulo for ranges for table table Accumulo Input Format set Ranges job ranges configure Input Format job protected Pair Text Text make Pair String first String second return new Pair Text Text null first null new Text first null second null new Text second Ensure that Accumulo s dependent jars are added to the Configuration to alleviate the need for clients to dependency jars param job The Mapreduce Job object throws Exception protected void load Dependent Jars Configuration conf throws Exception Thank you Base Utils add Dependency Jars conf org apache accumulo trace instrument Tracer class org apache accumulo core client Instance class org apache accumulo fate Fate class org apache zookeeper Zoo Keeper class org apache thrift Service Client class Method to allow specific implementations to add more elements to the Job for reading data from Accumulo param job protected void configure Input Format Job job Method to allow specific implementations to add more elements to the Job for writing data to Accumulo param job protected void configure Output Format Job job Override public String relative To Absolute Path String location Path cur Dir throws Exception return location Override public void set Context Signature String signature this context Signature signature Store Func methods public void set Store Func Context Signature String signature this context Signature signature Returns Properties based on code context Signature code protected Properties get Properties return Context get Context get Properties this get Class new String context Signature public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return relative To Absolute Path location cur Dir public void set Store Location String location Job job throws Exception set Location From Uri location load Dependent Jars job get Configuration Map String String entries get Output Format Entries job get Configuration unset Entries From Configuration job get Configuration entries try Accumulo Output Format set Connector Info job user new Password Token password catch Accumulo Security Exception e throw new Exception e Accumulo Output Format set Create Tables job true Accumulo Output Format set Zoo Keeper Instance job inst zookeepers Batch Writer Config bw Config new Batch Writer Config bw Config set Max Latency max Latency Time Unit bw Config set Max Memory max Mutation Buffer Size bw Config set Max Write Threads max Write Threads Accumulo Output Format set Batch Writer Options job bw Config log info Writing data to table configure Output Format job Suppress Warnings rawtypes public Output Format get Output Format return new Accumulo Output Format Suppress Warnings rawtypes unchecked public void prepare To Write Record Writer writer this writer writer protected abstract Collection Mutation get Mutations Tuple tuple throws Exec Exception Exception public void put Next Tuple tuple throws Exec Exception Exception Collection Mutation muts get Mutations tuple for Mutation mut muts try get Writer write table Name mut catch Interrupted Exception e throw new Exception e public void cleanup On Failure String failure Job job public void cleanup On Success String location Job job Override public void check Schema Resource Schema s throws Exception if caster instanceof Load Store Caster log error Caster must implement Load Store Caster for writing to Accumulo throw new Exception Bad Caster caster get Class schema s get Properties set Property context Signature schema Object Serializer serialize schema protected Text tuple To Text Tuple tuple int i Resource Field Schema field Schemas throws Exception Object o tuple get i byte type schema To Type o i field Schemas return obj To Text o type protected Text object To Text Object o Resource Field Schema field Schema throws Exception byte type schema To Type o field Schema return obj To Text o type protected byte schema To Type Object o Resource Field Schema field Schema return field Schema null Data Type find Type o field Schema get Type protected byte schema To Type Object o int i Resource Field Schema field Schemas return field Schemas null Data Type find Type o field Schemas i get Type protected byte tuple To Bytes Tuple tuple int i Resource Field Schema field Schemas throws Exception Object o tuple get i byte type schema To Type o i field Schemas return obj To Bytes o type protected Text obj To Text Object o byte type throws Exception byte bytes obj To Bytes o type if null bytes log warn Creating empty text from null value return new Text return new Text bytes Suppress Warnings unchecked protected byte obj To Bytes Object o byte type throws Exception if o null return null switch type case Data Type return Data Byte Array o get case Data Type return caster to Bytes Data Bag o case Data Type return caster to Bytes String o case Data Type return caster to Bytes Double o case Data Type return caster to Bytes Float o case Data Type return caster to Bytes Integer o case Data Type return caster to Bytes Long o case Data Type return caster to Bytes Big Integer o case Data Type return caster to Bytes Big Decimal o case Data Type return caster to Bytes Boolean o case Data Type return caster to Bytes Date Time o The type conversion here is unchecked Relying on Data Type find Type to do the right thing case Data Type return caster to Bytes Map String Object o case Data Type return null case Data Type return caster to Bytes Tuple o case Data Type throw new Exception Unable to determine type of o get Class default throw new Exception Unable to find a converter for tuple field o Override public Load Caster get Load Caster throws Exception return caster 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Data Input import java io Data Output import java io Exception import java util import org apache pig backend hadoop executionengine physical Layer relational Operators Accumulative Tuple Buffer public class Accumulative Bag implements Data Bag private static final long serial Version private transient Accumulative Tuple Buffer buffer private int index public Accumulative Bag Accumulative Tuple Buffer buffer int index this buffer buffer this index index public void add Tuple t throw new Runtime Exception Accumulative Bag does not support add operation public void add All Data Bag b throw new Runtime Exception Accumulative Bag does not support add operation public void clear throw new Runtime Exception Accumulative Bag does not support clear operation public boolean is Distinct return false public boolean is Sorted return false public Accumulative Tuple Buffer get Tuplebuffer return buffer public Iterator Tuple iterator return buffer get Tuples index public void mark Stale boolean stale public long size int size for Iterator Tuple it iterator it has Next it next size return size public long get Memory Size return public long spill return public void read Fields Data Input datainput throws Exception throw new Exception Accumulative Bag does not support read Fields operation public void write Data Output dataoutput throws Exception throw new Exception Accumulative Bag does not support write operation public int compare To Object other throw new Runtime Exception Accumulative Bag does not support compare To operation public boolean equals Object other if this other return true return false public int hash Code assert false hash Code not designed return 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Exception import org apache pig Accumulator import org apache pig Eval Func import org apache pig data Tuple This class is used to provide a free implementation of the Eval Func exec function given implementation of the Accumulator interface Instead of having to provide a redundant implementation this provides the base exec function for free given that the methods associated with the Accumulator interface are implemented For information on how to implement Accumulator see link Accumulator public abstract class Accumulator Eval Func extends Eval Func implements Accumulator Override public abstract void accumulate Tuple b throws Exception Override public abstract void cleanup Override public abstract get Value Override public exec Tuple input throws Exception accumulate input result get Value cleanup return result 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine spark optimizer import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig backend hadoop executionengine spark plan Spark Op Plan Visitor import org apache pig backend hadoop executionengine spark plan Spark Oper Plan import org apache pig backend hadoop executionengine spark plan Spark Operator import org apache pig backend hadoop executionengine util Accumulator Optimizer Util import org apache pig impl plan Depth First Walker import org apache pig impl plan Visitor Exception import java util List visitor to optimize plans that determines if a vertex plan can run in accumulative mode public class Accumulator Optimizer extends Spark Op Plan Visitor public Accumulator Optimizer Spark Oper Plan plan super plan new Depth First Walker Spark Operator Spark Oper Plan plan Override public void visit Spark Op Spark Operator spark Operator throws Visitor Exception Physical Plan plan spark Operator physical Plan List Physical Operator pos plan get Roots if pos null pos size return List Global Rearrange glrs Plan Helper get Physical Operators plan Global Rearrange class for Global Rearrange glr glrs List Physical Operator successors plan get Successors glr Accumulator Optimizer Util add Accumulator plan successors 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop accumulo import java io Byte Array Input Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java math Big Decimal import java math Big Integer import java util Map import org apache accumulo core Constants import org apache pig Load Store Caster import org apache pig Resource Schema Resource Field Schema import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Tuple import org joda time Date Time import com google common base Preconditions Load Store Caster implementation which stores most type implementations as bytes generated from the to String representation with a Charset Pulled some implementations from the Accumulo Lexicoder implementations in public class Accumulo Binary Converter implements Load Store Caster private static final int Integer Byte private static final int Long Byte Override public Data Bag bytes To Bag byte b Resource Field Schema field Schema throws Exception throw new Exec Exception Ca n t generate Data Bags from byte Override public Big Decimal bytes To Big Decimal byte b throws Exception throw new Exec Exception Ca n t generate a Big Integer from byte Override public Big Integer bytes To Big Integer byte b throws Exception Taken from Accumulo s Big Integer Lexicoder in Data Input Stream dis new Data Input Stream new Byte Array Input Stream b int len dis read Int len len x len Math abs len byte bytes new byte len dis read Fully bytes bytes byte x bytes return new Big Integer bytes Override public Boolean bytes To Boolean byte b throws Exception Preconditions check Argument b length return b byte Override public String bytes To Char Array byte b throws Exception return new String b Constants Override public Date Time bytes To Date Time byte b throws Exception String s new String b Constants return Date Time parse s Override public Double bytes To Double byte b throws Exception return Double long Bits To Double bytes To Long b Override public Float bytes To Float byte b throws Exception return Float int Bits To Float bytes To Integer b Override public Integer bytes To Integer byte b throws Exception Preconditions check Argument b length int n for int i i b length i n n b i x return n Override public Long bytes To Long byte b throws Exception Preconditions check Argument b length long l for int i i b length i l l b i x return l Override public Map String Object bytes To Map byte b Resource Field Schema field Schema throws Exception throw new Exec Exception Ca n t generate Map from byte Override public Tuple bytes To Tuple byte b Resource Field Schema field Schema throws Exception throw new Exec Exception Ca n t generate a Tuple from byte Not implemented Override public byte to Bytes Big Decimal bd throws Exception throw new Exception Ca n t generate bytes from Big Decimal Override public byte to Bytes Big Integer bi throws Exception Taken from Accumulo s Big Integer Lexicoder in byte bytes bi to Byte Array byte ret new byte bytes length Data Output Stream dos new Data Output Stream new Fixed Byte Array Output Stream ret flip the sign bit bytes byte x bytes int len bytes length if bi signum len len len len x try dos write Int len dos write bytes finally dos close return ret Override public byte to Bytes Boolean b throws Exception return new byte b byte byte Override public byte to Bytes Data Bag bag throws Exception throw new Exec Exception Cant generate bytes from Data Bag Override public byte to Bytes Data Byte Array a throws Exception return a get Override public byte to Bytes Date Time dt throws Exception return dt to String get Bytes Constants Override public byte to Bytes Double d throws Exception return to Bytes Double double To Raw Long Bits d Override public byte to Bytes Float f throws Exception return to Bytes Float float To Raw Int Bits f Override public byte to Bytes Integer val throws Exception int int Val val int Value byte b new byte for int i i i b i byte int Val int Val b byte int Val return b Override public byte to Bytes Long val throws Exception long long Val val long Value byte b new byte for int i i i b i byte long Val long Val b byte long Val return b Override public byte to Bytes Map String Object m throws Exception throw new Exception Ca n t generate bytes from Map Override public byte to Bytes String s throws Exception return s get Bytes Constants Override public byte to Bytes Tuple t throws Exception throw new Exception Ca n t generate bytes from Tuple 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop accumulo import java io Exception import java util Collection import java util Collections import java util Hash Map import java util Iterator import java util Map import java util Map Entry import java util Sorted Map import org apache accumulo core client Iterator Setting import org apache accumulo core client mapreduce Accumulo Input Format import org apache accumulo core data Key import org apache accumulo core data Mutation import org apache accumulo core data Partial Key import org apache accumulo core data Range import org apache accumulo core data Value import org apache accumulo core iterators user Whole Row Iterator import org apache commons cli Parse Exception import org apache hadoop io Text import org apache hadoop mapreduce Job import org apache log j Logger import org apache pig Resource Schema Resource Field Schema import org apache pig backend executionengine Exec Exception import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory Basic Pig Storage implementation that uses Accumulo as the backing store p When writing data the first entry in the link Tuple is treated as the row in the Accumulo key while subsequent entries in the tuple are handled as columns in that row link Map s are expanded placing the map key in the column family and the map value in the Accumulo value Scalars are placed directly into the value with an empty column qualifier If the columns argument on the constructor is omitted null or the empty String no column family is provided on the Keys created for Accumulo p p When reading data if aggregate Colfams is true elements in the same row and column family are aggregated into a single link Map This will result in a link Tuple of length unique column families for the given row If aggregate Colfams is false column family and column qualifier are concatenated separated by a colon and placed into a link Map This will result in a link Tuple with two entries where the latter element has a number of elements equal to the number of columns in the given row p public class Accumulo Storage extends Abstract Accumulo Storage private static final Logger log Logger get Logger Accumulo Storage class private static final String private static final Text new Text new byte private static final Data Byte Array new Data Byte Array new byte Not sure if Accumulo Storage instances need to be thread safe or not final Text cf Holder new Text cq Holder new Text Creates an Accumulo Storage which writes all values in a link Tuple with an empty column family and does n t group column families together on read creates on link Map for all columns public Accumulo Storage throws Parse Exception Exception this Create an Accumulo Storage with a of columns families to use on write and whether columns in a row should be grouped by family on read param columns comma separated list of column families to use when writing data aligned to the n th entry in the tuple param aggregate Colfams Should unique column qualifier and value pairs be grouped together by column family when reading data public Accumulo Storage String columns throws Parse Exception Exception this columns public Accumulo Storage String column Str String args throws Parse Exception Exception super column Str args Override protected Tuple get Tuple Key key Value value throws Exception Sorted Map Key Value row Vs Whole Row Iterator decode Row key value Tuple tuple Tuple Factory get Instance new Tuple columns size final Text cf Holder new Text final Text cq Holder new Text final Text row key get Row int tuple Offset tuple set tuple Offset new Data Byte Array Text decode row get Bytes row get Length for Column column this columns tuple Offset switch column get Type case cf Holder set column get Column Family if null column get Column Qualifier cq Holder set column get Column Qualifier else cq Holder set Get the key where our literal would exist accounting for colf colq or colf empty colq Key literal Start Key new Key row cf Holder cq Holder Sorted Map Key Value tail Map row Vs tail Map literal Start Key Find the element if tail Map is Empty tuple set tuple Offset else Key actual Key tail Map first Key Only place it in the tuple if it matches the user request avoid using a value from a key with the wrong colqual if literal Start Key compare To actual Key Partial Key tuple set tuple Offset new Data Byte Array tail Map get actual Key get else This row does n t have the column we were looking for tuple set tuple Offset break case cf Holder set column get Column Family Range colfam Prefix Range Range prefix row cf Holder Key colfam Prefix Start Key new Key row cf Holder Sorted Map Key Value cf Tail Map row Vs tail Map colfam Prefix Start Key Find the element if cf Tail Map is Empty tuple set tuple Offset else Hash Map String Data Byte Array tuple Map new Hash Map String Data Byte Array Build up a map for all the entries in this row that match the colfam prefix for Entry Key Value entry cf Tail Map entry Set if colfam Prefix Range contains entry get Key entry get Key get Column Family cf Holder entry get Key get Column Qualifier cq Holder Data Byte Array val new Data Byte Array entry get Value get Avoid adding an extra when colqual is empty if cq Holder get Length tuple Map put cf Holder to String val else tuple Map put cf Holder to String cq Holder to String val else break if tuple Map is Empty tuple set tuple Offset tuple Map break case cf Holder set column get Column Family cq Holder set column get Column Qualifier Range colqual Prefix Range Range prefix row cf Holder cq Holder Key colqual Prefix Start Key new Key row cf Holder cq Holder Sorted Map Key Value cq Tail Map row Vs tail Map colqual Prefix Start Key if cq Tail Map is Empty tuple set tuple Offset else Hash Map String Data Byte Array tuple Map new Hash Map String Data Byte Array Build up a map for all the entries in this row that match the colqual prefix for Entry Key Value entry cq Tail Map entry Set if colqual Prefix Range contains entry get Key entry get Key get Column Family cf Holder entry get Key get Column Qualifier cq Holder Data Byte Array val new Data Byte Array entry get Value get Avoid the extra on empty colqual if cq Holder get Length tuple Map put cf Holder to String val else tuple Map put cf Holder to String cq Holder to String val else break if tuple Map is Empty tuple set tuple Offset tuple Map break default break return tuple Override protected void configure Input Format Job job Accumulo Input Format add Iterator job new Iterator Setting Whole Row Iterator class Override protected Collection Mutation get Mutations Tuple tuple throws Exec Exception Exception final Resource Field Schema field Schemas schema null null schema get Fields Iterator Object tuple Iter tuple iterator if tuple size log debug Ignoring tuple of size tuple size return Collections empty List Mutation mutation new Mutation object To Text tuple Iter next null field Schemas null field Schemas int tuple Offset Iterator Column column Iter columns iterator while tuple Iter has Next column Iter has Next Object o tuple Iter next Column column column Iter next Grab the type for this field final byte type schema To Type o null field Schemas null field Schemas tuple Offset switch column get Type case byte bytes obj To Bytes o type if null bytes Value value new Value bytes We do n t have any column name from non Maps add Column mutation column get Column Family column get Column Qualifier value break case case Map String Object map try map Map String Object o catch Class Cast Exception e log error Expected Map at tuple offset tuple Offset but was o get Class get Simple Name throw e for Entry String Object entry map entry Set String key entry get Key Object obj Value entry get Value byte value Type Data Type find Type obj Value byte map Value obj To Bytes obj Value value Type if Column Type column get Type add Column mutation column get Column Family key null new Value map Value else if Column Type column get Type add Column mutation column get Column Family column get Column Qualifier key new Value map Value else throw new Exception Unknown column type break default log info Ignoring unhandled column type continue tuple Offset if mutation size return Collections empty List return Collections singleton List mutation Adds the given column family column qualifier and value to the given mutation param mutation param colfam param colqual param column Value protected void add Column Mutation mutation String colfam String colqual Value column Value if null colfam cf Holder set colfam else cf Holder clear if null colqual cq Holder set colqual else cq Holder clear mutation put cf Holder cq Holder column Value 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to You under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop accumulo import org apache accumulo core security Authorizations import org apache commons cli Command Line import org apache commons cli Gnu Parser import org apache commons cli Help Formatter import org apache commons cli Option import org apache commons cli Options import org apache commons cli Parse Exception import org apache commons lang String Utils public class Accumulo Storage Options public static final Option new Option c caster true Implementation of Load Store Caster to use typically String Converter or Accumulo Binary Converter new Option auths authorizations true Comma separated list of authorizations to use new Option s start true The row to begin reading from inclusive new Option e end true The row to read until inclusive new Option buff mutation buffer size true Number of bytes to buffer when writing data new Option wt write threads true Number of threads to use when writing data new Option ml max latency true Maximum latency in milliseconds before Mutations are flushed to Accumulo new Option sep separator true Separator string to use when parsing columns new Option iw ignore whitespace true Whether or not whitespace should be stripped from column list private Options options private Gnu Parser parser public Accumulo Storage Options parser new Gnu Parser options new Options options add Option options add Option options add Option options add Option options add Option options add Option options add Option options add Option options add Option public String get Help Message return c caster Load Store Caster Impl auths authorizations auth auth auth s start startrow e end endrow buff mutation buffer size bytes wt write threads threads ml max latency seconds sep separator iw ignore whitespace true false public Command Line get Command Line String args throws Parse Exception String split Args String Utils split args try return parser parse options split Args catch Parse Exception e Help Formatter formatter new Help Formatter formatter print Help get Help Message options throw e public boolean has Authorizations Command Line cli return cli has Option get Opt public Authorizations get Authorizations Command Line cli return new Authorizations cli get Option Value get Opt public long get Long Command Line cli Option o String value cli get Option Value o get Opt return null value null Long parse Long value public int get Int Command Line cli Option o String value cli get Option Value o get Opt return null value null Integer parse Int value 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java math Big Decimal import java math Big Integer import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Add extends Binary Expression Operator private static final long serial Version public Add Operator Key k super k public Add Operator Key k int rp super k rp Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Add this Override public String name return Add Data Type find Type Name result Type m Key to String This method is used to invoke the appropriate addition method as Java does not provide generic dispatch for it protected Number add Number a Number b byte data Type throws Exec Exception switch data Type case Data Type return Double value Of Double a Double b case Data Type return Integer value Of Integer a Integer b case Data Type return Long value Of Long a Long b case Data Type return Float value Of Float a Float b case Data Type return Big Integer a add Big Integer b case Data Type return Big Decimal a add Big Decimal b default throw new Exec Exception called on unsupported Number class Data Type find Type Name data Type protected Result generic Get Next byte data Type throws Exec Exception Result r accum Child null data Type if r null return r byte status Result res res lhs get Next data Type status res return Status if status Status res result null return res Number left Number res result res rhs get Next data Type status res return Status if status Status res result null return res Number right Number res result res result add left right data Type return res Override public Result get Next Double throws Exec Exception return generic Get Next Data Type Override public Result get Next Float throws Exec Exception return generic Get Next Data Type Override public Result get Next Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Long throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Decimal throws Exec Exception return generic Get Next Data Type Override public Add clone throws Clone Not Supported Exception Add clone new Add new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Add Operator public class Add Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Add Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Add plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Add Expression Add Expression ao Add Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null get Lhs get Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Add Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical optimizer import java util Collection import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Multi Map import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cube import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator visitor that walks a logical plan and then applies a given Logical Expression Visitor to all expressions it encounters public abstract class All Expression Visitor extends Logical Relational Nodes Visitor protected Logical Relational Operator current Op param plan Logical Plan to visit param walker Walker to use to visit the plan public All Expression Visitor Operator Plan plan Plan Walker walker throws Frontend Exception super plan walker Get a new instance of the expression visitor to apply to a given expression param expr Logical Expression Plan that will be visited return a new Logical Expression Visitor for that expression abstract protected Logical Expression Visitor get Visitor Logical Expression Plan expr throws Frontend Exception private void visit All Collection Logical Expression Plan lexp Plans throws Frontend Exception for Logical Expression Plan plan lexp Plans Logical Expression Visitor v get Visitor plan v visit Override public void visit Filter filter throws Frontend Exception current Op filter Logical Expression Visitor v get Visitor filter get Filter Plan v visit Override public void visit Limit limit throws Frontend Exception current Op limit if limit get Limit Plan null Logical Expression Visitor v get Visitor limit get Limit Plan v visit Override public void visit Join join throws Frontend Exception current Op join visit All join get Expression Plan Values Override public void visit Cube cu throws Frontend Exception current Op cu Multi Map Integer Logical Expression Plan expression Plans cu get Expression Plans for Integer key expression Plans key Set visit All expression Plans get key Override public void visit Cogroup cg throws Frontend Exception current Op cg Multi Map Integer Logical Expression Plan expression Plans cg get Expression Plans for Integer key expression Plans key Set visit All expression Plans get key Override public void visit For Each foreach throws Frontend Exception current Op foreach We have an Inner Operator Plan in For Each so we go ahead and work on that plan Operator Plan inner Plan foreach get Inner Plan Plan Walker new Walker current Walker spawn Child Walker inner Plan push Walker new Walker current Walker walk this pop Walker Override public void visit Generate gen throws Frontend Exception current Op gen visit All gen get Output Plans Override public void visit Inner Load load throws Frontend Exception the expression in Inner Load contains info relative from For Each so use Foreach as current Op current Op load get For Each Logical Expression Plan exp Logical Expression Plan load get Projection get Plan Logical Expression Visitor v get Visitor exp v visit Override public void visit Split Output split Output throws Frontend Exception current Op split Output Logical Expression Visitor v get Visitor split Output get Filter Plan v visit Override public void visit Rank rank throws Frontend Exception current Op rank visit All rank get Rank Col Plans Override public void visit Sort sort throws Frontend Exception current Op sort visit All sort get Sort Col Plans 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical optimizer import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Cube import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Native import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator visitor that walks the logical plan and calls the same method on every type of node Subclasses can extend this and implement the execute method and this method will be called on every node in the graph public abstract class All Same Ralational Nodes Visitor extends Logical Relational Nodes Visitor param plan Operator Plan to visit param walker Walker to use to visit the plan public All Same Ralational Nodes Visitor Operator Plan plan Plan Walker walker throws Frontend Exception super plan walker Method to call on every node in the logical plan param op Node that is currently being visited abstract protected void execute Logical Relational Operator op throws Frontend Exception Override public void visit Filter filter throws Frontend Exception execute filter Override public void visit Join join throws Frontend Exception execute join Override public void visit Cogroup cg throws Frontend Exception execute cg Override public void visit Load load throws Frontend Exception execute load Override public void visit Store store throws Frontend Exception execute store Override public void visit For Each foreach throws Frontend Exception execute foreach Override public void visit Split split throws Frontend Exception execute split Override public void visit Split Output split Output throws Frontend Exception execute split Output Override public void visit Union union throws Frontend Exception execute union Override public void visit Sort sort throws Frontend Exception execute sort Override public void visit Rank rank throws Frontend Exception execute rank Override public void visit Distinct distinct throws Frontend Exception execute distinct Override public void visit Cross cross throws Frontend Exception execute cross Override public void visit Stream stream throws Frontend Exception execute stream Override public void visit Limit limit throws Frontend Exception execute limit Override public void visit Native lo Native throws Frontend Exception execute lo Native Override public void visit Cube cube throws Frontend Exception execute cube 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Boolean and expression public class And Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public And Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super And plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof And Expression And Expression ao And Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new And Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig pen import java math Big Decimal import java math Big Integer import java util Array List import java util Collection import java util Hash Map import java util Hash Set import java util Iterator import java util Linked List import java util List import java util Map import java util Set import org joda time Date Time import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Multi Map import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical expression Add Expression import org apache pig newplan logical expression And Expression import org apache pig newplan logical expression Binary Expression import org apache pig newplan logical expression Cast Expression import org apache pig newplan logical expression Constant Expression import org apache pig newplan logical expression Divide Expression import org apache pig newplan logical expression Equal Expression import org apache pig newplan logical expression Greater Than Equal Expression import org apache pig newplan logical expression Greater Than Expression import org apache pig newplan logical expression Is Null Expression import org apache pig newplan logical expression Less Than Equal Expression import org apache pig newplan logical expression Less Than Expression import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Mod Expression import org apache pig newplan logical expression Multiply Expression import org apache pig newplan logical expression Not Equal Expression import org apache pig newplan logical expression Not Expression import org apache pig newplan logical expression Or Expression import org apache pig newplan logical expression Project Expression import org apache pig newplan logical expression Regex Expression import org apache pig newplan logical expression Subtract Expression import org apache pig newplan logical expression User Func Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Store import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig pen util Example Tuple import org apache pig pen util Pre Order Depth First Walker This is used to generate synthetic data Synthetic data generation is done by making constraint tuples for each operator as we traverse the plan and try to replace the constraints with values as far as possible We only deal with simple conditions right now public class Augment Base Data Visitor extends Logical Relational Nodes Visitor Map Load Data Bag base Data null Map Load Data Bag new Base Data new Hash Map Load Data Bag Map Operator Data Bag derived Data null private boolean limit false private final Map Operator Physical Operator log To Phys Map private Map Limit Long ori Limit Map Map Operator Data Bag output Constraints Map new Hash Map Operator Data Bag Log log Log Factory get Log get Class Augmentation moves from the leaves to root and hence needs a depthfirstwalker public Augment Base Data Visitor Operator Plan plan Map Operator Physical Operator log To Phys Map Map Load Data Bag base Data Map Operator Data Bag derived Data throws Frontend Exception super plan new Pre Order Depth First Walker plan this base Data base Data this derived Data derived Data this log To Phys Map log To Phys Map public void set Limit limit true public Map Load Data Bag get New Base Data throws Exec Exception consolidate base data from different Ds on the same inputs Multi Map File Spec Data Bag input Data Map new Multi Map File Spec Data Bag for Map Entry Load Data Bag e new Base Data entry Set input Data Map put e get Key get File Spec e get Value int index for File Spec fs input Data Map key Set int max Schema Size Tuple tuple Of Max Schema Size null for Data Bag bag input Data Map get fs if bag size int size Tuple t null t bag iterator next size t size if size max Schema Size max Schema Size size tuple Of Max Schema Size t for Data Bag bag input Data Map get fs if bag size for Iterator Tuple it bag iterator it has Next Tuple t it next for int i t size i max Schema Size i t append tuple Of Max Schema Size get i index for Map Entry Load Data Bag e base Data entry Set Data Bag bag new Base Data get e get Key if bag null bag Bag Factory get Instance new Default Bag new Base Data put e get Key bag bag add All e get Value return new Base Data public Map Limit Long get Ori Limit Map return ori Limit Map Override public void visit Cogroup cg throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return we first get the outputconstraints for the current cogroup Data Bag output Constraints output Constraints Map get cg output Constraints Map remove cg boolean able To Handle true we then check if we can handle this cogroup and try to collect some information about grouping List List Integer group Specs new Linked List List Integer int num Cols for int index index cg get Inputs Logical Plan plan size index Collection Logical Expression Plan group By Plans cg get Expression Plans get index List Integer group Cols new Array List Integer for Logical Expression Plan plan group By Plans Operator leaf plan get Sinks get if leaf instanceof Project Expression group Cols add Integer value Of Project Expression leaf get Col Num else able To Handle false break if num Cols num Cols group Cols size if group Cols size group By Plans size group Cols size num Cols we came across an unworkable cogroup plan break else group Specs add group Cols we should now have some workable data at this point to synthesize tuples try if able To Handle we need to go through the output constraints first int num Inputs cg get Inputs Logical Plan plan size if output Constraints null for Iterator Tuple it output Constraints iterator it has Next Tuple output Constraint it next Object group Label output Constraint get for int input input num Inputs input int num Input Fields Logical Relational Operator cg get Inputs Logical Plan plan get input get Schema size List Integer group Cols group Specs get input Data Bag output output Constraints Map get cg get Inputs Logical Plan plan get input if output null output Bag Factory get Instance new Default Bag output Constraints Map put cg get Inputs Logical Plan plan get input output for int i i i Tuple input Constraint Get Group By Input group Label group Cols num Input Fields if input Constraint null output add input Constraint then go through all organic data groups and add input constraints to make each group big enough Data Bag output Data derived Data get cg for Iterator Tuple it output Data iterator it has Next Tuple group Tup it next Object group Label group Tup get for int input input num Inputs input int num Input Fields Logical Relational Operator cg get Inputs Logical Plan plan get input get Schema size List Integer group Cols group Specs get input Data Bag output output Constraints Map get cg get Inputs Logical Plan plan get input if output null output Bag Factory get Instance new Default Bag output Constraints Map put cg get Inputs Logical Plan plan get input output int num Tups To Add int Data Bag group Tup get input size for int i i num Tups To Add i Tuple input Constraint Get Group By Input group Label group Cols num Input Fields if input Constraint null output add input Constraint catch Exception e log error Error visiting Cogroup during Augmentation phase of Example Generator e get Message throw new Frontend Exception Error visiting Cogroup during Augmentation phase of Example Generator e get Message Override public void visit Join join throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return we first get the outputconstraints for the current cogroup Data Bag output Constraints output Constraints Map get join output Constraints Map remove join boolean able To Handle true we then check if we can handle this cogroup and try to collect some information about grouping List List Integer group Specs new Linked List List Integer int num Cols for int index index join get Inputs Logical Plan plan size index Collection Logical Expression Plan group By Plans join get Expression Plans get index List Integer group Cols new Array List Integer for Logical Expression Plan plan group By Plans Operator leaf plan get Sinks get if leaf instanceof Project Expression group Cols add Integer value Of Project Expression leaf get Col Num else able To Handle false break if num Cols num Cols group Cols size if group Cols size group By Plans size group Cols size num Cols we came across an unworkable cogroup plan break else group Specs add group Cols we should now have some workable data at this point to synthesize tuples try if able To Handle we need to go through the output constraints first int num Inputs join get Inputs Logical Plan plan size if output Constraints null for Iterator Tuple it output Constraints iterator it has Next Tuple output Constraint it next for int input input num Inputs input int num Input Fields Logical Relational Operator join get Inputs Logical Plan plan get input get Schema size List Integer group Cols group Specs get input Data Bag output output Constraints Map get join get Inputs Logical Plan plan get input if output null output Bag Factory get Instance new Default Bag output Constraints Map put join get Inputs Logical Plan plan get input output Tuple input Constraint Get Join Input output Constraint group Cols num Input Fields if input Constraint null output add input Constraint then go through all organic data groups and add input constraints to make each group big enough Data Bag output Data derived Data get join if output Data size Data Bag output output Constraints Map get join get Inputs Logical Plan plan get if output null output size output derived Data get join get Inputs Logical Plan plan get Tuple input Constraint output iterator next for int input input num Inputs input Data Bag output output Constraints Map get join get Inputs Logical Plan plan get input if output null output Bag Factory get Instance new Default Bag output Constraints Map put join get Inputs Logical Plan plan get input output int num Input Fields Logical Relational Operator join get Inputs Logical Plan plan get input get Schema size Tuple input Constraint Get Join Input input Constraint group Specs get group Specs get input num Input Fields if input Constraint null output add input Constraint catch Exception e log error Error visiting Cogroup during Augmentation phase of Example Generator e get Message throw new Frontend Exception Error visiting Cogroup during Augmentation phase of Example Generator e get Message Override public void visit Cross cs throws Frontend Exception Override public void visit Distinct dt throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Data Bag output Constraints output Constraints Map get dt output Constraints Map remove dt Data Bag input Constraints output Constraints Map get dt get Input Logical Plan plan if input Constraints null input Constraints Bag Factory get Instance new Default Bag output Constraints Map put dt get Input Logical Plan plan input Constraints if output Constraints null output Constraints size for Iterator Tuple it output Constraints iterator it has Next input Constraints add it next boolean empty Input Constraints input Constraints size if empty Input Constraints Data Bag input Data derived Data get dt get Input Logical Plan plan for Iterator Tuple it input Data iterator it has Next input Constraints add it next Set Tuple distinct Set new Hash Set Tuple Iterator Tuple it for it input Constraints iterator it has Next if distinct Set add it next break if it has Next no duplicates found generate one if input Constraints size Tuple src Example Tuple input Constraints iterator next to Tuple tgt Tuple Factory get Instance new Tuple src get All Example Tuple input Constraint new Example Tuple tgt input Constraint synthetic true input Constraints add input Constraint else if empty Input Constraints input Constraints clear Override public void visit Filter filter throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Data Bag output Constraints output Constraints Map get filter output Constraints Map remove filter Logical Expression Plan filter Cond filter get Filter Plan Data Bag input Constraints output Constraints Map get filter get Input Logical Plan plan if input Constraints null input Constraints Bag Factory get Instance new Default Bag output Constraints Map put filter get Input Logical Plan plan input Constraints Data Bag output Data derived Data get filter Data Bag input Data derived Data get filter get Input Logical Plan plan try if output Constraints null output Constraints size there s one or more output constraints generate corresponding input constraints for Iterator Tuple it output Constraints iterator it has Next Tuple output Constraint it next Example Tuple input Constraint Generate Matching Tuple output Constraint filter Cond false if input Constraint null input Constraints add input Constraint else if output Data size no output constraints but output is empty generate one input that will pass the filter Example Tuple input Constraint Generate Matching Tuple filter get Schema filter Cond false if input Constraint null input Constraints add input Constraint if necessary insert a negative example i e a tuple that does not pass the filter if output Data size input Data size all tuples pass the filter generate one input that will not pass the filter Example Tuple input Constraint Generate Matching Tuple filter get Schema filter Cond true if input Constraint null input Constraints add input Constraint catch Exception e log error Error visiting Load during Augmentation phase of Example Generator e get Message e throw new Frontend Exception Error visiting Load during Augmentation phase of Example Generator e get Message e Override public void visit For Each for Each throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Data Bag output Constraints output Constraints Map get for Each output Constraints Map remove for Each Logical Plan plan for Each get Inner Plan boolean able To Handle true List Integer cols new Array List Integer boolean cast false if output Constraints null output Constraints size we dont have to do anything in this case return Operator op plan get Sinks get if op instanceof Cast Expression cast true op Cast Expression op get Expression if op instanceof Project Expression able To Handle false else cols add Integer value Of Project Expression op get Col Num if able To Handle we can only handle simple projections Data Bag output Bag Factory get Instance new Default Bag for Iterator Tuple it output Constraints iterator it has Next Tuple output Constraint it next try Tuple input Constraint Back Prop Constraint output Constraint cols Logical Relational Operator plan get Predecessors for Each get get Schema cast output add input Constraint catch Exception e e print Stack Trace throw new Frontend Exception Operator error during Augmenting Phase in Example Generator e get Message output Constraints Map put plan get Predecessors for Each get output Override public void visit Load load throws Frontend Exception Data Bag input Data base Data get load check if the input Data exists if input Data null input Data size log error No valid input data found throw new Runtime Exception No valid input data found Data Bag new Input Data new Base Data get load if new Input Data null new Input Data Bag Factory get Instance new Default Bag new Base Data put load new Input Data Logical Schema schema try schema load get Schema if schema null throw new Runtime Exception Example Generator requires a schema Please provide a schema while loading data catch Frontend Exception e log error Error visiting Load during Augmentation phase of Example Generator e get Message throw new Frontend Exception Error visiting Load during Augmentation phase of Example Generator e get Message Tuple example Tuple input Data iterator next Data Bag output Constraints output Constraints Map get load output Constraints Map remove load first of all we are required to guarantee that there is at least one output tuple if output Constraints null output Constraints size output Constraints Bag Factory get Instance new Default Bag output Constraints add Tuple Factory get Instance new Tuple schema get Fields size create example tuple to steal values from when we encounter do n t care fields i e null fields System out println example Tuple to String run through output constraints for each one synthesize a tuple and add it to the base data while synthesizing individual fields try to match fields that exist in the real data boolean new Input false for Iterator Tuple it output Constraints iterator it has Next Tuple output Constraint it next sanity check if output Constraint size schema get Fields size throw new Runtime Exception Internal error incorrect number of fields in constraint tuple Tuple input Tuple Factory get Instance new Tuple output Constraint size Example Tuple input Tuple new Example Tuple input try for int i i input Tuple size i Object d output Constraint get i if d null i example Tuple size d example Tuple get i input Tuple set i d if output Constraint instanceof Example Tuple input Tuple synthetic Example Tuple output Constraint synthetic else raw tuple should have been synthesized input Tuple synthetic true catch Exec Exception e log error Error visiting Load during Augmentation phase of Example Generator e get Message throw new Frontend Exception Error visiting Load during Augmentation phase of Example Generator e get Message try if input Tuple synthetic in Input input Tuple input Data schema input Tuple synthetic true new Input Data add input Tuple if new Input new Input true catch Exec Exception e throw new Frontend Exception Error visiting Load during Augmentation phase of Example Generator e get Message private boolean in Input Tuple new Tuple Data Bag input Logical Schema schema throws Exec Exception boolean result for Iterator Tuple iter input iterator iter has Next result true Tuple tmp iter next for int i i schema size i if new Tuple get i equals tmp get i result false break if result return true return false Override public void visit Sort s throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Data Bag output Constraints output Constraints Map get s output Constraints Map remove s if output Constraints null output Constraints Map put s get Input Logical Plan plan Bag Factory get Instance new Default Bag else output Constraints Map put s get Input Logical Plan plan output Constraints Override public void visit Split split throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Override public void visit Store store throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Data Bag output Constraints output Constraints Map get store if output Constraints null output Constraints Map put plan get Predecessors store get Bag Factory get Instance new Default Bag else output Constraints Map remove store output Constraints Map put plan get Predecessors store get output Constraints Override public void visit Union u throws Frontend Exception if limit Pre Order Depth First Walker current Walker get Branch Flag return Data Bag output Constraints output Constraints Map get u output Constraints Map remove u if output Constraints null output Constraints size we dont need to do anything we just find the inputs create empty bags as their output Constraints and return for Operator op u get Inputs Logical Plan plan Data Bag constraints Bag Factory get Instance new Default Bag output Constraints Map put op constraints return since we have some output Constraints we apply them to the inputs round robin int count List Operator inputs u get Inputs Logical Plan plan int no Inputs inputs size for Operator op inputs Data Bag constraint Bag Factory get Instance new Default Bag output Constraints Map put op constraint for Iterator Tuple it output Constraints iterator it has Next Data Bag constraint output Constraints Map get inputs get count constraint add it next count count no Inputs Override public void visit Limit lm throws Frontend Exception if limit not augment for in this traversal return if ori Limit Map null ori Limit Map new Hash Map Limit Long Data Bag output Constraints output Constraints Map get lm output Constraints Map remove lm Data Bag input Constraints output Constraints Map get lm get Input Logical Plan plan if input Constraints null input Constraints Bag Factory get Instance new Default Bag output Constraints Map put lm get Input Logical Plan plan input Constraints Data Bag input Data derived Data get lm get Input Logical Plan plan if output Constraints null output Constraints size there s one or more output constraints generate corresponding input constraints for Iterator Tuple it output Constraints iterator it has Next input Constraints add it next plus one more if only one if input Constraints size input Constraints add input Data iterator next Pre Order Depth First Walker current Walker set Branch Flag else if input Constraints size add all input to input constraints input Constraints add All input Data plus one more if only one if input Constraints size input Constraints add input Data iterator next Pre Order Depth First Walker current Walker set Branch Flag Limit po Limit Limit log To Phys Map get lm ori Limit Map put lm Long value Of po Limit get Limit po Limit set Limit input Constraints size lm set Limit po Limit get Limit Tuple Get Group By Input Object group Label List Integer group Cols int num Fields throws Exec Exception Tuple t Tuple Factory get Instance new Tuple num Fields if group Cols size Group Label would be a data atom t set group Cols get group Label else if group Label instanceof Tuple throw new Runtime Exception Unrecognized group label Tuple group Tuple group Label for int i i group Cols size i t set group Cols get i group get i return t Tuple Get Join Input Tuple group List Integer group Cols List Integer group Cols int num Fields throws Exec Exception Tuple t Tuple Factory get Instance new Tuple num Fields if group Cols size Group Label would be a data atom t set group Cols get group get group Cols get else if group instanceof Tuple throw new Runtime Exception Unrecognized group label for int i i group Cols size i t set group Cols get i group get group Cols get i return t Tuple Get Join Input Tuple group List Integer group Cols int num Fields throws Exec Exception Tuple t Tuple Factory get Instance new Tuple num Fields if group Cols size Group Label would be a data atom t set group Cols get group else if group instanceof Tuple throw new Runtime Exception Unrecognized group label for int i i group Cols size i t set group Cols get i group get i return t Tuple Back Prop Constraint Tuple output Constraint List Integer cols Logical Schema input Schema boolean cast throws Exec Exception Tuple input Const Tuple Factory get Instance new Tuple input Schema get Fields size Tuple input Constraint new Example Tuple input Const for int out Col out Col output Constraint size out Col int in Col cols get out Col Object out Val output Constraint get out Col Object in Val input Constraint get in Col if in Val null out Val null input Constraint set in Col out Val input Constraint set in Col cast new Data Byte Array out Val to String get Bytes out Val else if out Val null unable to back propagate due to conflicting column constraints so give up return null return input Constraint generate a constraint tuple that conforms to the schema and passes the predicate or null if unable to find such a tuple Example Tuple Generate Matching Tuple Logical Schema schema Logical Expression Plan plan boolean invert throws Frontend Exception Exec Exception return Generate Matching Tuple Tuple Factory get Instance new Tuple schema get Fields size plan invert generate a constraint tuple that conforms to the constraint and passes the predicate or null if unable to find such a tuple for now constraint tuples are tuples whose fields are a blend of actual data values and nulls where a null stands for do n t care in the future may want to replace do n t care with a more rich constraint language this would help e g in the case of two filters in a row you want the downstream filter to tell the upstream filter what predicate it wants satisfied in a given field Example Tuple Generate Matching Tuple Tuple constraint Logical Expression Plan predicate boolean invert throws Exec Exception Frontend Exception Tuple t Tuple Factory get Instance new Tuple constraint size Example Tuple t Out new Example Tuple t for int i i t size i t Out set i constraint get i Generate Matching Tuple Helper t Out predicate get Sources get invert t Out synthetic true return t Out void Generate Matching Tuple Helper Tuple t Operator pred boolean invert throws Frontend Exception Exec Exception if pred instanceof Binary Expression Generate Matching Tuple Helper t Binary Expression pred invert else if pred instanceof Not Expression Generate Matching Tuple Helper t Not Expression pred invert else if pred instanceof Is Null Expression Generate Matching Tuple Helper t Is Null Expression pred invert else if pred instanceof User Func Expression Do n t know how to generate input tuple for return null to suppress the generation t null else throw new Frontend Exception Unknown operator in filter predicate void Generate Matching Tuple Helper Tuple t Binary Expression pred boolean invert throws Frontend Exception Exec Exception if pred instanceof And Expression Generate Matching Tuple Helper t And Expression pred invert return else if pred instanceof Or Expression Generate Matching Tuple Helper t Or Expression pred invert return now we are sure that the expression operators are the roots of the plan boolean left Is Const false right Is Const false Object left Const null right Const null byte left Data Type right Data Type int left Col right Col if pred instanceof Add Expression pred instanceof Subtract Expression pred instanceof Multiply Expression pred instanceof Divide Expression pred instanceof Mod Expression pred instanceof Regex Expression return We do n t try to work around these operators right now if pred get Lhs instanceof Constant Expression left Is Const true left Const Constant Expression pred get Lhs get Value else Logical Expression lhs pred get Lhs if lhs instanceof Cast Expression lhs Cast Expression lhs get Expression if pred get Lhs Operand instanceof Project Expression Project Expression pred get Lhs Operand get Projection size return too hard if lhs instanceof Project Expression return left Col Project Expression lhs get Col Num left Data Type Project Expression lhs get Type Object d t get left Col if d null left Is Const true left Const d if pred get Rhs instanceof Constant Expression right Is Const true right Const Constant Expression pred get Rhs get Value else Operator rhs pred get Rhs if rhs instanceof Cast Expression rhs Cast Expression rhs get Expression if pred get Rhs Operand instanceof Project Expression Project Expression pred get Rhs Operand get Projection size return too hard if rhs instanceof Project Expression return right Col Project Expression rhs get Col Num right Data Type Project Expression rhs get Type Object d t get right Col if d null right Is Const true right Const d if left Is Const right Is Const return ca n t really change the result if both are constants now we try to change some nulls to constants convert some nulls to constants if invert if pred instanceof Equal Expression if left Is Const t set right Col generate Data right Data Type left Const to String else if right Is Const t set left Col generate Data left Data Type right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Not Equal Expression if left Is Const t set right Col generate Data right Data Type Get Unequal Value left Const to String else if right Is Const t set left Col generate Data left Data Type Get Unequal Value right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Greater Than Expression pred instanceof Greater Than Equal Expression if left Is Const t set right Col generate Data right Data Type Get Smaller Value left Const to String else if right Is Const t set left Col generate Data left Data Type Get Larger Value right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Less Than Expression pred instanceof Less Than Equal Expression if left Is Const t set right Col generate Data right Data Type Get Larger Value left Const to String else if right Is Const t set left Col generate Data left Data Type Get Smaller Value right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Equal Expression if left Is Const t set right Col generate Data right Data Type Get Unequal Value left Const to String else if right Is Const t set left Col generate Data left Data Type Get Unequal Value right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Not Equal Expression if left Is Const t set right Col generate Data right Data Type left Const to String else if right Is Const t set left Col generate Data left Data Type right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Greater Than Expression pred instanceof Greater Than Equal Expression if left Is Const t set right Col generate Data right Data Type Get Larger Value left Const to String else if right Is Const t set left Col generate Data left Data Type Get Smaller Value right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type else if pred instanceof Less Than Expression pred instanceof Less Than Equal Expression if left Is Const t set right Col generate Data right Data Type Get Smaller Value left Const to String else if right Is Const t set left Col generate Data left Data Type Get Larger Value right Const to String else t set left Col generate Data left Data Type t set right Col generate Data right Data Type void Generate Matching Tuple Helper Tuple t And Expression op boolean invert throws Frontend Exception Exec Exception Operator input op get Lhs Generate Matching Tuple Helper t input invert input op get Rhs Generate Matching Tuple Helper t input invert void Generate Matching Tuple Helper Tuple t Or Expression op boolean invert throws Frontend Exception Exec Exception Operator input op get Lhs Generate Matching Tuple Helper t input invert input op get Rhs Generate Matching Tuple Helper t input invert void Generate Matching Tuple Helper Tuple t Not Expression op boolean invert throws Frontend Exception Exec Exception Logical Expression input op get Expression Generate Matching Tuple Helper t input invert void Generate Matching Tuple Helper Tuple t Is Null Expression op boolean invert throws Frontend Exception Exec Exception byte type op get Expression get Type if invert t set null else t set generate Data type Object Get Unequal Value Object v byte type Data Type find Type v if type Data Type type Data Type type Data Type return null Object zero generate Data type if v equals zero return generate Data type return zero Object Get Smaller Value Object v byte type Data Type find Type v if type Data Type type Data Type type Data Type return null switch type case Data Type String str String v if str length return str substring str length else return null case Data Type Data Byte Array data Data Byte Array v if data size return new Data Byte Array data get data size else return null case Data Type return Integer value Of Integer v case Data Type return Long value Of Long v case Data Type return Float value Of Float v case Data Type return Double value Of Double v case Data Type return Big Integer v subtract Big Integer case Data Type return Big Decimal v subtract Big Decimal case Data Type Date Time dt Date Time v if dt get Millis Of Second return dt minus Millis else if dt get Second Of Minute return dt minus Seconds else if dt get Minute Of Hour return dt minus Minutes else if dt get Hour Of Day return dt minus Hours else return dt minus Days default return null Object Get Larger Value Object v byte type Data Type find Type v if type Data Type type Data Type type Data Type return null switch type case Data Type return String v case Data Type String str Data Byte Array v to String str str return new Data Byte Array str case Data Type return Integer value Of Integer v case Data Type return Long value Of Long v case Data Type return Float value Of Float v case Data Type return Double value Of Double v case Data Type return Big Integer v add Big Integer case Data Type return Big Decimal v add Big Decimal case Data Type Date Time dt Date Time v if dt get Millis Of Second return dt plus Millis else if dt get Second Of Minute return dt plus Seconds else if dt get Minute Of Hour return dt plus Minutes else if dt get Hour Of Day return dt plus Hours else return dt plus Days default return null Object generate Data byte type String data switch type case Data Type if data equals Ignore Case true return Boolean else if data equals Ignore Case false return Boolean else return null case Data Type return new Data Byte Array data get Bytes case Data Type return Double value Of data case Data Type return Float value Of data case Data Type return Integer value Of data case Data Type return Long value Of data case Data Type return new Big Integer data case Data Type return new Big Decimal data case Data Type return new Date Time data case Data Type return data default return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util Iterator import java util List import org apache pig Accumulator import org apache pig Algebraic import org apache pig Eval Func import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the average of a set of values This class implements link org apache pig Algebraic so if possible the execution will performed in a distributed fashion p can operate on any numeric type It can also operate on bytearrays which it will cast to doubles It expects a bag of tuples of one record each If Pig knows from the schema that this function will be passed a bag of integers or longs it will use a specially adapted version of that uses integer arithmetic for summing the data The return type of will always be double regardless of the input type p implements the link org apache pig Accumulator interface as well While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation public class extends Eval Func Double implements Algebraic Accumulator Double private static Tuple Factory m Tuple Factory Tuple Factory get Instance Override public Double exec Tuple input throws Exception try Double sum sum input if sum null either we were handed an empty bag or a bag filled with nulls return null in this case return null double count count input Double avg null if count avg new Double sum count return avg catch Exec Exception ee throw ee Override public String get Initial return Initial class get Name Override public String get Intermed return Intermediate class get Name Override public String get Final return Final class get Name static public class Initial extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception Tuple t m Tuple Factory new Tuple try input is a bag with one tuple containing the column we are trying to avg Data Bag bg Data Bag input get Data Byte Array dba null if bg iterator has Next Tuple tp bg iterator next dba Data Byte Array tp get t set dba null Double value Of dba to String null if dba null t set else t set return t catch Number Format Exception nfe invalid input treat this input as null try t set null t set catch Exec Exception e throw e return t catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Intermediate extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception try Data Bag b Data Bag input get return combine b catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Final extends Eval Func Double Override public Double exec Tuple input throws Exception try Data Bag b Data Bag input get Tuple combined combine b Double sum Double combined get if sum null return null double count Long combined get Double avg null if count avg new Double sum count return avg catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static protected Tuple combine Data Bag values throws Exec Exception double sum long count combine is called from Intermediate and Final In either case Initial would have been called before and would have sent in valid tuples Hence we do n t need to check if incoming bag is empty Tuple output m Tuple Factory new Tuple boolean saw Non Null false for Iterator Tuple it values iterator it has Next Tuple t it next Double d Double t get we count nulls in avg as contributing a departure from for performance of which implemented by just inspecting size of the bag if d null d else saw Non Null true sum d count Long t get if saw Non Null output set new Double sum else output set null output set Long value Of count return output static protected long count Tuple input throws Exec Exception Data Bag values Data Bag input get long cnt Iterator Tuple it values iterator while it has Next Tuple t Tuple it next if t null t size t get null cnt return cnt static protected Double sum Tuple input throws Exec Exception Exception Data Bag values Data Bag input get if we were handed an empty bag return if values null values size return null double sum boolean saw Non Null false for Iterator Tuple it values iterator it has Next Tuple t it next try Data Byte Array dba Data Byte Array t get Double d dba null Double value Of dba to String null if d null continue saw Non Null true sum d catch Runtime Exception exp int err Code String msg Problem while computing sum of doubles throw new Exec Exception msg err Code Pig Exception exp if saw Non Null return new Double sum else return null Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec func List add new Func Spec this get Class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Double Avg class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Float Avg class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Int Avg class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Long Avg class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Big Decimal Avg class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Big Integer Avg class get Name Schema generate Nested Schema Data Type Data Type return func List Accumulator interface implementation private Double intermediate Sum null private Double intermediate Count null Override public void accumulate Tuple b throws Exception try Double sum sum b if sum null return set default values if intermediate Sum null intermediate Count null intermediate Sum intermediate Count double count Long count b if count intermediate Count count intermediate Sum sum catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public void cleanup intermediate Sum null intermediate Count null Override public Double get Value Double avg null if intermediate Count null intermediate Count avg new Double intermediate Sum intermediate Count return avg 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io File Not Found Exception import java io Exception import java io Input Stream import java util Array List import java util List import java util Properties import org apache avro Schema import org apache avro Schema Parse Exception import org apache avro Schema Type import org apache avro file Data File Stream import org apache avro generic Generic Container import org apache avro generic Generic Datum Reader import org apache avro generic Generic Data import org apache avro mapred Avro Input Format import org apache avro mapred Avro Output Format import org apache commons cli Command Line import org apache commons cli Command Line Parser import org apache commons cli Gnu Parser import org apache commons cli Help Formatter import org apache commons cli Options import org apache commons cli Parse Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File Status import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop io Null Writable import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce Task Attempt Context import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib output File Output Format import org apache pig Expression import org apache pig Load Func import org apache pig Load Metadata import org apache pig Load Push Down import org apache pig Pig Warning import org apache pig Resource Schema import org apache pig Resource Statistics import org apache pig Store Func import org apache pig Store Func Interface import org apache pig Store Resources import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Jar Manager import org apache pig impl util Context import org apache pig impl util Utils import org apache pig impl util avro Avro Array Reader import org apache pig impl util avro Avro Record Reader import org apache pig impl util avro Avro Record Writer import org apache pig impl util avro Avro Storage Schema Conversion Utilities import org apache pig impl util avro Avro Tuple Wrapper import com google common collect Lists import com google common collect Maps Pig for reading and writing Avro data public class Avro Storage extends Load Func implements Store Func Interface Load Metadata Load Push Down Store Resources Creates new instance of Pig Storage function without specifying the schema Useful for just loading in data public Avro Storage this null null Creates new instance of Pig Storage function param sn Specifies the input output schema or record name public Avro Storage final String sn this sn null private String schema Name pig output private String schema Name Space null protected boolean allow Recursive false protected boolean double Colons To Double Underscores false protected Schema schema protected final Log log Log Factory get Log get Class Creates new instance of Avro Storage function specifying output schema properties param sn Specifies the input output schema or record name param opts Options for Avro Storage li code namespace code Namespace for an automatically generated output schema li li code schemafile code Specifies for avro schema file from which to read the input schema can be local file hdfs url etc li li code schemaclass code Specifies fully qualified class name for avro class in your classpath which implements Generic Container li li code examplefile code Specifies for avro data file from which to copy the input schema can be local file hdfs url etc li li code allowrecursive code Option to allow recursive schema definitions default is false li li code doublecolons code Option to translate Pig schema names with double colons to names with double underscores default is false li public Avro Storage final String sn final String opts super if sn null sn length try Schema s new Schema Parser parse sn must be a valid schema set Input Avro Schema s set Output Avro Schema s catch Schema Parse Exception e not a valid schema use as a record name schema Name sn if opts null String opts Arr opts split Options valid Options new Options try Command Line Parser parser new Gnu Parser valid Options add Option n namespace true Namespace for an automatically generated output schema valid Options add Option f schemafile true Specifies for avro schema file from which to read the input or output schema valid Options add Option c schemaclass true Specifies fully qualified class name for avro class in your classpath which implements Generic Container valid Options add Option e examplefile true Specifies for avro data file from which to copy the output schema valid Options add Option r allowrecursive false Option to allow recursive schema definitions default is false valid Options add Option d doublecolons false Option to translate Pig schema names with double colons to names with double underscores default is false Command Line configured Options parser parse valid Options opts Arr schema Name Space configured Options get Option Value namespace null allow Recursive configured Options has Option r double Colons To Double Underscores configured Options has Option d if configured Options has Option f try Path p new Path configured Options get Option Value f Configuration conf if Context get Context get Job Conf null conf new Configuration else conf Context get Context get Job Conf Schema s new Schema Parser parse File System get p to Uri conf open p set Input Avro Schema s set Output Avro Schema s catch File Not Found Exception fnfe System err printf file not found exception n log warn Schema file not found when instantiating Avro Storage If the schema was described in a local file on the front end and this message is in the back end log you can ignore this mesasge fnfe else if configured Options has Option c String schema Class configured Options get Option Value c try Schema s Generic Container Class for Name schema Class new Instance get Schema set Input Avro Schema s set Output Avro Schema s catch Class Not Found Exception Illegal Access Exception cnfe System err printf class not found exception n log error Schema class schema Class was not found in the classpath cnfe throw new Runtime Exception cnfe catch Instantiation Exception ie System err printf instantiation exception n log error Schema class schema Class must have a public empty args constructor ie throw new Runtime Exception ie catch Class Cast Exception cce System err printf class cast exception n log error Schema class schema Class must implement org apache avro generic Generic Container interface cce throw new Runtime Exception cce else if configured Options has Option e set Output Avro Schema get Avro Schema configured Options get Option Value e new Job new Configuration catch Parse Exception e log error Exception in Avro Storage e log error Avro Storage called with arguments sn opts warn Parse Exception in Avro Storage Pig Warning Help Formatter formatter new Help Formatter formatter print Help Avro Storage options valid Options throw new Runtime Exception e catch Exception e log warn Exception in Avro Storage e log warn Avro Storage called with arguments sn opts warn Exception in Avro Storage Pig Warning throw new Runtime Exception e Context signature for this instance protected String udf Context Signature null Override public final void set Context Signature final String signature udf Context Signature signature super set Context Signature signature update Schema From Input Avro Schema Internal function for getting the Properties object associated with this instance return The Properties object associated with this instance protected final Properties get Properties if udf Context Signature null return get Properties Avro Storage class null else return get Properties Avro Storage class udf Context Signature Internal function for getting the Properties object associated with this instance param c Class of this param signature Signature string return The Properties object associated with this instance Suppress Warnings rawtypes protected final Properties get Properties final Class c final String signature Context context Context get Context if signature null return context get Properties c else return context get Properties c new String signature see org apache pig Load Metadata get Schema java lang String org apache hadoop mapreduce Job Override public final Resource Schema get Schema final String location final Job job throws Exception if schema null Schema s get Avro Schema location job set Input Avro Schema s Resource Schema rs Avro Storage Schema Conversion Utilities avro Schema To Resource Schema schema allow Recursive return rs Reads the avro schema at the specified location param location Location of file param job Hadoop job object return an Avro Schema object derived from the specified file throws Exception protected final Schema get Avro Schema final String location final Job job throws Exception String locations get Path Strings location Path paths new Path locations length for int i i paths length i paths i new Path locations i return get Avro Schema paths job Reads the avro schemas at the specified location param p Location of file param job Hadoop job object return an Avro Schema object derived from the specified file throws Exception public Schema get Avro Schema final Path p final Job job throws Exception Generic Datum Reader Object avro Reader new Generic Datum Reader Object Array List File Status status List new Array List File Status File System fs File System get p to Uri job get Configuration for Path temp p for File Status tempf fs glob Status temp status List add tempf File Status status Array File Status status List to Array new File Status status List size if status Array null throw new Exception Path p to String does not exist if status Array length throw new Exception No path matches pattern p to String Path file Path Utils depth First Search For File status Array fs if file Path null throw new Exception No path matches pattern p to String Input Stream hdfs Input Stream fs open file Path Data File Stream Object avro Data Stream new Data File Stream Object hdfs Input Stream avro Reader Schema s avro Data Stream get Schema avro Data Stream close return s see org apache pig Load Metadata get Statistics java lang String org apache hadoop mapreduce Job Override public final Resource Statistics get Statistics final String location final Job job throws Exception return null see org apache pig Load Metadata get Partition Keys java lang String org apache hadoop mapreduce Job Override public final String get Partition Keys final String location final Job job throws Exception return null see org apache pig Load Metadata set Partition Filter org apache pig Expression Override public void set Partition Filter final Expression partition Filter throws Exception see org apache pig Store Func Interface rel To Abs Path For Store Location java lang String org apache hadoop fs Path Override public final String rel To Abs Path For Store Location final String location final Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir see org apache pig Store Func Interface get Output Format Override public Output Format Null Writable Object get Output Format throws Exception Hadoop output format for Avro Storage class Avro Storage Output Format extends File Output Format Null Writable Object Override public Record Writer Null Writable Object get Record Writer final Task Attempt Context tc throws Exception Interrupted Exception return new Avro Record Writer avro Storage Output Format Schema get Default Work File tc Avro Output Format tc get Configuration return new Avro Storage Output Format see org apache pig Store Func Interface set Store Location java lang String org apache hadoop mapreduce Job Override public final void set Store Location final String location final Job job throws Exception File Output Format set Output Path job new Path location Pig property name for the output avro schema public static final String org apache pig builtin Avro Storage output schema see org apache pig Store Func Interface check Schema org apache pig Resource Schema Override public final void check Schema final Resource Schema rs throws Exception if rs null throw new Exception check Schema called with null Resource Schema Schema avro Schema Avro Storage Schema Conversion Utilities resource Schema To Avro Schema rs schema Name null schema Name length pig output schema Name schema Name Space Maps String List Schema new Hash Map double Colons To Double Underscores if avro Schema null throw new Exception check Schema could not translate Resource Schema to Avro Schema set Output Avro Schema avro Schema Sets the output avro schema to s param s An Avro schema protected final void set Output Avro Schema final Schema s schema s get Properties set Property s to String Utility function that gets the output schema from the udf properties for this instance of the store function return the output schema associated with this protected final Schema get Output Avro Schema if schema null String schema String get Properties get Property if schema String null schema new Schema Parser parse schema String return schema Record Writer used by this instance private Record Writer Null Writable Object writer see org apache pig Store Func Interface prepare To Write org apache hadoop mapreduce Record Writer Suppress Warnings unchecked rawtypes Override public final void prepare To Write final Record Writer w throws Exception if this udf Context Signature null throw new Exception this get Class to String prepare To Write called without setting udf context signature writer Record Writer Null Writable Object w Avro Record Writer writer prepare To Write get Output Avro Schema see org apache pig Store Func Interface put Next org apache pig data Tuple Override public final void put Next final Tuple t throws Exception try writer write null t catch Interrupted Exception e log error Interrupted Exception in put Next throw new Exception e see org apache pig Store Func Interface set Store Func Context Signature java lang String Override public final void set Store Func Context Signature final String signature udf Context Signature signature super set Context Signature signature see org apache pig Store Func Interface cleanup On Failure java lang String org apache hadoop mapreduce Job Override public final void cleanup On Failure final String location final Job job throws Exception Store Func cleanup On Failure Impl location job Pig property name for the input avro schema public static final String org apache pig builtin Avro Storage input schema see org apache pig Load Func set Location java lang String org apache hadoop mapreduce Job Override public void set Location final String location final Job job throws Exception File Input Format set Input Paths job location if schema null schema get Input Avro Schema if schema null schema get Avro Schema location job if schema null throw new Exception Could not determine avro schema for location location set Input Avro Schema schema Sets the input avro schema to s param s The specified schema protected final void set Input Avro Schema final Schema s schema s get Properties set Property s to String Helper function reads the input avro schema from the Properties return The input avro schema public final Schema get Input Avro Schema if schema null update Schema From Input Avro Schema return schema Utility function that gets the input avro schema from the udf properties and updates schema for this instance private final void update Schema From Input Avro Schema String schema String get Properties get Property if schema String null Schema s new Schema Parser parse schema String schema s see org apache pig Load Func get Input Format Override public Input Format Null Writable Generic Data Record get Input Format throws Exception return new org apache pig backend hadoop executionengine map Reduce Layer Pig File Input Format Null Writable Generic Data Record Override public Record Reader Null Writable Generic Data Record create Record Reader final Input Split is final Task Attempt Context tc throws Exception Interrupted Exception Schema s get Input Avro Schema Record Reader Null Writable Generic Data Record rr null if s get Type Type rr new Avro Array Reader s else rr new Avro Record Reader s try rr initialize is tc finally rr close tc set Status is to String return rr Suppress Warnings rawtypes private Record Reader reader Pig Split split see org apache pig Load Func prepare To Read org apache hadoop mapreduce Record Reader org apache pig backend hadoop executionengine map Reduce Layer Pig Split Suppress Warnings rawtypes Override public final void prepare To Read final Record Reader r final Pig Split s throws Exception reader r split s see org apache pig Load Func get Next Override public final Tuple get Next throws Exception try if reader next Key Value return new Avro Tuple Wrapper Generic Data Record Generic Data Record reader get Current Value else return null catch Interrupted Exception e throw new Exception Wrapped Interrupted Exception e Override public void cleanup On Success final String location final Job job throws Exception Override public List Operator Set get Features return Lists new Array List Load Push Down Operator Set List of required fields passed by pig in a push down projection protected Required Field List required Field List see org apache pig Load Push Down push Projection org apache pig Load Push Down Required Field List Override public Required Field Response push Projection final Required Field List rfl throws Frontend Exception required Field List rfl Schema new Schema Avro Storage Schema Conversion Utilities new Schema From Required Field List schema rfl if new Schema null schema new Schema set Input Avro Schema schema return new Required Field Response true else log warn could not select fields subset rfl n warn could not select fields subset Pig Warning return new Required Field Response false Override public List String get Ship Files Class class List new Class Schema class Avro Input Format class return Func Utils get Ship Files class List 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer schema Schema This method should never be used directly use link public class Bag Size extends Eval Func Long Override public Long exec Tuple input throws Exception try Data Bag bag Data Bag input get return bag null null Long value Of bag size catch Exec Exception exp throw exp catch Exception e int err Code String msg Error while computing size in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema Flatten a bag into a string This will use the character as the default delimiter if one is not provided Example bag a b c Bag To String bag a b c Bag To String bag a b c bag a b c d e f Bag To String bag a b c d e f If input bag is null this will return null public class Bag To String extends Eval Func String private static final String Usage Bag To String data Bag or Bag To String data Bag delimiter private static final String Override public String exec Tuple input Tuple throws Exception if input Tuple size input Tuple size throw new Exec Exception Pig Exception Object first Arg input Tuple get if first Arg null return null if first Arg instanceof Data Bag throw new Exec Exception found type first Arg get Class get Name Pig Exception if input Tuple size input Tuple get instanceof String throw new Exec Exception Usage Bag To Tuple Data Bag String Pig Exception Data Bag bag Data Bag input Tuple get String delimeter if input Tuple size delimeter String input Tuple get String Builder buffer new String Builder try for Tuple t bag if t null for int i i t size i if buffer length buffer append delimeter buffer append t get i return buffer to String catch Exception e String msg Encourntered error while flattening a bag this get Class get Simple Name throw new Exec Exception msg Pig Exception e Override public Schema output Schema Schema input Schema try if input Schema null input Schema size input Schema size throw new Runtime Exception Expecting inputs found input Schema null input Schema size Field Schema input Field Schema input Schema get Field if input Field Schema type Data Type throw new Runtime Exception Expecting a bag of tuples found data type Data Type find Type Name input Field Schema type first field in the bag schema Field Schema first Field Schema input Field Schema schema get Field if first Field Schema null first Field Schema schema null first Field Schema schema size throw new Runtime Exception Expecting a bag and a delimeter found input Schema if first Field Schema type Data Type throw new Runtime Exception Expecting a bag and a delimeter found input Schema if input Schema size Field Schema second Input Field Schema input Schema get Field if second Input Field Schema type Data Type throw new Runtime Exception Expecting a bag and a delimeter found input Schema return new Schema new Schema Field Schema null Data Type catch Frontend Exception e e print Stack Trace return null Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Flatten a bag into a tuple This performs only flattening at the first level it does n t recursively flatten nested bags Example a b c a b c a b c d e f a b c d e f If input bag is null this will return null public class Bag To Tuple extends Eval Func Tuple Override public Tuple exec Tuple input Tuple throws Exception if input Tuple size throw new Exec Exception Expecting input found input Tuple size Pig Exception if input Tuple get null return null if input Tuple get instanceof Data Bag throw new Exec Exception Usage Bag To Tuple Data Bag Pig Exception Data Bag input Bag Data Bag input Tuple get try Tuple output Tuple null long output Tuple Size get Ouput Tuple Size input Bag Tuple Factory new Tuple int size can only support up to Integer if output Tuple Size Integer throw new Exec Exception Input bag is too large Pig Exception Tuple Factory tuple Factory Tuple Factory get Instance output Tuple tuple Factory new Tuple int output Tuple Size int field Num for Tuple t input Bag if t null for int i i t size i output Tuple set field Num t get i return output Tuple catch Exception e String msg Encourntered error while flattening a bag to tuple this get Class get Simple Name throw new Exec Exception msg Pig Exception e Calculate the size of the output tuple based on the sum of the size of each tuple in the input bag param bag return total of data elements in a tab private long get Ouput Tuple Size Data Bag bag long size if bag null for Tuple t bag size size t size return size Override public Schema output Schema Schema input Schema try if input Schema null input Schema size throw new Runtime Exception Expecting input found input Schema null input Schema size Schema Field Schema input Field Schema input Schema get Field if input Field Schema type Data Type throw new Runtime Exception Expecting a bag of tuples first field in the bag schema Schema Field Schema first Field Schema input Field Schema schema get Field if first Field Schema null first Field Schema schema null first Field Schema schema size throw new Runtime Exception Expecting a bag of tuples found input Schema if first Field Schema type Data Type throw new Runtime Exception Expecting a bag of tuples found input Schema now for output schema Schema tuple Output Schema new Schema for int i i first Field Schema schema size i tuple Output Schema add first Field Schema schema get Field i return new Schema new Schema Field Schema get Schema Name this get Class get Name to Lower Case input Schema tuple Output Schema Data Type catch Frontend Exception e e print Stack Trace return null Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan import java io Byte Array Output Stream import java io Print Stream import java util Array Deque import java util Array List import java util Deque import java util Hash Set import java util Iterator import java util List import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair public abstract class Base Operator Plan implements Operator Plan protected List Operator ops protected Plan Edge from Edges protected Plan Edge to Edges protected Plan Edge soft From Edges protected Plan Edge soft To Edges private List Operator roots private List Operator leaves protected static final Log log Log Factory get Log Base Operator Plan class public Base Operator Plan ops new Array List Operator roots new Array List Operator leaves new Array List Operator from Edges new Plan Edge to Edges new Plan Edge soft From Edges new Plan Edge soft To Edges new Plan Edge Suppress Warnings unchecked public Base Operator Plan Base Operator Plan other shallow copy constructor ops List Operator Array List Operator other ops clone roots List Operator Array List other roots clone leaves List Operator Array List other leaves clone from Edges other from Edges shallow Clone to Edges other to Edges shallow Clone soft From Edges other soft From Edges shallow Clone soft To Edges other soft To Edges shallow Clone Get number of nodes in the plan public int size return ops size Get all operators in the plan that have no predecessors return all operators in the plan that have no predecessors or an empty list if the plan is empty public List Operator get Sources if roots size ops size for Operator op ops if to Edges get op null roots add op return roots Get all operators in the plan that have no successors return all operators in the plan that have no successors or an empty list if the plan is empty public List Operator get Sinks if leaves size ops size for Operator op ops if from Edges get op null leaves add op return leaves For a given operator get all operators immediately before it in the plan param op operator to fetch predecessors of return list of all operators imeediately before op or an empty list if op is a root public List Operator get Predecessors Operator op return to Edges get op For a given operator get all operators immediately after it param op operator to fetch successors of return list of all operators imeediately after op or an empty list if op is a leaf public List Operator get Successors Operator op return from Edges get op For a given operator get all operators softly immediately before it in the plan param op operator to fetch predecessors of return list of all operators immediately before op or an empty list if op is a root public List Operator get Soft Link Predecessors Operator op return soft To Edges get op For a given operator get all operators softly immediately after it param op operator to fetch successors of return list of all operators immediately after op or an empty list if op is a leaf public List Operator get Soft Link Successors Operator op return soft From Edges get op Add a new operator to the plan It will not be connected to any existing operators param op operator to add public void add Operator op mark Dirty if ops contains op ops add op Remove an operator from the plan param op Operator to be removed throws Frontend Exception if the remove operation attempts to remove an operator that is still connected to other operators public void remove Operator op throws Frontend Exception if from Edges contains Key op to Edges contains Key op throw new Frontend Exception Attempt to remove operator op get Name that is still connected in the plan if soft From Edges contains Key op soft To Edges contains Key op throw new Frontend Exception Attempt to remove operator op get Name that is still softly connected in the plan mark Dirty ops remove op Connect two operators in the plan controlling which position in the edge lists that the from and to edges are placed param from Operator edge will come from param from Pos Position in the array for the from edge param to Operator edge will go to param to Pos Position in the array for the to edge public void connect Operator from int from Pos Operator to int to Pos if is Connected from to from null to null return mark Dirty from Edges put from to from Pos to Edges put to from to Pos Check if given two operators are directly connected param from Operator edge will come from param to Operator edge will go to public boolean is Connected Operator from Operator to List Operator preds get Predecessors to return preds null preds contains from Connect two operators in the plan param from Operator edge will come from param to Operator edge will go to public void connect Operator from Operator to if is Connected from to from null to null return mark Dirty from Edges put from to to Edges put to from Create an soft edge between two nodes param from Operator dependent upon param to Operator having the dependency public void create Soft Link Operator from Operator to soft From Edges put from to soft To Edges put to from Remove an soft edge param from Operator dependent upon param to Operator having the dependency public void remove Soft Link Operator from Operator to soft From Edges remove from to soft To Edges remove to from Disconnect two operators in the plan param from Operator edge is coming from param to Operator edge is going to return pair of positions indicating the position in the from and to arrays throws Frontend Exception if the two operators are n t connected public Pair Integer Integer disconnect Operator from Operator to throws Frontend Exception Pair Operator Integer f from Edges remove With Position from to if f null throw new Frontend Exception Attempt to disconnect operators from get Name and to get Name which are not connected Pair Operator Integer t to Edges remove With Position to from if t null throw new Frontend Exception Plan in inconssistent state from get Name and to get Name connected in from Edges but not to Edges mark Dirty return new Pair Integer Integer f second t second private void mark Dirty roots clear leaves clear public Iterator Operator get Operators return ops iterator public boolean is Equal Operator Plan other throws Frontend Exception return is Equal this other private static boolean check Predecessors Operator op Operator op throws Frontend Exception List Operator preds op get Plan get Predecessors op List Operator other Preds op get Plan get Predecessors op if preds null other Preds null intentionally blank else if preds null other Preds null return false else if preds size other Preds size return false for int i i preds size i Operator p preds get i Operator p other Preds get i if p is Equal p return false if check Predecessors p p return false return true protected static boolean is Equal Operator Plan p Operator Plan p throws Frontend Exception if p p return true if p null p null List Operator leaves p get Sinks List Operator other Leaves p get Sinks if leaves size other Leaves size return false Must find some leaf that is equal to each leaf There is no guarantee leaves will be returned in any particular order boolean found All true for Operator op leaves boolean found One false for Operator op other Leaves if op is Equal op check Predecessors op op found One true break found All found One if found All return false return found All return false public void explain Print Stream ps String format boolean verbose throws Frontend Exception Override public String to String Byte Array Output Stream os new Byte Array Output Stream Print Stream ps new Print Stream os try explain ps false catch Frontend Exception e return return os to String Override public void replace Operator old Operator Operator new Operator throws Frontend Exception add new Operator List Operator preds get Predecessors old Operator if preds null List Operator preds Copy new Array List Operator preds Copy add All preds for int i i preds Copy size i Operator pred preds Copy get i Pair Integer Integer pos disconnect pred old Operator connect pred pos first new Operator i List Operator succs get Successors old Operator if succs null List Operator succs Copy new Array List Operator succs Copy add All succs for int i i succs Copy size i Operator succ succs Copy get i Pair Integer Integer pos disconnect old Operator succ connect new Operator i succ pos second remove old Operator We assume if node has multiple inputs it only has one output if node has multiple outputs it only has one input Otherwise we do n t know how to connect inputs to outputs This assumption is true for logical plan physical plan and most plan Override public void remove And Reconnect Operator operator To Remove throws Frontend Exception List Operator preds Copy null if get Predecessors operator To Remove null get Predecessors operator To Remove size preds Copy new Array List Operator preds Copy add All get Predecessors operator To Remove List Operator succs Copy null if get Successors operator To Remove null get Successors operator To Remove size succs Copy new Array List Operator succs Copy add All get Successors operator To Remove if preds Copy null preds Copy size succs Copy null succs Copy size throw new Frontend Exception Can not remove and reconnect node with multiple inputs outputs if preds Copy null preds Copy size node has multiple inputs it can only has one output or no output reconnect inputs to output Operator succ null Pair Integer Integer pos null if succs Copy null succ succs Copy get pos disconnect operator To Remove succ for Operator pred preds Copy Pair Integer Integer pos disconnect pred operator To Remove if succ null connect pred pos first succ pos second else if succs Copy null succs Copy size node has multiple outputs it can only has one output or no output reconnect input to outputs Operator pred null Pair Integer Integer pos null if preds Copy null pred preds Copy get pos disconnect pred operator To Remove for Operator succ succs Copy Pair Integer Integer pos disconnect operator To Remove succ if pred null connect pred pos first succ pos second else Only have one input output Operator pred null Pair Integer Integer pos null if preds Copy null pred preds Copy get pos disconnect pred operator To Remove Operator succ null Pair Integer Integer pos null if succs Copy null succ succs Copy get pos disconnect operator To Remove succ if pred null succ null connect pred pos first succ pos second remove operator To Remove Override public void insert Between Operator pred Operator operator To Insert Operator succ throws Frontend Exception add operator To Insert Pair Integer Integer pos disconnect pred succ connect pred pos first operator To Insert connect operator To Insert succ pos second method to check if there is a path from a given node to another node param from the start node for checking param to the end node for checking return true if path exists false otherwise public boolean path Exists Operator from Operator to List Operator successors get Successors from if successors null successors size return false for Operator successor successors if successor equals to path Exists successor to return true return false Move everything below a given operator to the new operator plan The specified operator will be moved and will be the root of the new operator plan param root Operator to move everything after param new Plan new operator plan to move things into throws Plan Exception public void move Tree Operator root Base Operator Plan new Plan throws Frontend Exception Deque Operator queue new Array Deque Operator new Plan add root root set Plan new Plan queue add Last root while queue is Empty Operator node queue poll if get Successors node null for Operator succ get Successors node if queue contains succ queue add Last succ new Plan add succ succ set Plan new Plan new Plan connect node succ trim Below root Trim everything below a given operator The specified operator will be removed param op Operator to trim everything after throws Frontend Exception public void trim Below Operator op throws Frontend Exception if get Successors op null List Operator succs new Array List Operator succs add All get Successors op for Operator succ succs disconnect op succ trim Below succ remove succ 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Array List import java util List import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig impl util Identity Hash Set base class for all Binary expression operators Supports the lhs and rhs operators which are used to fetch the inputs and apply the appropriate operation with the appropriate type public abstract class Binary Expression Operator extends Expression Operator private static final long serial Version protected Expression Operator lhs protected Expression Operator rhs private transient List Expression Operator child public Binary Expression Operator Operator Key k this k public Binary Expression Operator Operator Key k int rp super k rp public Expression Operator get Lhs return lhs Get the child expressions of this expression public List Expression Operator get Child Expressions if child null child new Array List Expression Operator child add lhs child add rhs return child Override public boolean supports Multiple Inputs return true public void set Lhs Expression Operator lhs this lhs lhs public Expression Operator get Rhs return rhs public void set Rhs Expression Operator rhs this rhs rhs protected void clone Helper Binary Expression Operator op Do n t clone these as they are just references to things already in the plan lhs op lhs rhs op rhs super clone Helper op Override public Tuple illustrator Markup Object in Object out int eq Class Index return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Source Location public class Bin Cond Expression extends Logical Expression Will add this operator to the plan and connect it to the left and right hand side operators and the condition operator param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Bin Cond Expression Operator Plan plan Logical Expression condition Logical Expression lhs Logical Expression rhs super Bin Cond plan plan add this plan connect this condition plan connect this lhs plan connect this rhs Returns the operator which handles this condition return expression which handles the condition throws Frontend Exception public Logical Expression get Condition throws Frontend Exception return Logical Expression plan get Successors this get Get the left hand side of this expression return expression on the left hand side throws Frontend Exception public Logical Expression get Lhs throws Frontend Exception return Logical Expression plan get Successors this get Get the right hand side of this expression return expression on the right hand side throws Frontend Exception public Logical Expression get Rhs throws Frontend Exception return Logical Expression plan get Successors this get link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Bin Cond Expression Bin Cond Expression ao Bin Cond Expression other return ao get Condition is Equal get Condition ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema Type Checking Exp Visitor will ensure that lhs and rhs have same schema Logical Field Schema arg Fs get Lhs get Field Schema field Schema arg Fs deep Copy field Schema reset Uid uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Bin Cond Expression lg Exp Plan this get Condition deep Copy lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Byte Array Input Stream import java io Data Input import java io Data Input Stream import java io Data Output import java io Exception import java io Unsupported Encoding Exception import java math Big Decimal import java math Big Integer import java nio Byte Buffer import java util Hash Map import java util Iterator import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io Data Input Buffer import org apache hadoop io Writable import org apache hadoop io Writable Comparable import org apache hadoop io Writable Comparator import org apache hadoop mapred Job Conf import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig data utils Sedes Helper import org apache pig impl util Object Serializer import org joda time Date Time import org joda time Date Time Zone class to handle reading and writing of intermediate results of data types The serialization format used by this class more efficient than what was used in Data Reader Writer The format used by the functions in this class is subject to change so it should be used to store intermediate results within a pig query Interface Audience Private Interface Stability Stable public class Bin Inter Sedes implements Inter Sedes private static final int public static final byte public static final byte public static final byte public static final byte since boolean is not supported yet v as external type lot of people use int instead and some data with old schema is likely stay for some time so optimizing for that case as well public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte private static Tuple Factory m Tuple Factory Tuple Factory get Instance private static Bag Factory m Bag Factory Bag Factory get Instance public static final int public static final int public static final String public Tuple read Tuple Data Input in byte type throws Exception switch type case case case case case case case case case case case case case return Sedes Helper read Generic Tuple in type case case case return read Schema Tuple in type default throw new Exec Exception Unknown Tuple type found in stream type private Tuple read Schema Tuple Data Input in byte type throws Exception int id switch type case id in read Unsigned Byte break case id in read Unsigned Short break case id in read Int break default throw new Runtime Exception Invalid type given to read Schema Tuple type Tuple st Schema Tuple Factory get Instance id new Tuple st read Fields in return st public int get Tuple Size Data Input in byte type throws Exception int sz switch type case return case return case return case return case return case return case return case return case return case return case sz in read Unsigned Byte break case sz in read Unsigned Short break case sz in read Int break default int err Code String msg Unexpected datatype type while reading tuple from binary file throw new Exec Exception msg err Code Pig Exception if sz we construct an empty tuple presumably the writer wrote an empty tuple if sz throw new Exception Invalid size sz for a tuple return sz private Data Bag read Bag Data Input in byte type throws Exception Data Bag bag m Bag Factory new Default Bag long size determine size of bag switch type case size in read Unsigned Byte break case size in read Unsigned Short break case size in read Long break default int err Code String msg Unexpected data while reading bag from binary file throw new Exec Exception msg err Code Pig Exception for long i i size i try Object o read Datum in bag add Tuple o catch Exec Exception ee throw ee return bag private Map String Object read Map Data Input in byte type throws Exception int size switch type case size in read Unsigned Byte break case size in read Unsigned Short break case size in read Int break default int err Code String msg Unexpected data while reading map from binary file throw new Exec Exception msg err Code Pig Exception Map String Object m new Hash Map String Object size for int i i size i String key String read Datum in m put key read Datum in return m private Internal Map read Internal Map Data Input in throws Exception int size in read Int Internal Map m new Internal Map size for int i i size i Object key read Datum in m put key read Datum in return m private Writable Comparable read Writable Data Input in throws Exception String class Name String read Datum in create the writeable class It needs to have a default constructor Class obj Class null try obj Class Class for Name class Name catch Class Not Found Exception e throw new Exception Could not find class class Name while attempting to de serialize it e Writable Comparable writable null try writable Writable Comparable obj Class new Instance catch Exception e String msg Could create instance of class class Name while attempting to de serialize it no default constructor throw new Exception msg e read the fields of the object from Data Input writable read Fields in return writable non Javadoc see org apache pig data Inter Sedes read Datum java io Data Input Override public Object read Datum Data Input in throws Exception Exec Exception Read the data type byte b in read Byte return read Datum in b private static Object read Bytes Data Input in int size throws Exception byte ba new byte size in read Fully ba return new Data Byte Array ba Expects bin Inter Sedes data types Data Type types p see org apache pig data Inter Sedes read Datum java io Data Input byte Override public Object read Datum Data Input in byte type throws Exception Exec Exception switch type case case case case case case case case case case case case case return Sedes Helper read Generic Tuple in type case case case return read Bag in type case case case return read Map in type case return read Internal Map in case return Integer value Of case return Integer value Of case return Integer value Of in read Byte case return Integer value Of in read Short case return Integer value Of in read Int case return Long value Of case return Long value Of case return Long value Of in read Byte case return Long value Of in read Short case return Long value Of in read Int case return Long value Of in read Long case return new Date Time in read Long Date Time Zone for Offset Millis in read Short case return Float value Of in read Float case return Double value Of in read Double case return read Big Integer in case return read Big Decimal in case return Boolean value Of true case return Boolean value Of false case return Byte value Of in read Byte case case case return new Data Byte Array Sedes Helper read Bytes in type case case return Sedes Helper read Chararray in type case return read Writable in case case case return read Schema Tuple in type case return null default throw new Runtime Exception Unexpected data type type found in stream private Object read Big Decimal Data Input in throws Exception return new Big Decimal String read Datum in private Object read Big Integer Data Input in throws Exception return new Big Integer String read Datum in private void write Big Integer Data Output out Big Integer bi throws Exception write Datum out bi to String private void write Big Decimal Data Output out Big Decimal bd throws Exception write Datum out bd to String non Javadoc see org apache pig data Inter Sedes write Datum java io Data Output java lang Object Override public void write Datum Data Output out Object val throws Exception Read the data type byte type Data Type find Type val write Datum out val type Override Suppress Warnings unchecked public void write Datum Data Output out Object val byte type throws Exception switch type case Data Type write Tuple out Tuple val break case Data Type write Bag out Data Bag val break case Data Type write Map out Map String Object val break case Data Type out write Byte Map Object Object m Map Object Object val out write Int m size Iterator Map Entry Object Object i m entry Set iterator while i has Next Map Entry Object Object entry i next write Datum out entry get Key write Datum out entry get Value break case Data Type int i Integer val if i out write Byte else if i out write Byte else if Byte i i Byte out write Byte out write Byte i else if Short i i Short out write Byte out write Short i else out write Byte out write Int i break case Data Type long lng Long val if lng out write Byte else if lng out write Byte else if Byte lng lng Byte out write Byte out write Byte int lng else if Short lng lng Short out write Byte out write Short int lng else if Integer lng lng Integer out write Byte out write Int int lng else out write Byte out write Long lng break case Data Type out write Byte out write Long Date Time val get Millis out write Short Date Time val get Zone get Offset Date Time val break case Data Type out write Byte out write Float Float val break case Data Type out write Byte write Big Integer out Big Integer val break case Data Type out write Byte write Big Decimal out Big Decimal val break case Data Type out write Byte out write Double Double val break case Data Type if Boolean val out write Byte else out write Byte break case Data Type out write Byte out write Byte Byte val break case Data Type Data Byte Array bytes Data Byte Array val Sedes Helper write Bytes out bytes m Data break case Data Type Sedes Helper write Chararray out String val break case Data Type out write Byte store the class name so we know the class to create on read write Datum out val get Class get Name Writable writable Writable val writable write out break case Data Type out write Byte break default throw new Runtime Exception Unexpected data type val get Class get Name found in stream Note only standard Pig type is supported when you output from Load Func private void write Map Data Output out Map String Object m throws Exception final int sz m size if sz out write Byte out write Byte sz else if sz out write Byte out write Short sz else out write Byte out write Int sz Iterator Map Entry String Object i m entry Set iterator while i has Next Map Entry String Object entry i next write Datum out entry get Key write Datum out entry get Value private void write Bag Data Output out Data Bag bag throws Exception We do n t care whether this bag was sorted or distinct because using the iterator to write it will guarantee those things come correctly And on the other end there ll be no reason to waste time re sorting or re applying distinct final long sz bag size if sz out write Byte out write Byte int sz else if sz out write Byte out write Short int sz else out write Byte out write Long sz Iterator Tuple it bag iterator while it has Next write Tuple out it next private void write Tuple Data Output out Tuple t throws Exception if t instanceof Type Aware Tuple t write out else Sedes Helper write Generic Tuple out t non Javadoc see org apache pig data Inter Sedes add Cols To Tuple java io Data Input org apache pig data Tuple Override public void add Cols To Tuple Data Input in Tuple t throws Exception byte type in read Byte int sz get Tuple Size in type for int i i sz i t append read Datum in public static class Bin Inter Sedes Tuple Raw Comparator extends Writable Comparator implements Tuple Raw Comparator private final Log m Log Log Factory get Log get Class private boolean m Asc private boolean m Secondary Asc private static final boolean new boolean private boolean m Whole Tuple private boolean m Is Secondary Sort private boolean m Has Null Field private Tuple Factory m Fact private Inter Sedes m Sedes public Bin Inter Sedes Tuple Raw Comparator super Bin Sedes Tuple class Override public Configuration get Conf return null Override public void set Conf Configuration conf try m Asc boolean Object Serializer deserialize conf get pig sort Order m Secondary Asc boolean Object Serializer deserialize conf get pig secondary Sort Order m Is Secondary Sort true catch Exception ioe m Log error Unable to deserialize sort order object ioe get Message throw new Runtime Exception ioe if m Asc null m Asc new boolean m Asc true if m Secondary Asc null m Is Secondary Sort false If there s only one entry in m Asc it means it s for the whole tuple So we ca n t be looking for each column m Whole Tuple m Asc length m Fact Tuple Factory get Instance m Sedes Inter Sedes Factory get Inter Sedes Instance Override public boolean has Compared Tuple Null return m Has Null Field Compare two Bin Sedes Tuples as raw bytes We assume the Tuples are Pig Nullable Writable so client classes need to deal with Null and Index Override public int compare byte b int s int l byte b int s int l int rc Byte Buffer bb Byte Buffer wrap b s l Byte Buffer bb Byte Buffer wrap b s l try rc compare Bin Sedes Tuple bb bb catch Exception ioe m Log error error during tuple comparison ioe get Message throw new Runtime Exception ioe get Message ioe return rc Compare two Bin Sedes Tuples as raw bytes We deal with sort ordering in this method throws Exception private int compare Bin Sedes Tuple Byte Buffer bb Byte Buffer bb throws Exception m Has Null Field false store the position in case of deserialization int s bb position int s bb position treat the outermost tuple differently because we have to deal with sort order int result try first compare sizes int tsz read Size bb int tsz read Size bb if tsz tsz return else if tsz tsz return else if sizes are the same compare field by field if m Is Secondary Sort we have a compound tuple key main key secondary key Each key has its own sort order so we have to deal with them separately We delegate it to the first invocation of compare Bin Inter Sedes Datum assert tsz main key secondary key result compare Bin Inter Sedes Datum bb bb m Asc if result result compare Bin Inter Sedes Datum bb bb m Secondary Asc else we have just one tuple key we deal with sort order here for int i i tsz result i is used to distinguish original calls from recursive ones hack ish result compare Bin Inter Sedes Datum bb bb flip if the order is descending if result if m Whole Tuple m Asc i result else if m Whole Tuple m Asc result catch Unsupported Encoding Exception uee Tuple t m Fact new Tuple Tuple t m Fact new Tuple t read Fields new Data Input Stream new Byte Array Input Stream bb array s bb limit t read Fields new Data Input Stream new Byte Array Input Stream bb array s bb limit delegate to compare result compare t t return result private int compare Bin Inter Sedes Datum Byte Buffer bb Byte Buffer bb boolean asc throws Exception int rc byte type type byte dt bb get byte dt bb get switch dt case Bin Inter Sedes type Data Type type get Generalized Data Type dt if asc null we are scanning the top level Tuple original call m Has Null Field true if type type rc break case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type false true int bv dt Bin Inter Sedes int bv dt Bin Inter Sedes rc bv bv break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type byte bv bb get byte bv bb get rc bv bv bv bv break case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type int iv read Int bb dt int iv read Int bb dt rc iv iv iv iv break case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type long lv read Long bb dt long lv read Long bb dt rc lv lv lv lv break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type long lv bb get Long bb position bb position move cursor forward without read the timezone bytes long lv bb get Long bb position bb position rc lv lv lv lv break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type float fv bb get Float float fv bb get Float rc Float compare fv fv break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type double dv bb get Double double dv bb get Double rc Double compare dv dv break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type int sz read Size bb bb get int sz read Size bb bb get byte ca new byte sz byte ca new byte sz bb get ca bb get ca String str null str null try str new String ca Bin Inter Sedes str new String ca Bin Inter Sedes catch Unsupported Encoding Exception uee m Log warn Unsupported string encoding uee uee print Stack Trace if str null str null rc new Big Integer str compare To new Big Integer str break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type int sz read Size bb bb get int sz read Size bb bb get byte ca new byte sz byte ca new byte sz bb get ca bb get ca String str null str null try str new String ca Bin Inter Sedes str new String ca Bin Inter Sedes catch Unsupported Encoding Exception uee m Log warn Unsupported string encoding uee uee print Stack Trace if str null str null rc new Big Decimal str compare To new Big Decimal str break case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type int basz read Size bb dt int basz read Size bb dt rc Writable Comparator compare Bytes bb array bb position basz bb array bb position basz bb position bb position basz bb position bb position basz break case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type int casz read Size bb dt int casz read Size bb dt String str null str null try str new String bb array bb position casz Bin Inter Sedes str new String bb array bb position casz Bin Inter Sedes catch Unsupported Encoding Exception uee m Log warn Unsupported string encoding uee uee print Stack Trace finally bb position bb position casz bb position bb position casz if str null str null rc str compare To str break case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type first compare sizes int tsz read Size bb dt int tsz read Size bb dt if tsz tsz return else if tsz tsz return else if sizes are the same compare field by field If we are doing secondary sort use the sort order passed by the caller Inner tuples never have sort order so we pass null for int i i tsz rc i rc compare Bin Inter Sedes Datum bb bb null if rc asc null asc length asc i rc break case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type rc compare Bin Inter Sedes Bag bb bb dt dt break case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type rc compare Bin Inter Sedes Map bb bb dt dt break case Bin Inter Sedes type Data Type type get Generalized Data Type dt if type type rc compare Bin Inter Sedes Generic Writable Comparable bb bb break default m Log info Unsupported Data Type for binary comparison switching to object deserialization Data Type gen Type To Name Map get dt dt throw new Unsupported Encoding Exception compare generalized data types if type type rc type type apply sort order for keys that are not tuples or for whole tuples if asc null asc length asc rc return rc Suppress Warnings rawtypes Override public int compare Writable Comparable o Writable Comparable o Tuple t Tuple o Tuple t Tuple o m Has Null Field false treat the outermost tuple differently because we have to deal with sort order int result first compare sizes int tsz t size int tsz t size if tsz tsz return else if tsz tsz return else try if sizes are the same compare field by field if m Is Secondary Sort we have a compound tuple key main key secondary key Each key has its own sort order so we have to deal with them separately We delegate it to the first invocation of compare Datum assert tsz main key secondary key result compare Datum t get t get m Asc if result result compare Datum t get t get m Secondary Asc else we have just one tuple key and no chance of recursion we delegate dealing with sort order to compare Datum result compare Datum t t m Asc catch Exec Exception e throw new Runtime Exception Unable to compare tuples e return result private int compare Datum Object o Object o boolean asc int rc if o null o null o instanceof Tuple o instanceof Tuple objects are Tuples we may need to apply sort order inside them Tuple t Tuple o Tuple t Tuple o int sz t size int sz t size if sz sz return else if sz sz return else for int i i sz i try rc Data Type compare t get i t get i if rc asc null asc length asc i rc if t get i null record if the tuple has a null field m Has Null Field true if rc break catch Exec Exception e throw new Runtime Exception Unable to compare tuples e else objects are Tuples delegate to Data Type compare rc Data Type compare o o apply sort order for keys that are not tuples or for whole tuples if asc null asc length asc rc return rc Suppress Warnings unchecked rawtypes private int compare Bin Inter Sedes Generic Writable Comparable Byte Buffer bb Byte Buffer bb throws Exec Exception Exception Data Input Buffer buffer new Data Input Buffer Data Input Buffer buffer new Data Input Buffer buffer reset bb array bb position bb remaining buffer reset bb array bb position bb remaining Comparable writable Comparable m Sedes read Datum buffer Comparable writable Comparable m Sedes read Datum buffer bb position buffer get Position bb position buffer get Position return writable compare To writable Suppress Warnings unchecked private int compare Bin Inter Sedes Bag Byte Buffer bb Byte Buffer bb byte dt byte dt throws Exception int s bb position int s bb position int l bb remaining int l bb remaining first compare sizes int bsz read Size bb dt int bsz read Size bb dt if bsz bsz return else if bsz bsz return else Data Input Buffer buffer new Data Input Buffer Data Input Buffer buffer new Data Input Buffer buffer reset bb array s l buffer reset bb array s l Data Bag bag Data Bag m Sedes read Datum buffer dt Data Bag bag Data Bag m Sedes read Datum buffer dt bb position buffer get Position bb position buffer get Position return bag compare To bag Suppress Warnings unchecked private int compare Bin Inter Sedes Map Byte Buffer bb Byte Buffer bb byte dt byte dt throws Exec Exception Exception int s bb position int s bb position int l bb remaining int l bb remaining first compare sizes int bsz read Size bb dt int bsz read Size bb dt if bsz bsz return else if bsz bsz return else Data Input Buffer buffer new Data Input Buffer Data Input Buffer buffer new Data Input Buffer buffer reset bb array s l buffer reset bb array s l Map String Object map Map String Object m Sedes read Datum buffer dt Map String Object map Map String Object m Sedes read Datum buffer dt bb position buffer get Position bb position buffer get Position return Data Type compare map map Data Type Data Type private static byte get Generalized Data Type byte type switch type case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type case Bin Inter Sedes return Data Type default throw new Runtime Exception Unexpected data type type found in stream private static long read Long Byte Buffer bb byte type int bytes To Read switch type case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return bb get case Bin Inter Sedes return bb get Short case Bin Inter Sedes return bb get Int case Bin Inter Sedes return bb get Long default throw new Runtime Exception Unexpected data type type found in stream private static int read Int Byte Buffer bb byte type switch type case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return bb get case Bin Inter Sedes return bb get Short case Bin Inter Sedes return bb get Int default throw new Runtime Exception Unexpected data type type found in stream param bb Byte Buffer having serialized object including the type information param type serialized type information return the size of this type private static int read Size Byte Buffer bb return read Size bb bb get param bb Byte Buffer having serialized object minus the type information param type serialized type information return the size of this type private static int read Size Byte Buffer bb byte type switch type case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return get Unsigned Byte bb case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return get Unsigned Short bb case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes case Bin Inter Sedes return bb get Int case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return case Bin Inter Sedes return default throw new Runtime Exception Unexpected data type type found in stream same as format used by Data Input Data Output for unsigned short private static int get Unsigned Short Byte Buffer bb return bb get xff bb get xff same as format used by Data Input Data Output for unsigned byte private static int get Unsigned Byte Byte Buffer bb return bb get xff Override public Class extends Tuple Raw Comparator get Tuple Raw Comparator Class return Bin Inter Sedes Tuple Raw Comparator class public Tuple read Tuple Data Input in throws Exception return read Tuple in in read Byte public static boolean is Tuple Byte byte b return b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes b Bin Inter Sedes 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Data Input import java io Data Output import java io Exception import java util List import org apache pig classification Interface Audience This tuple has a faster de serialization mechanism It to be used for storing intermediate data between Map and Reduce and between jobs This is for internal pig use only The serialization format can change so do not use it for storing any persistant data ie in load store functions Interface Audience Private public class Bin Sedes Tuple extends Default Tuple private static final long serial Version private static final Inter Sedes sedes Inter Sedes Factory get Inter Sedes Instance Override public void write Data Output out throws Exception sedes write Datum out this Data Type Override public void read Fields Data Input in throws Exception Clear our fields in case we re being reused m Fields clear sedes add Cols To Tuple in this Default constructor Bin Sedes Tuple super Construct a tuple with a known number of fields Package level so that callers can not directly invoke it param size Number of fields to allocate in the tuple Bin Sedes Tuple int size super size Construct a tuple from an existing list of objects Package level so that callers can not directly invoke it param c List of objects to turn into a tuple Bin Sedes Tuple List Object c super c Construct a tuple from an existing list of objects Package level so that callers can not directly invoke it param c List of objects to turn into a tuple This list will be kept as part of the tuple param junk Just used to differentiate from the constructor above that copies the list Bin Sedes Tuple List Object c int junk super c junk public static Class extends Tuple Raw Comparator get Comparator Class return Inter Sedes Factory get Inter Sedes Instance get Tuple Raw Comparator Class 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java util List import org apache pig backend executionengine Exec Exception import org apache pig classification Interface Audience Default implementation of Tuple Factory Interface Audience Private public class Bin Sedes Tuple Factory extends Tuple Factory Override public Tuple new Tuple return new Bin Sedes Tuple Override public Tuple new Tuple int size return new Bin Sedes Tuple size Override Suppress Warnings unchecked public Tuple new Tuple List c return new Bin Sedes Tuple c Override Suppress Warnings unchecked public Tuple new Tuple No Copy List list return new Bin Sedes Tuple list Override public Tuple new Tuple Object datum Tuple t new Bin Sedes Tuple try t set datum catch Exec Exception e The world has come to an end we just allocated a tuple with one slot but we ca n t write to that slot throw new Runtime Exception Unable to write to field in newly allocated tuple of size e return t Override public Class extends Tuple tuple Class return Bin Sedes Tuple class Override public Class extends Tuple Raw Comparator tuple Raw Comparator Class return Bin Sedes Tuple get Comparator Class Override public boolean is Fixed Size return false 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Byte Array Output Stream import java io Data Output Stream import java io Exception import java math Big Decimal import java math Big Integer import java util Iterator import java util Map import java util Properties import org joda time Date Time import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib output File Output Format import org apache pig Expression import org apache pig File Input Load Func import org apache pig Load Caster import org apache pig Load Func import org apache pig Load Metadata import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Resource Statistics import org apache pig Store Func import org apache pig Store Func Interface import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop datastorage Data Storage import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig data Data Bag import org apache pig data Data Reader Writer import org apache pig data Tuple import org apache pig impl io Bin Storage Input Format import org apache pig impl io Bin Storage Output Format import org apache pig impl io Bin Storage Record Reader import org apache pig impl io Bin Storage Record Writer import org apache pig impl io File Localizer import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Utils Load and store data in a binary format This class is used by Pig to move data between Map Reduce jobs Use of this function for storing user data is supported public class Bin Storage extends File Input Load Func implements Store Func Interface Load Metadata static class Un Implemented Load Caster implements Load Caster private static final String un Implemented Error Message Can not cast bytes loaded from Bin Storage Please provide a custom converter Override public Data Bag bytes To Bag byte b Resource Field Schema field Schema throws Exception throw new Exec Exception un Implemented Error Message Override public String bytes To Char Array byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Double bytes To Double byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Float bytes To Float byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Integer bytes To Integer byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Long bytes To Long byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Boolean bytes To Boolean byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Date Time bytes To Date Time byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Map String Object bytes To Map byte b Resource Field Schema field Schema throws Exception throw new Exec Exception un Implemented Error Message Override public Tuple bytes To Tuple byte b Resource Field Schema field Schema throws Exception throw new Exec Exception un Implemented Error Message Override public Big Integer bytes To Big Integer byte b throws Exception throw new Exec Exception un Implemented Error Message Override public Big Decimal bytes To Big Decimal byte b throws Exception throw new Exec Exception un Implemented Error Message Iterator Tuple i null private static final Log m Log Log Factory get Log Bin Storage class protected long end Long private String caster String null private Load Caster caster null private Bin Storage Record Reader rec Reader null private Bin Storage Record Writer rec Writer null public Bin Storage If user knows how to cast the bytes for Bin Storage provide the class name for the caster When we later want to convert bytes to other types Bin Storage knows how This provides a way for user to store intermediate data without having to explicitly list all the fields and figure out their parts public Bin Storage String caster String this caster String caster String Override public Tuple get Next throws Exception if rec Reader next Key Value return rec Reader get Current Value else return null Override public void put Next Tuple t throws Exception try rec Writer write null t catch Interrupted Exception e throw new Exception e public byte to Bytes Data Bag bag throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos bag catch Exception ee int err Code String msg Error while converting bag to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes String s throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos s catch Exception ee int err Code String msg Error while converting chararray to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Double d throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos d catch Exception ee int err Code String msg Error while converting double to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Float f throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos f catch Exception ee int err Code String msg Error while converting float to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Integer i throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos i catch Exception ee int err Code String msg Error while converting int to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Long l throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos l catch Exception ee int err Code String msg Error while converting long to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Boolean b throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos b catch Exception ee int err Code String msg Error while converting boolean to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Date Time dt throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos dt catch Exception ee int err Code String msg Error while converting datetime to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Map String Object m throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos m catch Exception ee int err Code String msg Error while converting map to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array public byte to Bytes Tuple t throws Exception Byte Array Output Stream baos new Byte Array Output Stream Data Output Stream dos new Data Output Stream baos try Data Reader Writer write Datum dos t catch Exception ee int err Code String msg Error while converting tuple to bytes throw new Exec Exception msg err Code Pig Exception ee return baos to Byte Array Override public Input Format get Input Format return new Bin Storage Input Format Override public int hash Code return Suppress Warnings unchecked Override public Load Caster get Load Caster throws Exception if caster null Class Load Caster caster Class null if caster String null Class Loader cl Thread current Thread get Context Class Loader try Try caster String as a fully qualified name caster Class Class Load Caster cl load Class caster String catch Class Not Found Exception e if caster Class null try Try caster String as in builtin caster Class Class Load Caster cl load Class org apache pig builtin caster String catch Class Not Found Exception e throw new Frontend Exception Can not find Load Caster class caster String e try caster caster Class new Instance catch Exception e throw new Frontend Exception Can not instantiate class caster String e else caster new Un Implemented Load Caster return caster Override public void prepare To Read Record Reader reader Pig Split split rec Reader Bin Storage Record Reader reader Override public void set Location String location Job job throws Exception File Input Format set Input Paths job location Override public Output Format get Output Format return new Bin Storage Output Format Override public void prepare To Write Record Writer writer this rec Writer Bin Storage Record Writer writer Override public void set Store Location String location Job job throws Exception File Output Format set Output Path job new Path location Override public void check Schema Resource Schema s throws Exception Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir Override public String get Partition Keys String location Job job throws Exception return null Suppress Warnings deprecation Override public Resource Schema get Schema String location Job job throws Exception Configuration conf job get Configuration Properties props Configuration Util to Properties conf At compile time in batch mode the file may not exist such as intermediate file Just return null the same way as we would if we did not get a valid record String locations get Path Strings location for String loc locations since local mode now is implemented as hadoop s local mode we can treat either local or hadoop mode as hadoop mode hence we can use Data Storage and File Localizer open File below Data Storage storage try storage new Data Storage new org apache hadoop fs Path loc to Uri props catch Runtime Exception e throw new Exception e if File Localizer file Exists loc storage return null return Utils get Schema this location false job Override public Resource Statistics get Statistics String location Job job throws Exception return null Override public void set Partition Filter Expression plan throws Exception throw new Unsupported Operation Exception Override public void set Store Func Context Signature String signature Override public void cleanup On Failure String location Job job throws Exception Store Func cleanup On Failure Impl location job Override public void cleanup On Success String location Job job throws Exception do nothing 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig validator import java util Set import org apache pig Pig Configuration import org apache pig Pig Server import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import com google common base Splitter import com google common collect Sets This Filter handles black and whitelisting of Pig commands public final class Black And Whitelist Filter implements Pig Command Filter private static final int private static final Splitter Splitter on trim Results omit Empty Strings private final Pig Context context private final Set String whitelist private final Set String blacklist public Black And Whitelist Filter Pig Server pig Server this pig Server get Pig Context public Black And Whitelist Filter Pig Context context this context context whitelist Sets new Hash Set blacklist Sets new Hash Set init private void init String whitelist Config context get Properties get Property Pig Configuration if whitelist Config null Iterable String iter split whitelist Config for String elem iter whitelist add elem to Upper Case String blacklist Config context get Properties get Property Pig Configuration if blacklist Config null Iterable String iter split blacklist Config for String elem iter String u Elem elem to Upper Case if whitelist contains u Elem throw new Illegal State Exception Conflict between whitelist and blacklist elem appears in both blacklist add u Elem Override public void validate Command command throws Frontend Exception if blacklist contains command name throw new Frontend Exception command name command is not permitted check for size of whitelist as an empty whitelist should not disallow using Pig commands altogether if whitelist size whitelist contains command name throw new Frontend Exception command name command is not permitted 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig validator import java util Set import org apache pig Pig Configuration import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Depth First Walker import org apache pig newplan Operator Plan import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Cube import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Native import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical rules Logical Relational Node Validator import com google common base Splitter import com google common collect Sets This validator walks through the list of operators defined in link Pig Configuration and link Pig Configuration and checks whether the operation is permitted In case these properties are not defined default we let everything pass as usual public final class Black And Whitelist Validator implements Logical Relational Node Validator private final Pig Context pig Context private final Operator Plan operator Plan public Black And Whitelist Validator Pig Context pig Context Operator Plan operator Plan this pig Context pig Context this operator Plan operator Plan public void validate throws Frontend Exception Black And Whitelist Visitor visitor new Black And Whitelist Visitor this operator Plan visitor visit private class Black And Whitelist Visitor extends Logical Relational Nodes Visitor private static final int private final Splitter splitter private final Set String blacklist private final Set String whitelist protected Black And Whitelist Visitor Operator Plan plan throws Frontend Exception super plan new Depth First Walker plan blacklist Sets new Hash Set whitelist Sets new Hash Set splitter Splitter on trim Results omit Empty Strings init private void init String blacklist Config pig Context get Properties get Property Pig Configuration Set blacklist only if it s been defined by a user if blacklist Config null Iterable String iter splitter split blacklist Config for String elem iter blacklist add elem to Lower Case String whitelist Config pig Context get Properties get Property Pig Configuration Set whitelist only if it s been defined by a user if whitelist Config null Iterable String iter splitter split whitelist Config for String elem iter String l Elem elem to Lower Case if blacklist contains l Elem throw new Illegal State Exception Conflict between whitelist and blacklist elem appears in both whitelist add l Elem private void check String operator throws Frontend Exception throw an exception if the operator is not defined in whitelist if whitelist null whitelist size whitelist contains operator throw new Frontend Exception operator is disabled throw an exception if operator is defined in blacklist if blacklist null blacklist size blacklist contains operator throw new Frontend Exception operator is disabled Override public void visit Load load throws Frontend Exception check load Override public void visit Filter filter throws Frontend Exception check filter Override public void visit Store store throws Frontend Exception check store Override public void visit Join join throws Frontend Exception check join Override public void visit For Each foreach throws Frontend Exception check foreach Override public void visit Generate gen throws Frontend Exception public void visit Inner Load load throws Frontend Exception Override public void visit Cube cube throws Frontend Exception check cube public void visit Cogroup lo Cogroup throws Frontend Exception check group check cogroup Override public void visit Split lo Split throws Frontend Exception check split Override public void visit Split Output lo Split Output throws Frontend Exception Override public void visit Union lo Union throws Frontend Exception check union Override public void visit Sort lo Sort throws Frontend Exception check order Override public void visit Rank lo Rank throws Frontend Exception check rank Override public void visit Distinct lo Distinct throws Frontend Exception check distinct Override public void visit Limit lo Limit throws Frontend Exception check limit Override public void visit Cross lo Cross throws Frontend Exception check cross Override public void visit Stream lo Stream throws Frontend Exception check stream Override public void visit Native native throws Frontend Exception check mapreduce 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Byte Array Input Stream import java io Data Input Stream import java io File import java io File Input Stream import java io Filename Filter import java io Exception import java util Array List import java util List import org apache hadoop fs Path import org apache hadoop util bloom Bloom Filter import org apache hadoop util bloom Key import org apache pig Filter Func import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory Use a Bloom filter build previously by Build Bloom You would first build a bloom filter in a group all job For example in a group all job For example define bb Build Bloom jenkins load foo as x y group all foreach generate bb x store into mybloom The bloom filter can be on multiple keys by passing more than one field or the entire bag to Build Bloom The resulting file can then be used in a Bloom filter as define bloom Bloom mybloom load foo as x y load bar as z filter by bloom z join by z by x It uses link org apache hadoop util bloom Bloom Filter You can also pass the Bloom filter from Build Bloom directly to Bloom as a scalar instead of storing it to file and loading again This is simpler if the Bloom filter will not be reused and needs to be discarded after the run of the script define bb Build Bloom jenkins load foo as x y group all foreach generate bb x as bloomfilter load bar as z filter by Bloom bloomfilter z join by z by x public class Bloom extends Filter Func private static Tuple Factory m Tuple Factory Tuple Factory get Instance private String bloom File private Bloom Filter filter null public Bloom The filename containing the serialized Bloom filter If filename is null or the no arg constructor is used then the bloomfilter bytearray which is the output of Build Bloom should be passed as the first argument to the param filename file containing the serialized Bloom filter public Bloom String filename bloom File filename Override public Boolean exec Tuple input throws Exception if filter null init input byte b if bloom File null The first one is the bloom filter Skip that if input size b Data Type to Bytes input get else List Object input List input get All Tuple tuple m Tuple Factory new Tuple No Copy input List sub List input List size b Data Type to Bytes tuple Data Type else if input size b Data Type to Bytes input get else b Data Type to Bytes input Data Type Key k new Key b return filter membership Test k Override public List String get Cache Files if bloom File null List String list new Array List String We were passed the name of the file on Append a name for the file on the task node try list add bloom File get Filename From Path bloom File catch Exception e throw new Runtime Exception e return list return null private void init Tuple input throws Exception if bloom File null if input get instanceof Data Byte Array filter Build Bloom Base bloom In Data Byte Array input get else throw new Illegal Argument Exception The first argument to the Bloom should be the bloom filter if a bloom file is not specified in the constructor else filter new Bloom Filter String dir get Filename From Path bloom File String part Files new File dir list new Filename Filter Override public boolean accept File current String name return name starts With part String dc File dir part Files Data Input Stream dis new Data Input Stream new File Input Stream dc File try filter read Fields dis finally dis close For testing only do not use directly public void set Filter Data Byte Array dba throws Exception Data Input Stream dis new Data Input Stream new Byte Array Input Stream dba get filter new Bloom Filter filter read Fields dis private String get Filename From Path String p throws Exception Path path new Path p return path to Uri get Path replace 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Iterator import org apache hadoop util bloom Bloom Filter import org apache hadoop util bloom Key import org apache pig Algebraic import org apache pig Eval Func import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer schema Schema Build a bloom filter for use later in Bloom This is intended to run in a group all job For example define bb Build Bloom jenkins load foo as x y group all foreach generate bb x store into mybloom The bloom filter can be on multiple keys by passing more than one field or the entire bag to Build Bloom The resulting file can then be used in a Bloom filter as define bloom Bloom mybloom load foo as x y load bar as z filter by bloom z join by z by x It uses link org apache hadoop util bloom Bloom Filter public class Build Bloom extends Build Bloom Base Data Byte Array implements Algebraic Build a bloom filter of fixed size and number of hash functions param hash Type type of the hashing function see link org apache hadoop util hash Hash param mode Will be ignored though by convention it should be fixed or fixedsize param vector Size The vector size of this filter param nb Hash The number of hash functions to consider public Build Bloom String hash Type String mode String vector Size String nb Hash super hash Type mode vector Size nb Hash Construct a Bloom filter based on expected number of elements and desired accuracy param hash Type type of the hashing function see link org apache hadoop util hash Hash param num Elements The number of distinct elements expected to be placed in this filter param desired False Positive the acceptable rate of false positives This should be a floating point value between and where would be ie a totally useless filter public Build Bloom String hash Type String num Elements String desired False Positive super hash Type num Elements desired False Positive Override public Data Byte Array exec Tuple input throws Exception throw new Exception This must be used with algebraic public String get Initial return Initial class get Name public String get Intermed return Intermediate class get Name public String get Final return Final class get Name static public class Initial extends Build Bloom Base Tuple public Initial public Initial String hash Type String mode String vector Size String nb Hash super hash Type mode vector Size nb Hash public Initial String hash Type String num Elements String desired False Positive super hash Type num Elements desired False Positive Override public Tuple exec Tuple input throws Exception if input null input size return null Strip off the initial level of bag Data Bag values Data Bag input get Iterator Tuple it values iterator Tuple t it next If the input tuple has only one field then we ll extract that field and serialize it into a key If it has multiple fields we ll serialize the whole tuple byte b if t size b Data Type to Bytes t get else b Data Type to Bytes t Data Type Key k new Key b filter new Bloom Filter v Size num Hash h Type filter add k return Tuple Factory get Instance new Tuple bloom Out static public class Intermediate extends Build Bloom Base Tuple public Intermediate public Intermediate String hash Type String mode String vector Size String nb Hash super hash Type mode vector Size nb Hash public Intermediate String hash Type String num Elements String desired False Positive super hash Type num Elements desired False Positive Override public Tuple exec Tuple input throws Exception return Tuple Factory get Instance new Tuple bloom Or input static public class Final extends Build Bloom Base Data Byte Array public Final public Final String hash Type String mode String vector Size String nb Hash super hash Type mode vector Size nb Hash public Final String hash Type String num Elements String desired False Positive super hash Type num Elements desired False Positive Override public Data Byte Array exec Tuple input throws Exception return bloom Or input Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Byte Array Input Stream import java io Byte Array Output Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java util Iterator import org apache hadoop util bloom Bloom Filter import org apache hadoop util hash Hash import org apache pig Eval Func import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Tuple Base class for Build Bloom and its Algebraic implementations public abstract class Build Bloom Base extends Eval Func protected int v Size protected int num Hash protected int h Type protected Bloom Filter filter protected Build Bloom Base param hash Type type of the hashing function see link org apache hadoop util hash Hash param mode Will be ignored though by convention it should be fixed or fixedsize param vector Size The vector size of i this i filter param nb Hash The number of hash functions to consider public Build Bloom Base String hash Type String mode String vector Size String nb Hash v Size Integer value Of vector Size num Hash Integer value Of nb Hash h Type convert Hash Type hash Type param hash Type type of the hashing function see link org apache hadoop util hash Hash param num Elements The number of distinct elements expected to be placed in this filter param desired False Positive the acceptable rate of false positives This should be a floating point value between and where would be ie a totally useless filter public Build Bloom Base String hash Type String num Elements String desired False Positive set Size num Elements desired False Positive h Type convert Hash Type hash Type protected Data Byte Array bloom Or Tuple input throws Exception filter new Bloom Filter v Size num Hash h Type try Data Bag values Data Bag input get for Iterator Tuple it values iterator it has Next Tuple t it next filter or bloom In Data Byte Array t get catch Exec Exception ee throw new Exception ee return bloom Out protected Data Byte Array bloom Out throws Exception Byte Array Output Stream baos new Byte Array Output Stream v Size Data Output Stream dos new Data Output Stream baos filter write dos return new Data Byte Array baos to Byte Array public static Bloom Filter bloom In Data Byte Array b throws Exception Data Input Stream dis new Data Input Stream new Byte Array Input Stream b get Bloom Filter f new Bloom Filter f read Fields dis return f private int convert Hash Type String hash Type if hash Type to Lower Case contains jenkins return Hash else if hash Type to Lower Case contains murmur return Hash else throw new Runtime Exception Unknown hash type hash Type Valid values are jenkins and murmur private void set Size String num Elements String desired False Positive int num Integer value Of num Elements float fp Float value Of desired False Positive if num fp fp throw new Runtime Exception Number of elements must be greater than zero and desired False Positive must be between and v Size int num Math log fp Math pow Math log log info Build Bloom setting vector size to v Size num Hash int v Size num log info Build Bloom setting number of hashes to num Hash 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig Func Spec import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Cast Expression extends Unary Expression private Func Spec cast Func private Logical Schema Logical Field Schema cast Schema public Cast Expression Operator Plan plan Logical Expression exp Logical Schema Logical Field Schema fs super Cast plan exp cast Schema fs deep Copy cast Schema reset Uid Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Set the code Func Spec code that performs the casting functionality param spec the code Func Spec code that does the casting public void set Func Spec Func Spec spec cast Func spec Get the code Func Spec code that performs the casting functionality return the code Func Spec code that does the casting public Func Spec get Func Spec return cast Func Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Cast Expression Cast Expression of Cast Expression other return get Expression is Equal of get Expression else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null cast Schema schema cast Schema type uid Only Field Schema field Schema merge Uid uid Only Field Schema Bring back the top level uid this is not changed Logical Expression exp Logical Expression plan get Successors this get if exp get Field Schema null field Schema uid exp get Field Schema uid field Schema alias exp get Field Schema alias return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Cast Expression lg Exp Plan this get Expression deep Copy lg Exp Plan cast Schema deep Copy try Func Spec orig Func Spec this get Func Spec if orig Func Spec null Cast Expression copy set Func Spec orig Func Spec clone catch Clone Not Supported Exception e e print Stack Trace copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl util import java io Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Load Caster import org apache pig Pig Warning import org apache pig Resource Schema Resource Field Schema import org apache pig data Data Byte Array import org apache pig data Data Type public class Cast Utils private static Integer m Max Int Integer value Of Integer private static Long m Max Long Long value Of Long protected static final Log m Log Log Factory get Log Cast Utils class param caster Load Caster to be used to convert the bytes into a field param bytes param field Schema schema of Bag or Tuple pass in null if a simple type param data Type type from Data Type return converted object throws Exception public static Object convert To Type Load Caster caster byte bytes Resource Field Schema field Schema byte data Type throws Exception switch data Type case Data Type return caster bytes To Bag bytes field Schema case Data Type return new Data Byte Array bytes case Data Type return caster bytes To Char Array bytes case Data Type return caster bytes To Double bytes case Data Type return caster bytes To Float bytes case Data Type return caster bytes To Integer bytes case Data Type return caster bytes To Big Integer bytes case Data Type return caster bytes To Big Decimal bytes case Data Type return caster bytes To Long bytes case Data Type return caster bytes To Boolean bytes case Data Type return caster bytes To Date Time bytes case Data Type return caster bytes To Map bytes field Schema case Data Type return caster bytes To Tuple bytes field Schema default throw new Exception Unknown type data Type public static Double string To Double String str if str null return null else try return Double parse Double str catch Number Format Exception e Log Utils warn Cast Utils class Unable to interpret value str in field being converted to double caught Number Format Exception e get Message field discarded Pig Warning m Log return null public static Float string To Float String str if str null return null else try return Float parse Float str catch Number Format Exception e Log Utils warn Cast Utils class Unable to interpret value str in field being converted to float caught Number Format Exception e get Message field discarded Pig Warning m Log return null public static Integer string To Integer String str if str null return null else try return Integer parse Int str catch Number Format Exception e It s possible that this field can be interpreted as a double Unfortunately Java does n t handle this in Integer value Of So we need to try to convert it to a double and if that works then go to an int try Double d Double value Of str Need to check for an overflow error if d double Value m Max Int double Value Log Utils warn Cast Utils class Value d too large for integer Pig Warning m Log return null return Integer value Of d int Value catch Number Format Exception nfe Log Utils warn Cast Utils class Unable to interpret value str in field being converted to int caught Number Format Exception e get Message field discarded Pig Warning m Log return null public static Long string To Long String str if str null return null else try return Long parse Long str catch Number Format Exception e It s possible that this field can be interpreted as a double Unfortunately Java does n t handle this in Long value Of So we need to try to convert it to a double and if that works then go to an long try Double d Double value Of str Need to check for an overflow error if d double Value m Max Long double Value Log Utils warn Cast Utils class Value d too large for long Pig Warning m Log return null return Long value Of d long Value catch Number Format Exception nfe Log Utils warn Cast Utils class Unable to interpret value str in field being converted to long caught Number Format Exception nfe get Message field discarded Pig Warning m Log return null public static Boolean string To Boolean String str if str null return null else if str equals Ignore Case true return Boolean else if str equals Ignore Case false return Boolean else return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to You under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop accumulo import com google common base Preconditions Extracts necessary information from a user provide column specification colf colq Removes any trailing asterisk on colfam or colqual and appropriately sets the link Column Type public class Column public static enum Type private final Type column Type private String column Family private String column Qualifier public Column String col Preconditions check Not Null col int index col index Of Abstract Accumulo Storage if index column Family col column Qualifier null if column Family ends With Abstract Accumulo Storage column Family column Family substring column Family length column Type Type else column Type Type else if col length throw new Illegal Argument Exception Can not parse col column Family col substring index column Qualifier col substring index Handle colf colq if column Family ends With Abstract Accumulo Storage column Type Type column Family column Family substring column Family length else if column Qualifier is Empty column Type Type else if column Qualifier ends With Abstract Accumulo Storage column Type Type column Qualifier column Qualifier substring column Qualifier length else column Type Type public Type get Type return column Type public String get Column Family return column Family public String get Column Qualifier return column Qualifier public boolean match All return Type equals column Type equals column Family Override public boolean equals Object o if o instanceof Column Column other Column o if null column Family if null other column Family return false else if column Family equals other column Family return false if null column Qualifier if null other column Qualifier return false else if column Qualifier equals other column Qualifier return false return column Type other column Type return false Override public String to String return column Type column Family column Qualifier 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical visitor import java util Array List import java util List import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Plan Validation Exception import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator Plan import org apache pig newplan logical expression Dereference Expression import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical expression Project Expression import org apache pig newplan logical optimizer All Expression Visitor import org apache pig newplan logical relational Logical Schema Logical plan visitor which will convert all column alias references to column indexes using the underlying anonymous expression plan visitor public class Column Alias Conversion Visitor extends All Expression Visitor public Column Alias Conversion Visitor Operator Plan plan throws Frontend Exception super plan new Dependency Order Walker plan Override protected Logical Expression Visitor get Visitor Logical Expression Plan expr Plan throws Frontend Exception return new Logical Expression Visitor expr Plan new Dependency Order Walker expr Plan Override public void visit Project Expression expr throws Frontend Exception expr set Column Number From Alias public void visit Dereference Expression expr throws Frontend Exception List Object raw Cols expr get Raw Columns if raw Cols is Empty return List Integer cols new Array List Integer raw Cols size Logical Expression Plan plan Logical Expression Plan expr get Plan Logical Expression pred Logical Expression plan get Successors expr get Logical Schema schema null if pred get Field Schema type Data Type if pred get Field Schema schema null schema pred get Field Schema schema get Field schema if schema null schema size schema get Field type Data Type schema schema get Field schema else schema pred get Field Schema schema int col for Object rc raw Cols if rc instanceof Integer col Integer rc if schema null schema size col schema size throw new Plan Validation Exception expr Out of bound access Trying to access non existent column col Schema schema to String false has schema size column s cols add Integer rc else col schema null schema get Field Position String rc if col throw new Plan Validation Exception expr Invalid field reference Referenced field rc does not exist in schema schema null schema to String false cols add col expr set Bag Columns cols 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util Array List import java util List import org apache pig backend hadoop executionengine physical Layer expression Operators Project Representing one sort key Sort key may be compound if we sort on multiple columns if that is the case then this sort key contains multiple Column Chain Info public class Column Chain Info implements Cloneable private List Column Info column Infos new Array List Column Info public boolean equals Object o if o instanceof Column Chain Info return false Column Chain Info c Column Chain Info o if column Infos size c column Infos size return false for int i i column Infos size i if column Infos get i equals c column Infos get i return false return true public void insert List Integer columns byte type Column Info new Column Info new Column Info columns type column Infos add new Column Info Insert new Column Info for a project star or project range to end param start Col param type public void insert int start Col byte type Column Info new Column Info new Column Info start Col type column Infos add new Column Info In reduce the input represent the first input put instead of so that we can match the sort information collected from Local Rearrange public void insert In Reduce Project project if size int col if project is Project To End project get Columns size expecting first project to be projecting one of the bags so getting here is unexpected setting as the column so that it secondary sort optimization will not get used col return else col project get Columns get List Integer new Columns new Array List Integer new Columns add col Column Info new Column Info new Column Info new Columns project get Result Type column Infos add new Column Info else if project is Project To End insert project get Start Col project get Result Type else insert project get Columns project get Result Type public void insert Column Chain Info Column Chain Info column Chain Info column Infos add All column Chain Info column Infos public List Column Info get Column Infos return column Infos public String to String return column Infos to String Override public Object clone throws Clone Not Supported Exception super clone Column Chain Info result new Column Chain Info for Column Info column Info column Infos Column Info new Column Info Column Info column Info clone result column Infos add new Column Info return result public int size return column Infos size public Column Info get Column Info int i return column Infos get i Override public int hash Code int result for Column Info column Info column Infos result column Info hash Code return result 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util Array List import java util List import org apache pig data Data Type Represent one column inside order key this is a direct mapping from Project public class Column Info implements Cloneable List Integer columns byte result Type int start Col boolean is Range Project false public Column Info List Integer columns byte type this columns columns this result Type type Constructor for range projection or project star param start Col param type public Column Info int start Col byte type this start Col start Col this result Type type this is Range Project true public byte get Result Type return result Type public int get Start Col return start Col public boolean is Range Project return is Range Project public List Integer get Columns return columns Override public String to String String result if is Star result result columns to String result Data Type find Type Name result Type return result private boolean is Star return is Range Project start Col Override public boolean equals Object o if o null o instanceof Column Info return false Column Info c Column Info o if is Range Project c is Range Project start Col c start Col columns null c columns null columns null columns equals c columns return true return false Override public int hash Code return to String hash Code Override public Object clone throws Clone Not Supported Exception Column Info new Col Info Column Info super clone copy the mutable field List Integer cols new Array List Integer cols add All this columns new Col Info columns cols return new Col Info 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java util Collection import java util Hash Set import java util Iterator import java util List import java util Map import java util Set import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Operator Sub Plan import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig newplan logical relational Schema Not Defined Exception Helper class used by Column Map Key Prune to figure out what columns can be pruned It does n t make any changes to the operator plan public class Column Prune Helper protected static final String Column Prune Input Uids public static final String Column Prune Output Uids protected static final String Column Prune Required Columns private Operator Plan current Plan private Operator Sub Plan sub Plan public Column Prune Helper Operator Plan current Plan this current Plan current Plan private Operator Sub Plan get Sub Plan throws Frontend Exception Operator Sub Plan p null if current Plan instanceof Operator Sub Plan p new Operator Sub Plan Operator Sub Plan current Plan get Base Plan else p new Operator Sub Plan current Plan Iterator Operator iter current Plan get Operators while iter has Next Operator op iter next if op instanceof For Each add Operator op p return p private void add Operator Operator op Operator Sub Plan subplan throws Frontend Exception if op null return subplan add op List Operator ll current Plan get Predecessors op if ll null return for Operator pred ll add Operator pred subplan Suppress Warnings unchecked public boolean check throws Frontend Exception List Operator sources current Plan get Sources if this rule has run before just return false if sources size sources get get Annotation null clear Annotation return false create sub plan that ends with foreach sub Plan get Sub Plan if sub Plan size clear Annotation return false Column Dependency Visitor v new Column Dependency Visitor current Plan try v visit catch Schema Not Defined Exception e if any operator has an unknown schema just return false clear Annotation return false List Operator ll sub Plan get Sources boolean found false for Operator op ll if op instanceof Load Set Long uids Set Long op get Annotation Logical Schema s Load op get Schema Set Integer required get Columns s uids if required size s size op annotate required found true if found clear Annotation return found private void clear Annotation Iterator Operator iter current Plan get Operators while iter has Next Operator op iter next op remove Annotation op remove Annotation op remove Annotation get a set of column indexes from a set of uids protected Set Integer get Columns Logical Schema schema Set Long uids throws Frontend Exception if schema null throw new Schema Not Defined Exception Schema is not defined Set Integer cols new Hash Set Integer Iterator Long iter uids iterator while iter has Next long uid iter next int index schema find Field uid if index throw new Frontend Exception uid is not found in the schema schema cols add index return cols public Operator Plan report Changes return sub Plan Visitor to calculate the input and output uids for each operator It does n t change the plan only put calculated info as annotations The input and output uids are not necessarily the top level uids of a schema They may be the uids of lower level fields of complex fields that have their own schema static private class Column Dependency Visitor extends Logical Relational Nodes Visitor public Column Dependency Visitor Operator Plan plan throws Frontend Exception super plan new Reverse Dependency Order Walker plan Override public void visit Load load throws Frontend Exception Set Long output set Output Uids load for load input uids are same as output uids load annotate output Override public void visit Filter filter throws Frontend Exception Set Long output set Output Uids filter the input uids contains all the output uids and projections in filter conditions Set Long input new Hash Set Long output Logical Expression Plan exp filter get Filter Plan collect Uids filter exp input filter annotate input Override public void visit Store store throws Frontend Exception Set Long output set Output Uids store if output is Empty to deal with load store load store case Logical Schema s store get Schema if s null throw new Schema Not Defined Exception Schema for store get Name is not defined for int i i s size i output add s get Field i uid for store input uids are same as output uids store annotate output Override public void visit Join join throws Frontend Exception Set Long output set Output Uids join the input uids contains all the output uids and projections in join expressions Set Long input new Hash Set Long output Collection Logical Expression Plan exps join get Expression Plan Values Iterator Logical Expression Plan iter exps iterator while iter has Next Logical Expression Plan exp iter next collect Uids join exp input join annotate input Override public void visit Cogroup cg throws Frontend Exception Set Long output set Output Uids cg the input uids contains all the output uids and projections in join expressions Set Long input new Hash Set Long Add all the uids required for doing cogroup As in all the keys on which the cogroup is done for Logical Expression Plan plan cg get Expression Plans values collect Uids cg plan input Now check for the case where the output uid is a generated one If that is the case we need to add the uids which generated it in the input long first Uid Map Integer Long generated Input Uids cg get Generated Input Uids for Map Entry Integer Long entry generated Input Uids entry Set Long uid entry get Value Logical Relational Operator pred Logical Relational Operator cg get Plan get Predecessors cg get entry get Key if output contains uid Hence we need to all the full schema of the bag input add All get All Uids pred get Schema if pred get Schema null first Uid pred get Schema get Field uid if input is Empty first Uid input add first Uid cg annotate input Override public void visit Limit limit throws Frontend Exception Set Long output set Output Uids limit the input uids contains all the output uids and projections in limit expression Set Long input new Hash Set Long output Logical Expression Plan exp limit get Limit Plan if exp null collect Uids limit exp input limit annotate input Override public void visit Stream stream throws Frontend Exception output is not used set Output Uids is used to check if it has output schema Set Long output set Output Uids stream Every field is required Logical Relational Operator pred Logical Relational Operator plan get Predecessors stream get Set Long input get All Uids pred get Schema stream annotate input Override public void visit Distinct distinct throws Frontend Exception set Output Uids distinct Set Long input new Hash Set Long Every field is required Logical Schema s distinct get Schema if s null throw new Schema Not Defined Exception Schema for distinct get Name is not defined for int i i s size i input add s get Field i uid distinct annotate input Override public void visit Cross cross throws Frontend Exception Set Long output set Output Uids cross Since we do not change the topology of the plan we keep at least one input for each predecessor List Operator preds plan get Predecessors cross for Operator pred preds Logical Schema schema Logical Relational Operator pred get Schema Set Long uids get All Uids schema boolean all Pruned true for Long uid uids if output contains uid all Pruned false if all Pruned output add schema get Field uid cross annotate output Override public void visit Union union throws Frontend Exception Set Long output set Output Uids union Set Long input new Hash Set Long for long uid output input add All union get Input Uids uid union annotate input Override public void visit Split split throws Frontend Exception Set Long output set Output Uids split split annotate output Override public void visit Split Output split Output throws Frontend Exception Set Long output set Output Uids split Output the input uids contains all the output uids and projections in split Output conditions Set Long input new Hash Set Long for long uid output input add split Output get Input Uids uid Logical Expression Plan exp split Output get Filter Plan collect Uids split Output exp input split Output annotate input Override public void visit Sort sort throws Frontend Exception Set Long output set Output Uids sort Set Long input new Hash Set Long output for Logical Expression Plan exp sort get Sort Col Plans collect Uids sort exp input sort annotate input Override public void visit Rank rank throws Frontend Exception Set Long output set Output Uids rank Set Long input new Hash Set Long output for Logical Expression Plan exp rank get Rank Col Plans collect Uids rank exp input rank annotate input This function returns all uids present in the given schema private Set Long get All Uids Logical Schema schema Set Long uids new Hash Set Long if schema null return uids for Logical Field Schema field schema get Fields if field type Data Type field type Data Type field schema null uids add All get All Uids field schema uids add field uid return uids Suppress Warnings unchecked Override public void visit For Each foreach throws Frontend Exception Set Long output set Output Uids foreach Generate gen Optimizer Utils find Generate foreach gen annotate output visit gen Set Long input Set Long gen get Annotation Make sure at least one column will retain if input is Empty Logical Relational Operator pred Logical Relational Operator plan get Predecessors foreach get if pred get Schema null input add pred get Schema get Field uid foreach annotate input Override Suppress Warnings unchecked public void visit Generate gen throws Frontend Exception Set Long output Set Long gen get Annotation Set Long input new Hash Set Long List Logical Expression Plan ll gen get Output Plans Iterator Long iter output iterator while iter has Next long uid iter next for int i i ll size i Logical Expression Plan exp ll get i boolean found false Logical Schema plan Schema gen get Output Plan Schemas get i for Logical Field Schema fs plan Schema get Fields if fs uid uid found true break if found List Operator srcs exp get Sinks for Operator src srcs if src instanceof Project Expression List Inner Load inner Loads For Each find Reacheable Inner Load From Boundary Project Project Expression src first for Inner Load inner Load inner Loads Project Expression prj inner Load get Projection if prj is Project Star if prj find Referent get Schema null for Logical Schema Logical Field Schema fs prj find Referent get Schema get Fields input add fs uid else if prj find Referent get Schema null Logical Schema Logical Field Schema fs prj find Referent get Schema get Field prj get Col Num input add fs uid for the flatten bag we need to make sure at least one field is in the input for int i i ll size i if gen get Flatten Flags i continue Logical Expression Plan exp ll get i Logical Expression sink Logical Expression exp get Sources get if sink get Field Schema type Data Type sink get Field Schema type Data Type continue List Operator srcs exp get Sinks for Operator src srcs if src instanceof Project Expression continue List Inner Load inner Loads For Each find Reacheable Inner Load From Boundary Project Project Expression src first for Inner Load inner Load inner Loads Project Expression prj inner Load get Projection if prj is Project Star if prj find Referent get Schema null for Logical Schema Logical Field Schema fs prj find Referent get Schema get Fields input add fs uid else if prj find Referent get Schema null Logical Schema Logical Field Schema fs prj find Referent get Schema get Field prj get Col Num input add fs uid gen annotate input Override public void visit Inner Load load throws Frontend Exception Set Long output set Output Uids load load annotate output private void collect Uids Logical Relational Operator current Op Logical Expression Plan exp Set Long uids throws Frontend Exception List Operator ll exp get Sinks for Operator op ll if op instanceof Project Expression if Project Expression op is Range Or Star Project long uid Project Expression op get Field Schema uid uids add uid else Logical Relational Operator ref Project Expression op find Referent Logical Schema s ref get Schema if s null throw new Schema Not Defined Exception Schema not defined for ref get Alias for Logical Field Schema f s get Fields uids add f uid Suppress Warnings unchecked Get output uid from output schema If output schema does not exist throw exception private Set Long set Output Uids Logical Relational Operator op throws Frontend Exception List Operator ll plan get Successors op Set Long uids new Hash Set Long Logical Schema s op get Schema if s null throw new Schema Not Defined Exception Schema for op get Name is not defined if ll null if this is not sink the output uids are union of input uids of its successors for Operator succ ll Set Long input Uids Set Long succ get Annotation if input Uids null Iterator Long iter input Uids iterator while iter has Next long uid iter next if s find Field uid uids add uid else if it s leaf set to its schema for int i i s size i uids add s get Field i uid op annotate uids return uids 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java util Array List import java util Hash Map import java util Hash Set import java util List import java util Map import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Load Func import org apache pig Load Push Down import org apache pig Load Push Down Required Field import org apache pig Load Push Down Required Field List import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical Util import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema public class Column Prune Visitor extends Logical Relational Nodes Visitor protected static final Log log Log Factory get Log Column Prune Visitor class private Map Load Pair Map Integer Set String Set Integer required Items new Hash Map Load Pair Map Integer Set String Set Integer private boolean column Prune public Column Prune Visitor Operator Plan plan Map Load Pair Map Integer Set String Set Integer required Items boolean column Prune throws Frontend Exception super plan new Reverse Dependency Order Walker plan this column Prune column Prune this required Items required Items public void add Required Items Load load Pair Map Integer Set String Set Integer required Item required Items put load required Item Override public void visit Load load throws Frontend Exception if required Items contains Key load return Pair Map Integer Set String Set Integer required required Items get load Required Field List required Fields new Required Field List Logical Schema s load get Schema for int i i s size i Required Field required Field null As we have done processing ahead we assume that a column is not present in both Column Pruner and Map Pruner if required first null required first contains Key i required Field new Required Field required Field set Index i required Field set Alias s get Field i alias required Field set Type s get Field i type List Required Field sub Fields new Array List Required Field for String key required first get i Required Field sub Field new Required Field key null Data Type sub Fields add sub Field required Field set Sub Fields sub Fields required Fields add required Field if required second null required second contains i required Field new Required Field required Field set Index i required Field set Alias s get Field i alias required Field set Type s get Field i type required Fields add required Field boolean column Required new boolean s size for Required Field rf required Fields get Fields column Required rf get Index true List Pair Integer Integer prune List new Array List Pair Integer Integer for int i i column Required length i if column Required i prune List add new Pair Integer Integer i String Buffer message new String Buffer if prune List size message append Columns pruned for load get Alias for int i i prune List size i message append prune List get i second if i prune List size message append log info message message new String Buffer for Required Field rf required Fields get Fields List Required Field sub rf get Sub Fields if sub null message append Map key required for load get Alias rf get Index sub n if message length log info message Load Push Down Required Field Response response null try Load Func load Func load get Load Func if load Func instanceof Load Push Down response Load Push Down load Func push Projection required Fields catch Frontend Exception e log warn push Projection on load throw an exception skip it Loader does not support column pruning insert foreach if column Prune if response null response get Required Field Response Logical Plan p Logical Plan load get Plan Operator next p get Successors load get if there is already a For Each after load we do n t need to add another For Each if next instanceof For Each return For Each foreach new For Each load get Plan add foreach to the base plan p add foreach p insert Between load foreach next Logical Plan inner Plan new Logical Plan foreach set Inner Plan inner Plan build foreach inner plan List Logical Expression Plan exps new Array List Logical Expression Plan Generate gen new Generate inner Plan exps new boolean required Fields get Fields size inner Plan add gen for int i i required Fields get Fields size i Load Push Down Required Field rf required Fields get Fields get i Inner Load inner Load new Inner Load inner Plan foreach rf get Index inner Plan add inner Load inner Plan connect inner Load gen Logical Expression Plan exp new Logical Expression Plan Project Expression prj new Project Expression exp i gen exp add prj exps add exp else columns are pruned reset schema for Loader List Integer required Indexes new Array List Integer List Load Push Down Required Field field List required Fields get Fields for int i i field List size i required Indexes add field List get i get Index load set Required Fields required Indexes Logical Schema new Schema new Logical Schema for int i i field List size i new Schema add Field s get Field field List get i get Index load set Schema new Schema Override public void visit Filter filter throws Frontend Exception Override public void visit Limit limit throws Frontend Exception Override public void visit Split Output split Output throws Frontend Exception Suppress Warnings unchecked Override public void visit Split split throws Frontend Exception List Operator branch Outputs split get Plan get Successors split for int i i branch Outputs size i Operator branch Output branch Outputs get i Set Long branch Output Uids Set Long branch Output get Annotation Column Prune Helper if branch Output Uids null Set Integer columns To Drop new Hash Set Integer for int j j split get Schema size j if branch Output Uids contains split get Schema get Field j uid columns To Drop add j if columns To Drop is Empty For Each foreach Util add For Each After Logical Plan split get Plan split i columns To Drop foreach get Schema Override public void visit Sort sort throws Frontend Exception Override public void visit Rank rank throws Frontend Exception Override public void visit Store store throws Frontend Exception Override public void visit Cogroup cg throws Frontend Exception Logical Plan p Logical Plan cg get Plan List Operator successors p get Successors cg if there is already a For Each after this operator skip adding another foreach since it can conflict with Accumulator Optimizer Util add Accumulator if successors size successors get instanceof For Each return add For Each If Necessary cg Override public void visit Join join throws Frontend Exception Override public void visit Cross cross throws Frontend Exception Override Suppress Warnings unchecked public void visit For Each foreach throws Frontend Exception if column Prune return get column numbers from input uids Set Long input Uids Set Long foreach get Annotation Column Prune Helper Get all top level projects Logical Plan inner Plan foreach get Inner Plan List Inner Load inner Loads new Array List Inner Load List Operator sources inner Plan get Sources for Operator s sources if s instanceof Inner Load inner Loads add Inner Load s If project of the inner Load is not in remove this inner Load Set Inner Load inner Loads To Remove new Hash Set Inner Load for Inner Load inner Load inner Loads Project Expression project inner Load get Projection if project is Project Star Logical Schema Logical Field Schema tuple project get Field Schema Check the first component of the star projection long uid tuple schema get Field uid if input Uids contains uid inner Loads To Remove add inner Load else if input Uids contains project get Field Schema uid inner Loads To Remove add inner Load Find the logical operator immediate precede Generate which should be removed the whole branch Set Logical Relational Operator branch Head To Remove new Hash Set Logical Relational Operator for Inner Load inner Load inner Loads To Remove Operator op inner Load while inner Plan get Successors op get instanceof Generate op inner Plan get Successors op get branch Head To Remove add Logical Relational Operator op Find the expression plan to remove Generate gen Generate inner Plan get Sinks get List Logical Expression Plan gen Plans To Remove new Array List Logical Expression Plan List Logical Expression Plan gen Plans gen get Output Plans for int i i gen Plans size i Logical Expression Plan exp Plan gen Plans get i List Operator exp Sources exp Plan get Sinks for Operator exp Src exp Sources if exp Src instanceof Project Expression Logical Relational Operator reference Project Expression exp Src find Referent if branch Head To Remove contains reference gen Plans To Remove add exp Plan Build the temporary structure based on gen Plans To Remove which include flatten List output Plan Schemas uid Only Schemas inputs Removed We first construct inputs Needed and inputs Removed all inputs inputs Needed We can not figure out inputs Removed directly since the inputs may be used by other output plan We can only get inputs Removed after visiting all output plans List Boolean flatten List new Array List Boolean Set Integer inputs Needed new Hash Set Integer Set Integer inputs Removed new Hash Set Integer List Logical Schema output Plan Schemas new Array List Logical Schema List Logical Schema uid Only Schemas new Array List Logical Schema List Logical Schema user Defined Schemas null if gen get User Defined Schema null user Defined Schemas new Array List Logical Schema for int i i gen Plans size i Logical Expression Plan gen Plan gen Plans get i if gen Plans To Remove contains gen Plan flatten List add gen get Flatten Flags i output Plan Schemas add gen get Output Plan Schemas get i uid Only Schemas add gen get Uid Only Schemas get i if gen get User Defined Schema null user Defined Schemas add gen get User Defined Schema get i List Operator sinks gen Plan get Sinks for Operator s sinks if s instanceof Project Expression inputs Needed add Project Expression s get Input Num List Operator preds inner Plan get Predecessors gen if preds null otherwise all gen plan are based on constant no need to adjust for int i i preds size i if inputs Needed contains i inputs Removed add i Change Generate remove unneeded output expression plan change flatten flag output Plan Schema uid Only Schemas boolean flatten new boolean flatten List size for int i i flatten List size i flatten i flatten List get i gen set Flatten Flags flatten gen set Output Plan Schemas output Plan Schemas gen set Uid Only Schemas uid Only Schemas gen set User Defined Schema user Defined Schemas for Logical Expression Plan gen Plan To Remove gen Plans To Remove gen Plans remove gen Plan To Remove shift project input if inputs Removed is Empty for Logical Expression Plan gen Plan gen Plans List Operator sinks gen Plan get Sinks for Operator s sinks if s instanceof Project Expression int input Project Expression s get Input Num int num To Shift for int i inputs Removed if i input num To Shift Project Expression s set Input Num input num To Shift Prune unneeded Inner Load List Logical Relational Operator pred To Remove new Array List Logical Relational Operator for int i inputs Removed pred To Remove add Logical Relational Operator preds get i for Logical Relational Operator pred pred To Remove remove Sub Tree pred Override public void visit Union union throws Frontend Exception Add For Each before union if necessary List Operator preds new Array List Operator preds add All plan get Predecessors union for Operator pred preds add For Each If Necessary Logical Relational Operator pred remove all the operators starting from an operator private void remove Sub Tree Logical Relational Operator op throws Frontend Exception Logical Plan p Logical Plan op get Plan List Operator ll p get Predecessors op if ll null for Operator pred ll to Array new Operator ll size remove Sub Tree Logical Relational Operator pred if p get Successors op null Operator succs p get Successors op to Array new Operator for Operator s succs p disconnect op s p remove op Add For Each after op to prune unnecessary columns Suppress Warnings unchecked private void add For Each If Necessary Logical Relational Operator op throws Frontend Exception Set Long output Uids Set Long op get Annotation Column Prune Helper if output Uids null Logical Schema schema op get Schema Set Integer columns To Drop new Hash Set Integer for int i i schema size i if output Uids contains schema get Field i uid columns To Drop add i if columns To Drop is Empty For Each foreach Util add For Each After Logical Plan op get Plan op columns To Drop foreach get Schema 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine spark optimizer import java util Hash Map import java util Linked List import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Combiner Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Pre Combiner Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Packager import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig backend hadoop executionengine spark operator Global Rearrange Spark import org apache pig backend hadoop executionengine spark operator Reduce By Spark import org apache pig backend hadoop executionengine spark plan Spark Op Plan Visitor import org apache pig backend hadoop executionengine spark plan Spark Oper Plan import org apache pig backend hadoop executionengine spark plan Spark Operator import org apache pig backend hadoop executionengine util Combiner Optimizer Util import org apache pig data Data Type import org apache pig impl plan Depth First Walker import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl plan optimizer Optimizer Exception import org apache pig impl util Pair import com google common collect Maps This class goes through the physical plan are replaces Global Rearrange with Reduce By where there are algebraic operations public class Combiner Optimizer extends Spark Op Plan Visitor private static Log Log Factory get Log Combiner Optimizer class public Combiner Optimizer Spark Oper Plan plan super plan new Depth First Walker plan Override public void visit Spark Op Spark Operator spark Op throws Visitor Exception try add Combiner spark Op physical Plan catch Exception e throw new Visitor Exception e Checks for algebraic operations and if they exist Replaces global rearrange cogroup with reduce By as follows Input foreach using algebraic Op packager global Rearrange local Rearrange Output foreach using algebraic Op Final reduce By uses algebraic Op Intermediate foreach using algebraic Op Initial Combiner Rearrange private void add Combiner Physical Plan phy Plan throws Visitor Exception Plan Exception Clone Not Supported Exception List Physical Operator leaves phy Plan get Leaves if leaves null leaves size return Ensure there is grouping List Global Rearrange glrs Plan Helper get Physical Operators phy Plan Global Rearrange class if glrs null glrs size return for Global Rearrange glr glrs List Physical Operator glr Successors phy Plan get Successors glr if glr Successors null glr Successors is Empty continue if glr Successors get instanceof Package continue Package po Package Package glr Successors get List Physical Operator po Package Successors phy Plan get Successors po Package if po Package Successors null po Package Successors size continue Physical Operator successor po Package Successors get Retaining the original successor to be used later in modifying the plan Physical Operator package Successor successor if successor instanceof Limit Limit is acceptable as long as it has a single foreach as successor List Physical Operator limit Sucs phy Plan get Successors successor if limit Sucs null limit Sucs size limit Sucs get instanceof For Each the code below will now further examine the foreach successor limit Sucs get if successor instanceof For Each For Each foreach For Each successor List Physical Operator foreach Successors phy Plan get Successors foreach multi query if foreach Successors null foreach Successors size continue Clone foreach so it can be modified to a post reduce foreach For Each post Reduce foreach clone List Physical Plan fe Inners post Reduce get Input Plans find algebraic operators and also check if the foreach statement is suitable for combiner use List Pair Physical Operator Physical Plan algebraic Ops Combiner Optimizer Util find Algebraic Ops fe Inners if algebraic Ops null algebraic Ops size the plan is not combinable or there is nothing to combine we re done continue try List Physical Operator glr Predecessors phy Plan get Predecessors glr Exclude co group from optimization if glr Predecessors null glr Predecessors size continue if glr Predecessors get instanceof Local Rearrange continue Local Rearrange rearrange Local Rearrange glr Predecessors get info Algebraic operations found Optimizing plan to use combiner Trim the global rearrange and the preceeding package convert To Map Side For Each phy Plan po Package replace Distinct Project with distinct udf which is Algebraic for Pair Physical Operator Physical Plan op plan algebraic Ops if op plan first instanceof Distinct continue Combiner Optimizer Util Distinct Patcher distinct Patcher new Combiner Optimizer Util Distinct Patcher op plan second distinct Patcher visit if distinct Patcher get Distinct null int err Code String msg Problem with replacing distinct operator with distinct built in function throw new Plan Exception msg err Code Pig Exception op plan first distinct Patcher get Distinct create new map foreach For Each mfe Combiner Optimizer Util create For Each With Grp Proj post Reduce po Package get Pkgr get Key Type Map Physical Operator Integer op newpos Maps new Hash Map Integer pos create plan for each algebraic udf and add as inner plan in map foreach for Pair Physical Operator Physical Plan op plan algebraic Ops Physical Plan udf Plan Combiner Optimizer Util create Plan With Predecessors op plan first op plan second mfe add Input Plan udf Plan false op newpos put op plan first pos Combiner Optimizer Util change Func mfe User Func since we will only be creating Single Tuple Bag as input to the map foreach we should flag the Projects in the map foreach inner plans to also use Single Tuple Bag for Physical Plan mpl mfe get Input Plans try new Combiner Optimizer Util fix Map Projects mpl visit catch Visitor Exception e int err Code String msg Unable to flag project operator to use single tuple bag throw new Plan Exception msg err Code Pig Exception e create new combine foreach For Each cfe Combiner Optimizer Util create For Each With Grp Proj post Reduce po Package get Pkgr get Key Type add algebraic functions with appropriate projection Combiner Optimizer Util add Algebraic Func To Combine cfe op newpos we have modified the foreach inner plans so set them again for the foreach so that foreach can do any re initialization around them mfe set Input Plans mfe get Input Plans cfe set Input Plans cfe get Input Plans tell Combiner Package which fields need projected and which placed in bags First field is simple project rest need to go into bags int num Fields algebraic Ops size algebraic funcs group key boolean bags new boolean num Fields bags false for int i i num Fields i bags i true Use the Combiner package in the combine plan as it needs to act differently than the regular package operator Combiner Packager pkgr new Combiner Packager po Package get Pkgr bags Package combine Pack po Package clone combine Pack set Pkgr pkgr specialized local rearrange operator will replace the normal local rearrange in the map plan Local Rearrange new Rearrange Combiner Optimizer Util get New Rearrange rearrange Pre Combiner Local Rearrange combiner Local Rearrange Combiner Optimizer Util get Pre Combiner rearrange phy Plan replace rearrange combiner Local Rearrange Create a reduce By operator Reduce By Spark reduce Operator new Reduce By Spark cfe get Operator Key combiner Local Rearrange get Requested Parallelism cfe get Input Plans cfe get To Be Flattened combine Pack new Rearrange reduce Operator set Custom Partitioner glr get Custom Partitioner fix Reduce Side post Reduce algebraic Ops Combiner Optimizer Util change Func reduce Operator User Func update Packager reduce Operator new Rearrange Add the new operators phy Plan add reduce Operator phy Plan add mfe Connect the new operators as follows reduce By using algebraic Op Intermediate foreach using algebraic Op Initial phy Plan connect mfe reduce Operator Insert the reduce stage between combiner rearrange and its successor phy Plan disconnect combiner Local Rearrange package Successor phy Plan connect reduce Operator package Successor phy Plan connect combiner Local Rearrange mfe Replace foreach with post reduce foreach phy Plan add post Reduce phy Plan replace foreach post Reduce catch Exception e int err Code String msg Internal error Unable to introduce the combiner for optimization throw new Optimizer Exception msg err Code Pig Exception e Modifies the input plans of the post reduce foreach to match the output of reduce stage private void fix Reduce Side For Each post Reduce List Pair Physical Operator Physical Plan algebraic Ops throws Exec Exception Plan Exception int i for Pair Physical Operator Physical Plan algebraic Op algebraic Ops User Func combine Udf User Func algebraic Op first Physical Plan pplan algebraic Op second combine Udf set Algebraic Function User Func Project new Proj new Project Combiner Optimizer Util create Operator Key post Reduce get Operator Key get Scope i new Proj set Result Type Data Type for Physical Operator original Udf Input pplan get Predecessors combine Udf to Array new Physical Operator if pplan get Predecessors original Udf Input null pplan trim Above original Udf Input pplan remove original Udf Input pplan add new Proj pplan connect new Proj combine Udf i post Reduce set Result Type Data Type Modifies the map side of foreach before reduce private void convert To Map Side For Each Physical Plan physical Plan Package po Package throws Plan Exception Linked List Physical Operator operators To Remove new Linked List for Physical Operator physical Operator physical Plan get Predecessors po Package if physical Operator instanceof Global Rearrange Spark operators To Remove add physical Operator break Remove global rearranges preceeding Package for Physical Operator po operators To Remove physical Plan remove And Reconnect po Remove Package itself physical Plan remove And Reconnect po Package Update the Reduce By Operator with the packaging used by Local rearrange private void update Packager Reduce By Spark reduce Operator Local Rearrange lrearrange throws Optimizer Exception Packager pkgr reduce Operator get Op get Pkgr annotate the package with information from the Rearrange update the key Info information if already present in the Package Map Integer Pair Boolean Map Integer Integer key Info pkgr get Key Info if key Info null key Info new Hash Map if key Info get Integer value Of lrearrange get Index null something is wrong we should not be getting key info for the same index from two different Local Rearranges int err Code String msg Unexpected problem during optimization Found index lrearrange get Index in multiple Local Rearrange operators throw new Optimizer Exception msg err Code Pig Exception key Info put Integer value Of lrearrange get Index new Pair Boolean Map Integer Integer lrearrange is Project Star lrearrange get Projected Cols Map pkgr set Key Info key Info pkgr set Key Tuple lrearrange is Key Tuple pkgr set Key Compound lrearrange is Key Compound Look for a algebraic User Func that is the leaf of an input plan param pplan physical plan return null if any operator other Project or non algebraic User Func is found while going down the plan otherwise algebraic User Func is returned private static User Func get Algebraic Successor Physical Plan pplan check if it ends in an List Physical Operator leaves pplan get Leaves if leaves null leaves size return null Physical Operator succ leaves get if succ instanceof User Func User Func succ combinable return User Func succ some other operator ca n t combine return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine util import java util Array List import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Func Spec import org apache pig Pig Exception import org apache pig Pig Warning import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Combiner Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Pre Combiner Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig classification Interface Audience import org apache pig data Data Type import org apache pig impl plan Compilation Message Collector import org apache pig impl plan Compilation Message Collector Message Type import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Depth First Walker import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Plan Walker import org apache pig impl plan Visitor Exception import org apache pig impl plan optimizer Optimizer Exception import org apache pig impl util Pair import com google common collect Lists import com google common collect Maps Interface Audience Private public class Combiner Optimizer Util private static final String org apache pig builtin Distinct class get Name private static final Log Log Factory get Log Combiner Optimizer Util class private Combiner Optimizer Util Algebraic functions and distinct in nested plan of a foreach are partially computed in the map and combine phase new foreach statement with initial and intermediate forms of algebraic functions are added to map and combine plans respectively If bag portion of group by result is projected or a non algebraic expression udf has bag as input combiner will not be used This is because the use of combiner in such case is likely to degrade performance as there will not be much reduction in data size in combine stage to offset the cost of the additional number of times de serialization is done Major areas for enhancement use of combiner in cogroup queries with order by limit or sort in a nested foreach after group by case where group by is followed by filter that has algebraic expression public static void add Combiner Physical Plan map Plan Physical Plan reduce Plan Physical Plan combine Plan Compilation Message Collector message Collector boolean do Map Agg throws Visitor Exception part one check if this job represents a group by foreach Find the Local Rearrange in the map ll need it later List Physical Operator map Leaves map Plan get Leaves if map Leaves null map Leaves size message Collector collect Expected map to have single leaf Message Type Warning Pig Warning return Physical Operator map Leaf map Leaves get if map Leaf instanceof Local Rearrange return Local Rearrange rearrange Local Rearrange map Leaf List Physical Operator reduce Roots reduce Plan get Roots if reduce Roots size message Collector collect Expected reduce to have single root Message Type Warning Pig Warning return expect that the first root should always be a Package If not do n t know what s going on so m out of here Physical Operator root reduce Roots get if root instanceof Package message Collector collect Expected reduce root to be a Package Message Type Warning Pig Warning return Package pack Package root List Physical Operator pack Successors reduce Plan get Successors root if pack Successors null pack Successors size return Physical Operator successor pack Successors get if successor instanceof Limit Limit is acceptable as long has it has a single foreach as successor List Physical Operator limit Sucs reduce Plan get Successors successor if limit Sucs null limit Sucs size limit Sucs get instanceof For Each the code below will now further examine the foreach successor limit Sucs get if successor instanceof For Each For Each foreach For Each successor List Physical Plan fe Inners foreach get Input Plans find algebraic operators and also check if the foreach statement is suitable for combiner use List Pair Physical Operator Physical Plan algebraic Ops find Algebraic Ops fe Inners if algebraic Ops null algebraic Ops size the plan is not combinable or there is nothing to combine we re done return if combine Plan null combine Plan get Roots size message Collector collect Was n t expecting to find anything already in the combiner Message Type Warning Pig Warning return info Choosing to move algebraic foreach to combiner try replace Distinct Project with distinct udf which is Algebraic for Pair Physical Operator Physical Plan op plan algebraic Ops if op plan first instanceof Distinct continue Distinct Patcher distinct Patcher new Distinct Patcher op plan second distinct Patcher visit if distinct Patcher get Distinct null int err Code String msg Problem with replacing distinct operator with distinct built in function throw new Plan Exception msg err Code Pig Exception op plan first distinct Patcher get Distinct create new map foreach For Each mfe create For Each With Grp Proj foreach rearrange get Key Type Map Physical Operator Integer op newpos Maps new Hash Map Integer pos create plan for each algebraic udf and add as inner plan in map foreach for Pair Physical Operator Physical Plan op plan algebraic Ops Physical Plan udf Plan create Plan With Predecessors op plan first op plan second mfe add Input Plan udf Plan false op newpos put op plan first pos change Func mfe User Func since we will only be creating Single Tuple Bag as input to the map foreach we should flag the Projects in the map foreach inner plans to also use Single Tuple Bag for Physical Plan mpl mfe get Input Plans try new fix Map Projects mpl visit catch Visitor Exception e int err Code String msg Unable to flag project operator to use single tuple bag throw new Plan Exception msg err Code Pig Exception e create new combine foreach For Each cfe create For Each With Grp Proj foreach rearrange get Key Type add algebraic functions with appropriate projection add Algebraic Func To Combine cfe op newpos change Func cfe User Func fix projection and function time for algebraic functions in reduce foreach for Pair Physical Operator Physical Plan op plan algebraic Ops set Project Input op plan first op plan second op newpos get op plan first byte result Type op plan first get Result Type User Func op plan first set Algebraic Function User Func op plan first set Result Type result Type we have modified the foreach inner plans so set them again for the foreach so that foreach can do any re initialization around them this is a necessary evil right now because the leaves are explicitly stored in the Foreach as a list rather than computed each time at run time from the plans for optimization Do we want to have the Foreach compute the leaves each time and have Java optimize it will Java optimize mfe set Input Plans mfe get Input Plans cfe set Input Plans cfe get Input Plans foreach set Input Plans foreach get Input Plans tell Combiner Package which fields need projected and which placed in bags First field is simple project rest need to go into bags int num Fields algebraic Ops size algebraic funcs group key boolean bags new boolean num Fields bags false for int i i num Fields i bags i true Use the Combiner package in the combine plan as it needs to act differently than the regular package operator Combiner Packager pkgr new Combiner Packager pack get Pkgr bags Package combine Pack pack clone combine Pack set Pkgr pkgr combine Pack set Parent Plan null combine Plan add combine Pack combine Plan add cfe combine Plan connect combine Pack cfe No need to connect projections in cfe to cp because Pig Combiner directly attaches output from package to root of remaining plan Local Rearrange mlr get New Rearrange rearrange Partial Agg map Agg null if do Map Agg map Agg create Partial Agg cfe is Group All rearrange specialized local rearrange operator will replace the normal local rearrange in the map plan This behaves like the regular local rearrange in the get Next as far as getting its input and constructing the key out of the input It then returns a tuple with two fields the key in the first position and the value inside a bag in the second position This output format resembles the format out of a Package This output will feed to the map foreach which expects this format If the key field is n t in the project of the combiner or map foreach it is added to the end This is required so that we can set up the inner plan of the new Local Rearrange leaf in the map and combine plan to contain just the project of the key patch Up Map map Plan get Pre Combiner rearrange mfe map Agg mlr Local Rearrange clr get New Rearrange rearrange clr set Parent Plan null combine Plan add clr combine Plan connect cfe clr Change the package operator in the reduce plan to be the Combiner package as it needs to act differently than the regular package operator pack set Pkgr pkgr clone catch Exception e int err Code String msg Internal error Unable to introduce the combiner for optimization throw new Optimizer Exception msg err Code Pig Exception e private static boolean is Group All Local Rearrange lr Local Rearrange tuple chararray false scope scope Constant all scope boolean is Group All false if lr get Plans size Physical Plan plan lr get Plans get if plan get Keys size plan get Roots get instanceof Constant Expression Constant Expression const Expr Constant Expression plan get Roots get is Group All all equals const Expr get Value return is Group All Translate For Each in combiner into a Partial Agg param combine return partial aggregate operator throws Clone Not Supported Exception private static Partial Agg create Partial Agg For Each combine boolean is Group All throws Clone Not Supported Exception String scope combine get Operator Key scope Partial Agg po Agg new Partial Agg new Operator Key scope Node Id Generator get Generator get Next Node Id scope is Group All po Agg add Original Location combine get Alias combine get Original Locations po Agg set Result Type combine get Result Type first plan in combine foreach is the group key po Agg set Key Plan combine get Input Plans get clone List Physical Plan value Plans Lists new Array List for int i i combine get Input Plans size i value Plans add combine get Input Plans get i clone po Agg set Value Plans value Plans return po Agg find algebraic operators and also check if the foreach statement is suitable for combiner use param fe Inners inner plans of foreach return null if plan is not combinable otherwise list of combinable operators throws Visitor Exception public static List Pair Physical Operator Physical Plan find Algebraic Ops List Physical Plan fe Inners throws Visitor Exception List Pair Physical Operator Physical Plan algebraic Ops Lists new Array List check each foreach inner plan for Physical Plan pplan fe Inners check for presence of non combinable operators Algebraic Plan Checker alg Checker new Algebraic Plan Checker pplan alg Checker visit if alg Checker saw Non Algebraic return null if we found a combinable distinct add that to list if alg Checker saw Distinct Agg algebraic Ops add new Pair Physical Operator Physical Plan alg Checker get Distinct pplan continue List Physical Operator roots pplan get Roots combinable operators have to be attached to Project root s if root does not have a successor that is combinable the project has to be projecting the group column Otherwise this job is considered not combinable as we do n t want to use combiner for cases where this foreach statement is projecting bags likely to bad for performance because of additional de serialization costs for Physical Operator root roots if root instanceof Constant Expression continue if root instanceof Project how can this happen expect root of inner plan to be constant or project not combining it return null Project proj Project root User Func combine Udf get Algebraic Successor proj pplan if combine Udf null if proj is Project To End project star or project to end not combinable return null Check to see if this is a projection of the grouping column If so it will be a projection of col List Integer cols proj get Columns if cols null cols size cols get it is project of grouping column so the plan is still combinable continue else not combinable return null The algebraic udf can have more than one input Add the udf only once boolean exist false for Pair Physical Operator Physical Plan pair algebraic Ops if pair first equals combine Udf exist true break if exist algebraic Ops add new Pair Physical Operator Physical Plan combine Udf pplan return algebraic Ops Look for a algebraic User Func as successor to this project called recursively to skip any other projects seen on the way param proj project param pplan physical plan return null if any operator other Project or algebraic User Func is found while going down the plan otherwise algebraic User Func is returned private static User Func get Algebraic Successor Project proj Physical Plan pplan check if root is followed by combinable operator List Physical Operator succs pplan get Successors proj if succs null succs size return null if succs size project shared by more than one operator does not happen in plans generated today wo n t try to combine this return null Physical Operator succ succs get if succ instanceof Project return get Algebraic Successor Project succ pplan if succ instanceof User Func User Func succ combinable return User Func succ some other operator ca n t combine return null Create a new foreach with same scope alias as given foreach add an inner plan that projects the group column which is going to be the first input param foreach source foreach param key Type type for group by key return new Foreach public static For Each create For Each With Grp Proj For Each foreach byte key Type String scope foreach get Operator Key scope For Each new new For Each create Operator Key scope new Array List Physical Plan new add Original Location foreach get Alias foreach get Original Locations new set Result Type foreach get Result Type create plan that projects the group column Physical Plan grp Proj Plan new Physical Plan group by column is the first column Project proj new Project create Operator Key scope proj set Result Type key Type grp Proj Plan add proj new add Input Plan grp Proj Plan false return new Create new plan and add to it the clones of operator alge Op and its predecessors from the physical plan pplan param alge Op algebraic operator param pplan physical plan that has alge Op return new plan throws Clone Not Supported Exception throws Plan Exception public static Physical Plan create Plan With Predecessors Physical Operator alge Op Physical Plan pplan throws Clone Not Supported Exception Plan Exception Physical Plan newplan new Physical Plan add Predecessors To Plan alge Op pplan newplan return newplan Recursively clone op and its predecessors from pplan and add them to newplan param op param pplan param newplan return throws Clone Not Supported Exception throws Plan Exception private static Physical Operator add Predecessors To Plan Physical Operator op Physical Plan pplan Physical Plan newplan throws Clone Not Supported Exception Plan Exception Physical Operator new Op op clone newplan add new Op if pplan get Predecessors op null pplan get Predecessors op size return new Op for Physical Operator pred pplan get Predecessors op Physical Operator new Pred add Predecessors To Plan pred pplan newplan newplan connect new Pred new Op return new Op add algebraic functions with appropriate projection to new foreach in combiner param cfe the new foreach in combiner param op newpos mapping of physical operator to position in input throws Clone Not Supported Exception throws Plan Exception public static void add Algebraic Func To Combine For Each cfe Map Physical Operator Integer op newpos throws Clone Not Supported Exception Plan Exception an array that we will first populate with physical operators in order of their position in input Used while adding plans to combine foreach just so that output of combine foreach same positions as input That means the same operator to position mapping can be used by reduce as well Physical Operator ops In Order new Physical Operator op newpos size for Map Entry Physical Operator Integer op pos op newpos entry Set ops In Order op pos get Value op pos get Key first position is used by group column and a plan has been added for it so start with for int i i ops In Order length i create new inner plan for foreach add cloned copy of given physical operator and a new project Even if the udf in query takes multiple input only one project needs to be added because input to this udf will be the version of udf evaluated in map Physical Plan new Plan new Physical Plan Physical Operator new Op ops In Order i clone new Plan add new Op Project proj new Project create Operator Key cfe get Operator Key get Scope i proj set Result Type Data Type new Plan add proj new Plan connect proj new Op cfe add Input Plan new Plan false Replace old Local Rearrange with new pre combine add new map foreach new map local rearrange and connect them param map Plan param pre Combiner param mfe param map Agg param mlr throws Plan Exception private static void patch Up Map Physical Plan map Plan Pre Combiner Local Rearrange pre Combiner For Each mfe Partial Agg map Agg Local Rearrange mlr throws Plan Exception Local Rearrange old Local Rearrange map Plan get Leaves get map Plan replace old pre Combiner map Plan add mfe map Plan connect pre Combiner mfe the operator before local rearrange Physical Operator op Before mfe if map Agg null map Plan add map Agg map Plan connect mfe map Agg op Before map Agg map Plan add mlr map Plan connect op Before mlr param rearrange return public static Pre Combiner Local Rearrange get Pre Combiner Local Rearrange rearrange String scope rearrange get Operator Key scope Pre Combiner Local Rearrange pclr new Pre Combiner Local Rearrange create Operator Key scope rearrange get Requested Parallelism rearrange get Inputs pclr set Plans rearrange get Plans return pclr public static Operator Key create Operator Key String scope return new Operator Key scope Node Id Generator get Generator get Next Node Id scope param op param index param plan throws Plan Exception private static void set Project Input Physical Operator op Physical Plan plan int index throws Plan Exception String scope op get Operator Key scope Project proj new Project new Operator Key scope Node Id Generator get Generator get Next Node Id scope op get Requested Parallelism index proj set Result Type Data Type Remove old connections and elements from the plan plan trim Above op plan add proj plan connect proj op List Physical Operator inputs Lists new Array List inputs add proj op set Inputs inputs Change the algebriac function type for algebraic functions in map and combine In map and combine the algebraic functions will be leaf of the plan param fe param type throws Plan Exception public static void change Func For Each fe byte type throws Plan Exception for Physical Plan plan fe get Input Plans List Physical Operator leaves plan get Leaves if leaves null leaves size int err Code String msg Expected to find plan with single leaf Found leaves size leaves throw new Plan Exception msg err Code Pig Exception Physical Operator leaf leaves get if leaf instanceof Project continue if leaf instanceof User Func int err Code String msg Expected to find plan with or project leaf Found leaf get Class get Simple Name throw new Plan Exception msg err Code Pig Exception User Func func User Func leaf try func set Algebraic Function type catch Exec Exception e int err Code String msg Could not set algebraic function type throw new Plan Exception msg err Code Pig Exception e create new Local rearrange by cloning existing rearrange and add plan for projecting the key param rearrange return throws Plan Exception throws Clone Not Supported Exception public static Local Rearrange get New Rearrange Local Rearrange rearrange throws Plan Exception Clone Not Supported Exception Local Rearrange new Rearrange rearrange clone Set the projection to be the key Physical Plan new Plan new Physical Plan String scope new Rearrange get Operator Key scope Project proj new Project new Operator Key scope Node Id Generator get Generator get Next Node Id scope proj set Result Type new Rearrange get Key Type new Plan add proj List Physical Plan plans new Array List Physical Plan plans add new Plan new Rearrange set Plans From Combiner plans return new Rearrange Checks if there is something that prevents the use of algebraic interface and looks for the Distinct that can be used as algebraic private static class Algebraic Plan Checker extends Phy Plan Visitor boolean saw Non Algebraic false boolean saw Distinct Agg false private boolean saw Foreach false private Distinct distinct null Algebraic Plan Checker Physical Plan plan super plan new Dependency Order Walker Physical Operator Physical Plan plan Override public void visit throws Visitor Exception super visit if we saw foreach and distinct agg its ok else if we only saw foreach mark it as non algebraic if saw Foreach saw Distinct Agg saw Non Algebraic true Override public void visit Distinct Distinct distinct throws Visitor Exception this distinct distinct if saw Distinct Agg we want to combine only in the case where there is only one Distinct which is the only input to an agg we apparently have seen a Distinct before so lets not combine saw Non Algebraic true return check that this distinct is the only input to an agg We could have the following two cases script b group a by c foreach b x distinct a generate x The above script leads to the following plan for x User Func org apache pig builtin long Project bag Distinct bag Project tuple script b group a by c foreach b x distinct a generate x The above script leads to the following plan for x User Func org apache pig builtin Int Sum long Project bag Project bag Distinct bag Project tuple So tracing from the Distinct to its successors upto the leaf we should see a Project bag as the immediate successor and an optional Project bag as the next successor till we see the leaf Physical Operator leaf m Plan get Leaves get the leaf has to be a User Func need not be algebraic if leaf instanceof User Func we want to combine only in the case where there is only one Distinct which is the only input to an agg Do not combine if there are additional inputs List Physical Operator preds m Plan get Predecessors leaf if preds size saw Non Algebraic true return List Physical Operator immediate Succs m Plan get Successors distinct if immediate Succs size immediate Succs get instanceof Project if check Successor Is Leaf leaf immediate Succs get script above saw Distinct Agg true return else check for script scenario above List Physical Operator next Succs m Plan get Successors immediate Succs get if next Succs size Physical Operator op next Succs get if op instanceof Project if check Successor Is Leaf leaf op saw Distinct Agg true return if we did not return above that means we did not see the pattern we expected saw Non Algebraic true return the distinct public Distinct get Distinct if saw Non Algebraic return null return distinct Override public void visit Limit Limit limit throws Visitor Exception saw Non Algebraic true private boolean check Successor Is Leaf Physical Operator leaf Physical Operator op To Check List Physical Operator succs m Plan get Successors op To Check if succs size Physical Operator op succs get if op leaf return true return false Override public void visit Filter Filter filter throws Visitor Exception saw Non Algebraic true Override public void visit For Each For Each fe throws Visitor Exception we need to allow foreach as input for distinct but do n t want it for other things why So lets flag the presence of Foreach and if this is present with a distinct agg it will be allowed saw Foreach true Override public void visit Sort Sort sort throws Visitor Exception saw Non Algebraic true visitor to replace Project bag Distinct bag with User Func org apache pig builtin Distinct Data Bag public static class Distinct Patcher extends Phy Plan Visitor private User Func distinct null param plan param walker public Distinct Patcher Physical Plan plan Plan Walker Physical Operator Physical Plan walker super plan walker param physical Plan public Distinct Patcher Physical Plan physical Plan this physical Plan new Dependency Order Walker Physical Operator Physical Plan physical Plan Override public void visit Project Project proj throws Visitor Exception check if this project is preceded by Distinct and has the return type bag List Physical Operator preds m Plan get Predecessors proj if preds null return this is a leaf project and so not interesting for patching Physical Operator pred preds get if preds size pred instanceof Distinct if distinct null we should not already have been patched since the Project Distinct pair should occur only once int err Code String msg Unexpected Project Distinct pair while trying to set up plans for use with combiner throw new Optimizer Exception msg err Code Pig Exception we have stick in the Userfunc org apache pig builtin Distinct Data Bag in place of the Project Distinct pair Physical Operator distinct Predecessor m Plan get Predecessors pred get User Func func null try String scope proj get Operator Key scope List Physical Operator func Input new Array List Physical Operator Func Spec f Spec new Func Spec func Input add distinct Predecessor explicitly set distinct Predecessor s result type to be tuple this is relevant when distinct Predecessor is originally a Foreach with return type we need to set it to tuple so we get a stream of tuples distinct Predecessor set Result Type Data Type func new User Func new Operator Key scope Node Id Generator get Generator get Next Node Id scope func Input f Spec func set Result Type Data Type m Plan replace proj func m Plan remove pred connect the the newly added func to the predecessor to the earlier Distinct m Plan connect distinct Predecessor func catch Plan Exception e int err Code String msg Problem with reconfiguring plan to add distinct built in function throw new Optimizer Exception msg err Code Pig Exception e distinct func public User Func get Distinct return distinct public static class fix Map Projects extends Phy Plan Visitor public fix Map Projects Physical Plan plan this plan new Depth First Walker Physical Operator Physical Plan plan param plan param walker public fix Map Projects Physical Plan plan Plan Walker Physical Operator Physical Plan walker super plan walker Override public void visit Project Project proj throws Visitor Exception if proj get Result Type Data Type we should be calling this visitor only for fixing up the projects in the map s foreach inner plan In the map side we are dealing with single tuple bags so set the flag in the project to use single tuple bags If in future we do n t have single tuple bags in the input to map s foreach we should be doing this proj set Result Single Tuple Bag true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl util import java util Array List import java util List import org apache pig Eval Func import org apache pig Func Spec import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators And import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig builtin Is Empty import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Non Spillable Data Bag import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl logical Layer schema Schema import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception class to add util functions that gets used by Log To Phy Translator and Compiler public class Compiler Utils public static void add Empty Bag Outer Join Physical Plan fe Plan Schema input Schema boolean skewed Right Outer Join String is First Reduce Of Key Class Name throws Plan Exception we currently have Project bag as the only operator in the plan If the bag is an empty bag we should replace it with a bag with one tuple with null fields so that when we flatten we do not drop records flatten will drop records if the bag is left as an empty bag and actually project nulls for the fields in the empty bag So we need to get to the following state Project Bag User Func Is Empty Const Bag bag with null fields Project Bag Bin Cond Further if it is skewed right outer join only the first reduce of the key will generate tuple with null fields See Project key Project Bag Is First Reduce Of Key User Func Is Empty Const Bag bag with null fields Project Bag Bin Cond Project relation Project Project fe Plan get Roots get try condition of the bincond Project relation Project For Is Empty relation Project clone fe Plan add relation Project For Is Empty String scope relation Project get Operator Key scope Func Spec is Empty Spec new Func Spec Is Empty class get Name Object f Pig Context instantiate Func From Spec is Empty Spec User Func is Empty new User Func new Operator Key scope Node Id Generator get Generator get Next Node Id scope null is Empty Spec Eval Func f is Empty set Result Type Data Type fe Plan add is Empty fe Plan connect relation Project For Is Empty is Empty Expression Operator cond if skewed Right Outer Join Project project For Key new Project new Operator Key scope Node Id Generator get Generator get Next Node Id scope project For Key set Column project For Key set Overloaded false project For Key set Result Type input Schema get Field type And and new And new Operator Key scope Node Id Generator get Generator get Next Node Id scope Func Spec is First Reduce Of Key Spec new Func Spec is First Reduce Of Key Class Name Object f Pig Context instantiate Func From Spec is First Reduce Of Key Spec User Func is First Reduce Of Key new User Func new Operator Key scope Node Id Generator get Generator get Next Node Id scope null is First Reduce Of Key Spec Eval Func f fe Plan add project For Key fe Plan add is First Reduce Of Key fe Plan add and fe Plan connect project For Key is First Reduce Of Key fe Plan connect is First Reduce Of Key and fe Plan connect is Empty and and set Lhs is First Reduce Of Key and set Rhs is Empty cond and else cond is Empty lhs of bincond const bag with null fields Constant Expression ce new Constant Expression new Operator Key scope Node Id Generator get Generator get Next Node Id scope the following should give a tuple with the required number of nulls Tuple t Tuple Factory get Instance new Tuple input Schema size for int i i input Schema size i t set i null List Tuple bag Contents new Array List Tuple bag Contents add t Data Bag bg new Non Spillable Data Bag bag Contents ce set Value bg ce set Result Type Data Type this operator does n t have any predecessors fe Plan add ce rhs of bincond is the original project let s set up the bincond now Bin Cond bincond new Bin Cond new Operator Key scope Node Id Generator get Generator get Next Node Id scope bincond set Cond cond bincond set Lhs ce bincond set Rhs relation Project bincond set Result Type Data Type fe Plan add bincond fe Plan connect cond bincond fe Plan connect ce bincond fe Plan connect relation Project bincond catch Exception e throw new Plan Exception Error setting up outerjoin e 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util List import org apache pig Eval Func import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the concatenation of two or more arguments It can be used with two or more bytearrays or two or more chararrays but not a mixture of the two public class extends Eval Func Data Byte Array Override public Data Byte Array exec Tuple input throws Exception try if input null input size return null Data Byte Array db new Data Byte Array for int i i input size i if input get i null return null db append Data Byte Array input get i return db catch Exec Exception exp throw exp catch Exception e int err Code String msg Error while computing concat in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec Schema s new Schema s add new Schema Field Schema null Data Type s add new Schema Field Schema null Data Type func List add new Func Spec this get Class get Name s s new Schema s add new Schema Field Schema null Data Type s add new Schema Field Schema null Data Type func List add new Func Spec String Concat class get Name s return func List Override public Schema Type get Schema Type return Schema Type Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop datastorage import java util Enumeration import java util Iterator import java util Map import java util Map Entry import java util Properties import org apache hadoop conf Configuration import org apache pig Exec Type import org apache pig Pig Constants import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine util Map Red Util public class Configuration Util public static Configuration to Configuration Properties properties return to Configuration properties false public static Configuration to Configuration Properties properties boolean load Defaults assert properties null final Configuration config new Configuration load Defaults final Enumeration Object iter properties keys while iter has More Elements final String key String iter next Element final String val properties get Property key config set key val return config public static Properties to Properties Configuration configuration Properties properties new Properties assert configuration null Iterator Map Entry String String iter configuration iterator while iter has Next Map Entry String String entry iter next properties put entry get Key entry get Value return properties param orig Conf param replace Conf public static void merge Conf Configuration orig Conf Configuration replace Conf for Entry String String entry replace Conf orig Conf set entry get Key entry get Value public static Properties get Local Properties Configuration local Conf if Pig Map Reduce s Job Context null Pig Map Reduce s Job Context get Configuration get exectype equals Exec Type to String local Conf new Configuration false local Conf add Resource core default xml else if Pig Map Reduce s Job Context null local Conf new Configuration Pig Map Reduce s Job Context get Configuration else local Conf new Configuration true It s really hacky try to get unit test working under hadoop Hadoop Mini Cluster currently need setup Distributed cache before start so build classes hadoop site xml contains such entry This prevents some tests from successful They expect those files in hdfs so we need to unset it in hadoop This should go away once Mini Cluster fix the distributed cache issue local Conf unset Configuration local Conf set Map Red Util file Properties props Configuration Util to Properties local Conf return props public static void replace Config For Local Mode Configuration configuration for Entry String String entry configuration String key entry get Key String value entry get Value if key starts With Pig Constants String real Conf Key key substring Pig Constants length configuration set real Conf Key value Returns Properties containing alternative names of given property and same values can be used to solve deprecations return public static Properties expand For Alternative Names String name String value final Configuration config new Configuration false config set name value return Configuration Util to Properties config 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java io Exception import java util Array List import java util Hash Map import java util Iterator import java util List import java util Map import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine map Reduce Layer Pig Hadoop Logger import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Context import org apache pig newplan Base Operator Plan import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan Reverse Dependency Order Walker Seen Chk import org apache pig newplan logical expression All Same Expression Visitor import org apache pig newplan logical expression Constant Expression import org apache pig newplan logical expression Exp To Phy Translation Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical expression User Func Expression import org apache pig newplan logical optimizer All Expression Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan optimizer Rule import org apache pig newplan optimizer Transformer import org joda time Date Time Zone public abstract class Constant Calculator extends Rule private List Logical Relational Operator processed Operators new Array List Logical Relational Operator private Pig Context pc public Constant Calculator String n Pig Context pc super n false this pc pc Override public Transformer get New Transformer return new Constant Calculator Transformer processed Operators pc public static class Constant Calculator Transformer extends Transformer private List Logical Relational Operator processed Operators new Array List Logical Relational Operator private Operator Plan plan private Pig Context pc public Constant Calculator Transformer List Logical Relational Operator processed Operators Pig Context pc this processed Operators processed Operators this pc pc Override public boolean check Operator Plan matched throws Frontend Exception Iterator Operator operators matched get Operators while operators has Next Logical Relational Operator operator Logical Relational Operator operators next If the operator is already processed we quit if processed Operators contains operator continue processed Operators add operator return true return false public static class Constant Calculator Logical Plan Visitor extends All Expression Visitor private Pig Context pc public Constant Calculator Logical Plan Visitor Operator Plan plan Pig Context pc throws Frontend Exception super plan new Dependency Order Walker plan this pc pc Override protected Logical Expression Visitor get Visitor Logical Expression Plan expr throws Frontend Exception return new Constant Calculator Expression Visitor expr current Op pc public static class Constant Calculator Expression Visitor extends All Same Expression Visitor private Logical Relational Operator current Op private Pig Context pc private Date Time Zone current null public Constant Calculator Expression Visitor Operator Plan exp Plan Logical Relational Operator current Op Pig Context pc throws Frontend Exception super exp Plan new Reverse Dependency Order Walker Seen Chk exp Plan this current Op current Op this pc pc Override protected void execute Logical Expression op throws Frontend Exception if op instanceof User Func Expression User Func Expression udf User Func Expression op if udf get Eval Func allow Compile Time Calculation return boolean val Set false Object val null if current Walker get Plan get Successors op null If has successors and all successors are constant calculate the constant for Operator succ current Walker get Plan get Successors op if succ instanceof Constant Expression return All successors are constant calculate the value Operator Plan exp Logical Plan new Logical Expression Plan Base Operator Plan current Walker get Plan move Tree op Base Operator Plan exp Logical Plan Physical Plan exp Physical Plan new Physical Plan Map Operator Physical Operator log To Phy Map new Hash Map Operator Physical Operator Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk exp Logical Plan Save the old walker and use child Walker as current Walker push Walker child Walker Exp To Phy Translation Visitor exp Translation Visitor new Exp To Phy Translation Visitor exp Logical Plan child Walker current Op exp Physical Plan log To Phy Map exp Translation Visitor visit pop Walker Physical Operator root exp Physical Plan get Leaves get try Context get Context add Job Conf Configuration Util to Configuration pc get Properties true Pig Hadoop Logger pig Hadoop Logger Pig Hadoop Logger get Instance Physical Operator set Pig Logger pig Hadoop Logger set Default Time Zone val root get Next root get Result Type result restore Default Time Zone Context get Context add Job Conf null catch Exec Exception e throw new Frontend Exception e val Set true else if op instanceof User Func Expression If solo calculate User Func Expression udf User Func Expression op try Context get Context add Job Conf Configuration Util to Configuration pc get Properties true set Default Time Zone val udf get Eval Func exec null restore Default Time Zone Context get Context add Job Conf null catch Exception e throw new Frontend Exception e val Set true if val Set Constant Expression constant Expr constant Expr new Constant Expression current Walker get Plan val constant Expr inherit Schema op current Walker get Plan replace op constant Expr private void set Default Time Zone String dtz Str pc get Properties get Property pig datetime default tz if dtz Str null dtz Str length current Date Time Zone get Default Date Time Zone set Default Date Time Zone for dtz Str private void restore Default Time Zone if current null Date Time Zone set Default current current null Override public void transform Operator Plan matched throws Frontend Exception All Expression Visitor expression Visitor new Constant Calculator Logical Plan Visitor matched pc expression Visitor visit Override public Operator Plan report Changes return plan 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig Pig Exception import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical Util import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Source Location constant public class Constant Expression extends Column Expression Stupid Java needs a union Object val Remember data type when the value is null byte type Data Type Adds expression to the plan param plan Logical Expression Plan this constant is a part of param val Value of this constant public Constant Expression Operator Plan plan Object val super Constant plan this val val plan add this link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Get the value of this constant return value of the constant public Object get Value return val public void set Value Object val this val val Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Constant Expression Constant Expression co Constant Expression other return co get Field Schema is Equal get Field Schema co val null val null co null co val equals val else return false Override public Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema try field Schema Util translate Field Schema Data Type determine Field Schema val catch Exception e throw new Frontend Exception Error determining field schema from object in constant expression Pig Exception e uid Only Field Schema field Schema merge Uid uid Only Field Schema if type Data Type field Schema type Data Type val null field Schema type type return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Constant Expression copy new Constant Expression lg Exp Plan this get Value copy type this type copy set Location new Source Location location return copy public void inherit Schema Logical Expression expr throws Frontend Exception field Schema expr get Field Schema uid Only Field Schema field Schema merge Uid uid Only Field Schema if field Schema type Data Type type field Schema type 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util Iterator import java util List import org apache pig Accumulator import org apache pig Algebraic import org apache pig Eval Func import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the count of the number of values in a bag This count does not include null values and thus matches semantics for a where a is field but not for where in indicates all p This class implements link org apache pig Algebraic so if possible the execution will performed in a distributed fashion p There are no restrictions as to the data types inside the bag to be counted p implements the link org apache pig Accumulator interface as well While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation public class extends Eval Func Long implements Algebraic Accumulator Long private static Tuple Factory m Tuple Factory Tuple Factory get Instance Override public Long exec Tuple input throws Exception try Data Bag bag Data Bag input get if bag null return null Iterator it bag iterator long cnt while it has Next Tuple t Tuple it next if t null t size t get null cnt return cnt catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing count in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e public String get Initial return Initial class get Name public String get Intermed return Intermediate class get Name public String get Final return Final class get Name static public class Initial extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception Since Initial is guaranteed to be called only in the map it will be called with an input of a bag with a single tuple the count should always be if bag is non empty Data Bag bag Data Bag input get Iterator it bag iterator if it has Next Tuple t Tuple it next if t null t size t get null return m Tuple Factory new Tuple Long value Of return m Tuple Factory new Tuple Long value Of static public class Intermediate extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception try return m Tuple Factory new Tuple sum input catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing count in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Final extends Eval Func Long Override public Long exec Tuple input throws Exception try return sum input catch Exception ee int err Code String msg Error while computing count in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception ee static protected Long sum Tuple input throws Exec Exception Number Format Exception Data Bag values Data Bag input get long sum for Iterator Tuple it values iterator it has Next Tuple t it next Have faith here Checking each value before the cast is just too much sum Long t get return sum Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec Schema s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec this get Class get Name s return func List Accumulator interface implementation private long intermediate Count Override public void accumulate Tuple b throws Exception try Data Bag bag Data Bag b get Iterator it bag iterator while it has Next Tuple t Tuple it next if t null t size t get null intermediate Count catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing min in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public void cleanup intermediate Count Override public Long get Value return intermediate Count 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util List import org apache pig Eval Func import org apache pig backend executionengine Exec Exception import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema import com google common collect Lists Produces a Data Bag with all combinations of the argument tuple members as in a data cube Meaning a b c will produce the following bag pre a b c null null null a b null a null c a null null null b c null null c null b null pre p The all marker is null by default but can be set to an arbitrary string by invoking a constructor via a The constructor takes a single argument the string you want to represent all p Usage goes something like this pre code events load logs events using Event Loader as lang event app id cubed foreach x generate piggybank Cube Dimensions lang event app id as lang event app id measure cube foreach group cubed by lang event app id parallel generate flatten group as lang event app id cubed measure store cube into event cube pre p b Note b doing this with non algebraic aggregations on large data can result in very slow reducers since one of the groups is going to get i all i the records in your relation public class Cube Dimensions extends Eval Func Data Bag private static Bag Factory bf Bag Factory get Instance private static Tuple Factory tf Tuple Factory get Instance private final String all Marker private static final String unknown unknown public Cube Dimensions this null public Cube Dimensions String all Marker super this all Marker all Marker Override public Data Bag exec Tuple tuple throws Exception List Tuple result Lists new Array List With Capacity int Math pow tuple size convert Null To Unknown tuple Tuple newt tf new Tuple tuple size recursively Cube result tuple newt return bf new Default Bag result if the dimension values contain null then replace it with unknown value since null will be used for rollups public static void convert Null To Unknown Tuple tuple throws Exec Exception int idx for Object obj tuple get All if obj null tuple set idx unknown idx private void recursively Cube List Tuple result Tuple input int index Tuple newt throws Exec Exception newt set index input get index if index input size result add newt else recursively Cube result input index newt tf new Tuple makes a copy tf new Tuple No Copy does n t Tuple newnewt tf new Tuple newt get All newnewt set index all Marker if index input size result add newnewt else recursively Cube result input index newnewt Override public Schema output Schema Schema input try return new Schema new Field Schema dimensions input Data Type catch Frontend Exception e we are specifying explicitly so this should not happen throw new Runtime Exception e Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java lang reflect Parameterized Type import java lang reflect Type import java math Big Decimal import java math Big Integer import java util Array List import java util Arrays import java util Hash Map import java util Iterator import java util Map import java util Tree Map import org apache hadoop io Writable Comparable import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig backend executionengine Exec Exception import org apache pig builtin To Date import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Merge Exception import org joda time Date Time class of static final values used to encode data type and a number of static helper functions for manipulating data objects The data type values could be done as an enumeration but it is done as byte codes instead to save creating objects Interface Audience Public Interface Stability Stable public class Data Type This list can be used to record values of data on disk so do not change the values You may strand user data Order matters here as compare below uses the order to order unlike datatypes Do n t change this ordering Spaced unevenly to leave room for new entries without changing values or creating order issues public static final byte public static final byte public static final byte public static final byte internal use only public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte public static final byte Internal use only public static final byte internal use only for storing loading chararray bigger than characters in Bin Storage public static final byte public static final byte public static final byte Internal use only used to store Writeable Comparable objects for creating ordered index in Merge Join Expecting a object that implements Writable interface and has default constructor public static final byte Internal use only public static final byte internal use only for maps that are object object Used by Find Quantiles public static final byte Determine the datatype of an object param o Object to test return byte code of the type or if we do n t know public static byte find Type Object o if o null return Try to put the most common first if o instanceof Data Byte Array return else if o instanceof String return else if o instanceof Tuple return else if o instanceof Data Bag return else if o instanceof Integer return else if o instanceof Long return else if o instanceof Internal Map return else if o instanceof Map return else if o instanceof Float return else if o instanceof Double return else if o instanceof Boolean return else if o instanceof Date Time return else if o instanceof Byte return else if o instanceof Big Integer return else if o instanceof Big Decimal return else if o instanceof Writable Comparable return else return Given a Type object determine the data type it represents This is n t cheap as it uses reflection so use sparingly param t Type to examine return byte code of the type or if we do n t know public static byte find Type Type t if t null return Try to put the most common first if t Data Byte Array class return else if t String class return else if t Integer class return else if t Long class return else if t Float class return else if t Double class return else if t Boolean class return else if t Byte class return else if t Big Integer class return else if t Big Decimal class return else if t Date Time class return else if t Internal Map class return else Might be a tuple or a bag need to check the interfaces it implements if t instanceof Class return extract Type From Class t else if t instanceof Parameterized Type Parameterized Type impl Parameterized Type t Class c Class impl get Raw Type return extract Type From Class c return private static byte extract Type From Class Type t Class c Class t Class ioe Interfaces c get Interfaces Class interfaces null if c is Interface interfaces new Class ioe Interfaces length interfaces c for int i i interfaces length i interfaces i ioe Interfaces i else interfaces ioe Interfaces boolean matched Writable Comparable false for int i i interfaces length i if interfaces i get Name equals org apache pig data Tuple return else if interfaces i get Name equals org apache pig data Data Bag return else if interfaces i get Name equals java util Map return else if interfaces i get Name equals org apache hadoop io Writable Comparable use type only as last resort matched Writable Comparable true if matched Writable Comparable return return Return the number of types Pig knows about return number of types public static int num Types byte types gen All Types return types length Get an array of all type values return byte array with an entry for each type public static byte gen All Types byte types Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type return types private static String gen All Type Names String names return names Get a map of type values to type names return map public static Map Byte String gen Type To Name Map byte types gen All Types String names gen All Type Names Map Byte String ret new Hash Map Byte String for int i i types length i ret put types i names i return ret Get a map of type names to type values return map public static Map String Byte gen Name To Type Map byte types gen All Types String names gen All Type Names Map String Byte ret new Hash Map String Byte for int i i types length i ret put names i types i return ret Get the type name param o Object to test return type name as a String public static String find Type Name Object o return find Type Name find Type o Get the type name from the type byte code param dt Type byte code return type name as a String public static String find Type Name byte dt switch dt case return case return boolean case return byte case return int case return biginteger case return bigdecimal case return long case return float case return double case return datetime case return bytearray case return bigchararray case return chararray case return map case return internalmap case return tuple case return bag case return generic writablecomparable default return Unknown public static Class find Type Class byte dt switch dt case return Void case return Boolean case return Byte case return Integer case return Big Integer class case return Big Decimal class case return Long case return Float case return Double case return Date Time class case return Data Byte Array class case return String class case return String class case return Map class case return Internal Map class case return Tuple class case return Data Bag class case return Writable Comparable class default throw new Runtime Exception Invalid type has no corresponding class dt Get the type code from the type name param name Type name return type code public static byte find Type By Name String name if name null return else if boolean equals Ignore Case name return else if byte equals Ignore Case name return else if int equals Ignore Case name return else if biginteger equals Ignore Case name return else if bigdecimal equals Ignore Case name return else if long equals Ignore Case name return else if float equals Ignore Case name return else if double equals Ignore Case name return else if datetime equals Ignore Case name return else if bytearray equals Ignore Case name return else if bigchararray equals Ignore Case name return else if chararray equals Ignore Case name return else if map equals Ignore Case name return else if internalmap equals Ignore Case name return else if tuple equals Ignore Case name return else if bag equals Ignore Case name return else if generic writablecomparable equals Ignore Case name return else return Determine whether the this data type is complex param data Type Data type code to test return true if data Type is bag tuple or map public static boolean is Complex byte data Type return data Type data Type data Type data Type Determine whether the object is complex or atomic param o Object to determine type of return true if data Type is bag tuple or map public static boolean is Complex Object o return is Complex find Type o Determine whether the this data type is atomic param data Type Data type code to test return true if data Type is bytearray bigchararray chararray integer long float or boolean public static boolean is Atomic byte data Type return data Type data Type data Type data Type data Type data Type data Type data Type data Type data Type data Type data Type data Type Determine whether the this data type is atomic param o Object to determine type of return true if data Type is bytearray chararray integer long float or boolean public static boolean is Atomic Object o return is Atomic find Type o Determine whether the this object can have a schema param o Object to determine if it has a schema return true if the type can have a valid schema i e bag or tuple public static boolean is Schema Type Object o return is Schema Type find Type o Determine whether the this data type can have a schema param data Type data Type to determine if it has a schema return true if the type can have a valid schema i e bag or tuple public static boolean is Schema Type byte data Type return data Type data Type data Type Compare two objects to each other This function is necessary because there s no super class that implements compare To This function provides an arbitrary ordering of objects of different types as follows lt lt lt lt lt lt lt lt lt lt lt lt No other functions should implement this cross object logic They should call this function for it instead param o First object param o Second object return if o is less if they are equal if o is less public static int compare Object o Object o byte dt find Type o byte dt find Type o return compare o o dt dt Same as link compare Object Object but does not use reflection to determine the type of passed in objects relying instead on the caller to provide the appropriate values as determined by link Data Type find Type Object Use this version in cases where multiple objects of the same type have to be repeatedly compared param o first object param o second object param dt type as byte value of o param dt type as byte value of o return if o is lt o if they are equal if o gt o Suppress Warnings unchecked public static int compare Object o Object o byte dt byte dt if dt dt if o null if o null return else return else if o null return switch dt case return case return Boolean o compare To Boolean o case return Byte o compare To Byte o case return Integer o compare To Integer o case return Long o compare To Long o case return Float o compare To Float o case return Double o compare To Double o case return Date Time o compare To Date Time o case return Data Byte Array o compare To o case return String o compare To String o case return Big Integer o compare To Big Integer o case return Big Decimal o compare To Big Decimal o case Map String Object m Map String Object o Map String Object m Map String Object o int sz m size int sz m size if sz sz return else if sz sz return else This is bad but we have to sort the keys of the maps in order to be commutative Tree Map String Object tm new Tree Map String Object m Tree Map String Object tm new Tree Map String Object m Iterator Map Entry String Object i tm entry Set iterator Iterator Map Entry String Object i tm entry Set iterator while i has Next Map Entry String Object entry i next Map Entry String Object entry i next int c entry get Key compare To entry get Key if c return c else c compare entry get Value entry get Value if c return c return case return Comparable o compare To o case return Do n t think anyway will want to do this case return Tuple o compare To o case return Data Bag o compare To o default throw new Runtime Exception Unkown type dt in compare else if dt dt return else return public static byte to Bytes Object o throws Exec Exception return to Bytes o find Type o Suppress Warnings unchecked public static byte to Bytes Object o byte type throws Exec Exception switch type case return Boolean o boolean Value new byte new byte return Boolean o to String get Bytes case return new byte Byte o case case case case case case return Number o to String get Bytes case return Date Time o to String get Bytes case return String o get Bytes case return map To String Map String Object o get Bytes case return Tuple o to String get Bytes case return Data Byte Array o get case return Data Bag o to String get Bytes case return null default int err Code String msg Can not convert a find Type Name o to a Byte Array throw new Exec Exception msg err Code Pig Exception Force a data object to a Boolean if possible Any numeric type can be forced to a Boolean as well as Char Array Byte Array Complex types can not be forced to a Boolean This is n t particularly efficient so if you already b know b that the object you have is a Boolean you should just cast it param o object to cast param type of the object you are casting return The object as a Boolean throws Exec Exception if the type ca n t be forced to a Boolean public static Boolean to Boolean Object o byte type throws Exec Exception try switch type case return null case return Boolean o case return Boolean value Of Byte o byte Value case return Boolean value Of Integer o int Value case return Boolean value Of Long o long Value case return Boolean value Of Big Integer equals Big Integer o case return Boolean value Of Big Decimal equals Big Decimal o case return Boolean value Of Float o float Value case return Boolean value Of Double o double Value case String str Data Byte Array o to String if str equals Ignore Case true return Boolean else if str equals Ignore Case false return Boolean else return null case if String o equals Ignore Case true return Boolean else if String o equals Ignore Case false return Boolean else return null case case case case case case default int err Code String msg Can not convert a find Type Name o to a Boolean throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Float throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Float throw new Exec Exception msg err Code Pig Exception public static Boolean to Boolean Object o throws Exec Exception return to Boolean o find Type o Force a data object to an Integer if possible Any numeric type can be forced to an Integer though precision may be lost as well as Char Array Byte Array or Boolean Complex types can not be forced to an Integer This is n t particularly efficient so if you already b know b that the object you have is an Integer you should just cast it param o object to cast param type of the object you are casting return The object as an Integer throws Exec Exception if the type ca n t be forced to an Integer public static Integer to Integer Object o byte type throws Exec Exception try switch type case if Boolean o true return Integer value Of else return Integer value Of case return Integer value Of Byte o int Value case return Integer o case return Integer value Of Long o int Value case return Integer value Of Float o int Value case return Integer value Of Double o int Value case return Integer value Of Data Byte Array o to String case return Integer value Of String o case return Integer value Of Big Integer o int Value case return Integer value Of Big Decimal o int Value case return null case return Integer value Of Long value Of Date Time o get Millis int Value case case case case case default int err Code String msg Can not convert a find Type Name o to an Integer throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Integer throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Integer throw new Exec Exception msg err Code Pig Exception Force a data object to an Integer if possible Any numeric type can be forced to an Integer though precision may be lost as well as Char Array Byte Array or Boolean Complex types can not be forced to an Integer This is n t particularly efficient so if you already b know b that the object you have is an Integer you should just cast it Unlike link to Integer Object byte this method will first determine the type of o and then do the cast Use link to Integer Object byte if you already know the type param o object to cast return The object as an Integer throws Exec Exception if the type ca n t be forced to an Integer public static Integer to Integer Object o throws Exec Exception return to Integer o find Type o Force a data object to a Long if possible Any numeric type can be forced to a Long though precision may be lost as well as Char Array Byte Array or Boolean Complex types can not be forced to a Long This is n t particularly efficient so if you already b know b that the object you have is a Long you should just cast it param o object to cast param type of the object you are casting return The object as a Long throws Exec Exception if the type ca n t be forced to a Long public static Long to Long Object o byte type throws Exec Exception try switch type case if Boolean o true return Long value Of else return Long value Of case return Long value Of Byte o long Value case return Long value Of Integer o long Value case return Long o case return Long value Of Float o long Value case return Long value Of Double o long Value case return Long value Of Data Byte Array o to String case return Long value Of String o case return Long value Of Big Integer o long Value case return Long value Of Big Decimal o long Value case return null case return Long value Of Date Time o get Millis case case case case case default int err Code String msg Can not convert a find Type Name o to a Long throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Long throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Long throw new Exec Exception msg err Code Pig Exception Force a data object to a Long if possible Any numeric type can be forced to a Long though precision may be lost as well as Char Array Byte Array or Boolean Complex types can not be forced to an Long This is n t particularly efficient so if you already b know b that the object you have is a Long you should just cast it Unlike link to Long Object byte this method will first determine the type of o and then do the cast Use link to Long Object byte if you already know the type param o object to cast return The object as a Long throws Exec Exception if the type ca n t be forced to an Long public static Long to Long Object o throws Exec Exception return to Long o find Type o Force a data object to a Float if possible Any numeric type can be forced to a Float though precision may be lost as well as Char Array Byte Array Complex types can not be forced to a Float This is n t particularly efficient so if you already b know b that the object you have is a Float you should just cast it param o object to cast param type of the object you are casting return The object as a Float throws Exec Exception if the type ca n t be forced to a Float public static Float to Float Object o byte type throws Exec Exception try switch type case return Boolean o Float value Of Float value Of case return new Float Integer o float Value case return new Float Long o float Value case return Float o case return new Float Double o float Value case return new Float Long value Of Date Time o get Millis float Value case return Float value Of Data Byte Array o to String case return Float value Of String o case return Float value Of Big Integer o float Value case return Float value Of Big Decimal o float Value case return null case case case case case case default int err Code String msg Can not convert a find Type Name o to a Float throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Float throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Float throw new Exec Exception msg err Code Pig Exception Force a data object to a Float if possible Any numeric type can be forced to a Float though precision may be lost as well as Char Array Byte Array or Boolean Complex types can not be forced to an Float This is n t particularly efficient so if you already b know b that the object you have is a Float you should just cast it Unlike link to Float Object byte this method will first determine the type of o and then do the cast Use link to Float Object byte if you already know the type param o object to cast return The object as a Float throws Exec Exception if the type ca n t be forced to an Float public static Float to Float Object o throws Exec Exception return to Float o find Type o Force a data object to a Double if possible Any numeric type can be forced to a Double as well as Char Array Byte Array Complex types can not be forced to a Double This is n t particularly efficient so if you already b know b that the object you have is a Double you should just cast it param o object to cast param type of the object you are casting return The object as a Double throws Exec Exception if the type ca n t be forced to a Double public static Double to Double Object o byte type throws Exec Exception try switch type case return Boolean o Double value Of Double value Of case return new Double Integer o double Value case return new Double Long o double Value case return new Double Float o double Value case return Double o case return new Double Long value Of Date Time o get Millis double Value case return Double value Of Data Byte Array o to String case return Double value Of String o case return Double value Of Big Integer o double Value case return Double value Of Big Decimal o double Value case return null case case case case case case default int err Code String msg Can not convert a find Type Name o to a Double throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Double throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Double throw new Exec Exception msg err Code Pig Exception Force a data object to a Date Time if possible Only Char Array Byte Array can be forced to a Date Time Numeric types and complex types can not be forced to a Date Time This is n t particularly efficient so if you already b know b that the object you have is a Date Time you should just cast it param o object to cast param type of the object you are casting return The object as a Boolean throws Exec Exception if the type ca n t be forced to a Boolean public static Date Time to Date Time Object o byte type throws Exec Exception try switch type case return null case return new Date Time Data Byte Array o to String case the string can contain just date part or date part plus time part return To Date extract Date Time String o case return new Date Time Integer o long Value case return new Date Time Long o long Value case return new Date Time Float o long Value case return new Date Time Double o long Value case return new Date Time Big Integer o long Value case return new Date Time Big Decimal o long Value case return Date Time o case case case case case case case default int err Code String msg Can not convert a find Type Name o to a Date Time throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Date Time throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Date Time throw new Exec Exception msg err Code Pig Exception public static Date Time to Date Time Object o throws Exec Exception return to Date Time o find Type o Force a data object to a Double if possible Any numeric type can be forced to a Double as well as Char Array Byte Array or Boolean Complex types can not be forced to an Double This is n t particularly efficient so if you already b know b that the object you have is a Double you should just cast it Unlike link to Double Object byte this method will first determine the type of o and then do the cast Use link to Double Object byte if you already know the type param o object to cast return The object as a Double throws Exec Exception if the type ca n t be forced to an Double public static Double to Double Object o throws Exec Exception return to Double o find Type o public static Big Integer to Big Integer Object o throws Exec Exception return to Big Integer o find Type o public static Big Integer to Big Integer Object o byte type throws Exec Exception try switch type case return Boolean o Big Integer Big Integer case return Big Integer value Of Integer o long Value case return Big Integer value Of Long o long Value case return Big Integer value Of Float o long Value case return Big Integer value Of Double o long Value case return new Big Integer Data Byte Array o to String case return new Big Integer String o case return Big Integer o case return Big Decimal o to Big Integer case return Big Integer value Of Date Time o get Millis case return null case case case case case case default int err Code String msg Can not convert a find Type Name o to a Big Integer throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Big Integer throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Big Integer throw new Exec Exception msg err Code Pig Exception public static Big Decimal to Big Decimal Object o throws Exec Exception return to Big Decimal o find Type o public static Big Decimal to Big Decimal Object o byte type throws Exec Exception try switch type case return Boolean o Big Decimal Big Decimal case return Big Decimal value Of Integer o long Value case return Big Decimal value Of Long o long Value case return Big Decimal value Of Float o double Value case return Big Decimal value Of Double o double Value case return new Big Decimal Data Byte Array o to String case return new Big Decimal String o case return new Big Decimal Big Integer o case return Big Decimal o case return Big Decimal value Of Date Time o get Millis case return null case case case case case case default int err Code String msg Can not convert a find Type Name o to a Big Decimal throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Number Format Exception nfe int err Code String msg Problem with formatting Could not convert o to Big Decimal throw new Exec Exception msg err Code Pig Exception nfe catch Exception e int err Code String msg Internal error Could not convert o to Big Decimal throw new Exec Exception msg err Code Pig Exception Force a data object to a String if possible Any simple atomic type can be forced to a String including Byte Array Complex types can not be forced to a String This is n t particularly efficient so if you already b know b that the object you have is a String you should just cast it param o object to cast param type of the object you are casting return The object as a String throws Exec Exception if the type ca n t be forced to a String public static String to String Object o byte type throws Exec Exception try switch type case return Integer o to String case return Long o to String case return Float o to String case return Double o to String case return Date Time o to String case return Data Byte Array o to String case return String o case return Big Integer o to String case return Big Decimal o to String case return null case return Boolean o to String case return Byte o to String case case case case case default int err Code String msg Can not convert a find Type Name o to a String throw new Exec Exception msg err Code Pig Exception catch Class Cast Exception cce throw cce catch Exec Exception ee throw ee catch Exception e int err Code String msg Internal error Could not convert o to String throw new Exec Exception msg err Code Pig Exception Force a data object to a String if possible Any simple atomic type can be forced to a String including Byte Array Complex types can not be forced to a String This is n t particularly efficient so if you already b know b that the object you have is a String you should just cast it Unlike link to String Object byte this method will first determine the type of o and then do the cast Use link to String Object byte if you already know the type param o object to cast return The object as a String throws Exec Exception if the type ca n t be forced to a String public static String to String Object o throws Exec Exception return to String o find Type o If this object is a map return it as a map This is n t particularly efficient so if you already b know b that the object you have is a Map you should just cast it param o object to cast return The object as a Map throws Exec Exception if the type ca n t be forced to a Double Suppress Warnings unchecked public static Map String Object to Map Object o throws Exec Exception if o null return null if o instanceof Map o instanceof Internal Map try return Map String Object o catch Exception e int err Code String msg Internal error Could not convert o to Map throw new Exec Exception msg err Code Pig Exception else int err Code String msg Can not convert a find Type Name o to a Map throw new Exec Exception msg err Code Pig Exception If this object is a tuple return it as a tuple This is n t particularly efficient so if you already b know b that the object you have is a Tuple you should just cast it param o object to cast return The object as a Double throws Exec Exception if the type ca n t be forced to a Double public static Tuple to Tuple Object o throws Exec Exception if o null return null if o instanceof Tuple try return Tuple o catch Exception e int err Code String msg Internal error Could not convert o to Tuple throw new Exec Exception msg err Code Pig Exception else int err Code String msg Can not convert a find Type Name o to a Tuple throw new Exec Exception msg err Code Pig Exception If this object is a bag return it as a bag This is n t particularly efficient so if you already b know b that the object you have is a bag you should just cast it param o object to cast return The object as a Double throws Exec Exception if the type ca n t be forced to a Double public static Data Bag to Bag Object o throws Exec Exception if o null return null if o instanceof Data Bag try return Data Bag o catch Exception e int err Code String msg Internal error Could not convert o to Bag throw new Exec Exception msg err Code Pig Exception else int err Code String msg Can not convert a find Type Name o to a Data Bag throw new Exec Exception msg err Code Pig Exception Purely for debugging public static void spill Tuple Contents Tuple t String label System out print Tuple label Iterator Object i t get All iterator for int j i has Next j System out print j i next get Class get Name System out println t to String Determine if this type is a numeric type param t type as byte value to test return true if this is a numeric type false otherwise public static boolean is Number Type byte t switch t case return true case return true case return true case return true case return true case return true default return false Determine if this is a type that can work can be done on param t type as a byte value to test return false if the type is unknown null or error true otherwise public static boolean is Usable Type byte t switch t case return false case return false case return false default return true Test if one type can cast to the other param cast Type data type of the cast type param input Type data type of the input return true or false public static boolean castable byte cast Type byte input Type Only legal types can be cast to if Data Type is Usable Type cast Type Data Type is Usable Type input Type return false Same type is castable if cast Type input Type return true Numerical type is castable if Data Type is Number Type cast Type Data Type is Number Type input Type return true databyte can cast to anything if input Type Data Type return true Cast numerical type to string or vice versa is valid if Data Type is Number Type input Type cast Type Data Type Data Type is Number Type cast Type input Type Data Type return true else return false return false Merge types if possible Merging types means finding a type that one or both types can be upcast to param type param type return the merged type or Data Type if not successful public static byte merge Type byte type byte type Only legal types can be merged if Data Type is Usable Type type Data Type is Usable Type type return Data Type Same type is if type type return type Both are number so we return the bigger type if Data Type is Number Type type Data Type is Number Type type return type type type type One is bytearray and the other is number or chararray if type Data Type return type if type Data Type return type else return just return Data Type Given a map turn it into a String param m map return string representation of the map public static String map To String Map String Object m boolean has Next false String Builder sb new String Builder sb append for Map Entry String Object e m entry Set if has Next sb append else has Next true sb append e get Key sb append Object val e get Value if val null sb append val to String sb append return sb to String Test whether two byte arrays Java byte arrays not Pig byte arrays are equal have no idea why we have this function param lhs byte array param rhs byte array return true if both are null or the two are the same length and have the same bytes public static boolean equal Byte Arrays byte lhs byte rhs if lhs null rhs null return true if lhs null rhs null return false if lhs length rhs length return false for int i i lhs length i if lhs i rhs i return false return true Utility method that determines the schema from the passed in data Type If the data Type is Bag or Tuple then we need to determine the schemas inside this data Type for this we iterate through the fields inside this field This method works both for raw objects and Resource Schema Resource Field Schema field descriptions the specific behavior is determined by the klass parameter param data Type Data Type Data Type and so on param field Iter iterator over the fields if this is a tuple or a bag param field Num number of fields inside the field if a tuple param klass should be Object or Resource Schema Resource Field Schema return throws Exec Exception throws Frontend Exception throws Schema Merge Exception Suppress Warnings deprecation private static Schema Field Schema determine Field Schema byte data Type Iterator field Iter long field Num Class klass throws Exec Exception Frontend Exception Schema Merge Exception switch data Type case return new Schema Field Schema null case case case case case case case case case case case return new Schema Field Schema null data Type case Schema schema null if field Num schema new Schema for int i i field Num i schema add determine Field Schema klass cast field Iter next return new Schema Field Schema null schema case Schema schema null Schema bag Schema null if field Num Array List Schema schemas new Array List Schema while field Iter has Next schemas add determine Field Schema klass cast field Iter next schema schema schemas get if null schema Schema Field Schema tuple Fs new Schema Field Schema null null bag Schema new Schema tuple Fs bag Schema set Two Level Access Required true return new Schema Field Schema null bag Schema int schema Size schema size for int i i schemas size i Schema curr Schema schemas get i if null curr Schema curr Schema size schema Size Schema Field Schema tuple Fs new Schema Field Schema null null bag Schema new Schema tuple Fs bag Schema set Two Level Access Required true return new Schema Field Schema null bag Schema schema Schema merge Schema schema curr Schema false false false Schema Field Schema tuple Fs new Schema Field Schema null schema bag Schema new Schema tuple Fs since this schema has tuple field schema which internally has a list of field schemas for the actual items in the bag an access to any field in the bag is a two level access bag Schema set Two Level Access Required true return new Schema Field Schema null bag Schema default int err Code String msg Can not determine field schema throw new Exec Exception msg err Code Pig Exception Determine the field schema of an Resource Field Schema param rc Field Schema the rc Field Schema we want translated return the field schema corresponding to the object throws Exec Exception Frontend Exception Schema Merge Exception public static Schema Field Schema determine Field Schema Resource Schema Resource Field Schema rc Field Schema throws Exec Exception Frontend Exception Schema Merge Exception byte dt rc Field Schema get Type Iterator Resource Schema Resource Field Schema field Iter null long field Num if dt dt field Iter Arrays as List rc Field Schema get Schema get Fields iterator field Num rc Field Schema get Schema get Fields length return determine Field Schema dt field Iter field Num Resource Schema Resource Field Schema class Determine the field schema of an object param o the object whose field schema is to be determined return the field schema corresponding to the object throws Exec Exception Frontend Exception Schema Merge Exception public static Schema Field Schema determine Field Schema Object o throws Exec Exception Frontend Exception Schema Merge Exception byte dt find Type o Iterator field Iter null long field Num if dt field Iter Tuple o get All iterator field Num Tuple o size else if dt field Num Data Bag o size field Iter Data Bag o iterator return determine Field Schema dt field Iter field Num Object class 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Buffered Input Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java io File Input Stream import java io File Not Found Exception import java io Exception import java util Array List import java util Iterator import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Counters import org apache pig Pig Warning An unordered collection of Tuples possibly with multiples The tuples are stored in a List since there is no concern for order or distinctness public class Default Data Bag extends Default Abstract Bag private static final long serial Version private static final Log log Log Factory get Log Default Data Bag class private static final Inter Sedes Inter Sedes Factory get Inter Sedes Instance public Default Data Bag m Contents new Array List Tuple This constructor creates a bag out of an existing list of tuples by taking ownership of the list and copying the contents of the list param list Of Tuples List Tuple containing the tuples public Default Data Bag List Tuple list Of Tuples m Contents list Of Tuples m Size list Of Tuples size mark Spillable If Necessary Override public boolean is Sorted return false Override public boolean is Distinct return false Override public Iterator Tuple iterator return new Default Data Bag Iterator Override public long spill Make sure we have something to spill Do n t create empty files as that will make a mess if m Contents size return Lock the container before spill so that iterators are n t trying to read while m mucking with the container long spilled synchronized m Contents Data Output Stream out null try out get Spill File catch Exception ioe Do not remove last file from spilled array It was not added as File create Tmp File threw an Exception warn Unable to create tmp file to spill to disk Pig Warning ioe return try Iterator Tuple i m Contents iterator while i has Next write Datum out i next Data Type spilled This will spill every records if spilled x fff report Progress out flush out close out null m Contents clear catch Throwable e Remove the last file from the spilled array since we failed to write to it m Spill Files remove m Spill Files size warn Unable to spill contents to disk Pig Warning e return finally if out null try out close catch Exception e warn Error closing spill Pig Warning e Increment the spill count inc Spill Count Pig Counters return spilled An iterator that handles getting the next tuple from the bag This iterator has a couple of issues to deal with First data can be stored in a combination of in memory and on disk Second the bag may be asked to spill while the iterator is reading it This means that it will be pointing to someplace in memory and suddenly it will need to switch to a disk file private class Default Data Bag Iterator implements Iterator Tuple We have to buffer a tuple because there s no easy way for next to tell whether or not there s another tuple available other than to read it private Tuple m Buf null private int m Memory Ptr private int m File Ptr private Data Input Stream m In null private int m Cntr private boolean has Cached Tuple false Default Data Bag Iterator Override public boolean has Next Once we call has Next set the flag so we can call has Next repeated without fetching next tuple if has Cached Tuple return m Buf null m Buf next has Cached Tuple true return m Buf null Override public Tuple next This will report progress every times through next This should be much faster than using mod if m Cntr x ff report Progress If there s one in the buffer use that one if has Cached Tuple Tuple t m Buf has Cached Tuple false return t See if we ve been reading from memory or not if m Memory Ptr If there s still data in memory keep reading from there Lock before we check the size obtain a reader lock from this point forward we ca n t have them spilling on us synchronized m Contents if m Contents size return read From Memory The container spilled since our last read Do n t need to the hold the lock now as it s already spilled on us Our file pointer will already point to the new spill file because it was either already or had been incremented past the end of the old m Spill Files size We need to open the new file and then fast forward past all of the tuples we ve already read Then we need to reset m Memory Ptr so we know to read from the file next time we come through try m In new Data Input Stream new Buffered Input Stream new File Input Stream m Spill Files get m File Ptr catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe for int i i m Memory Ptr i try read Datum m In catch Exception eof This should never happen it means we did n t dump all of our tuples to disk String msg Ran out of tuples to read prematurely log fatal msg eof throw new Runtime Exception msg eof catch Exception ioe String msg Unable to read our spill file log fatal msg ioe throw new Runtime Exception msg ioe m Memory Ptr return read From File We have n t read from memory yet so keep trying to read from the file return read From File Not implemented Override public void remove private Tuple read From File if m In null We already have a file open Tuple t try t Tuple read Datum m In return t catch Exception eof Fall through to the next case where we find the next file or go to memory try m In close catch Exception e log warn Failed to close spill file e catch Exception ioe String msg Unable to read our spill file log fatal msg ioe throw new Runtime Exception msg ioe Need to open the next file if there is one Have to lock here because otherwise we could decide there s no more files and between the time we decide that and start trying to read from memory the container could spill and then we re stuck If there s another file to read we can unlock immediately If there is n t we need to hold the lock and go into read From Memory synchronized m Contents if m Spill Files null m File Ptr m Spill Files size We ve read everything there is to read from the files go look in memory return read From Memory Open the next file then call ourselves again as it will enter the if above try m In new Data Input Stream new Buffered Input Stream new File Input Stream m Spill Files get m File Ptr catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe return read From File This should only be called once we know we have n t spilled It assumes that the m Contents lock is already held before we enter this function private Tuple read From Memory if m Contents size return null if m Memory Ptr m Contents size return List Tuple m Contents get m Memory Ptr else return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Byte Array Input Stream import java io Data Input import java io Data Input Stream import java io Data Output import java io Exception import java io Unsupported Encoding Exception import java math Big Decimal import java math Big Integer import java nio Byte Buffer import java util Array List import java util Iterator import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io Writable Comparator import org apache hadoop mapred Job Conf import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig impl io Nullable Tuple import org apache pig impl util Object Serializer default implementation of Tuple This class will be created by the Default Tuple Factory public class Default Tuple extends Abstract Tuple private static final long serial Version protected List Object m Fields Default constructor This constructor is public so that hadoop can call it directly However inside pig you should never be calling this function Use Tuple Factory instead br Time complexity after allocation public Default Tuple m Fields new Array List Object Construct a tuple with a known number of fields Package level so that callers can not directly invoke it br Resulting tuple is filled pre filled with null elements Time complexity after allocation param size Number of fields to allocate in the tuple Default Tuple int size m Fields new Array List Object size for int i i size i m Fields add null Construct a tuple from an existing list of objects Package level so that callers can not directly invoke it br Time complexity plus running time of input object iteration after allocation param c List of objects to turn into a tuple Default Tuple List Object c m Fields new Array List Object c Construct a tuple from an existing list of objects Package level so that callers can not directly invoke it br Time complexity param c List of objects to turn into a tuple This list will be kept as part of the tuple param junk Just used to differentiate from the constructor above that copies the list Default Tuple List Object c int junk m Fields c Find the size of the tuple Used to be called arity return number of fields in the tuple Override public int size return m Fields size Get the value in a given field param field Num Number of the field to get the value for return value as an Object throws Exec Exception if the field number is greater than or equal to the number of fields in the tuple Override public Object get int field Num throws Exec Exception return m Fields get field Num Get all of the fields in the tuple as a list return List lt Object gt containing the fields of the tuple in order Override public List Object get All return m Fields Set the value in a given field param field Num Number of the field to set the value for param val Object to put in the indicated field throws Exec Exception if the field number is greater than or equal to the number of fields in the tuple Override public void set int field Num Object val throws Exec Exception m Fields set field Num val Append a field to a tuple This method is not efficient as it may force copying of existing data in order to grow the data structure Whenever possible you should construct your Tuple with the new Tuple int method and then fill in the values with set rather than construct it with new Tuple and append values param val Object to append to the tuple Override public void append Object val m Fields add val Determine the size of tuple in memory This is used by data bags to determine their memory size This need not be exact but it should be a decent estimation return estimated memory size Override public long get Memory Size Iterator Object i m Fields iterator rest of the fixed portion of mfields size is accounted within empty tuple size long mfields var size Size Util round To Eight m Fields size in java hotspot bit vm there seems to be a minimum tuple size of which is probably from the minimum size of this array list mfields var size Math max mfields var size fixed overhead bytes tuple object header m Fields reference m Fields array list fixed size long sum mfields var size while i has Next sum Size Util get Pig Obj Mem Size i next return sum Override public int compare To Object other if other instanceof Tuple Tuple t Tuple other int my Sz m Fields size int t Sz t size if t Sz my Sz return else if t Sz my Sz return else for int i i my Sz i try int c Data Type compare m Fields get i t get i if c return c catch Exec Exception e throw new Runtime Exception Unable to compare tuples e return else return Data Type compare this other public static class Default Tuple Raw Comparator extends Writable Comparator implements Tuple Raw Comparator private final Log m Log Log Factory get Log get Class private boolean m Asc private boolean m Whole Tuple private boolean m Has Null Field private Tuple Factory m Fact public Default Tuple Raw Comparator super Default Tuple class Override public Configuration get Conf return null Override public void set Conf Configuration conf try m Asc boolean Object Serializer deserialize conf get pig sort Order catch Exception ioe m Log error Unable to deserialize pig sort Order ioe get Message throw new Runtime Exception ioe if m Asc null m Asc new boolean m Asc true If there s only one entry in m Asc it means it s for the whole tuple So we ca n t be looking for each column m Whole Tuple m Asc length m Fact Tuple Factory get Instance Override public boolean has Compared Tuple Null return m Has Null Field Compare two Default Tuples as raw bytes We assume the Tuples are Pig Nullable Writable so client classes need to deal with Null and Index Override public int compare byte b int s int l byte b int s int l Byte Buffer bb Byte Buffer wrap b s l Byte Buffer bb Byte Buffer wrap b s l int rc compare Default Tuple bb bb true adjust for secondary sort asc return rc Compare two Default Tuples as raw bytes private int compare Default Tuple Byte Buffer bb Byte Buffer bb boolean top Level m Has Null Field false store the position in case of deserialization int s bb position int s bb position int rc byte tuple Type bb get byte tuple Type bb get assert tuple Type tuple Type tuple Type Data Type first compare sizes int sz bb get Int int sz bb get Int if sz sz return else if sz sz return else if sizes are the same compare field by field for int i i sz rc i byte dt bb get byte dt bb get if dt dt switch dt case Data Type if top Level we are scanning the top level Tuple original call m Has Null Field true rc break case Data Type case Data Type byte bv bb get byte bv bb get rc bv bv bv bv break case Data Type int iv bb get Int int iv bb get Int rc iv iv iv iv break case Data Type long lv bb get Long long lv bb get Long rc lv lv lv lv break case Data Type float fv bb get Float float fv bb get Float rc Float compare fv fv break case Data Type double dv bb get Double double dv bb get Double rc Double compare dv dv break case Data Type if bb get Data Type bb get Data Type throw new Runtime Exception Issue in comparing raw bytes for Default Tuple was not serialized with int basz bb get Int int basz bb get Int byte ba new byte basz byte ba new byte basz bb get ba bb get ba rc new Big Integer ba compare To new Big Integer ba break case Data Type byte catype bb get byte catype bb get int casz catype Data Type bb get Short bb get Int int casz catype Data Type bb get Short bb get Int byte ca new byte casz byte ca new byte casz bb get ca bb get ca String str null str null try str new String ca Data Reader Writer str new String ca Data Reader Writer catch Unsupported Encoding Exception uee m Log warn Unsupported string encoding uee uee print Stack Trace if str null str null rc new Big Decimal str compare To new Big Decimal str break case Data Type long dtv bb get Long bb position bb position move cursor forward without read the timezone bytes long dtv bb get Long bb position bb position rc dtv dtv dtv dtv break case Data Type int basz bb get Int int basz bb get Int byte ba new byte basz byte ba new byte basz bb get ba bb get ba rc Data Byte Array compare ba ba break case Data Type case Data Type int casz dt Data Type bb get Short bb get Int int casz dt Data Type bb get Short bb get Int byte ca new byte casz byte ca new byte casz bb get ca bb get ca String str null str null try str new String ca Data Reader Writer str new String ca Data Reader Writer catch Unsupported Encoding Exception uee m Log warn Unsupported string encoding uee uee print Stack Trace if str null str null rc str compare To str break case Data Type put back the cursor to before Data Type bb position bb position bb position bb position rc compare Default Tuple bb bb false break default m Log info Unsupported Data Type for binary comparison switching to object deserialization Data Type gen Type To Name Map get dt dt Tuple t m Fact new Tuple Tuple t m Fact new Tuple try t read Fields new Data Input Stream new Byte Array Input Stream bb array s bb limit t read Fields new Data Input Stream new Byte Array Input Stream bb array s bb limit catch Exception ioe m Log error Unable to instantiate tuples for comparison ioe get Message throw new Runtime Exception ioe get Message ioe delegate to compare Tuple return compare Tuple t t else compare Data Types if dt dt rc else rc flip if the order is descending if rc if m Whole Tuple m Asc i rc else if m Whole Tuple m Asc rc return rc Override public int compare Object o Object o Nullable Tuple nt Nullable Tuple o Nullable Tuple nt Nullable Tuple o int rc if either are null handle differently if nt is Null nt is Null rc compare Tuple Tuple nt get Value As Pig Type Tuple nt get Value As Pig Type else for sorting purposes two nulls are equal if nt is Null nt is Null rc else if nt is Null rc else rc if m Whole Tuple m Asc rc return rc private int compare Tuple Tuple t Tuple t int sz t size int sz t size if sz sz return else if sz sz return else for int i i sz i try int c Data Type compare t get i t get i if c if m Whole Tuple m Asc i c else if m Whole Tuple m Asc c return c catch Exec Exception e throw new Runtime Exception Unable to compare tuples e return Override public int hash Code int hash for Iterator Object it m Fields iterator it has Next Object o it next if o null hash hash o hash Code return hash Override public void write Data Output out throws Exception out write Byte Data Type int sz size out write Int sz for int i i sz i Data Reader Writer write Datum out m Fields get i Override public void read Fields Data Input in throws Exception Clear our fields in case we re being reused m Fields clear Make sure it s a tuple byte b in read Byte if b Data Type int err Code String msg Unexpected data while reading tuple from binary file throw new Exec Exception msg err Code Pig Exception Read the number of fields int sz in read Int for int i i sz i try append Data Reader Writer read Datum in catch Exec Exception ee throw ee public static Class extends Tuple Raw Comparator get Comparator Class return Default Tuple Raw Comparator class 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import java util Array List import java util List import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Source Location get one or elements out of a tuple or a bag in case of Tuple a int b bag b a int b b float c int the number after represents the uid Dereference a int dereference of single column in a tuple gives the field Dereference Tuple a int c int dereference of more than one column gives a tuple Dereference Dereference b bag b b float dereference of a bag gives a bag public class Dereference Expression extends Column Expression private List Object raw Columns new Array List Object private List Integer columns new Array List Integer The column in the input bag which the project references Count is zero based public Dereference Expression Operator Plan plan super Dereference plan plan add this public Dereference Expression Operator Plan plan int col Num this plan columns add col Num public Dereference Expression Operator Plan plan List Integer column Nums this plan columns add All column Nums public void set Raw Columns List Object cols raw Columns add All cols link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this public List Integer get Bag Columns return columns public void set Bag Columns List Integer columns this columns columns this raw Columns clear We do n t need this any more Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Dereference Expression Dereference Expression po Dereference Expression other if po columns size columns size return false return po columns contains All columns get Referred Expression is Equal po get Referred Expression else return false public Logical Expression get Referred Expression throws Frontend Exception if plan get Successors this size throw new Frontend Exception Could not find a related project Expression for Dereference return Logical Expression plan get Successors this get public String to String String Builder msg new String Builder msg append Name name Type if field Schema null msg append Data Type find Type Name field Schema type else msg append null msg append Uid if field Schema null msg append field Schema uid else msg append null msg append Column columns msg append return msg to String Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema Logical Expression successor Logical Expression plan get Successors this get Logical Field Schema pred successor get Field Schema if pred null if pred type Data Type Logical Schema inner Schema null if pred schema null inner Schema new Logical Schema Get the tuple inner schema Logical Schema orig Schema pred schema get Field schema Slice the tuple inner schema if raw Columns is Empty columns translate Alias To Pos orig Schema raw Columns for int column columns if orig Schema null orig Schema size inner Schema add Field orig Schema get Field column else inner Schema add Field new Logical Field Schema null null Data Type Logical Schema bag Schema new Logical Schema bag Schema add Field new Logical Schema Logical Field Schema null inner Schema Data Type Logical Expression get Next Uid field Schema new Logical Schema Logical Field Schema null bag Schema Data Type Logical Expression get Next Uid uid Only Field Schema field Schema merge Uid uid Only Field Schema else Dereference a field out of a tuple if pred schema null if raw Columns is Empty columns translate Alias To Pos pred schema raw Columns if pred schema null pred schema size if columns size field Schema pred schema get Field columns get else Logical Schema inner Schema new Logical Schema String alias pred alias for int column columns inner Schema add Field pred schema get Field column String sub Alias pred schema get Field column alias if sub Alias null sub Alias alias alias sub Alias field Schema new Logical Schema Logical Field Schema alias inner Schema Data Type Logical Expression get Next Uid else field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema else field Schema new Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema private List Integer translate Alias To Pos Logical Schema schema List Object raw Columns throws Frontend Exception List Integer columns new Array List Integer for Object raw Column raw Columns if raw Column instanceof Integer if schema null Integer raw Column schema size Integer raw Column throw new Frontend Exception Index raw Column out of range in schema schema to String false columns add Integer raw Column else if schema null int pos schema get Field Position String raw Column if pos columns add pos continue else throw new Frontend Exception Can not find field raw Column in schema to String false return columns Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception List Integer columns Copy new Array List Integer this get Bag Columns Dereference Expression copy new Dereference Expression lg Exp Plan columns Copy List Object raw Columns Copy new Array List Object this raw Columns copy set Raw Columns raw Columns Copy Only one input is expected Logical Expression input Logical Expression plan get Successors this get Logical Expression input Copy input deep Copy lg Exp Plan lg Exp Plan add input Copy lg Exp Plan connect copy input Copy copy set Location new Source Location location return copy public List Object get Raw Columns return this raw Columns 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Hash Set import java util Iterator import java util Set import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Tuple import org apache pig data Tuple Factory takes two bags as arguments and compares them Any tuples that are in one bag but not the other are returned If the fields are not bags then they will be returned if they do not match or an empty bag will be returned if the two records match p The implementation assumes that both bags being passed to this function will fit entirely into memory simultaneously If that is not the case the will still function but it will be strong very strong slow public class extends Eval Func Data Bag Tuple Factory m Tuple Factory Tuple Factory get Instance Bag Factory m Bag Factory Bag Factory get Instance Compares a tuple with two fields Emits any differences param input a tuple with exactly two fields throws Exception if there are not exactly two fields in a tuple Override public Data Bag exec Tuple input throws Exception if input size int err Code String msg expected two inputs but received input size inputs throw new Exec Exception msg err Code Pig Exception try Data Bag output m Bag Factory new Default Bag Object o input get if o instanceof Data Bag Data Bag bag Data Bag o Data Bag bag Data Bag input get compute Diff bag bag output else Object d input get Object d input get if d equals d output add m Tuple Factory new Tuple d output add m Tuple Factory new Tuple d return output catch Exec Exception ee throw ee private void compute Diff Data Bag bag Data Bag bag Data Bag emit To Build two hash tables and probe with first one then the other This does make the assumption that the distinct set of keys from each bag will fit in memory Set Tuple s new Hash Set Tuple Iterator Tuple i bag iterator while i has Next s add i next Set Tuple s new Hash Set Tuple Iterator Tuple i bag iterator while i has Next s add i next for Tuple t s if s contains t emit To add t for Tuple t s if s contains t emit To add t Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Buffered Input Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java io File import java io File Input Stream import java io File Not Found Exception import java io Exception import java util Array List import java util Arrays import java util Collections import java util Hash Set import java util Iterator import java util Linked List import java util List Iterator import java util Tree Set import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Counters import org apache pig Pig Warning An unordered collection of Tuples with no multiples Data is stored without duplicates as it comes in When it is time to spill that data is sorted and written to disk It must also be sorted upon the first read otherwise if a spill happened after that the iterators would have no way to find their place in the new file The data is stored in a Hash Set When it is time to sort it is placed in an Array List and then sorted Dispite all these machinations this was found to be faster than storing it in a Tree Set public class Distinct Data Bag extends Default Abstract Bag private static final long serial Version private static final Log log Log Factory get Log Distinct Data Bag class private static final Inter Sedes Inter Sedes Factory get Inter Sedes Instance public Distinct Data Bag m Contents new Hash Set Tuple Override public boolean is Sorted return false Override public boolean is Distinct return true Override public long size if m Spill Files null m Spill Files size We need to racalculate size to guarantee a count of unique entries including those on disk Iterator Tuple iter iterator int new Size while iter has Next new Size iter next synchronized m Contents we do n t want adds to change our numbers the lock may need to cover more of the method m Size new Size return m Size Override public Iterator Tuple iterator return new Distinct Data Bag Iterator Override public void add Tuple t synchronized m Contents if m Contents add t m Size mark Spillable If Necessary Override public long spill Make sure we have something to spill Do n t create empty files as that will make a mess if m Contents size return Lock the container before spill so that iterators are n t trying to read while m mucking with the container long spilled synchronized m Contents Data Output Stream out null try out get Spill File catch Exception ioe Do not remove last file from spilled array It was not added as File create Tmp File threw an Exception warn Unable to create tmp file to spill to disk Pig Warning ioe return try If we ve already started reading then it will already be sorted into an array list If not we need to sort it before writing if m Contents instanceof Array List Iterator Tuple i m Contents iterator while i has Next write Datum out i next Data Type spilled This will spill every records if spilled x fff report Progress else Tuple array new Tuple m Contents size m Contents to Array array Arrays sort array for int i i array length i array i write out spilled This will spill every records if spilled x fff report Progress out flush out close out null m Contents clear catch Throwable e Remove the last file from the spilled array since we failed to write to it m Spill Files remove m Spill Files size warn Unable to spill contents to disk Pig Warning e return finally if out null try out close catch Exception e warn Error closing spill Pig Warning e Increment the spill count inc Spill Count Pig Counters return spilled An iterator that handles getting the next tuple from the bag This iterator has a couple of issues to deal with First data can be stored in a combination of in memory and on disk Second the bag may be asked to spill while the iterator is reading it This means that it will be pointing to someplace in memory and suddenly it will need to switch to a disk file private class Distinct Data Bag Iterator implements Iterator Tuple private class Container implements Comparable Container public Tuple tuple public int file Num Override Suppress Warnings unchecked public int compare To Container other return tuple compare To other tuple Override public boolean equals Object obj if obj instanceof Container return tuple equals Container obj tuple else return false Override public int hash Code return tuple hash Code We have to buffer a tuple because there s no easy way for next to tell whether or not there s another tuple available other than to read it private Tuple m Buf null private int m Memory Ptr private Tree Set Container m Merge Tree null private Array List Data Input Stream m Streams null private int m Cntr Suppress Warnings unchecked Distinct Data Bag Iterator If this is the first read we need to sort the data synchronized m Contents if m Contents instanceof Hash Set pre Merge We re the first reader we need to sort the data This is in case it gets dumped under us Array List Tuple l new Array List Tuple m Contents Collections sort l m Contents l Override public boolean has Next See if we can find a tuple If so buffer it m Buf next return m Buf null Override public Tuple next This will report progress every times through next This should be much faster than using mod if m Cntr x ff report Progress If there s one in the buffer use that one if m Buf null Tuple t m Buf m Buf null return t Check to see if we just need to read from memory boolean spilled false synchronized m Contents if m Spill Files null m Spill Files size return read From Memory if m Memory Ptr m Contents size spilled true Check to see if we were reading from memory but we spilled if spilled Data Input Stream in We need to open the new file and then fast forward past all of the tuples we ve already read Then we need to place the first tuple from that file in the priority queue Whatever tuples from memory that were already in the queue will be fine as they re guaranteed to be ahead of the point we fast foward to We re guaranteed that the file we want to read from for the fast forward is the last element in m Spill Files because we do n t support calls to add after calls to iterator and spill wo n t create empty files try in new Data Input Stream new Buffered Input Stream new File Input Stream m Spill Files get m Spill Files size if m Streams null m Merge Tree new Tree Set Container We did n t have any files before this spill m Streams new Array List Data Input Stream m Streams add in catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Fast forward past the tuples we ve already put in the queue for int i i m Memory Ptr i try read Datum in catch Exception eof This should never happen it means we did n t dump all of our tuples to disk throw new Runtime Exception Ran out of tuples to read prematurely eof catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe m Memory Ptr Add the next tuple from this file to the queue add To Queue null m Spill Files size Fall through to read the next entry from the priority queue We have spill files so we need to read the next tuple from one of those files or from memory return read From Tree Not implemented Override public void remove private Tuple read From Tree if m Merge Tree null First read we need to set up the queue and the array of file streams m Merge Tree new Tree Set Container Add one to the size in case we spill later m Streams new Array List Data Input Stream m Spill Files size Iterator File i m Spill Files iterator while i has Next try Data Input Stream in new Data Input Stream new Buffered Input Stream new File Input Stream i next m Streams add in Add the first tuple from this file into the merge queue add To Queue null m Streams size catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Prime one from memory too if m Contents size add To Queue null if m Merge Tree size return null Pop the top one off the queue Container c m Merge Tree first m Merge Tree remove c Add the next tuple from whereever we read from into the queue Buffer the tuple we re returning as we ll be reusing c Tuple t c tuple add To Queue c c file Num return t private void add To Queue Container c int file Num if c null c new Container c file Num file Num if file Num Need to read from memory We may have spilled since this tuple was put in the queue and hence memory might be empty But do n t care as then just wo n t add any more from memory synchronized m Contents do c tuple read From Memory if c tuple null If we find a unique entry then add it to the queue Otherwise ignore it and keep reading if m Merge Tree add c return while c tuple null return Read the next tuple from the indicated file Data Input Stream in m Streams get file Num if in null There s still data in this file do try c tuple Tuple read Datum in If we find a unique entry then add it to the queue Otherwise ignore it and keep reading If we run out of tuples to read that s fine we just wo n t add a new one from this file if m Merge Tree add c return catch Exception eof Out of tuples in this file Set our slot in the array to null so we do n t keep trying to read from this file try in close catch Exception e log warn Failed to close spill file e m Streams set file Num null return catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe while true Function assumes that the reader lock is already held before we enter this function private Tuple read From Memory if m Contents size return null if m Memory Ptr m Contents size return Array List Tuple m Contents get m Memory Ptr else return null Pre merge if there are too many spill files This avoids the issue of having too large a fan out in our merge Experimentation by the hadoop team has shown that is about the optimal number of spill files This function modifies the m Spill Files array and assumes the write lock is already held It will not unlock it Tuples are reconstituted as tuples evaluated and rewritten as tuples This is expensive but do n t know how to read tuples from the file otherwise This function is slightly different than the one in Sorted Data Bag as it uses a Tree Set instead of a Priority private void pre Merge if m Spill Files null m Spill Files size return While there are more than max spill files gather max spill files together and merge them into one file Then remove the others from m Spill Files The new spill files are attached at the end of the list so can just keep going until get a small enough number without too much concern over uneven size merges Convert m Spill Files to a linked list since we ll be removing pieces from the middle and we want to do it efficiently try Linked List File ll new Linked List File m Spill Files Linked List File files To Delete new Linked List File while ll size List Iterator File i ll list Iterator m Streams new Array List Data Input Stream m Merge Tree new Tree Set Container for int j j j try File f i next Data Input Stream in new Data Input Stream new Buffered Input Stream new File Input Stream f m Streams add in add To Queue null m Streams size i remove files To Delete add f catch File Not Found Exception fnfe We ca n t find our own spill file That should neer happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Get a new spill file This adds one to the end of the spill files list So need to append it to my linked list as well so that it s still there when move my linked list back to the spill files Data Output Stream out null try out get Spill File ll add m Spill Files get m Spill Files size Tuple t while t read From Tree null t write out out flush catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe finally if out null try out close catch Exception e warn Error closing spill Pig Warning e delete files that have been merged into new files for File f files To Delete if f delete false log warn Failed to delete spill file f get Path clear the list so that finalize does not delete any files when m Spill Files is assigned a new value m Spill Files clear Now move our new list back to the spill files array m Spill Files new File List ll finally Reset m Streams and m Merge so that they ll be allocated properly for regular merging m Streams null m Merge Tree null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java math Big Decimal import java math Big Integer import java math Rounding Mode import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Warning import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Divide extends Binary Expression Operator private static final long serial Version public static final short private static final Log Log Factory get Log Divide class public Divide Operator Key k super k public Divide Operator Key k int rp super k rp Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Divide this Override public String name return Divide Data Type find Type Name result Type m Key to String This method is used to invoke the appropriate method as Java does not provide generic dispatch for it protected Number divide Number a Number b byte data Type throws Exec Exception switch data Type case Data Type return Double value Of Double a Double b case Data Type return Integer value Of Integer a Integer b case Data Type return Long value Of Long a Long b case Data Type return Float value Of Float a Float b case Data Type return Big Integer a divide Big Integer b case Data Type return big Decimal Divide With Scale a b default throw new Exec Exception called on unsupported Number class Data Type find Type Name data Type private Number big Decimal Divide With Scale Number a Number b Using same result scaling as Hive See Arithmetic Rules https cwiki apache org confluence download attachments Hive Decimal Precision Scale Support pdf int result Scale Math max Big Decimal a scale Big Decimal b precision if is Debug Enabled debug For bigdecimal divide using result Scale as result scale return Big Decimal a divide Big Decimal b result Scale Rounding Mode This method is used to invoke the appropriate method as Java does not provide generic dispatch for it protected boolean equals Zero Number a byte data Type throws Exec Exception switch data Type case Data Type return Double a equals case Data Type return Integer a equals case Data Type return Long a equals case Data Type return Float a equals f case Data Type return Big Integer equals Big Integer a case Data Type return Big Decimal equals Big Decimal a default throw new Exec Exception Called on unsupported Number class Data Type find Type Name data Type protected Result generic Get Next byte data Type throws Exec Exception Result r accum Child null data Type if r null return r byte status Result res res lhs get Next data Type status res return Status if status Status res result null return res Number left Number res result res rhs get Next data Type status res return Status if status Status res result null return res Number right Number res result if equals Zero right data Type if pig Logger null pig Logger warn this Divide by zero Converting it to Pig Warning res result null else res result divide left right data Type return res Override public Result get Next Double throws Exec Exception return generic Get Next Data Type Override public Result get Next Float throws Exec Exception return generic Get Next Data Type Override public Result get Next Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Long throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Decimal throws Exec Exception return generic Get Next Data Type Override public Divide clone throws Clone Not Supported Exception Divide clone new Divide new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Divide Operator public class Divide Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Divide Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Divide plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Divide Expression Divide Expression ao Divide Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null get Lhs get Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Divide Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Iterator import org apache pig Accumulator import org apache pig Algebraic import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer schema Schema This method should never be used directly use link public class Double Avg extends Eval Func Double implements Algebraic Accumulator Double private static Tuple Factory m Tuple Factory Tuple Factory get Instance Override public Double exec Tuple input throws Exception try Double sum sum input if sum null either we were handed an empty bag or a bag filled with nulls return null in this case return null double count count input Double avg null if count avg new Double sum count return avg catch Exec Exception ee throw ee Override public String get Initial return Initial class get Name Override public String get Intermed return Intermediate class get Name Override public String get Final return Final class get Name static public class Initial extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception try Tuple t m Tuple Factory new Tuple input is a bag with one tuple containing the column we are trying to avg on Data Bag bg Data Bag input get Double d null if bg iterator has Next Tuple tp bg iterator next d Double tp get t set d if d null t set else t set return t catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Intermediate extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception try Data Bag b Data Bag input get return combine b catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Final extends Eval Func Double Override public Double exec Tuple input throws Exception try Data Bag b Data Bag input get Tuple combined combine b Double sum Double combined get if sum null return null double count Long combined get Double avg null if count avg new Double sum count return avg catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static protected Tuple combine Data Bag values throws Exec Exception double sum long count combine is called from Intermediate and Final In either case Initial would have been called before and would have sent in valid tuples Hence we do n t need to check if incoming bag is empty Tuple output m Tuple Factory new Tuple boolean saw Non Null false for Iterator Tuple it values iterator it has Next Tuple t it next Double d Double t get we count nulls in avg as contributing a departure from for performance of which implemented by just inspecting size of the bag if d null d else saw Non Null true sum d count Long t get if saw Non Null output set new Double sum else output set null output set Long value Of count return output static protected long count Tuple input throws Exec Exception Data Bag values Data Bag input get Iterator it values iterator long cnt while it has Next Tuple t Tuple it next if t null t size t get null cnt return cnt static protected Double sum Tuple input throws Exec Exception Exception Data Bag values Data Bag input get if we were handed an empty bag return if values null values size return null double sum boolean saw Non Null false for Iterator Tuple it values iterator it has Next Tuple t it next try Double d Double t get if d null continue saw Non Null true sum d catch Runtime Exception exp int err Code String msg Problem while computing sum of doubles throw new Exec Exception msg err Code Pig Exception exp if saw Non Null return new Double sum else return null Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type Accumulator interface private Double intermediate Sum null private Double intermediate Count null Override public void accumulate Tuple b throws Exception try Double sum sum b if sum null return set default values if intermediate Sum null intermediate Count null intermediate Sum intermediate Count double count Long count b if count intermediate Count count intermediate Sum sum catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public void cleanup intermediate Sum null intermediate Count null Override public Double get Value Double avg null if intermediate Count null intermediate Count avg new Double intermediate Sum intermediate Count return avg 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin This method should never be used directly use link public class Double Sum extends Algebraic Double Math Base public Double Sum set Op public static class Intermediate extends Algebraic Double Math Base Intermediate Override public get Op return public static class Final extends Algebraic Double Math Base Final Override public get Op return 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig parser import java io Exception import java io Reader import java util Abstract List import java util Array List import java util Arrays import java util Hash Map import java util List import org antlr runtime Common Token Stream import org antlr runtime Recognition Exception import org antlr runtime tree Common Tree import org antlr runtime tree Common Tree Node Stream import org antlr runtime tree Tree import org apache pig Load Func import org apache pig impl Pig Context import org apache pig tools pigscript parser Parse Exception import org apache pig tools pigscript parser Pig Script Parser public class Dry Run Grunt Parser extends Pig Script Parser private String Builder sb new String Builder private Pig Context context private Tree parser Tree private String source private int to Skip private boolean done false private boolean has Macro false public Dry Run Grunt Parser Reader stream String source Pig Context context super stream this source source this context context public String get Result return sb to String public boolean parse Stop On Error throws Exception done false while done try parse catch Parse Exception e throw new Parser Exception Dry run parsing failed e return has Macro Override public void prompt Override protected void quit done true Override protected void print Aliases throws Exception Override protected void process Fs Command String cmd Tokens throws Exception String cmds Load Func join Abstract List String Arrays as List cmd Tokens sb append fs append cmds append n Override protected void process Sh Command String cmd Tokens throws Exception String cmds Load Func join Abstract List String Arrays as List cmd Tokens sb append sh append cmds append n Override protected void process Command String cmd throws Exception sb append sql append cmd append n Override protected void process Describe String alias throws Exception sb append describe append alias append n Override protected void process Explain String alias String script boolean is Verbose String format String target List String params List String files throws Exception Parse Exception sb append explain if script null sb append script append script append if target null sb append out append target append if is Verbose sb append brief if format null format equals dot sb append dot if format null format equals xml sb append xml if params null for String param params sb append param append param append if files null for String file files sb append param file append file append if alias null sb append alias sb append n Override protected void process Register String jar throws Exception sb append register append jar append n Override protected void process Register String path String scripting Engine String namespace throws Exception Parse Exception sb append register append path append if scripting Engine null sb append using append scripting Engine if namespace null sb append as append namespace sb append n Override protected void process Set String key String value throws Exception Parse Exception sb append set append key append append value append n Override protected void process Set throws Exception sb append set n Override protected void process Cat String path throws Exception sb append cat append path append n Override protected void process String path throws Exception sb append cd append path append n Override protected void process Dump String alias throws Exception sb append dump append alias append n Override protected void process Kill String jobid throws Exception sb append kill append jobid append n Override protected void process String path throws Exception sb append ls if path null sb append append path sb append n Override protected void process throws Exception sb append pwd n Override protected void print Help Override protected void process History boolean with Numbers Override protected void process Move String src String dst throws Exception sb append mv append src append append dst append n Override protected void process Copy String src String dst throws Exception sb append cp append src append append dst append n Override protected void process Copy To Local String src String dst throws Exception sb append Copy To Local append src append append dst append n Override protected void process Copy From Local String src String dst throws Exception sb append Copy From Local append src append append dst append n Override protected void process Mkdir String dir throws Exception sb append mkdir append dir append n Override protected void process Pig String cmd throws Exception int start get Line Number String Builder blder new String Builder for int i i start i blder append n if cmd char At cmd length cmd blder append cmd cmd blder to String Common Token Stream token Stream Query Parser Driver tokenize cmd source Tree ast null try ast Query Parser Driver parse token Stream catch Runtime Exception ex throw new Parser Exception ex get Message if has Macro List Common Tree import Nodes new Array List Common Tree List Common Tree macro Nodes new Array List Common Tree List Common Tree inline Nodes new Array List Common Tree Query Parser Driver traverse Import ast import Nodes Query Parser Driver traverse ast macro Nodes inline Nodes if import Nodes is Empty macro Nodes is Empty inline Nodes is Empty has Macro true if parser Tree null parser Tree ast else int n ast get Child Count for int i i n i parser Tree add Child ast get Child i Common Tree dup Common Tree parser Tree dup Node dup add Children Common Tree parser Tree get Children Query Parser Driver driver new Query Parser Driver context new Hash Map String String Tree new Ast driver expand Macro dup Common Tree Node Stream nodes new Common Tree Node Stream new Ast Ast Printer walker new Ast Printer nodes try walker query catch Recognition Exception e throw new Parser Exception Failed to print for command cmd e String result walker get Result trim if result is Empty String lines result split n for int i to Skip i lines length i sb append lines i append n to Skip Override protected void process Remove String path String opt throws Exception if opt null opt equals Ignore Case force sb append rm else sb append rmf sb append path append n Override protected void process Illustrate String alias String script String target List String params List String files throws Exception Parse Exception sb append illustrate if script null sb append script append script append if target null sb append out append target append if params null for String param params sb append param append param append if files null for String file files sb append param file append file append if alias null sb append alias sb append n Override protected void process Script String script boolean batch List String params List String files throws Exception Parse Exception if batch sb append exec else sb append run if params null for String param params sb append param append param append if files null for String file files sb append param file append file append sb append script append n Override protected void print Clear Override protected void process Default String key String value throws Exception Override protected void process Declare String key String value throws Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer plans import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Poisson Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Reservoir Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine tez plan operator Build Bloom Rearrange Tez import org apache pig impl plan Depth First Walker import org apache pig impl plan Visitor Exception This visitor visits the Plan and does the following for each Oper If the map plan or the reduce plan of the Oper has an end of all input flag present in it this marks in the Oper whether the map has an end of all input flag set or if the reduce has an end of all input flag set public class End Of All Input Setter extends Op Plan Visitor param plan plan to visit public End Of All Input Setter Oper Plan plan super plan new Depth First Walker Map Reduce Oper Oper Plan plan Override public void visit Op Map Reduce Oper mr throws Visitor Exception End Of All Input Checker checker new End Of All Input Checker mr map Plan checker visit if checker is End Of All Input Present mr set End Of All Input In Map true checker new End Of All Input Checker mr reduce Plan checker visit if checker is End Of All Input Present mr set End Of All Input In Reduce true public static class End Of All Input Checker extends Phy Plan Visitor private boolean end Of All Input Flag false public End Of All Input Checker Physical Plan plan super plan new Depth First Walker Physical Operator Physical Plan plan Override public void visit Stream Stream stream throws Visitor Exception stream present end Of All Input Flag true Override public void visit Merge Join Merge Join join throws Visitor Exception merge join present end Of All Input Flag true Override public void visit Collected Group Collected Group mg throws Visitor Exception map side group present end Of All Input Flag true Override public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception end Of All Input Flag true Override public void visit Partial Agg Partial Agg part Agg throws Visitor Exception end Of All Input Flag true Override public void visit Reservoir Sample Reservoir Sample reservoir Sample throws Visitor Exception end Of All Input Flag true Override public void visit Poisson Sample Poisson Sample poisson Sample throws Visitor Exception end Of All Input Flag true Override public void visit For Each For Each foreach throws Visitor Exception try if foreach need End Of All Input Processing end Of All Input Flag true catch Exception e throw new Visitor Exception e Override public void visit Local Rearrange Local Rearrange lr throws Visitor Exception if lr instanceof Build Bloom Rearrange Tez end Of All Input Flag true super visit Local Rearrange lr return if end of all input is present public boolean is End Of All Input Present return end Of All Input Flag 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Equality test expression public class Equal Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Equal Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Equal plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Equal Expression Equal Expression eo Equal Expression other return eo get Lhs is Equal get Lhs eo get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Equal Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Hash Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Equal To Expr extends Binary Comparison Operator private static final long serial Version transient private final Log log Log Factory get Log get Class public Equal To Expr Operator Key k this k public Equal To Expr Operator Key k int rp super k rp result Type Data Type Override public String name return Equal To Data Type find Type Name result Type m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Equal To this Override public Result get Next Boolean throws Exec Exception try Result left right switch operand Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type Result r accum Child null operand Type if r null return r left lhs get Next operand Type right rhs get Next operand Type return do Comparison left right default int err Code String msg this get Class get Simple Name does not know how to handle type Data Type find Type Name operand Type throw new Exec Exception msg err Code Pig Exception catch Runtime Exception e throw new Exec Exception exception while executing this to String e to String Pig Exception e Suppress Warnings unchecked private Result do Comparison Result left Result right throws Exec Exception if left return Status Status return left if right return Status Status return right if either operand is null the result should be null if left result null right result null left result null left return Status Status return left if left result instanceof Comparable right result instanceof Comparable if Comparable left result compare To right result left result Boolean else left result Boolean else if left result instanceof Hash Map right result instanceof Hash Map Hash Map left Map Hash Map left result Hash Map right Map Hash Map right result if left Map equals right Map left result Boolean else left result Boolean else throw new Exec Exception The left side and right side has the different types illustrator Markup null left result Boolean left result return left Override public Equal To Expr clone throws Clone Not Supported Exception Equal To Expr clone new Equal To Expr new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend hadoop executionengine physical Layer Pig Logger import org apache pig backend hadoop executionengine physical Layer Pig Progressable import org apache pig builtin Output Schema import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl util Context import org apache pig impl util Utils import org apache pig parser Parser Exception import java io Exception import java lang reflect import java util Hash Map import java util List import java util Map The class is used to implement functions to be applied to fields in a dataset The function is applied to each Tuple in the set The programmer should not make assumptions about state maintained between invocations of the exec method since the Pig runtime will schedule and localize invocations based on information provided at runtime The programmer also should not make assumptions about when or how many times the class will be instantiated since it may be instantiated multiple times in both the front and back end Interface Audience Public Interface Stability Stable public abstract class Eval Func Reporter to send heartbeats to Hadoop If exec will take more than a a few seconds link Pig Progressable progress should be called occasionally to avoid timeouts Default Hadoop timeout is seconds protected Pig Progressable reporter Logging object Log calls made on the front end will be sent to pig s log on the client Log calls made on the backend will be sent to stdout and can be seen in the Hadoop logs protected Log log Log Factory get Log get Class Logger for aggregating warnings Any warnings to be sent to the user should be logged to this via link Pig Logger warn protected Pig Logger pig Logger private static int next Schema Id for assigning unique ids to columns protected String get Schema Name String name Schema input String alias name if input null input get Aliases size alias input get Aliases iterator next alias next Schema Id return alias Return type of this instance of Eval Func protected Type return Type Eval Func s schema type see link Eval Func get Schema Type public static enum Schema Type default field type if the last field of the udf schema is of type vararg public Eval Func Resolve concrete type for of Eval Func Build map from type param to type for class hierarchy from current class to Eval Func Map Type Variable Type types By Type Variable new Hash Map Type Variable Type Class cls get Class Type type cls get Generic Superclass cls cls get Superclass while Eval Func class is Assignable From cls Type Variable extends Class type Params cls get Type Parameters if type instanceof Parameterized Type Parameterized Type p Type Parameterized Type type Type type Args p Type get Actual Type Arguments for int i i type Params length i types By Type Variable put type Params i type Args i type cls get Generic Superclass cls cls get Superclass Use type param to type map to determine concrete type of for of Eval Func Type target Type Eval Func class get Type Parameters while target Type null target Type instanceof Type Variable target Type types By Type Variable get target Type if target Type null target Type instanceof Generic Array Type target Type instanceof Wildcard Type throw new Runtime Exception String format Failed to determine concrete type for type parameter of Eval Func for derived class s get Class get Name return Type target Type Type check the initial intermediate and final functions if this instanceof Algebraic Algebraic a Algebraic this String err Msg function of get Class get Name is not of the expected type if get Return Type From Spec new Func Spec a get Initial Tuple class throw new Runtime Exception Initial err Msg if get Return Type From Spec new Func Spec a get Intermed Tuple class throw new Runtime Exception Intermediate err Msg if get Return Type From Spec new Func Spec a get Final equals return Type throw new Runtime Exception Final err Msg private Type get Return Type From Spec Func Spec func Spec try return Eval Func Pig Context instantiate Func From Spec func Spec get Return Type catch Class Cast Exception e throw new Runtime Exception func Spec does not specify an eval func e Get the Type that this Eval Func returns return Type public Type get Return Type return return Type report that progress is being made otherwise hadoop times out after seconds working on one outer tuple Utility method to allow to report progress If exec will take more than a a few seconds link Pig Progressable progress should be called occasionally to avoid timeouts Default Hadoop timeout is seconds public final void progress if reporter null reporter progress else warn No reporter object provided to Pig Warning Issue a warning Warning messages are aggregated and reported to the user param msg String message of the warning param warning Enum type of warning public final void warn String msg Enum warning Enum if pig Logger null pig Logger warn this msg warning Enum else log warn No logger object provided to this get Class get Name msg Placeholder for cleanup to be performed at the end User defined functions can override Default implementation is a no op public void finish This callback method must be implemented by all subclasses This is the method that will be invoked on every Tuple of a given dataset Since the dataset may be divided up in a variety of ways the programmer should not make assumptions about state that is maintained between invocations of this method param input the Tuple to be processed return result of type throws Exception abstract public exec Tuple input throws Exception Report the schema of the output of this Pig will make use of this in error checking optimization and planning The schema of input data to this is provided p The default implementation interprets the link Output Schema annotation if one is present Otherwise it returns code null code no known output schema param input Schema of the input return Schema of the output public Schema output Schema Schema input Output Schema schema this get Class get Annotation Output Schema class try return schema null null Utils get Schema From String schema value catch Parser Exception e throw new Runtime Exception e This function should be overriden to return true for functions that return their values asynchronously Currently pig never attempts to execute a function asynchronously return true if the function can be executed asynchronously Deprecated public boolean is Asynchronous return false public Pig Progressable get Reporter return reporter Set the reporter Called by Pig to provide a reference of the reporter to the param reporter Hadoop reporter public final void set Reporter Pig Progressable reporter this reporter reporter Allow a to specify type specific implementations of itself For example an implementation of arithmetic sum might have int and float implementations since integer arithmetic performs much better than floating point arithmetic Pig s typechecker will call this method and using the returned list plus the schema of the function s input data decide which implementation of the to use return List containing Func Spec objects representing the Eval Func class which can handle the inputs corresponding to the schema in the objects Each Func Spec should be constructed with a schema that describes the input for that implementation For example the sum function above would return two elements in its list ol li Func Spec this get Class get Name new Schema new Schema Field Schema null Data Type li Func Spec Int Sum get Class get Name new Schema new Schema Field Schema null Data Type ol This would indicate that the main implementation is used for doubles and the special implementation Int Sum is used for ints public List Func Spec get Arg To Func Mapping throws Frontend Exception return null Allow a to specify a list of hdfs files it would like placed in the distributed cache These files will be put in the cache for every job the is used in The default implementation returns null return list of files public List String get Cache Files return null Allow a to specify a list of local files it would like placed in the distributed cache These files will be put in the cache for every job the is used in Check for link Func Utils for utility function to facilitate it The default implementation returns null return list of files public List String get Ship Files return null public Pig Logger get Pig Logger return pig Logger Set the Pig Logger object Called by Pig to provide a reference to the param pig Logger Pig Logger object public final void set Pig Logger Pig Logger pig Logger this pig Logger pig Logger public Log get Logger return log private Schema input Schema Internal null This method will be called by Pig both in the front end and back end to pass a unique signature to the link Eval Func The signature can be used to store into the link Context any information which the link Eval Func needs to store between various method invocations in the front end and back end param signature a unique signature to identify this Eval Func public void set Context Signature String signature This method is for internal use It is called by Pig core in both front end and back end to setup the right input schema for Eval Func public void set Input Schema Schema input this input Schema Internal input This method is intended to be called by the user in link Eval Func to get the input schema of the Eval Func public Schema get Input Schema return this input Schema Internal Returns the link Schema Type of the Eval Func User defined functions can override this method to return link Schema Type In this case the last Field Schema added to the Schema in link get Arg To Func Mapping will be considered as a vararg field return the schema type of the public Schema Type get Schema Type return Schema Type Whether the should be evaluated at compile time if all inputs are constant This is applicable for most however if a will access hdfs file which is not available at compile time it has to be false return Whether or not compile time calculation is allowed default to false to ensure legacy will get the right behavior public boolean allow Compile Time Calculation return false public boolean need End Of All Input Processing return false public void set End Of All Input boolean end Of All Input This will be called on both the front end and the back end during execution return the link Load Caster associated with this eval Returning null indicates that casts from bytearray will pick the one associated with the parameters when they all come from the same loadcaster type throws Exception if there is an exception during Load Caster public Load Caster get Load Caster throws Exception return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig pen import java io Exception import java util Collection import java util Hash Map import java util Iterator import java util Linked List import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Execution Engine import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig data Data Bag import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Localizer import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Identity Hash Set import org apache pig newplan Operator import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig pen util Display Examples import org apache pig pen util Lineage Tracer This class is used to generate example tuples for the purpose public class Example Generator Logical Plan plan Logical Plan new Plan Map Load Data Bag base Data null Pig Context pig Context Physical Plan phys Plan Physical Plan Resetter phys Plan Reseter private Execution Engine exec Engine private Local Map Reduce Simulator local Runner Log log Log Factory get Log get Class private int private Map Operator Physical Operator log To Phy Map private Map Physical Operator Operator po Load To Log Map private Map Physical Operator Operator po To Log Map private Hash Map Physical Operator Collection Identity Hash Set Tuple po To Eqclasses Map private Lineage Tracer lineage private Map Operator Data Bag log To Data Map null private Map For Each Map Logical Relational Operator Data Bag for Each Inner Log To Data Map Map For Each Map Logical Relational Operator Physical Operator for Each Inner Log To Phy Map Map Limit Long ori Limit Map null Map Load Logical Schema po Load To Schema Map public Example Generator Logical Plan plan Pig Context hadoop Pig Context this plan plan pig Context new Pig Context Exec Type hadoop Pig Context get Properties pig Context hadoop Pig Context pig Context set Exec Type Exec Type File Localizer set Initialized false try pig Context connect catch Exec Exception e log error Error connecting to the cluster e get Localized Message exec Engine new Execution Engine pig Context local Runner new Local Map Reduce Simulator po Load To Schema Map new Hash Map Load Logical Schema public Lineage Tracer get Lineage return lineage public Map Operator Physical Operator get Log To Phy Map return log To Phy Map public void set Max Records int max max public Map Operator Data Bag get Examples throws Exception Interrupted Exception if pig Context get Properties get Property pig usenewlogicalplan true equals false throw new Exec Exception must use the new logical plan pig Context in Illustrator true phys Plan compile Plan plan phys Plan Reseter new Physical Plan Resetter phys Plan List Operator loads new Plan get Sources List Physical Operator p Roots phys Plan get Roots if loads size p Roots size throw new Exec Exception Logical and Physical plans have different number of roots log To Phy Map exec Engine get Log To Phy Map for Each Inner Log To Phy Map exec Engine get For Each Inner Log To Phy Map plan po Load To Log Map new Hash Map Physical Operator Operator log To Data Map new Hash Map Operator Data Bag po To Log Map new Hash Map Physical Operator Operator set up foreach inner data map for Each Inner Log To Data Map new Hash Map For Each Map Logical Relational Operator Data Bag for Map Entry For Each Map Logical Relational Operator Physical Operator entry for Each Inner Log To Phy Map entry Set Map Logical Relational Operator Data Bag inner Map new Hash Map Logical Relational Operator Data Bag for Each Inner Log To Data Map put entry get Key inner Map for Operator load loads po Load To Log Map put log To Phy Map get load load boolean has Limit false for Operator lo log To Phy Map key Set po To Log Map put log To Phy Map get lo lo if has Limit lo instanceof Limit has Limit true try read Base Data loads catch Exec Exception e log error Error reading data e get Message throw e catch Frontend Exception e log error Error reading data e get Message throw new Runtime Exception e Map Operator Data Bag derived Data null create derived data and trim base data Lineage Trimming Visitor trimmer new Lineage Trimming Visitor new Plan base Data this log To Phy Map phys Plan pig Context trimmer visit base Data trimmer get Base Data System out println Obtained the first level derived and trimmed data create new derived data from trimmed basedata derived Data get Data phys Plan System out println Got new derived data from the trimmed base data augment base data Augment Base Data Visitor augment new Augment Base Data Visitor new Plan log To Phy Map base Data derived Data augment visit this base Data augment get New Base Data System out println Obtained augmented base data create new derived data and trim the base data after augmenting base data with synthetic tuples trimmer new Lineage Trimming Visitor new Plan base Data this log To Phy Map phys Plan pig Context trimmer visit base Data trimmer get Base Data System out println Final trimming create the final version of derived Data to give to the output derived Data get Data phys Plan System out println Obtaining final derived data for output if has Limit augment set Limit augment visit this base Data augment get New Base Data ori Limit Map augment get Ori Limit Map derived Data get Data Display Examples print Simple plan get Leaves get derived Data derived Data System out println Display Examples print Tabular new Plan derived Data for Each Inner Log To Data Map pig Context in Illustrator false return derived Data private void read Base Data List Operator loads throws Exception Interrupted Exception Frontend Exception Exec Exception Physical Plan this Phy Plan new Physical Plan for Operator op loads Logical Schema schema Load op get Schema if schema null throw new Exec Exception Example Generator requires a schema Please provide a schema while loading data po Load To Schema Map put Load log To Phy Map get op schema this Phy Plan add log To Phy Map get op base Data null Map Operator Data Bag result get Data this Phy Plan base Data new Hash Map Load Data Bag for Operator lo result key Set if lo instanceof Load base Data put Load lo result get lo Physical Plan compile Plan Logical Plan plan throws Exec Exception Frontend Exception new Plan new Logical Plan plan plan optimize pig Context Physical Plan result exec Engine compile plan null return result public Map Operator Data Bag get Data throws Exception Interrupted Exception return get Data phys Plan private Map Operator Data Bag get Data Physical Plan plan throws Pig Exception Exception Interrupted Exception get data on a physical plan possibly trimmed of one branch lineage new Lineage Tracer Illustrator Attacher attacher new Illustrator Attacher plan lineage po Load To Schema Map pig Context attacher visit if ori Limit Map null for Map Entry Limit Long entry ori Limit Map entry Set log To Phy Map get entry get Key get Illustrator set Original Limit entry get Value get Log To Data Map attacher get Data Map if base Data null set Load Data Map phys Plan Reseter visit local Runner launch Pig plan base Data lineage attacher this pig Context if base Data null po To Eqclasses Map attacher po To Eqclasses Map else for Map Entry Physical Operator Collection Identity Hash Set Tuple entry attacher po To Eqclasses Map entry Set if entry get Key instanceof Load po To Eqclasses Map put entry get Key entry get Value if base Data null only for non derived data generation phy To Transform plan attacher get Data Map return log To Data Map public Map Operator Data Bag get Data Map Load Data Bag new Base Data throws Exception base Data new Base Data return get Data phys Plan private void phy To Transform Physical Plan plan Map Physical Operator Data Bag phy To Data Map remap the to as result of the compilation may have changed in the plans Map Physical Operator Physical Operator phy To Map local Runner get Phy To Map for Map Entry Physical Operator Operator entry po To Log Map entry Set if phy To Map get entry get Key null Physical Operator po In phy To Map get entry get Key log To Data Map put entry get Value phy To Data Map get po In po To Eqclasses Map put entry get Key po To Eqclasses Map get po In private void get Log To Data Map Map Physical Operator Data Bag phy To Data Map log To Data Map clear for Operator lo log To Phy Map key Set if log To Phy Map get lo null log To Data Map put lo phy To Data Map get log To Phy Map get lo set the to Data mapping for the For Each inner plans for Map Entry For Each Map Logical Relational Operator Data Bag entry for Each Inner Log To Data Map entry Set entry get Value clear for Map Entry Logical Relational Operator Physical Operator inner Entry for Each Inner Log To Phy Map get entry get Key entry Set entry get Value put inner Entry get Key phy To Data Map get inner Entry get Value private void set Load Data Map This function sets up the Data map eq class and lineage for the base data used in the coming runner this must be called after log To Data Map has been properly re set and before the runner is started if base Data null if po To Eqclasses Map null po To Eqclasses Map new Hash Map Physical Operator Collection Identity Hash Set Tuple else po To Eqclasses Map clear for Load lo base Data key Set log To Data Map get lo add All base Data get lo Linked List Identity Hash Set Tuple equivalence Classes new Linked List Identity Hash Set Tuple Identity Hash Set Tuple equivalence Class new Identity Hash Set Tuple equivalence Classes add equivalence Class for Tuple t base Data get lo lineage insert t equivalence Class add t po To Eqclasses Map put log To Phy Map get lo equivalence Classes public Collection Identity Hash Set Tuple get Eq Classes Map Logical Relational Operator Collection Identity Hash Set Tuple log To Eqclasses Map get Lo To Eq Class Map Linked List Identity Hash Set Tuple ret new Linked List Identity Hash Set Tuple for Map Entry Logical Relational Operator Collection Identity Hash Set Tuple entry log To Eqclasses Map entry Set if entry get Value null ret add All entry get Value return ret public Map Logical Relational Operator Collection Identity Hash Set Tuple get Lo To Eq Class Map Map Logical Relational Operator Collection Identity Hash Set Tuple ret Equivalence Classes get Lo To Eq Class Map phys Plan new Plan log To Phy Map log To Data Map for Each Inner Log To Phy Map po To Eqclasses Map eq classes adjustments based upon logical operators for Map Entry Logical Relational Operator Collection Identity Hash Set Tuple entry ret entry Set if entry get Key instanceof Sort Collection Identity Hash Set Tuple eq Classes entry get Value for Iterator Identity Hash Set Tuple it eq Classes iterator it has Next Object t null Identity Hash Set Tuple eq Class it next if eq Class size eq Class clear continue boolean first true all Identical true for Iterator Tuple it eq Class iterator it has Next if first first false t it next else if it next equals t all Identical false break if all Identical eq Class clear return ret 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig pen util import java io Data Input import java io Data Output import java io Exception import java util List import org apache pig backend executionengine Exec Exception import org apache pig data Tuple import org apache pig data Abstract Tuple Example tuple adds booleans to Tuple synthetic say whether the tuple was generated synthetically omittable is for future use in case we want to attach weights to tuples that have been displayed earlier public class Example Tuple extends Abstract Tuple private static final long serial Version public boolean synthetic false public boolean omittable true Object expr null Tuple t null public Example Tuple public Example Tuple Object expr this expr expr public Example Tuple Tuple t Have to do it like this because Tuple is an interface we do n t have access to its internal structures this t t Override public String to String return t to String Writable methods Override public void write Data Output out throws Exception t write out out write Boolean synthetic out write Boolean omittable Override public void read Fields Data Input in throws Exception t read Fields in this synthetic in read Boolean this omittable in read Boolean public Tuple to Tuple return t Override public void append Object val t append val Override public Object get int field Num throws Exec Exception return t get field Num Override public List Object get All return t get All Override public long get Memory Size return t get Memory Size Override public byte get Type int field Num throws Exec Exception return t get Type field Num Override public boolean is Null int field Num throws Exec Exception return t is Null field Num Override public void reference Tuple t t reference t Override public void set int field Num Object val throws Exec Exception t set field Num val Override public int size return t size Override Suppress Warnings unchecked public int compare To Object o return t compare To o 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend executionengine import java util Iterator import java util Properties import java io Output Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig data Tuple import org apache pig tools pigstats Pig Stats Abstraction on a job that the execution engine runs It allows the front end to retrieve information on job status and manage a running job public interface Exec Job public enum public static final String job progress public get Status true is the physical plan has executed successfully and results are ready to be retrieved return true if execution has completed false otherwise throws Exec Exception public boolean has Completed throws Exec Exception if query has executed successfully we want to retrieve the results via iterating over them return iterator for resulting tuples throws Exec Exception public Iterator Tuple get Results throws Exec Exception Returns the alias of relation generated by this job public String get Alias throws Exec Exception Get configuration information return configuration information for the execution engine public Properties get Configuration Can be information about the state not submitted e g the execute method has not been called yet not running e g execute has been issued but job is waiting running completed aborted progress information return statistics relevant to the execution engine public Pig Stats get Statistics return link Store object associated with the store public Store get Store hook for asynchronous notification of job completion pushed from the back end public void completion Notification Object cookie Kills current job throws Exec Exception public void kill throws Exec Exception Collecting various forms of outputs public void get Logs Output Stream log throws Exec Exception public void get Out Output Stream out throws Exec Exception public void get Error Output Stream error throws Exec Exception Get exceptions that happened during execution public Exception get Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Serializable import java util Properties import org apache pig backend executionengine Execution Engine import org apache pig backend hadoop executionengine map Reduce Layer Local Exec Type import org apache pig backend hadoop executionengine map Reduce Layer Exec Type import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl Pig Context The type of query execution Pig will cycle through all implementations of Exec Type and choose the first one that matches the Properties passed in This Exec Type then dictates the Execution Engine used for processing and other behaviour throughout Pig Any implementing classes should be noted in the services folder titled org apache pig Exec Type as per the Java Service Loader specification Interface Audience Public Interface Stability Evolving public interface Exec Type extends Serializable public static final Exec Type new Local Exec Type public static final Exec Type new Exec Type An Exec Type is selected based off the Properties for the given script There may be multiple settings that trigger the selection of a given Exec Type For example distributed mode is currently triggered if the user specifies mapred or mapreduce It is desirable to override the to String method of the given Exec Type to uniquely identify the Exec Type The initialize method should return true if it accepts the properties or false if it does not The Java Service Loader framework will be used to iterate through and select the correct Exec Type public boolean accepts Properties properties Returns the Execution Engine that this Exec Type is associated with Once the Exec Type the script is running in is determined by the Pig Server it will then call this method to get an instance of the Execution Engine associated with this Exec Type to delegate all further execution to on the backend public Execution Engine get Execution Engine Pig Context pig Context Returns the Execution Engine class that this Exec Type is associated with This method simply returns the class of the Execution Engine associated with this Exec Type and not an instance of it public Class extends Execution Engine get Execution Engine Class An Exec Type is classified as local if it runs in process and through the local filesystem While an Execution Engine may have a more nuanced notion of local mode these are the qualifications Pig requires Execution Engines can extend the Exec Type interface to additionally differentiate between Exec Types as necessary public boolean is Local Returns the canonical name for this Exec Type return public String name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java util Properties import java util Service Loader import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend hadoop executionengine map Reduce Layer Local Exec Type import org apache pig backend hadoop executionengine map Reduce Layer Exec Type import org apache pig impl util Properties Util public class Exec Type Provider private static final Log log Log Factory get Log Exec Type Provider class public static Exec Type select Exec Type Properties properties throws Pig Exception Service Loader Exec Type framework Loader Service Loader load Exec Type class for Exec Type exec Type framework Loader log info Trying Exec Type exec Type if exec Type accepts properties log info Picked exec Type as the Exec Type return get Singleton exec Type else log debug Can not pick exec Type as the Exec Type throw new Pig Exception Unknown exec type properties get Property exectype This method attempts to return a singleton instance of the given exec type Only works for Exec Types as these are the only Exec Types that we have constants in the Pig codebase for param exec Type return private static Exec Type get Singleton Exec Type exec Type if exec Type instanceof Exec Type return Exec Type if exec Type instanceof Local Exec Type return Exec Type if it is not specific but rather a different execution engine we do n t have access to any constants that can act as singletons so we just use the given instance return exec Type public static Exec Type from String String exec Type throws Pig Exception Properties properties Properties Util load Default Properties properties set Property exectype exec Type return select Exec Type properties 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl streaming import java io Buffered Input Stream import java io Buffered Output Stream import java io Buffered Reader import java io Data Input Stream import java io Data Output Stream import java io Exception import java io Input Stream import java io Input Stream Reader import java util concurrent Blocking Queue import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig data Tuple import org apache pig impl io Buffered Positioned Input Stream import org apache pig impl streaming Input Handler Input Type import org apache pig impl streaming Output Handler Output Type import org apache pig impl util Context link Executable Manager manages an external executable which processes data in a Pig query The code Executable Manager code is responsible for startup teardown of the external process and also for managing it It feeds input records to the executable via it s code stdin code collects the output records from the code stdout code and also diagnostic information from the code stdout code public class Executable Manager private static final Log Log Factory get Log Executable Manager class private static final int private static final Result new Result Status null protected Streaming Command command Streaming command to be run Process process Handle to the process protected int exit Code Exit code of the process protected Data Output Stream stdin stdin of the process Process Input Thread stdin Thread thread to send input to process Process Output Thread stdout Thread thread to get process stdout Input Stream stdout stdout of the process Process Error Thread stderr Thread thread to get process stderr Input Stream stderr stderr of the process Input Output handlers Input Handler input Handler Output Handler output Handler Statistics protected long input Records protected long input Bytes protected long output Records protected long output Bytes protected volatile Throwable outerr Threads Error private Stream po Stream private Process Input Thread file Input Thread Create a new link Executable Manager public Executable Manager Configure and initialize the link Executable Manager param stream Stream operator throws Exception throws Exec Exception public void configure Stream stream throws Exception Exec Exception this po Stream stream this command stream get Command Create the input output handlers this input Handler Handler Factory create Input Handler command this output Handler Handler Factory create Output Handler command Close and cleanup the link Executable Manager throws Exception public void close throws Exception Close the Input Handler which in some cases lets the process terminate input Handler close process Check if we need to start the process now if input Handler get Input Type Input Type exec Wait for the process to exit try exit Code process wait For catch Interrupted Exception ie error Unexpected exception while waiting for streaming binary to complete ie kill Process process Wait for stdout thread to complete try if stdout Thread null stdout Thread join stdout Thread null catch Interrupted Exception ie error Unexpected exception while waiting for output thread for streaming binary to complete ie kill Process process Wait for stderr thread to complete try if stderr Thread null stderr Thread join stderr Thread null catch Interrupted Exception ie error Unexpected exception while waiting for input thread for streaming binary to complete ie kill Process process debug Process exited with exit Code if exit Code String err Msg command to String failed with exit status exit Code error err Msg Result res new Result Status err Msg send Output po Stream get Binary Output Queue res if exit Code output Handler get Output Type Output Type Trigger the output Handler output Handler bind To null start thread to process output from executable s stdout stdout Thread new Process Output Thread output Handler po Stream stdout Thread start Check if there was a problem with the managed process if outerr Threads Error null error Output Error thread failed with outerr Threads Error Helper function to close input and output streams to the process and kill it param process the process to be killed throws Exception private void kill Process Process process if process null try input Handler close process catch Exception e info Exception in kill Process while closing input Handler Ignoring e get Message try output Handler close catch Exception e info Exception in kill Process while closing output Handler Ignoring e get Message process destroy Start execution of the external process This takes care of setting up the environment of the process and also starts Process Error Thread to process the code stderr code of the managed process throws Exception protected void exec throws Exception Process Builder process Builder Streaming Util create Process this command process process Builder start debug Started the process for command command Pick up the process stderr stream and start the thread to process the stderr stream stderr new Data Input Stream new Buffered Input Stream process get Error Stream stderr Thread new Process Error Thread stderr Thread start Check if we need to handle the process stdout directly if output Handler get Output Type Output Type Get hold of the stdout of the process stdout new Data Input Stream new Buffered Input Stream process get Input Stream Bind the stdout to the Output Handler output Handler bind To new Buffered Positioned Input Stream stdout Long start thread to process output from executable s stdout stdout Thread new Process Output Thread output Handler po Stream stdout Thread start Start execution of the link Executable Manager throws Exception public void run throws Exception Check if we need to exec the process if input Handler get Input Type Input Type start the thread to handle input we pass the Context to the file Input Thread because when input type is asynchronous the exec is called by file Input Thread and it needs to access to the Context file Input Thread new Process Input Thread input Handler po Stream Context get Context file Input Thread start If Input type is that means input to the streaming binary is from a file that means we can not exec the process till the input file is completely written This will be done in close so now we return return Start the executable exec set up input to the executable stdin new Data Output Stream new Buffered Output Stream process get Output Stream input Handler bind To stdin Start the thread to send input to the executable s stdin stdin Thread new Process Input Thread input Handler po Stream null stdin Thread start The thread which consumes input from Stream s binary Input queue and feeds it to the the Process class Process Input Thread extends Thread Input Handler input Handler private Stream po Stream private Context udf Context private Blocking Queue Result binary Input Queue Process Input Thread Input Handler input Handler Stream po Stream Context udf Context set Daemon true this input Handler input Handler this po Stream po Stream a copy of Context passed from the Executable Manager thread this udf Context udf Context the input queue from where this thread will read input tuples this binary Input Queue po Stream get Binary Input Queue Override public void run If input type is asynchronous set the udf Context of the current thread to the copy of Executable Manager thread s udf Context This is necessary because the exec method is called by the current thread file Input Thread instead of the Executable Manager thread if input Handler get Input Type Input Type udf Context null Context set Udf Context udf Context try Read tuples from the previous operator in the pipeline and pass it to the executable while true Result inp null inp binary Input Queue take synchronized po Stream notify waiting producer the if check is to keep findbugs happy if inp null po Stream notify All We should receive an only when input for this process has already been sent and no more input is expected if inp null inp return Status Status signal cleanup in Executable Manager close return if inp null inp return Status Status Check if there was a problem with the managed process if outerr Threads Error null throw new Exception Output Error thread failed with outerr Threads Error Pass the serialized tuple to the executable via the Input Handler Tuple t null try t Tuple inp result input Handler put Next t catch Exception e if input type is synchronous then it could be related to the process terminating if input Handler get Input Type Input Type warn Exception while trying to write to stream binary s input e could be because the process died closed the input stream we will only call close here and not worry about deducing whether the process died normally or abnormally if there was any real issue we should see a non zero exit code from the process and send a Status back what if we got an Exception because there was only an issue with writing to input of the binary hmm hope that means the process died abnormally close return else asynchronous case then this is a real exception throw e input Bytes t get Memory Size input Records catch Throwable t Note that an error occurred outerr Threads Error t Result res new Result Status Error while reading from Stream and passing it to the streaming process t get Message error Error while reading from Stream and passing it to the streaming process t send Output po Stream get Binary Output Queue res kill Process process private void send Output Blocking Queue Result binary Output Queue Result res try binary Output Queue put res catch Interrupted Exception e error Error while sending binary output to Stream e synchronized po Stream notify waiting consumer the if is to satisfy findbugs if res null po Stream notify All The thread which gets output from the streaming binary and puts it onto the binary output Queue of Stream class Process Output Thread extends Thread Output Handler output Handler private Blocking Queue Result binary Output Queue Process Output Thread Output Handler output Handler Stream po Stream set Daemon true this output Handler output Handler the output queue where this thread will put output tuples for Stream this binary Output Queue po Stream get Binary Output Queue Override public void run try Read tuples from the executable and send it to Queue of Stream Tuple tuple null while tuple output Handler get Next null process Output tuple output Bytes tuple get Memory Size output from binary is done process Output null output Handler close catch Throwable t Note that an error occurred outerr Threads Error t error Caught Exception in Output Handler of Streaming binary sending error signal to pipeline t send to Stream try Result res new Result res result Error reading output from Streaming binary command to String t get Message res return Status Status send Output binary Output Queue res kill Process process catch Exception e error Error while trying to signal Error status to pipeline e void process Output Tuple t Result res new Result if t null we have a valid tuple to pass back res result t res return Status Status output Records else t null means end of output from binary wait for the process to exit and harvest exit code try exit Code process wait For catch Interrupted Exception ie kill Process process signal error String err Msg Failure while waiting for process command to String ie get Message error err Msg ie res result err Msg res return Status Status send Output binary Output Queue res return if exit Code signal End Of Stream output res else signal Error String err Msg command to String failed with exit status exit Code error err Msg res result err Msg res return Status Status send Output binary Output Queue res Workhorse to process the stderr stream of the managed process By default code Execuatble Manager code just sends out the received error message to the code stderr code of itself param error error message from the managed process protected void process Error String error Just send it out to our stderr System err print error class Process Error Thread extends Thread public Process Error Thread set Daemon true Override public void run try String error Buffered Reader reader new Buffered Reader new Input Stream Reader stderr while error reader read Line null process Error error n if stderr null stderr close debug Process Error Thread done catch Throwable t Note that an error occurred outerr Threads Error t error t try if stderr null stderr close catch Exception ioe warn ioe throw new Runtime Exception t 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen Illustrator base class for all types of expressions All expression operators must extend this class public abstract class Expression Operator extends Physical Operator private static final Log log Log Factory get Log Expression Operator class private static final long serial Version public Expression Operator Operator Key k this k public Expression Operator Operator Key k int rp super k rp Override public void set Illustrator Illustrator illustrator this illustrator illustrator Override public boolean supports Multiple Outputs return false Override public Result get Next Data Bag throws Exec Exception return new Result Override public abstract void visit Phy Plan Visitor v throws Visitor Exception Make a deep copy of this operator This is declared here to make it possible to call clone on Expression Operators throws Clone Not Supported Exception Override public Expression Operator clone throws Clone Not Supported Exception String s This expression operator does not implement clone log error s throw new Clone Not Supported Exception s Get the sub expressions of this expression This is called if reducer is run as accumulative mode all the child expression must be called if they have any to drive the accumulate protected abstract List Expression Operator get Child Expressions check whether this expression contains any this is called if reducer is run as accumulative mode in this case all Fs must be called public boolean contain if this instanceof User Func return true List Expression Operator l get Child Expressions if l null for Expression Operator e l if e contain return true return false Drive all the Fs in accumulative mode protected Result accum Child List Expression Operator child byte data Type throws Exec Exception try if is Accum Started if child null child get Child Expressions Result res null if child null for Expression Operator e child if e contain res e get Next data Type if res return Status Status return res res new Result res return Status Status return res return null catch Runtime Exception e throw new Exec Exception Exception while executing this to String e to String e Override public String to String return this get Class get Simple Name super to String children get Child Expressions at get Original Locations 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import java io Exception import java util Array List import java util Deque import java util Linked List import java util List import java util Map import org apache pig Comparison Func import org apache pig Eval Func import org apache pig Func Spec import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig backend hadoop executionengine physical Layer Logical To Physical Translator Exception import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Add import org apache pig backend hadoop executionengine physical Layer expression Operators Binary Comparison Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Binary Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Divide import org apache pig backend hadoop executionengine physical Layer expression Operators Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Greater Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Less Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Mod import org apache pig backend hadoop executionengine physical Layer expression Operators Multiply import org apache pig backend hadoop executionengine physical Layer expression Operators Not Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators And import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Cast import org apache pig backend hadoop executionengine physical Layer expression Operators Is Null import org apache pig backend hadoop executionengine physical Layer expression Operators Map Look Up import org apache pig backend hadoop executionengine physical Layer expression Operators Negative import org apache pig backend hadoop executionengine physical Layer expression Operators Not import org apache pig backend hadoop executionengine physical Layer expression Operators Or import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators Regexp import org apache pig backend hadoop executionengine physical Layer expression Operators Relation To Expr Project import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer expression Operators Subtract import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Logical Relational Operator public class Exp To Phy Translation Visitor extends Logical Expression Visitor This value points to the current Logical Relational Operator we are working on protected Logical Relational Operator current Op public Exp To Phy Translation Visitor Operator Plan plan Logical Relational Operator op Physical Plan phy Plan Map Operator Physical Operator map throws Frontend Exception this plan new Dependency Order Walker plan op phy Plan map public Exp To Phy Translation Visitor Operator Plan plan Plan Walker walker Logical Relational Operator op Physical Plan phy Plan Map Operator Physical Operator map throws Frontend Exception super plan walker current Op op log To Phy Map map current Plan phy Plan current Plans new Linked List Physical Plan protected Map Operator Physical Operator log To Phy Map protected Deque Physical Plan current Plans protected Physical Plan current Plan protected Node Id Generator node Gen Node Id Generator get Generator protected Pig Context pc public void set Pig Context Pig Context pc this pc pc public Physical Plan get Physical Plan return current Plan private void attach Binary Comparison Operator Binary Expression op Binary Comparison Operator expr Op throws Frontend Exception We dont have aliases in Expression Operators expr Op set Alias op get Alias expr Op set Operand Type op get Lhs get Type expr Op set Lhs Expression Operator log To Phy Map get op get Lhs expr Op set Rhs Expression Operator log To Phy Map get op get Rhs Operator Plan o Plan op get Plan current Plan add expr Op log To Phy Map put op expr Op List Operator successors o Plan get Successors op if successors null return for Operator lo successors Physical Operator from log To Phy Map get lo try current Plan connect from expr Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e private void attach Binary Expression Operator Binary Expression op Binary Expression Operator expr Op throws Frontend Exception We dont have aliases in Expression Operators expr Op set Alias op get Alias expr Op set Result Type op get Lhs get Type expr Op set Lhs Expression Operator log To Phy Map get op get Lhs expr Op set Rhs Expression Operator log To Phy Map get op get Rhs Operator Plan o Plan op get Plan current Plan add expr Op log To Phy Map put op expr Op List Operator successors o Plan get Successors op if successors null return for Operator lo successors Physical Operator from log To Phy Map get lo try current Plan connect from expr Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit And Expression op throws Frontend Exception System err println Entering And Binary Comparison Operator expr Op new And new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Or Expression op throws Frontend Exception System err println Entering Or Binary Comparison Operator expr Op new Or new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Equal Expression op throws Frontend Exception Binary Comparison Operator expr Op new Equal To Expr new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Not Equal Expression op throws Frontend Exception Binary Comparison Operator expr Op new Not Equal To Expr new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Greater Than Expression op throws Frontend Exception Binary Comparison Operator expr Op new Greater Than Expr new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Greater Than Equal Expression op throws Frontend Exception Binary Comparison Operator expr Op new Or Equal To Expr new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Less Than Expression op throws Frontend Exception Binary Comparison Operator expr Op new Less Than Expr new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Less Than Equal Expression op throws Frontend Exception Binary Comparison Operator expr Op new Or Equal To Expr new Operator Key node Gen get Next Node Id attach Binary Comparison Operator op expr Op Override public void visit Project Expression op throws Frontend Exception Project expr Op if op get Attached Relational Op instanceof Generate op get Plan get Successors op null op find Referent instanceof Inner Load expr Op new Relation To Expr Project new Operator Key node Gen get Next Node Id else expr Op new Project new Operator Key node Gen get Next Node Id if op get Field Schema null op is Range Or Star Project expr Op set Result Type Data Type else expr Op set Result Type op get Type if op is Project Star expr Op set Star op is Project Star else if op is Range Project if op get End Col all other project range should have been expanded by project star expander throw new Assertion Error project range that is not a project to end seen in translation to physical plan expr Op set Project To End op get Start Col else expr Op set Column op get Col Num implement this expr Op set Overloaded op get Overloaded log To Phy Map put op expr Op current Plan add expr Op Override public void visit Map Lookup Expression op throws Frontend Exception Expression Operator phys Op new Map Look Up new Operator Key node Gen get Next Node Id Map Look Up phys Op set Look Up Key op get Lookup Key phys Op set Result Type op get Type phys Op add Original Location op get Field Schema alias op get Location current Plan add phys Op log To Phy Map put op phys Op Expression Operator from Expression Operator log To Phy Map get op get Map try current Plan connect from phys Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit org apache pig newplan logical expression Constant Expression op throws Frontend Exception System err println Entering Constant Constant Expression ce new Constant Expression new Operator Key node Gen get Next Node Id We dont have aliases in Expression Operators ce set Alias op get Alias ce set Value op get Value ce set Result Type op get Type this operator does n t have any predecessors current Plan add ce log To Phy Map put op ce System err println Exiting Constant Override public void visit Cast Expression op throws Frontend Exception Cast p Cast new Cast new Operator Key node Gen get Next Node Id p Cast add Original Location op get Field Schema alias op get Location phys Op set Alias op get Alias current Plan add p Cast log To Phy Map put op p Cast Expression Operator from Expression Operator log To Phy Map get op get Expression p Cast set Result Type op get Type p Cast set Field Schema new Resource Schema Resource Field Schema op get Field Schema Func Spec lf Spec op get Func Spec if null lf Spec try p Cast set Func Spec lf Spec catch Exception e int err Code String msg Can not resolve load function to use for casting from Data Type find Type Name op get Expression get Type to Data Type find Type Name op get Type throw new Logical To Physical Translator Exception msg err Code Pig Exception e try current Plan connect from p Cast catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Not Expression op throws Frontend Exception Not p Not new Not new Operator Key node Gen get Next Node Id phys Op set Alias op get Alias current Plan add p Not log To Phy Map put op p Not Expression Operator from Expression Operator log To Phy Map get op get Expression p Not set Expr from p Not set Result Type op get Type p Not set Operand Type op get Type try current Plan connect from p Not catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Is Null Expression op throws Frontend Exception Is Null p Is Null new Is Null new Operator Key node Gen get Next Node Id phys Op set Alias op get Alias current Plan add p Is Null log To Phy Map put op p Is Null Expression Operator from Expression Operator log To Phy Map get op get Expression p Is Null set Expr from p Is Null set Result Type op get Type p Is Null set Operand Type op get Expression get Type try current Plan connect from p Is Null catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Negative Expression op throws Frontend Exception Negative p Negative new Negative new Operator Key node Gen get Next Node Id phys Op set Alias op get Alias current Plan add p Negative log To Phy Map put op p Negative Expression Operator from Expression Operator log To Phy Map get op get Expression p Negative set Expr from p Negative set Result Type op get Type try current Plan connect from p Negative catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Add Expression op throws Frontend Exception Binary Expression Operator expr Op new Add new Operator Key node Gen get Next Node Id attach Binary Expression Operator op expr Op Override public void visit Regex Expression op throws Frontend Exception Binary Expression Operator expr Op new Regexp new Operator Key node Gen get Next Node Id attach Binary Expression Operator op expr Op List Operator successors op get Plan get Successors op if successors get instanceof org apache pig newplan logical expression Constant Expression Regexp expr Op set Const Expr true Override public void visit Subtract Expression op throws Frontend Exception Binary Expression Operator expr Op new Subtract new Operator Key node Gen get Next Node Id attach Binary Expression Operator op expr Op Override public void visit Multiply Expression op throws Frontend Exception Binary Expression Operator expr Op new Multiply new Operator Key node Gen get Next Node Id attach Binary Expression Operator op expr Op Override public void visit Divide Expression op throws Frontend Exception Binary Expression Operator expr Op new Divide new Operator Key node Gen get Next Node Id attach Binary Expression Operator op expr Op Override public void visit Mod Expression op throws Frontend Exception Binary Expression Operator expr Op new Mod new Operator Key node Gen get Next Node Id attach Binary Expression Operator op expr Op Override public void visit Bin Cond Expression op throws Frontend Exception Bin Cond expr Op new Bin Cond new Operator Key node Gen get Next Node Id expr Op set Result Type op get Type expr Op set Cond Expression Operator log To Phy Map get op get Condition expr Op set Lhs Expression Operator log To Phy Map get op get Lhs expr Op set Rhs Expression Operator log To Phy Map get op get Rhs Operator Plan o Plan op get Plan current Plan add expr Op log To Phy Map put op expr Op List Operator successors o Plan get Successors op if successors null return for Operator lo successors Physical Operator from log To Phy Map get lo try current Plan connect from expr Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Suppress Warnings unchecked Override public void visit User Func Expression op throws Frontend Exception Object f Pig Context instantiate Func From Spec op get Func Spec Physical Operator p if f instanceof Eval Func p new User Func new Operator Key node Gen get Next Node Id null op get Func Spec Eval Func f User Func p set Signature op get Signature reinitialize input schema from signature if User Func p get Func get Input Schema null User Func p set Func Input Schema op get Signature Eval Func f set Input Schema User Func p get Func get Input Schema List String cache Files Eval Func f get Cache Files if cache Files null User Func p set Cache Files cache Files List String ship Files Eval Func f get Ship Files if ship Files null User Func p set Ship Files ship Files else p new User Comparison Func new Operator Key node Gen get Next Node Id null op get Func Spec Comparison Func f p set Result Type op get Type current Plan add p List Logical Expression from List op get Arguments if from List null for Logical Expression input Operator from List Physical Operator from log To Phy Map get input Operator try current Plan connect from p catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put op p We need to track all the scalars if op instanceof Scalar Expression Operator ref Op Scalar Expression op get Implicit Referenced Operator User Func p set Referenced Operator log To Phy Map get ref Op Override public void visit Dereference Expression op throws Frontend Exception Project expr Op new Project new Operator Key node Gen get Next Node Id expr Op set Result Type op get Type expr Op set Columns Array List Integer op get Bag Columns expr Op set Star false log To Phy Map put op expr Op current Plan add expr Op Physical Operator from log To Phy Map get op get Referred Expression if from null current Plan connect from expr Op 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl io import java io Buffered Output Stream import java io Buffered Reader import java io File import java io File Not Found Exception import java io File Output Stream import java io Exception import java io Input Stream import java io Input Stream Reader import java io Output Stream import java net import java util Array List import java util List import java util Map import java util Properties import java util Random import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File Status import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop fs permission Fs Action import org apache hadoop fs permission Fs Permission import org apache hadoop util Shell import org apache pig Exec Type import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig backend datastorage Container Descriptor import org apache pig backend datastorage Data Storage import org apache pig backend datastorage Data Storage Exception import org apache pig backend datastorage Element Descriptor import org apache pig backend datastorage Seekable Input Stream import org apache pig backend datastorage Seekable Input Stream import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop datastorage Data Storage import org apache pig backend hadoop datastorage Directory import org apache pig backend hadoop datastorage Path import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig impl Pig Context import org apache pig impl util Utils import com google common annotations Visible For Testing public class File Localizer private static final Log log Log Factory get Log File Localizer class static public final String file static public final int static public final int public static Fs Permission new Fs Permission Fs Action Fs Action Fs Action rwx public static class Data Storage Input Stream Iterator extends Input Stream Input Stream current Element Descriptor elements int current Element public Data Storage Input Stream Iterator Element Descriptor elements this elements elements private boolean is throws Exception if current null if current Element elements length return true current elements current Element open return false private void do Next throws Exception current close current null Override public int read throws Exception while is int rc current read if rc return rc do Next return Override public int available throws Exception if is return return current available Override public void close throws Exception if current null current close current null current Element elements length Override public int read byte b int off int len throws Exception int count while is len int rc current read b off len if rc do Next continue off rc len rc count rc return count is count Override public int read byte b throws Exception return read b b length Override public long skip long n throws Exception while is n n current skip n return n static String check Default Prefix Exec Type exec Type String file Spec if file Spec starts With return file Spec return exec Type Exec Type file Spec This function is meant to be used if the mappers reducers want to access any file param file Name return Input Stream of the open file throws Exception public static Input Stream open File String file Name throws Exception Configuration conf Pig Map Reduce s Job Conf Internal get if conf null throw new Runtime Exception ca n t open file while executing locally return open File file Name Configuration Util to Properties conf public static Input Stream open File String file Name Properties properties throws Exception Data Storage dds new Data Storage properties Element Descriptor elem dds as Element file Name return open File elem public static long get Size String file Name throws Exception Configuration conf Pig Map Reduce s Job Conf Internal get if conf null throw new Runtime Exception ca n t open file while executing locally return get Size file Name Configuration Util to Properties conf public static long get Size String file Name Properties properties throws Exception Data Storage dds new Data Storage properties Element Descriptor elem dds as Element file Name recursively get all the files under this path Element Descriptor all Elems get File Element Descriptors elem long size add up the sizes of all files found for int i i all Elems length i Map String Object stats all Elems i get Statistics size Long stats get Element Descriptor return size private static Input Stream open File Element Descriptor elem throws Exception Element Descriptor elements null if elem exists try if elem get Data Storage is Container elem to String if elem system Element throw new Exception Attempt is made to open system file elem to String return elem open catch Data Storage Exception e throw new Exception Failed to determine if elem elem is container e elem is a directory recursively get all files in it elements get File Element Descriptors elem else It might be a glob if glob Matches Files elem elem get Data Storage throw new Exception elem to String does not exist else elements get File Element Descriptors elem return new Data Storage Input Stream Iterator elements return new Data Storage Input Stream Iterator elements recursively get all File element descriptors present in the input element descriptor param elem input element descriptor return an array of Element descriptors for files present found by traversing all levels of dirs in the input element descriptor throws Data Storage Exception private static Element Descriptor get File Element Descriptors Element Descriptor elem throws Data Storage Exception Data Storage store elem get Data Storage Element Descriptor elems store as Collection elem to String elems could have directories in it if so get the files out so that it contains only files List Element Descriptor paths new Array List Element Descriptor List Element Descriptor file Paths new Array List Element Descriptor for int m m elems length m paths add elems m for int j j paths size j Element Descriptor full Path store as Element store get Active Container paths get j Skip hadoop s private meta files if full Path system Element continue if full Path instanceof Container Descriptor for Element Descriptor child Container Descriptor full Path paths add child continue else this is a file add it to file Paths file Paths add full Path elems new Element Descriptor file Paths size file Paths to Array elems return elems private static Input Stream open File Element Descriptor elem throws Exception Currently we use classes to represent files and dirs in local mode so we can just delegate this call to open File elem When we have true local mode files and dirs return open File elem This function returns an input stream to a local file system file or a file residing on Hadoop s param file Name The filename to open param exec Type exec Type indicating whether executing in local mode or Map Reduce mode Hadoop param storage The Data Storage object used to open the file Spec return Input Stream to the file Spec throws Exception deprecated Use link open String Pig Context instead Deprecated static public Input Stream open String file Name Exec Type exec Type Data Storage storage throws Exception file Name check Default Prefix exec Type file Name if file Name starts With Element Descriptor elem storage as Element full Path file Name storage return open File elem else file Name file Name substring length Element Descriptor elem storage as Element full Path file Name storage return open File elem deprecated Use link full Path String Pig Context instead Deprecated public static String full Path String file Name Data Storage storage String full Path try if file Name char At Element Descriptor current Dir storage get Active Container Element Descriptor elem storage as Element current Dir to String file Name full Path elem to String else full Path file Name catch Data Storage Exception e full Path file Name return full Path static public Input Stream open String file Spec Pig Context pig Context throws Exception file Spec check Default Prefix pig Context get Exec Type file Spec if file Spec starts With Element Descriptor elem pig Context get Dfs as Element full Path file Spec pig Context return open File elem else file Spec file Spec substring length buffering because we only want buffered streams to be passed to load functions return new Buffered Input Stream new File Input Stream file Spec Element Descriptor elem pig Context get Lfs as Element full Path file Spec pig Context return open File elem param file Spec param offset param pig Context return Seekable Input Stream throws Exception This is an overloaded version of open where there is a need to seek in stream Currently seek is supported only in file not in directory or glob static public Seekable Input Stream open String file Spec long offset Pig Context pig Context throws Exception file Spec check Default Prefix pig Context get Exec Type file Spec Element Descriptor elem if file Spec starts With elem pig Context get Dfs as Element full Path file Spec pig Context else file Spec file Spec substring length elem pig Context get Lfs as Element full Path file Spec pig Context if elem exists elem get Data Storage is Container elem to String try if elem system Element throw new Exception Attempt is made to open system file elem to String Seekable Input Stream sis elem sopen sis seek offset return sis catch Data Storage Exception e throw new Exception Failed to determine if elem elem is container e Either a directory or a glob else throw new Exception Currently seek is supported only in a file not in glob or directory static public Output Stream create String file Spec Pig Context pig Context throws Exception return create file Spec false pig Context static public Output Stream create String file Spec boolean append Pig Context pig Context throws Exception file Spec check Default Prefix pig Context get Exec Type file Spec if file Spec starts With Element Descriptor elem pig Context get Dfs as Element file Spec return elem create else file Spec file Spec substring length probably this should be replaced with the local file system File f new File file Spec get Parent File if f null boolean res f mkdirs if res log warn File Localizer create failed to create f return new File Output Stream file Spec append static public boolean delete String file Spec Pig Context pig Context throws Exception file Spec check Default Prefix pig Context get Exec Type file Spec Element Descriptor elem null if file Spec starts With elem pig Context get Dfs as Element file Spec else elem pig Context get Lfs as Element file Spec elem delete return true static Random r new Random Thread local relative Root Container Descriptor Do not access this object directly since it s lazy initialized in the relative Root Pig Context method which should be used instead private static Thread Local Container Descriptor relative Root new Thread Local Container Descriptor private static Container Descriptor resource Path This method is only used by test code to reset state param initialized public static void set Initialized boolean initialized if initialized relative Root set null Accessor method to get the root Container Descriptor used for temporary files bound to this thread Calling this method lazy initialized the relative Root object param pig Context return throws Data Storage Exception private static synchronized Container Descriptor relative Root final Pig Context pig Context throws Data Storage Exception if relative Root get null Container Descriptor relative get Temp Container pig Context relative Root set relative return relative Root get Accessor method to get the resource Container Descriptor used for tez resource path bound to this thread Calling this method lazy initialized the resource Path object This path is different than relative Root in that calling Pig Server shutdown will only remove relative Root but not resourth Path since resourth Path should be available in the entire session param pig Context return temporary resource path throws Data Storage Exception public static synchronized Path get Temporary Resource Path final Pig Context pig Context throws Data Storage Exception if resource Path null resource Path get Temp Container pig Context return Path resource Path get Path private static synchronized Container Descriptor get Temp Container final Pig Context pig Context throws Data Storage Exception Container Descriptor temp Container null String tdir Utils substitute Vars pig Context get Properties get Property Pig Configuration tmp try do temp Container pig Context get Dfs as Container tdir temp r next Int while temp Container exists create Container temp Container catch Exception e try one last time in case this was due Exception caused by dir operations on directory created by another at the same instant temp Container pig Context get Dfs as Container tdir temp r next Int try create Container temp Container catch Exception e throw new Data Storage Exception e return temp Container private static void create Container Container Descriptor container throws Exception container create if container instanceof Directory Directory container set Permission public static void delete Temp Files if relative Root get null try relative Root get delete catch Exception e log error e set Initialized false public static void delete Temp Resource Files if resource Path null try resource Path delete catch Exception e log error e public static Path get Temporary Path Pig Context pig Context throws Exception return get Temporary Path pig Context public static Path get Temporary Path Pig Context pig Context String suffix throws Exception Element Descriptor relative relative Root pig Context Element Descriptor elem pig Context get Dfs as Element relative to String tmp r next Int suffix return Path elem get Path public static String hadoopify String filename Pig Context pig Context throws Exception if Shell filename filename replace if filename starts With filename filename substring length Element Descriptor local Elem pig Context get Lfs as Element filename if local Elem exists throw new File Not Found Exception filename Element Descriptor distrib Elem pig Context get Dfs as Element get Temporary Path pig Context to String int suffix Start filename last Index Of if suffix Start distrib Elem pig Context get Dfs as Element distrib Elem to String filename substring suffix Start currently the copy method in Data Storage does not allow to specify overwrite so the work around is to delete the dst file first if it exists if distrib Elem exists distrib Elem delete local Elem copy distrib Elem null false return distrib Elem to String public static String full Path String filename Pig Context pig Context throws Exception try if filename char At Element Descriptor current Dir pig Context get Dfs get Active Container Element Descriptor elem pig Context get Dfs as Element current Dir to String filename return elem to String return filename catch Data Storage Exception e return filename public static boolean file Exists String filename Pig Context context throws Exception return file Exists filename context get Fs deprecated Use link file Exists String Pig Context instead Deprecated public static boolean file Exists String filename Data Storage store throws Exception Element Descriptor elem store as Element filename return elem exists glob Matches Files elem store public static boolean is File String filename Pig Context context throws Exception return is Directory filename context get Dfs deprecated Use link is File String Pig Context instead Deprecated public static boolean is File String filename Data Storage store throws Exception return is Directory filename store public static boolean is Directory String filename Pig Context context throws Exception return is Directory filename context get Dfs deprecated Use link is Directory String Pig Context instead Deprecated public static boolean is Directory String filename Data Storage store throws Exception Element Descriptor elem store as Element filename return elem instanceof Container Descriptor private static boolean glob Matches Files Element Descriptor elem Data Storage fs throws Exception try Currently if you give a glob with non special glob characters hadoop returns an array with your file name in it So check for that Element Descriptor elems fs as Collection elem to String switch elems length case return false case return elems equals elem default return true catch Data Storage Exception e throw e Visible For Testing public static void set Random r File Localizer r r Convert path from Windows convention to Unix convention Invoked under cygwin param path path in Windows convention return path in Unix convention null if fail static public String parse Cyg Path String path int style String command if style command new String cygpath w path else command new String cygpath u path Process p null try p Runtime get Runtime exec command catch Exception e return null int exit Val try exit Val p wait For catch Interrupted Exception e return null if exit Val return null String line null Buffered Reader br null try Input Stream Reader isr new Input Stream Reader p get Input Stream br new Buffered Reader isr line br read Line isr close catch Exception e return null finally if br null try br close catch Exception e return line static File local Temp Dir null static File f boolean success true try f File create Temp File pig tmp success f delete success f mkdir local Temp Dir f local Temp Dir delete On Exit catch Exception e if success throw new Runtime Exception Error creating File Localizer temp directory public static class Fetch File Ret public Fetch File Ret File file boolean did Fetch this file file this did Fetch did Fetch public File file public boolean did Fetch Ensures that the passed path is on the local file system fetching it to the java io tmpdir if necessary If pig jars relative to dfs is true and dfs is not null then a relative path is assumed to be relative to the passed dfs active directory Else they are assumed to be relative to the local working directory public static Fetch File Ret fetch File Properties properties String file Path throws Exception return fetch Files Internal properties file Path false Ensures that the passed files pointed to by path are on the local file system fetching them to the java io tmpdir if necessary If pig jars relative to dfs is true and dfs is not null then a relative path is assumed to be relative to the passed dfs active directory Else they are assumed to be relative to the local working directory public static Fetch File Ret fetch Files Properties properties String file Path throws Exception return fetch Files Internal properties file Path true Copies the files from remote to local filesystem When multiple Files is set the path could point to multiple files through globs or a directory In this case return array contains multiple files otherwise a single file is returned If pig jars relative to dfs is true then a relative path is assumed to be relative to the default filesystem s active directory Else they are assumed to be relative to the local working directory param properties param file Path param multiple Files return private static Fetch File Ret fetch Files Internal Properties properties String file Path boolean multiple Files throws Exception Path path new Path file Path if path get Name is Empty return new Fetch File Ret uri path to Uri Configuration conf new Configuration Configuration Util merge Conf conf Configuration Util to Configuration properties if there is no schema or if the schema is local then it is expected to be a local path File System local Fs File System get Local conf File System src Fs if true equals properties get Property pig jars relative to dfs uri get Scheme null For Windows local files uri get Scheme null uri get Path matches Za z uri get Scheme null uri get Scheme equals local src Fs local Fs else src Fs path get File System conf File Status files if multiple Files files src Fs glob Status path else files new File Status src Fs get File Status path if files null files length throw new Exec Exception file file Path does not exist Pig Exception Fetch File Ret fetch Files new Fetch File Ret files length int idx for File Status file files should throw an exception if this is not a file String pathname file get Path to Uri get Path String filename file get Path get Name if src Fs local Fs fetch Files idx new Fetch File Ret new File pathname false else fetch from remote File dest new File local Temp Dir filename dest delete On Exit try src Fs copy To Local File file get Path new Path dest get Absolute Path catch Exception e throw new Exec Exception Could not copy file Path to local destination dest Pig Exception e fetch Files idx new Fetch File Ret dest true return fetch Files Ensures that the passed resource is available from the local file system fetching it to a temporary directory throws Resource Not Found Exception public static Fetch File Ret fetch Resource String name throws Exception Resource Not Found Exception Fetch File Ret local File Ret null Input Stream resource Stream Pig Context get Class Loader get Resource As Stream name if resource Stream null File dest new File local Temp Dir name dest get Parent File mkdirs dest delete On Exit Output Stream output Stream null try output Stream new Buffered Output Stream new File Output Stream dest byte buffer new byte int len while len resource Stream read buffer output Stream write buffer len finally resource Stream close if output Stream null output Stream close local File Ret new Fetch File Ret dest false else throw new Resource Not Found Exception name return local File Ret 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java util Array List import java util Iterator import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Operator Sub Plan import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan optimizer Rule import org apache pig newplan optimizer Transformer This Rule moves Filter Above Foreach It checks if uid on which filter works on is present in the predecessor of foreach If so it transforms it public class Filter Above Foreach extends Rule public Filter Above Foreach String n super n false Override protected Operator Plan build Pattern the pattern that this rule looks for is foreach filter Logical Plan plan new Logical Plan Logical Relational Operator foreach new For Each plan Logical Relational Operator filter new Filter plan plan add foreach plan add filter plan connect foreach filter return plan Override public Transformer get New Transformer return new Filter Above For Each Transformer public class Filter Above For Each Transformer extends Transformer Filter filter null For Each foreach null Logical Relational Operator for Each Pred null Operator Sub Plan sub Plan null Override public boolean check Operator Plan matched throws Frontend Exception Iterator Operator iter matched get Operators while iter has Next Operator op iter next if op instanceof For Each foreach For Each op break This would be a strange case if foreach null return false iter matched get Operators while iter has Next Operator op iter next if op instanceof Filter filter Filter op break This is for cheating we look up more than one filter in the plan while filter null Get uids of Filter Pair List Long List Byte uid With Types get Filter Projection Uids filter See if the previous operators have uids from project List Operator preds current Plan get Predecessors foreach for int j j preds size j Logical Relational Operator log Rel Op Logical Relational Operator preds get j if has All log Rel Op uid With Types for Each Pred Logical Relational Operator preds get j If a filter is nondeterministic we should n t push it up return Optimizer Utils plan Has Non Deterministic Udf filter get Filter Plan Chances are there are filters below this filter which can be moved up So searching for those filters List Operator successors current Plan get Successors filter if successors null successors size successors get instanceof Filter filter Filter successors get else filter null return false Get all uids from Projections of this Filter Operator param filter return Set of uid private Pair List Long List Byte get Filter Projection Uids Filter filter throws Frontend Exception List Long uids new Array List Long List Byte types new Array List Byte if filter null Logical Expression Plan filter Plan filter get Filter Plan Iterator Operator iter filter Plan get Operators Operator op null while iter has Next op iter next if op instanceof Project Expression Project Expression proj Project Expression op if proj is Project Star project range is always expanded when schema is available so nothing to do here for it Logical Relational Operator pred Logical Relational Operator filter get Plan get Predecessors filter get Logical Schema pred Schema pred get Schema if pred Schema null for int i i pred Schema size i uids add pred Schema get Field i uid types add pred Schema get Field i type else uids add proj get Field Schema uid types add proj get Field Schema type Pair List Long List Byte result new Pair List Long List Byte uids types return result checks if a relational operator contains all of the specified uids param op Logical Relational operator that should contain the uid param uids Uids to check for return true if given Logical Relational Operator has all the given uids private boolean has All Logical Relational Operator op Pair List Long List Byte uid With Types throws Frontend Exception Logical Schema schema op get Schema if schema null return false List Long uids uid With Types first List Byte types uid With Types second for int i i uids size i boolean found false for Logical Schema Logical Field Schema fs schema get Fields if fs uid uids get i fs type types get i found true if found return false return true Override public Operator Plan report Changes return sub Plan Override public void transform Operator Plan matched throws Frontend Exception List Operator op Set current Plan get Predecessors filter if op Set null op Set size return Operator filter Pred op Set get op Set current Plan get Successors filter if op Set null op Set size return Operator filter Suc op Set get sub Plan new Operator Sub Plan current Plan Steps below do the following For Each Pred For Each Filter These are filters which can not be moved Filter Pred is a Filter Filter To be moved Filter Suc Transforms into For Each Pred Filter After being Moved For Each Filter These are filters which can not be moved Filter Pred is a Filter Filter Suc Above plan is assuming we are modifying the filter in middle If we are modifying the first filter after For Each then kleene star becomes zero And For Each is Filter Pred Pair Integer Integer for Each Pred Places current Plan disconnect for Each Pred foreach Pair Integer Integer filter Pred Places current Plan disconnect filter Pred filter Pair Integer Integer filter Suc Places current Plan disconnect filter filter Suc current Plan connect for Each Pred for Each Pred Places first filter filter Pred Places second current Plan connect filter filter Suc Places first foreach for Each Pred Places second current Plan connect filter Pred filter Pred Places first filter Suc filter Suc Places second sub Plan add for Each Pred sub Plan add foreach sub Plan add filter Pred sub Plan add filter sub Plan add filter Suc 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import org apache pig impl Pig Context import org apache pig newplan Operator Plan import org apache pig newplan logical relational Filter import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator public class Filter Constant Calculator extends Constant Calculator public Filter Constant Calculator String n Pig Context pc super n pc Override protected Operator Plan build Pattern Logical Plan plan new Logical Plan Logical Relational Operator op new Filter plan plan add op return plan 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import org apache pig impl Pig Context import org apache pig newplan Operator Plan import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator public class For Each Constant Calculator extends Constant Calculator public For Each Constant Calculator String n Pig Context pc super n pc Override protected Operator Plan build Pattern Logical Plan plan new Logical Plan Logical Relational Operator op new For Each plan plan add op return plan 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import org apache pig Exceptional Function import org apache pig Primitive Eval Func import org apache pig impl Pig Context import java io Exception import java lang reflect Parameterized Type import java lang reflect Type import java util Hash Map Eval Func that wraps an implementation of the Function interface which is passed as a String in the constructor When resolving the Function class the Pig package import list is used The Function must have a default no arg constructor which will be used For Functions that take args in the constructor initialize the function in a subclass of this one and call code super function code Example code my Udf Function Wrapper Eval Func My Function code public class Function Wrapper Eval Func extends Primitive Eval Func Object Object cache the types we resolve to limit reflection private static Hash Map Class Type resolved Types new Hash Map Class Type private Exceptional Function function private String counter Group Takes the class name of a Function initializes it using the default constructor and passes it to Function Wrapper Eval Func Exceptional Function function Functions must implement either com google common base Function or Exceptional Function param function Class Name function class to initialize public Function Wrapper Eval Func String function Class Name throws Class Not Found Exception No Such Method Exception Illegal Access Exception Instantiation Exception Exception this initialize Function function Class Name Determines the input and output types of the Function and initializes the superclass Subclass and call this constructor if a Function with a non default constructor is required param function Function to be used by the protected Function Wrapper Eval Func com google common base Function function throws Exception Class Not Found Exception No Such Method Exception this Exceptional Function new Google Function Bridge function Determines the input and output types of the Function and initializes the superclass Subclass and call this constructor if a Function with a non default constructor is required param function Function to be used by the protected Function Wrapper Eval Func Exceptional Function function throws Exception Class Not Found Exception No Such Method Exception super get Function In Class function get Function Out Class function this function function String function Name function instanceof Google Function Bridge Google Function Bridge function get Wrapped Function get Class get Simple Name function get Class get Simple Name this counter Group get Class get Name function Name Override Suppress Warnings unchecked public Object exec Object input throws Exception try return function apply input catch Exception e safe Incr Counter get Counter Group e get Class get Canonical Name throw new Exception e Override protected String get Counter Group return this counter Group private static Class get Function In Class Exceptional Function function Class Name throws Class Not Found Exception No Such Method Exception Exception return get Function Type Class function Class Name private static Class get Function Out Class Exceptional Function function Class Name throws Class Not Found Exception No Such Method Exception Exception return get Function Type Class function Class Name For a given class that implements the parameterized interface code Exceptional Function code return the type class at the code index code position If the Function class is code Google Function Bridge code return the type class for the wrapped function private static Class get Function Type Class Exceptional Function function int index throws Class Not Found Exception No Such Method Exception Exception Class clazz Class expected Interface if function instanceof Google Function Bridge clazz Google Function Bridge function get Wrapped Function get Class expected Interface com google common base Function class else clazz function get Class expected Interface Exceptional Function class check the cache if resolved Types contains Key clazz return Class resolved Types get clazz index Type interface Types clazz get Generic Interfaces for Type interface Type interface Types Parameterized Type parameterized Type Parameterized Type interface Type if expected Interface is Assignable From Class parameterized Type get Raw Type Type types parameterized Type get Actual Type Arguments resolved Types put clazz types return Class types index throw new No Such Method Exception Unrecognized function class passed clazz get Class Function must implement either com google common base Function class get Name or Exceptional Function class get Name Suppress Warnings unchecked private static Exceptional Function initialize Function String function Class Name throws Exception Illegal Access Exception Instantiation Exception Object function Object Pig Context resolve Class Name function Class Name new Instance if function Object instanceof Exceptional Function return Exceptional Function function Object else if function Object instanceof com google common base Function return new Google Function Bridge com google common base Function function Object throw new Instantiation Exception Unrecognized function class passed function Object get Class Function must implement either com google common base Function class get Name or Exceptional Function class get Name Used so we can handle both Google s Function as well as an Pig s Exceptional Function private static class Google Function Bridge implements org apache pig Function private com google common base Function function private Google Function Bridge com google common base Function function this function function public com google common base Function get Wrapped Function return function Override Suppress Warnings unchecked public apply Object item return function apply item 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java util Array List import java util Arrays import java util List import org apache pig impl util Jar Manager public class Func Utils Utility function to get a list of containing jars via classes param classes Identifying Jars classes used to identify containing jars return list of containing jars public static List String get Ship Files Class classes Identifying Jars return get Ship Files Arrays as List classes Identifying Jars public static List String get Ship Files List Class classes Identifying Jars List String cache Files new Array List String for Class clz classes Identifying Jars String jar Jar Manager find Containing Jar clz cache Files add jar return cache Files 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig backend executionengine Exec Exception import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema The generic Invoker class does all the common grunt work of setting up an invoker Class specific non generic extensions of this class are needed for Pig to know what type of return to expect from exec and to find the appropriate classes through reflection All they have to do is implement the constructors that call into super Note that the no parameter constructor is b required b if seemingly nonsensical for Pig to do its work p The Invoker family of udfs understand the following class names all case independent li String li Long li Float li Double li Int p Invokers can also work with array arguments represented in Pig as Data Bags of single tuple elements Simply refer to code string code for example p This allows one to dynamically invoke Java methods that return a code code p Usage of the Invoker family of Fs adjust as appropriate p pre code invoking a static method String To Long Invoke For Long java lang Long value Of String longs strings String To Long some chararray invoking a method on an object String Concat Invoke For String java lang String concat String String false concatenations strings String Concat str str pre p The first argument to the constructor is the full path to desired method br The second argument is a list of classes of the method parameters br If the method is not static the first element in this list is the object to invoke the method on br The second argument is optional a no argument static method is assumed if it is not supplied br The third argument is the keyword static or true to signify that the method is static br The third argument is optional and true by default br p param public abstract class Generic Invoker extends Eval Func private Invoker invoker public Generic Invoker public Generic Invoker String full Name throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception invoker new Invoker full Name public Generic Invoker String full Name String param Specs Str throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception invoker new Invoker full Name param Specs Str public Generic Invoker String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception invoker new Invoker full Name param Specs Str is Static Override public exec Tuple input throws Exception if invoker null throw new Exec Exception exec attempted on an unitialized invoker Invokers must be constructed with the method to invoke and parameter signature to same return invoker invoke input Override public Schema output Schema Schema input if invoker null return null Field Schema fs new Field Schema null Data Type find Type invoker get Return Type return new Schema fs 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Greater Than Equal Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Greater Than Equal Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Greater Than Equal plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Greater Than Equal Expression Greater Than Equal Expression eo Greater Than Equal Expression other return eo get Lhs is Equal get Lhs eo get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Greater Than Equal Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Greater Than Expr extends Binary Comparison Operator private static final long serial Version public Greater Than Expr Operator Key k this k public Greater Than Expr Operator Key k int rp super k rp result Type Data Type Override public String name return Greater Than Data Type find Type Name result Type m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Greater Than this Override public Result get Next Boolean throws Exec Exception Result left right switch operand Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type Result r accum Child null operand Type if r null return r left lhs get Next operand Type right rhs get Next operand Type return do Comparison left right default int err Code String msg this get Class get Simple Name does not know how to handle type Data Type find Type Name operand Type throw new Exec Exception msg err Code Pig Exception Suppress Warnings unchecked private Result do Comparison Result left Result right if left return Status Status return left if right return Status Status return right if either operand is null the result should be null if left result null right result null left result null left return Status Status return left assert left result instanceof Comparable assert right result instanceof Comparable if Comparable left result compare To right result left result Boolean else left result Boolean illustrator Markup null left result Boolean left result return left Override public Greater Than Expr clone throws Clone Not Supported Exception Greater Than Expr clone new Greater Than Expr new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Greater Than Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Greater Than Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Greater Than plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Greater Than Expression Greater Than Expression eo Greater Than Expression other return eo get Lhs is Equal get Lhs eo get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Greater Than Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools grunt import java io Buffered Reader import java util Array List import jline console Console Reader import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Server import org apache pig impl Pig Context import org apache pig tools grunt Grunt Parser import org apache pig tools grunt Pig Completor import org apache pig tools grunt Pig Completor Aliases import org apache pig tools pigstats Pig Stats Util import org apache pig backend executionengine Exec Exception import org apache pig impl util Log Utils public class Grunt private final Log log Log Factory get Log get Class Buffered Reader in Pig Server pig Grunt Parser parser public Grunt Buffered Reader in Pig Context pig Context throws Exec Exception this in in this pig new Pig Server pig Context if in null parser new Grunt Parser in pig public void set Console Reader Console Reader c c add Completer new Pig Completor Aliases pig c add Completer new Pig Completor parser set Console Reader c public void run boolean verbose true equals Ignore Case pig get Pig Context get Properties get Property verbose while true try Pig Stats Util get Empty Pig Stats parser set Interactive true parser parse Stop On Error break catch Throwable t Log Utils write Log t pig get Pig Context get Properties get Property pig logfile log verbose Pig Stack Trace parser Re Init in public int exec throws Throwable boolean verbose true equals Ignore Case pig get Pig Context get Properties get Property verbose try Pig Stats Util get Empty Pig Stats parser set Interactive false return parser parse Stop On Error catch Throwable t Log Utils write Log t pig get Pig Context get Properties get Property pig logfile log verbose Pig Stack Trace throw t public void check Script String script File throws Throwable boolean verbose true equals Ignore Case pig get Pig Context get Properties get Property verbose try parser set Interactive false parser set Validate Each Statement true boolean dont Print Output true parser process Explain null script File false text null new Array List String new Array List String dont Print Output catch Throwable t Log Utils write Log t pig get Pig Context get Properties get Property pig logfile log verbose Pig Stack Trace throw t 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools grunt import java io Buffered Reader import java io Byte Array Input Stream import java io File import java io File Not Found Exception import java io File Output Stream import java io File Reader import java io Exception import java io Input Stream import java io Input Stream Reader import java io Print Stream import java io Reader import java io String Reader import java util Abstract List import java util Array List import java util Arrays import java util Collections import java util Date import java util Hash Set import java util Iterator import java util List import java util Map import java util Properties import java util Set import jline console Console Reader import org apache commons io output Null Output Stream import org apache commons lang String Utils import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop fs File System import org apache hadoop fs Fs Shell import org apache hadoop fs Path import org apache pig Load Func import org apache pig Pig Server import org apache pig backend datastorage Container Descriptor import org apache pig backend datastorage Data Storage import org apache pig backend datastorage Data Storage Exception import org apache pig backend datastorage Element Descriptor import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop datastorage Data Storage import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Localizer import org apache pig impl io File Localizer Fetch File Ret import org apache pig impl util Log Utils import org apache pig impl util Tuple Format import org apache pig parser Register Resolver import org apache pig tools pigscript parser Parse Exception import org apache pig tools pigscript parser Pig Script Parser import org apache pig tools pigscript parser Pig Script Parser Token Manager import org apache pig tools pigscript parser Token Mgr Error import org apache pig tools pigstats Job Stats import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Pig Stats Job Graph import org apache pig validator Black And Whitelist Filter import org apache pig validator Pig Command Filter import org fusesource jansi Ansi import org fusesource jansi Ansi Console import com google common collect Lists public class Grunt Parser extends Pig Script Parser private static final Log log Log Factory get Log Grunt Parser class private Pig Command Filter filter public Grunt Parser Reader reader this reader null init public Grunt Parser Reader reader Pig Server pig Server super reader m Pig Server pig Server init public Grunt Parser Input Stream stream String encoding this stream encoding null public Grunt Parser Input Stream stream String encoding Pig Server pig Server super stream encoding m Pig Server pig Server init public Grunt Parser Input Stream stream super stream init public Grunt Parser Input Stream stream Pig Server pig Server super stream m Pig Server pig Server init public Grunt Parser Pig Script Parser Token Manager tm this tm null public Grunt Parser Pig Script Parser Token Manager tm Pig Server pig Server super tm m Pig Server pig Server init private void init m Done false m Load Only false m Explain null m Script Illustrate false set Props filter new Black And Whitelist Filter m Pig Server private void set Props m Dfs m Pig Server get Pig Context get Dfs m Lfs m Pig Server get Pig Context get Lfs m Conf m Pig Server get Pig Context get Properties shell new Fs Shell Configuration Util to Configuration m Conf Override public void set Interactive boolean is Interactive super set Interactive is Interactive if is Interactive set Validate Each Statement true public void set Validate Each Statement boolean b m Pig Server set Validate Each Statement b private void set Batch On m Pig Server set Batch On private void execute Batch throws Exception if m Pig Server is Batch On if m Explain null explain Current Batch if m Load Only m Pig Server execute Batch Pig Stats stats Pig Stats get Job Graph jg stats get Job Graph Iterator Job Stats iter jg iterator while iter has Next Job Stats js iter next if js is Successful m Num Failed Jobs Exception exp js get Exception null js get Exception new Exec Exception Job js get Job Id null js get Job Id failed hadoop does not return any error message Log Utils write Log exp m Pig Server get Pig Context get Properties get Property pig logfile log true equals Ignore Case m Pig Server get Pig Context get Properties get Property verbose Pig Stack Trace else m Num Succeeded Jobs private void discard Batch throws Exception if m Pig Server is Batch On m Pig Server discard Batch public int parse Stop On Error throws Exception Parse Exception return parse Stop On Error false Parses Pig commands in either interactive mode or batch mode In interactive mode executes the plan right away whenever a command is encountered throws Exception Parse Exception public int parse Stop On Error boolean same Batch throws Exception Parse Exception if m Pig Server null throw new Illegal State Exception if m Interactive same Batch set Batch On m Pig Server set Skip Parse In Register For Batch true try prompt m Done false while m Done parse if same Batch execute Batch catch Token Mgr Error tme This error from Pig Script Parser Token Manager is not intuitive and always refers to the last line if we are reading whole query without parsing it line by line during batch So execute Batch and get the error from Query Parser if m Interactive same Batch execute Batch throw tme finally if same Batch discard Batch int res m Num Succeeded Jobs m Num Failed Jobs return res public void set Load Only boolean load Only m Load Only load Only public void set Script Illustrate m Script Illustrate true Override public void prompt if m Interactive m Console Reader set Prompt grunt Override protected void quit m Done true public boolean is Done return m Done parse Only method added for supporting penny public void parse Only throws Exception Parse Exception if m Pig Server null throw new Illegal State Exception m Done false while m Done parse Override protected void process Describe String alias throws Exception String nested Alias null if m Explain null process only if not in explain mode if m Pig Server is Batch On m Pig Server parse And Build if alias null alias m Pig Server get Pig Context get Last Alias if describe is used immediately after launching grunt shell then last defined alias will be null if alias null throw new Exception No previously defined alias found Please define an alias and use describe operator if alias contains nested Alias alias substring alias index Of alias alias substring alias index Of m Pig Server dump Schema Nested alias nested Alias else if equals alias alias m Pig Server get Last Rel m Pig Server dump Schema alias else log warn describe statement is ignored while processing explain script or check Override protected void process Explain String alias String script boolean is Verbose String format String target List String params List String files throws Exception Parse Exception if m Pig Server is Batch On m Pig Server parse And Build if alias null script null if m Interactive alias m Pig Server get Pig Context get Last Alias if explain is used immediately after launching grunt shell then last defined alias will be null if alias null throw new Parse Exception explain statement must be on an alias or on a script if equals alias alias m Pig Server get Last Rel process Explain alias script is Verbose format target params files false protected void process Explain String alias String script boolean is Verbose String format String target List String params List String files boolean dont Print Output throws Exception Parse Exception filter validate Pig Command Filter Command if null m Explain return try m Explain new Explain State alias target script is Verbose format if script null set Batch On try load Script script true true false params files catch Exception e discard Batch throw e catch Parse Exception e discard Batch throw e m Explain m Last true explain Current Batch dont Print Output finally if script null discard Batch m Explain null protected void explain Current Batch throws Exception explain Current Batch false protected void explain Current Batch boolean dont Print Output throws Exception Print Stream lp dont Print Output new Print Stream new Null Output Stream System out Print Stream ep dont Print Output new Print Stream new Null Output Stream System out if m Explain m Last m Explain m Count if m Pig Server is Batch Empty return m Explain m Count boolean mark As Executed m Explain m Script null if m Explain m Target null File file new File m Explain m Target if file is Directory String s Count m Explain m Last m Explain m Count m Explain m Count String suffix m Explain m Time s Count m Explain m Format lp new Print Stream new File file logical plan suffix m Pig Server explain m Explain m Alias m Explain m Format m Explain m Verbose mark As Executed lp null file suffix lp close ep close else boolean append m Explain m Count lp ep new Print Stream new File Output Stream m Explain m Target append m Pig Server explain m Explain m Alias m Explain m Format m Explain m Verbose mark As Executed lp ep null null lp close else m Pig Server explain m Explain m Alias m Explain m Format m Explain m Verbose mark As Executed lp ep null null Override protected void print Aliases throws Exception if m Explain null process only if not in explain mode m Pig Server print Aliases else log warn aliases statement is ignored while processing explain script or check Override protected void print Clear Ansi Console system Install Ansi ansi Ansi ansi System out println ansi erase Screen System out println ansi cursor Ansi Console system Uninstall Override protected void process Register String jar throws Exception filter validate Pig Command Filter Command jar parameter Substitution In Grunt jar m Pig Server register Jar jar Override protected void process Register String path String scripting Lang String namespace throws Exception Parse Exception filter validate Pig Command Filter Command path parameter Substitution In Grunt path scripting Lang parameter Substitution In Grunt scripting Lang namespace parameter Substitution In Grunt namespace new Register Resolver m Pig Server parse Register path scripting Lang namespace private String run Preprocessor String script Path List String params List String param Files throws Exception Parse Exception Pig Context context m Pig Server get Pig Context Buffered Reader reader new Buffered Reader new File Reader script Path String result context do Param Substitution reader params param Files reader close return result Override protected void process Script String script boolean batch List String params List String files throws Exception Parse Exception if batch filter validate Pig Command Filter Command else filter validate Pig Command Filter Command if m Explain null process only if not in explain mode if script null execute Batch return if batch set Batch On m Pig Server set Job Name script try load Script script true false m Load Only params files execute Batch finally discard Batch else load Script script false false m Load Only params files else log warn run exec statement is ignored while processing explain script or check private void load Script String script boolean batch boolean load Only boolean illustrate List String params List String files throws Exception Parse Exception Reader input Reader Console Reader reader boolean interactive Pig Context pc m Pig Server get Pig Context if load Only pc get Preprocessor Context param Scope Push pc set Params params pc set Param Files files try Fetch File Ret fetch File File Localizer fetch File m Conf script String cmds run Preprocessor fetch File file get Absolute Path params files if m Interactive batch Write prompt and echo commands Console reader treats tabs in a special way cmds cmds replace All t reader new Console Reader new Byte Array Input Stream cmds get Bytes System out reader set History m Console Reader get History Input Stream in new Console Reader Input Stream reader input Reader new Buffered Reader new Input Stream Reader in interactive true else Quietly parse the statements input Reader new String Reader cmds reader null interactive false catch File Not Found Exception fnfe throw new Parse Exception File not found script catch Security Exception se throw new Parse Exception Can not access file script Grunt Parser parser new Grunt Parser input Reader m Pig Server parser set Console Reader reader parser set Interactive interactive parser set Load Only load Only if illustrate parser set Script Illustrate parser m Explain m Explain parser prompt while parser is Done parser parse if interactive System out println if load Only pc get Preprocessor Context param Scope Pop Override protected void process Set String key String value throws Exception Parse Exception filter validate Pig Command Filter Command key parameter Substitution In Grunt key value parameter Substitution In Grunt value if key equals debug if value null if value equals on m Pig Server debug On else if value equals off m Pig Server debug Off else throw new Parse Exception Invalid value value provided for key else System out println key m Pig Server is Debug On else if key equals job name if value null m Pig Server set Job Name value else System out println key m Pig Server get Job Name else if key equals job priority if value null m Pig Server set Job Priority value else System out println key m Pig Server get Job Priority else if key equals stream skippath if value null Validate File file new File value if file exists file is Directory throw new Exception Invalid value for stream skippath value m Pig Server add Path To Skip value else System out println key String Utils join m Pig Server get Pig Context get Paths To Skip else if key equals default parallel if value null Validate try m Pig Server set Default Parallel Integer parse Int value catch Number Format Exception e throw new Parse Exception Invalid value for default parallel else System out println key m Pig Server get Pig Context get Default Parallel else if value null m Pig Server get Pig Context get Execution Engine set Property key value else if m Pig Server get Pig Context get Properties contains Key key System out println key m Pig Server get Pig Context get Properties get Property key else System out println key is not defined Override protected void process Set throws Exception Parse Exception filter validate Pig Command Filter Command Properties job Props m Pig Server get Pig Context get Properties Properties sys Props System get Properties List String job Props List Lists new Array List List String sys Props List Lists new Array List for Object key job Props key Set String prop Str key job Props get Property String key if sys Props contains Key key sys Props List add system prop Str else job Props List add prop Str Collections sort job Props List Collections sort sys Props List job Props List add All sys Props List for String prop job Props List System out println prop Override protected void process Cat String path throws Exception filter validate Pig Command Filter Command path parameter Substitution In Grunt path if m Explain null process only if not in explain mode execute Batch try byte buffer new byte Element Descriptor dfs Path m Dfs as Element path int rc if dfs Path exists throw new Exception Directory path does not exist if m Dfs is Container path Container Descriptor dfs Dir Container Descriptor dfs Path Iterator Element Descriptor paths dfs Dir iterator while paths has Next Element Descriptor cur Elem paths next if m Dfs is Container cur Elem to String continue Input Stream is cur Elem open while rc is read buffer System out write buffer rc is close else Input Stream is dfs Path open while rc is read buffer System out write buffer rc is close catch Data Storage Exception e throw new Exception Failed to Cat path e else log warn cat statement is ignored while processing explain script or check Override protected void process String path throws Exception filter validate Pig Command Filter Command path parameter Substitution In Grunt path Container Descriptor container if m Explain null process only if not in explain mode execute Batch try if path null container m Dfs as Container Data Storage m Dfs get get Home Directory to String m Dfs set Active Container container else container m Dfs as Container path if container exists throw new Exception Directory path does not exist if m Dfs is Container path throw new Exception path is not a directory m Dfs set Active Container container catch Data Storage Exception e throw new Exception Failed to change working directory to path null Data Storage m Dfs get get Home Directory to String path e else log warn cd statement is ignored while processing explain script or check Override protected void process Dump String alias throws Exception filter validate Pig Command Filter Command if alias null if m Pig Server is Batch On m Pig Server parse And Build alias m Pig Server get Pig Context get Last Alias if dump is used immediately after launching grunt shell then last defined alias will be null if alias null throw new Exception No previously defined alias found Please define an alias and use dump operator if m Explain null process only if not in explain mode execute Batch if equals alias alias m Pig Server get Last Rel Iterator Tuple result m Pig Server open Iterator alias while result has Next Tuple t result next System out println Tuple Format format t else log warn dump statement is ignored while processing explain script or check Override protected void process Illustrate String alias String script String target List String params List String files throws Exception Parse Exception filter validate Pig Command Filter Command if m Script Illustrate throw new Parse Exception illustrate statement can not appear in a script that is illustrated opon if alias null script null throw new Parse Exception illustrate statement on an alias does not work when a script is in effect else if m Explain null log warn illustrate statement is ignored while processing explain script or check else try if script null if true equals Ignore Case m Pig Server get Pig Context get Properties get Property opt multiquery true throw new Parse Exception Can not explain script if multiquery is disabled set Batch On try load Script script true true true params files catch Exception e discard Batch throw e catch Parse Exception e discard Batch throw e else if alias null if m Pig Server is Batch On m Pig Server parse And Build alias m Pig Server get Pig Context get Last Alias if illustrate is used immediately after launching grunt shell then last defined alias will be null if alias null throw new Parse Exception illustrate statement must be on an alias or on a script if equals alias if m Pig Server is Batch On m Pig Server parse And Build alias m Pig Server get Last Rel m Pig Server get Examples alias finally if script null discard Batch Override protected void process Kill String jobid throws Exception filter validate Pig Command Filter Command m Pig Server get Pig Context get Execution Engine kill Job jobid Override protected void process String path throws Exception filter validate Pig Command Filter Command path parameter Substitution In Grunt path if m Explain null process only if not in explain mode execute Batch try Element Descriptor path Descriptor if path null path Descriptor m Dfs get Active Container else path Descriptor m Dfs as Element path if path Descriptor exists throw new Exception File or directory path does not exist if m Dfs is Container path Descriptor to String Container Descriptor container Container Descriptor path Descriptor Iterator Element Descriptor elems container iterator while elems has Next Element Descriptor cur Elem elems next if m Dfs is Container cur Elem to String System out println cur Elem to String t dir else print Length And Replication cur Elem else print Length And Replication path Descriptor catch Data Storage Exception e throw new Exception Failed to on path e else log warn ls statement is ignored while processing explain script or check private void print Length And Replication Element Descriptor elem throws Exception Map String Object stats elem get Statistics long replication Short stats get Element Descriptor long len Long stats get Element Descriptor System out println elem to String r replication t len Override protected void process throws Exception filter validate Pig Command Filter Command if m Explain null process only if not in explain mode execute Batch System out println m Dfs get Active Container to String else log warn pwd statement is ignored while processing explain script or check Override protected void process History boolean with Numbers m Pig Server print History with Numbers Override protected void print Help System out println Commands System out println pig latin statement See the Pig Latin manual for details http hadoop apache org pig System out println File system commands System out println fs fs arguments Equivalent to Hadoop dfs command http hadoop apache org common docs current hdfs shell html System out println Diagnostic commands System out println describe alias alias Show the schema for the alias Inner aliases can be described as System out println explain script pigscript out path brief dot xml param param name param value System out println param file file name alias Show the execution plan to compute the alias or for entire script System out println script Explain the entire script System out println out Store the output into directory rather than print to stdout System out println brief Do n t expand nested plans presenting a smaller graph for overview System out println dot Generate the output in dot format Default is text format System out println xml Generate the output in xml format Default is text format System out println param param name See parameter substitution for details System out println param file file name See parameter substitution for details System out println alias Alias to explain System out println dump alias Compute the alias and writes the results to stdout System out println Utility Commands System out println exec param param name param value param file file name script System out println Execute the script with access to grunt environment including aliases System out println param param name See parameter substitution for details System out println param file file name See parameter substitution for details System out println script Script to be executed System out println run param param name param value param file file name script System out println Execute the script with access to grunt environment System out println param param name See parameter substitution for details System out println param file file name See parameter substitution for details System out println script Script to be executed System out println sh shell command Invoke a shell command System out println kill job id Kill the hadoop job specified by the hadoop job id System out println set key value Provide execution parameters to Pig Keys and values are case sensitive System out println The following keys are supported System out println default parallel Script level reduce parallelism Basic input size heuristics used by default System out println debug Set debug on or off Default is off System out println job name Single quoted name for jobs Default is Pig Latin script name System out println job priority Priority for jobs Values very low low normal high very high Default is normal System out println stream skippath String that contains the path This is used by streaming System out println any hadoop property System out println help Display this message System out println history n Display the list statements in cache System out println n Hide line numbers System out println quit Quit the grunt shell Override protected void process Move String src String dst throws Exception filter validate Pig Command Filter Command src parameter Substitution In Grunt src dst parameter Substitution In Grunt dst if m Explain null process only if not in explain mode execute Batch try Element Descriptor src Path m Dfs as Element src Element Descriptor dst Path m Dfs as Element dst if src Path exists throw new Exception File or directory src does not exist src Path rename dst Path catch Data Storage Exception e throw new Exception Failed to move src to dst e else log warn mv statement is ignored while processing explain script or check Override protected void process Copy String src String dst throws Exception filter validate Pig Command Filter Command src parameter Substitution In Grunt src dst parameter Substitution In Grunt dst if m Explain null process only if not in explain mode execute Batch try Element Descriptor src Path m Dfs as Element src Element Descriptor dst Path m Dfs as Element dst src Path copy dst Path m Conf false catch Data Storage Exception e throw new Exception Failed to copy src to dst e else log warn cp statement is ignored while processing explain script or check Override protected void process Copy To Local String src String dst throws Exception filter validate Pig Command Filter Command src parameter Substitution In Grunt src dst parameter Substitution In Grunt dst if m Explain null process only if not in explain mode execute Batch try Element Descriptor src Path m Dfs as Element src Element Descriptor dst Path m Lfs as Element dst src Path copy dst Path false catch Data Storage Exception e throw new Exception Failed to copy src to locally dst e else log warn copy To Local statement is ignored while processing explain script or check Override protected void process Copy From Local String src String dst throws Exception filter validate Pig Command Filter Command src parameter Substitution In Grunt src dst parameter Substitution In Grunt dst if m Explain null process only if not in explain mode execute Batch try Element Descriptor src Path m Lfs as Element src Element Descriptor dst Path m Dfs as Element dst src Path copy dst Path false catch Data Storage Exception e throw new Exception Failed to copy loally src to dst e else log warn copy From Local statement is ignored while processing explain script or check Override protected void process Mkdir String dir throws Exception filter validate Pig Command Filter Command dir parameter Substitution In Grunt dir if m Explain null process only if not in explain mode execute Batch Container Descriptor dir Descriptor m Dfs as Container dir dir Descriptor create else log warn mkdir statement is ignored while processing explain script or check Override protected void process Pig String cmd throws Exception int start cmd parameter Substitution In Grunt cmd if m Interactive start get Line Number if cmd char At cmd length m Pig Server register Query cmd start else m Pig Server register Query cmd start Override protected void process Remove String path String options throws Exception filter validate Pig Command Filter Command filter validate Pig Command Filter Command path parameter Substitution In Grunt path int int if m Explain null process only if not in explain mode Path file Path new Path path Element Descriptor dfs Path null File System fs file Path get File System Configuration Util to Configuration m Conf execute Batch if fs exists file Path if options null options equals Ignore Case force throw new Exception File or directory path does not exist else boolean delete Success fs delete file Path true if delete Success log warn Unable to delete path return long start Time System current Time Millis long duration while fs exists file Path duration System current Time Millis start Time if duration throw new Exception Timed out waiting to delete file dfs Path else try Thread sleep catch Interrupted Exception e throw new Exception Error waiting for file deletion e log info Waited duration ms to delete file else log warn rm rmf statement is ignored while processing explain script or check Override protected void process Fs Command String cmd Tokens throws Exception filter validate Pig Command Filter Command for int i i cmd Tokens length i cmd Tokens i parameter Substitution In Grunt cmd Tokens i if m Explain null process only if not in explain mode execute Batch int ret Code try ret Code shell run cmd Tokens catch Exception e throw new Exception e if ret Code m Interactive String s Load Func join Abstract List String Arrays as List cmd Tokens throw new Exception fs command s failed Please check output logs for details else log warn fs statement is ignored while processing explain script or check Override protected void process Sh Command String cmd Tokens throws Exception filter validate Pig Command Filter Command for int i i cmd Tokens length i cmd Tokens i parameter Substitution In Grunt cmd Tokens i if m Explain null process only if not in explain mode try execute Batch For sh command create a process with the following syntax shell exe invoke arg command as string String shell Name sh String shell Invoke Arg c Insert cmd in front of the array list to execute to support built in shell commands like mkdir on Windows if System get Property os name starts With Windows shell Name cmd shell Invoke Arg List String string List new Array List String string List add shell Name string List add shell Invoke Arg String Buffer command String new String Buffer for String curr Token cmd Tokens command String append command String append curr Token string List add command String to String String new Cmd Tokens string List to Array new String Process executor Runtime get Runtime exec new Cmd Tokens Stream Printer out Printer new Stream Printer executor get Input Stream null System out Stream Printer err Printer new Stream Printer executor get Error Stream null System err out Printer start err Printer start int ret executor wait For out Printer join err Printer join if ret m Interactive String s Load Func join Abstract List String Arrays as List cmd Tokens throw new Exception sh command s failed Please check output logs for details catch Exception e throw new Exception e else log warn sh statement is ignored while processing explain script or check public static int run Command String hcat Bin String cmd boolean m Interactive throws Exception List String tokens List new Array List String if hcat Bin ends With py tokens List add python tokens List add hcat Bin else tokens List add hcat Bin cmd cmd trim if cmd substring to Lower Case equals sql Should never happen throw new Exception sql command not start with sql keyword cmd cmd substring trim tokens List add e tokens List add cmd replace All n String tokens tokens List to Array new String create new environment environment This is because of antlr version conflict between Pig and Hive Map String String envs System getenv Set String env Set new Hash Set String for Map Entry String String entry envs entry Set if entry get Key equals env Set add entry get Key entry get Value log info Going to run hcat command tokens tokens length Process executor Runtime get Runtime exec tokens env Set to Array new String Stream Printer out Printer new Stream Printer executor get Input Stream null System out Stream Printer err Printer new Stream Printer executor get Error Stream null System err out Printer start err Printer start int ret try ret executor wait For out Printer join err Printer join if ret m Interactive throw new Exception sql command cmd failed catch Interrupted Exception e log warn Exception raised from sql command e get Localized Message return Override protected void process Command String cmd throws Exception cmd parameter Substitution In Grunt cmd if m Explain null process only if not in explain mode if m Pig Server get Pig Context get Properties get pig sql type equals hcat throw new Exception sql command only support hcat currently String hcat Bin String m Pig Server get Pig Context get Properties get hcat bin if hcat Bin null throw new Exception hcat bin is not defined Define it to be your hcat script Usually bin hcat if new File hcat Bin exists throw new Exception hcat Bin does not exist Please check your hcat bin setting in pig properties execute Batch run Command hcat Bin cmd m Interactive else log warn sql statement is ignored while processing explain script or check Override protected void process Default String key String value throws Exception parameter Substitution In Grunt default key value Override protected void process Declare String key String value throws Exception parameter Substitution In Grunt declare key value private String parameter Substitution In Grunt String input throws Exception if m Interactive input null return m Pig Server get Pig Context do Param Substitution new Buffered Reader new String Reader input trim return input Stream Printer public static class Stream Printer extends Thread Input Stream is String type Print Stream os public Stream Printer Input Stream is String type Print Stream os this is is this type type this os os Override public void run try Input Stream Reader isr new Input Stream Reader is Buffered Reader br new Buffered Reader isr String line null if type null while line br read Line null os println type line else while line br read Line null os println line catch Exception ioe ioe print Stack Trace protected static class Explain State public long m Time public int m Count public String m Alias public String m Target public String m Script public boolean m Verbose public String m Format public boolean m Last public Explain State String alias String target String script boolean verbose String format m Time new Date get Time m Count m Alias alias m Target target m Script script m Verbose verbose m Format format m Last false private Pig Server m Pig Server private Data Storage m Dfs private Data Storage m Lfs private Properties m Conf private boolean m Done private boolean m Load Only private Explain State m Explain private int m Num Failed Jobs private int m Num Succeeded Jobs private Fs Shell shell private boolean m Script Illustrate For Testing Only protected void set Explain State Explain State explain State this m Explain explain State 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Or Equal To Expr extends Binary Comparison Operator private static final long serial Version transient private final Log log Log Factory get Log get Class public Or Equal To Expr Operator Key k this k public Or Equal To Expr Operator Key k int rp super k rp result Type Data Type Override public String name return Greater Than or Equal Data Type find Type Name result Type m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Or Equal this Override public Result get Next Boolean throws Exec Exception Result left right switch operand Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type Result r accum Child null operand Type if r null return r left lhs get Next operand Type right rhs get Next operand Type return do Comparison left right default int err Code String msg this get Class get Simple Name does not know how to handle type Data Type find Type Name operand Type throw new Exec Exception msg err Code Pig Exception Suppress Warnings unchecked private Result do Comparison Result left Result right if left return Status Status return left if right return Status Status return right if either operand is null the result should be null if left result null right result null left result null left return Status Status return left assert left result instanceof Comparable assert right result instanceof Comparable if Comparable left result compare To right result left result Boolean else left result Boolean illustrator Markup null left result Boolean left result return left Override public Or Equal To Expr clone throws Clone Not Supported Exception Or Equal To Expr clone new Or Equal To Expr new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine shims import java io Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop mapred Job Conf import org apache hadoop mapreduce Context Factory import org apache hadoop mapreduce Job Context import org apache hadoop mapreduce Job import org apache hadoop mapreduce Task Attempt Context import org apache hadoop mapreduce Task Attempt import org apache hadoop mapreduce Task Type import org apache hadoop mapreduce task Job Context Impl import org apache hadoop mapreduce task Task Attempt Context Impl public class Hadoop Shims private static Log Log Factory get Log Hadoop Shims class static public Job Context clone Job Context Job Context original throws Exception Interrupted Exception Job Context new Context Context Factory clone Context original new Job Conf original get Configuration return new Context static public Task Attempt Context create Task Attempt Context Configuration conf Task Attempt task Id if conf instanceof Job Conf return new Task Attempt Context Impl new Job Conf conf task Id else return new Task Attempt Context Impl conf task Id static public Job Context create Job Context Configuration conf Job job Id if conf instanceof Job Conf return new Job Context Impl new Job Conf conf job Id else return new Job Context Impl conf job Id static public boolean is Map Task Attempt task Attempt Task Type type task Attempt get Task Type if type Task Type return true return false static public Task Attempt get New Task Attempt Task Attempt task Attempt new Task Attempt Task Type return task Attempt static public Task Attempt create Task Attempt String jt Identifier int job Id boolean is Map int task Id int id if is Map return new Task Attempt jt Identifier job Id Task Type task Id id else return new Task Attempt jt Identifier job Id Task Type task Id id Returns whether the give path has a File System implementation param path path param conf configuration return true if the give path s scheme has a File System implementation false otherwise public static boolean has File System Impl Path path Configuration conf String scheme path to Uri get Scheme if scheme null Hadoop if conf get fs file impl null String fs Impl conf get fs scheme impl if fs Impl null return false else try Object fs File System get File System Class scheme conf return fs null false true catch Exception e return false return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop hbase import java io Exception import java lang reflect Invocation Target Exception import java lang reflect Undeclared Throwable Exception import java math Big Decimal import java math Big Integer import java net Malformed Exception import java net import java util Array List import java util Arrays import java util Hash Map import java util List import java util Map import java util Map Entry import java util Navigable Map import java util Properties import org apache commons cli Command Line import org apache commons cli Command Line Parser import org apache commons cli Gnu Parser import org apache commons cli Help Formatter import org apache commons cli Option import org apache commons cli Option Builder import org apache commons cli Options import org apache commons cli Parse Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache hadoop hbase Base Configuration import org apache hadoop hbase client Delete import org apache hadoop hbase client Put import org apache hadoop hbase client Result import org apache hadoop hbase client Scan import org apache hadoop hbase filter Binary Comparator import org apache hadoop hbase filter Column Prefix Filter import org apache hadoop hbase filter Compare Filter Compare Op import org apache hadoop hbase filter Family Filter import org apache hadoop hbase filter Filter import org apache hadoop hbase filter Filter List import org apache hadoop hbase filter Qualifier Filter import org apache hadoop hbase filter Regex String Comparator import org apache hadoop hbase filter Row Filter import org apache hadoop hbase filter While Match Filter import org apache hadoop hbase io Immutable Bytes Writable import org apache hadoop hbase mapreduce Table Input Format import org apache hadoop hbase mapreduce Table Map Reduce Util import org apache hadoop hbase mapreduce Table Output Format import org apache hadoop hbase mapreduce Table Split import org apache hadoop hbase security token Token Util import org apache hadoop hbase util Bytes import org apache hadoop io Writable Comparable import org apache hadoop mapred Job Conf import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop security User Group Information import org apache pig Collectable Load Func import org apache pig Load Caster import org apache pig Load Func import org apache pig Load Push Down import org apache pig Load Store Caster import org apache pig Ordered Load Func import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Store Func Interface import org apache pig Store Resources import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig backend hadoop hbase Base Table Input Format Base Table Builder import org apache pig builtin Func Utils import org apache pig builtin Utf Storage Converter import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Object Serializer import org apache pig impl util Context import org apache pig impl util Utils import org joda time Date Time import com google common collect Lists Base implementation of Load Func and Store Func Below is an example showing how to load data from Base pre code raw hbase Sample Table org apache pig backend hadoop hbase Base Storage info first name info last name friends info load Key true limit id bytearray first name chararray last name chararray friends map map info map map pre This example loads data redundantly from the info column family just to illustrate usage Note that the row key is inserted first in the result schema To load only column names that start with a given prefix specify the column name with a trailing For example passing code friends bob code to the constructor in the above example would cause only columns that start with i bob i to be loaded Note that when using a prefix like code friends bob code explicit Base filters are set for all columns and prefixes specified Querying Base with many filters can cause performance degredation This is typically seen when mixing one or more prefixed descriptors with a large list of columns In that case better perfomance will be seen by either loading the entire family via code friends code or by specifying explicit column descriptor names Below is an example showing how to store data into Base pre code copy raw hbase Sample Table Copy org apache pig backend hadoop hbase Base Storage info first name info last name friends info pre Note that will expect the first value in the tuple to be the row key Scalars values need to map to an explicit column descriptor and maps need to map to a column family name In the above examples the code friends code column family data from code Sample Table code will be written to a code buddies code column family in the code Sample Table Copy code table public class Base Storage extends Load Func implements Store Func Interface Load Push Down Ordered Load Func Store Resources Collectable Load Func private static final Log Log Factory get Log Base Storage class private final static String Storage Converter private final static String Base Binary Converter private final static String pig hbase caster private final static String private final static String private final static String hbase security authentication private final static String hbase config set private final static String hbase token set private List Column Info column Info Lists new Array List Use Job Conf to store hbase delegation token private Job Conf m conf private Record Reader reader private Record Writer writer private Table Output Format output Format null private Scan scan private String context Signature null private final Command Line configured Options private final static Options valid Options new Options private final static Command Line Parser parser new Gnu Parser private boolean load Row Key private String delimiter private boolean ignore Whitespace private final long limit private final boolean cache Blocks private final int caching private boolean no private final long min Timestamp private final long max Timestamp private final long timestamp private boolean include Timestamp private boolean include Tombstone protected transient byte gt protected transient byte gte protected transient byte lt protected transient byte lte private String regex private Load Caster caster private Resource Schema schema private Required Field List required Field List private static void populate Valid Options Option load Key Option Builder has Optional Args with Arg Name load Key with Long Opt load Key with Description Load Key create valid Options add Option load Key valid Options add Option gt true Records must be greater than this value binary double slash escaped valid Options add Option lt true Records must be less than this value binary double slash escaped valid Options add Option gte true Records must be greater than or equal to this value valid Options add Option lte true Records must be less than or equal to this value valid Options add Option regex true Record must match this regular expression valid Options add Option cache Blocks true Set whether blocks should be cached for the scan valid Options add Option caching true Number of rows scanners should cache valid Options add Option limit true Per region limit valid Options add Option max Results Per Column Family true Limit the maximum number of values returned per row per column family valid Options add Option delim true Column delimiter valid Options add Option ignore Whitespace true Ignore spaces when parsing columns valid Options add Option caster true Caster to use for converting values class name Base Binary Converter or Utf Storage Converter For storage casters must implement Load Store Caster Option no Wal Option Builder has Optional Args with Arg Name no with Long Opt no with Description Sets the write ahead to false for faster loading To be used with extreme caution since this could result in data loss see http hbase apache org book html perf hbase client putwal create valid Options add Option no Wal valid Options add Option min Timestamp true Record must have timestamp greater or equal to this value valid Options add Option max Timestamp true Record must have timestamp less then this value valid Options add Option timestamp true Record must have timestamp equal to this value valid Options add Option include Timestamp false Record will include the timestamp after the rowkey on store rowkey timestamp valid Options add Option include Tombstone false Record will include a tombstone marker on store after the row Key and timestamp if included rowkey timestamp tombstone Constructor Construct a Base Table Load Func and Store Func to load or store the cells of the provided columns param column List columnlist that is a presented string delimited by space and or commas To retreive all columns in a column family code Foo code specify a column as either code Foo code or code Foo code To fetch only columns in the that start with bar specify code Foo bar code The resulting tuple will always be the size of the number of tokens in code column List code Items in the tuple will be scalar values when a full column descriptor is specified or a map of column descriptors to values when a column family is specified throws Parse Exception when unable to parse arguments throws Exception public Base Storage String column List throws Parse Exception Exception this column List Constructor Construct a Base Table Load Func and Store Func to load or store param column List param opt String Loader options Known options ul li load Key true false Load the row key as the first column li gt min Key Val li lt max Key Val li gte min Key Val li lte max Key Val li regex match regex on Key Val li limit num Rows Per Region max number of rows to retrieve per region li max Results Per Column Family Limit the maximum number of values returned per row per column family li delim char delimiter to use when parsing column names default is space or comma li ignore Whitespace true false ignore spaces when parsing column names default true li cache Blocks true false Set whether blocks should be cached for the scan default false li caching num Rows number of rows to cache faster scans more memory li no true false Sets the write ahead to false for faster loading li min Timestamp Scan s timestamp for min time Range li max Timestamp Scan s timestamp for max time Range li timestamp Scan s specified timestamp li include Timestamp Record will include the timestamp after the rowkey on store rowkey timestamp li include Tombstone Record will include a tombstone marker on store after the row Key and timestamp if included rowkey timestamp tombstone li caster Base Binary Converter Utf Storage Converter Utf Storage Converter is the default To be used with extreme caution since this could result in data loss see http hbase apache org book html perf hbase client putwal ul throws Parse Exception throws Exception public Base Storage String column List String opt String throws Parse Exception Exception populate Valid Options String opts Arr opt String split try configured Options parser parse valid Options opts Arr catch Parse Exception e Help Formatter formatter new Help Formatter formatter print Help load Key gt gte lt lte regex cache Blocks caching caster no limit max Results Per Column Family delim ignore Whitespace min Timestamp max Timestamp timestamp include Timestamp include Tombstone valid Options throw e load Row Key false if configured Options has Option load Key String value configured Options get Option Value load Key if true equals Ignore Case value equals Ignore Case value value null the empty string and null check is for backward compat load Row Key true delimiter if configured Options get Option Value delim null delimiter configured Options get Option Value delim ignore Whitespace true if configured Options has Option ignore Whitespace String value configured Options get Option Value ignore Whitespace if true equals Ignore Case value ignore Whitespace false column Info parse Column List column List delimiter ignore Whitespace In mr Context deserialize is first called and then Context get Context get Client System Props is called the value is not null In spark mode when spark executor first initializes all the object Context get Context get Client System Props is null and then Context deserialize is called so we need check whether Context get Context get Client System Props is null or not if is null default Caster otherwise is Context get Context get Client System Props get Property Detail see String default Caster Context get Context get Client System Props null Context get Context get Client System Props get Property String caster Option configured Options get Option Value caster default Caster if equals Ignore Case caster Option caster new Utf Storage Converter else if equals Ignore Case caster Option caster new Base Binary Converter else try caster Load Caster Pig Context instantiate Func From Spec caster Option catch Class Cast Exception e error Configured caster does not implement Load Caster interface throw new Exception e catch Runtime Exception e error Configured caster class not found e throw new Exception e debug Using caster caster get Class caching Integer value Of configured Options get Option Value caching cache Blocks Boolean value Of configured Options get Option Value cache Blocks false limit Long value Of configured Options get Option Value limit no false if configured Options has Option no String value configured Options get Option Value no if true equals Ignore Case value equals Ignore Case value value null the empty string and null check is for backward compat no true if configured Options has Option min Timestamp min Timestamp Long parse Long configured Options get Option Value min Timestamp else min Timestamp if configured Options has Option max Timestamp max Timestamp Long parse Long configured Options get Option Value max Timestamp else max Timestamp Long if configured Options has Option timestamp timestamp Long parse Long configured Options get Option Value timestamp else timestamp include Timestamp false if configured Options has Option include Timestamp String value configured Options get Option Value include Timestamp if true equals Ignore Case value equals Ignore Case value value null the empty string and null check is for backward compat include Timestamp true include Tombstone false if configured Options has Option include Tombstone String value configured Options get Option Value include Tombstone if true equals Ignore Case value equals Ignore Case value value null include Tombstone true init Scan Returns Properties based on code context Signature code private Properties get Properties return Context get Context get Properties this get Class new String context Signature return code context Signature projected Fields code private String projected Fields Name return context Signature projected Fields param column List param delimiter param ignore Whitespace return private List Column Info parse Column List String column List String delimiter boolean ignore Whitespace List Column Info column Info new Array List Column Info Default behavior is to allow combinations of spaces and delimiter which defaults to a comma Setting to not ignore whitespace will include the whitespace in the columns names String col Names column List split delimiter if ignore Whitespace List String columns new Array List String for String col Name col Names String sub Col Names col Name split for String sub Col Name sub Col Names sub Col Name sub Col Name trim if sub Col Name length columns add sub Col Name col Names columns to Array new String columns size for String col Name col Names column Info add new Column Info col Name return column Info private void init Scan throws Exception scan new Scan scan set Cache Blocks cache Blocks scan set Caching caching Set filters if any if configured Options has Option gt gt Bytes to Bytes Binary Utils slashisize configured Options get Option Value gt add Row Filter Compare Op gt scan set Start Row gt if configured Options has Option lt lt Bytes to Bytes Binary Utils slashisize configured Options get Option Value lt add Row Filter Compare Op lt scan set Stop Row lt if configured Options has Option gte gte Bytes to Bytes Binary Utils slashisize configured Options get Option Value gte scan set Start Row gte if configured Options has Option lte lte Bytes to Bytes Binary Utils slashisize configured Options get Option Value lte byte lt increment lte if is Debug Enabled debug String format Incrementing lte value of s from bytes s to s to set stop row Bytes to String lte to String lte to String lt if lt null scan set Stop Row increment lte The While Match Filter will short circuit the scan after we no longer match The set Stop Row call will limit the number of regions we need to scan add Filter new While Match Filter new Row Filter Compare Op new Binary Comparator lte if configured Options has Option regex regex Utils slashisize configured Options get Option Value regex add Filter new Row Filter Compare Op new Regex String Comparator regex if configured Options has Option min Timestamp configured Options has Option max Timestamp scan set Time Range min Timestamp max Timestamp if configured Options has Option timestamp scan set Time Stamp timestamp if configured Options has Option max Results Per Column Family int max Results Per Column Family Integer value Of configured Options get Option Value max Results Per Column Family scan set Max Results Per Column Family max Results Per Column Family if the group of column Infos for this family does n t contain a prefix we do n t need to set any filters we can just call add Column or add Family See javadocs below boolean column Prefix Exists false for Column Info column Info column Info if column Info get Column Prefix null column Prefix Exists true break if column Prefix Exists add Filters Without Column Prefix column Info else add Filters With Column Prefix column Info If there is no column with a prefix we do n t need filters we can just call add Column and add Family on the scan private void add Filters Without Column Prefix List Column Info column Infos Need to check for mixed types in a family so we do n t call add Column after add Family on the same family Map String List Column Info grouped Map group By Family column Infos for Entry String List Column Info entry Set grouped Map entry Set boolean only Columns true for Column Info column Info entry Set get Value if column Info is Column Map only Columns false break if only Columns for Column Info column Info entry Set get Value if is Debug Enabled debug Adding column to scan via add Column with cf name Bytes to String column Info get Column Family Bytes to String column Info get Column Name scan add Column column Info get Column Family column Info get Column Name else String family entry Set get Key if is Debug Enabled debug Adding column family to scan via add Family with cf name family scan add Family Bytes to Bytes family If we have a qualifier with a prefix and a wildcard i e cf foo we need a filter on every possible column to be returned as shown below This will become very inneficient for long lists of columns mixed with a prefixed wildcard Filter List must pass of Family Filter a must pass Filter List of either Qualifier or Column Prefix Filter If we have only column family filters i e cf or explicit column descriptors i e cf foo or a mix of both then we do n t need filters since the scan will take care of that private void add Filters With Column Prefix List Column Info column Infos we need to apply a column list filter for each family Filter List all Column Filters null Map String List Column Info grouped Map group By Family column Infos for String cf String grouped Map key Set List Column Info column Info List grouped Map get cf String byte cf Bytes to Bytes cf String all filters roll up to one parent filter if all Column Filters null all Column Filters new Filter List Filter List Operator each group contains a column family filter all and an one of of the column filters Filter List this Column Group Filter new Filter List Filter List Operator this Column Group Filter add Filter new Family Filter Compare Op new Binary Comparator cf Filter List column Filters new Filter List Filter List Operator for Column Info col Info column Info List if col Info is Column Map if is Debug Enabled debug Adding family prefix filters with values Bytes to String col Info get Column Family Bytes to String col Info get Column Prefix add a Prefix Filter to the list of column filters if col Info get Column Prefix null column Filters add Filter new Column Prefix Filter col Info get Column Prefix else if is Debug Enabled debug Adding family descriptor filters with values Bytes to String col Info get Column Family Bytes to String col Info get Column Name add a Qualifier Filter to the list of column filters column Filters add Filter new Qualifier Filter Compare Op new Binary Comparator col Info get Column Name if column Filters get Filters size this Column Group Filter add Filter column Filters all Column Filters add Filter this Column Group Filter if all Column Filters null add Filter all Column Filters private void add Row Filter Compare Op op byte val if is Debug Enabled debug Adding filter op to String with value Bytes to String Binary val add Filter new Row Filter op new Binary Comparator val private void add Filter Filter filter Filter List scan Filter Filter List scan get Filter if scan Filter null scan Filter new Filter List Filter List Operator scan Filter add Filter filter scan set Filter scan Filter Returns the Column Info list so external objects can inspect it return List of Column Info objects public List Column Info get Column Info List return column Info Updates the Column Info List Use this if you need to implement custom projections protected void set Column Info List List Column Info column Info List this column Info column Info List Stores the required Fields List as a serialized object so it can be fetched on the cluster If you plan to overwrite push Projection you need to call this with the required Field List so it they can be accessed on the cluster protected void store Projected Field Names Required Field List required Field List throws Frontend Exception try get Properties set Property projected Fields Name Object Serializer serialize required Field List catch Exception e throw new Frontend Exception e Override public Tuple get Next throws Exception try if reader next Key Value Immutable Bytes Writable row Key Immutable Bytes Writable reader get Current Key Result result Result reader get Current Value int tuple Size column Info size use a map of families qualifiers with the most recent version of the cell Fetching multiple vesions could be a useful feature Navigable Map byte Navigable Map byte byte results Map result get No Version Map if load Row Key tuple Size Tuple tuple Tuple Factory get Instance new Tuple tuple Size int start Index if load Row Key tuple set new Data Byte Array row Key get start Index for int i i column Info size i int current Index start Index i Column Info column Info column Info get i if column Info is Column Map It s a column family so we need to iterate and set all values found Navigable Map byte byte cf Results results Map get column Info get Column Family Map String Data Byte Array cf Map new Hash Map String Data Byte Array if cf Results null for byte quantifier cf Results key Set We need to check against the prefix filter to see if this value should be included We ca n t just rely on the server side filter since a user could specify multiple filters for the same if column Info get Column Prefix null column Info has Prefix Match quantifier byte cell cf Results get quantifier Data Byte Array value cell null null new Data Byte Array cell cf Map put Bytes to String quantifier value tuple set current Index cf Map else It s a column so set the value byte cell result get Value column Info get Column Family column Info get Column Name Data Byte Array value cell null null new Data Byte Array cell tuple set current Index value if is Debug Enabled for int i i tuple size i debug tuple value tuple get i return tuple catch Interrupted Exception e throw new Exception e return null Override public Input Format get Input Format Table Input Format input Format new Base Table Builder with Limit limit with Gt gt with Gte gte with Lt lt with Lte lte with Conf m conf build input Format set Scan scan return input Format Override public void prepare To Read Record Reader reader Pig Split split this reader reader Override public void set Context Signature String signature this context Signature signature Override public void set Location String location Job job throws Exception Properties udf Props get Properties job get Configuration set Boolean pig no Split Combination true m conf initialize Local Job Config job String delegation Token Set udf Props get Property if delegation Token Set null add Base Delegation Token m conf job udf Props set Property true String tablename location if location starts With hbase tablename location substring m conf set Table Input Format tablename String projected Fields udf Props get Property projected Fields Name if projected Fields null update column Info push Projection Required Field List Object Serializer deserialize projected Fields add Filters Without Column Prefix column Info if required Field List null Properties p Context get Context get Properties this get Class new String context Signature p set Property context Signature projected Fields Object Serializer serialize required Field List Override public List String get Ship Files Depend on Base to do the right thing when available as of try Configuration conf new Configuration Table Map Reduce Util add Base Dependency Jars conf if conf get tmpjars null String tmpjars conf get Strings tmpjars List String ship Files new Array List String tmpjars length for String tmpjar tmpjars ship Files add new tmpjar get Path return ship Files catch Exception e if e instanceof Malformed Exception debug Table Map Reduce Utils add Base Dependency Jars tmpjars had malformed url Falling back to previous logic e else debug Table Map Reduce Utils add Base Dependency Jars invocation failed Falling back to previous logic e List Class class List new Array List Class class List add org apache hadoop hbase client Table class main hbase jar or hbase client class List add org apache hadoop hbase mapreduce Table Split class main hbase jar or hbase server class List add org apache zookeeper Zoo Keeper class zookeeper Additional jars that are specific to v add Class To List org cloudera htrace Trace class List htrace add Class To List org apache hadoop hbase protobuf generated Base Protos class List hbase protocol add Class To List org apache hadoop hbase Table Name class List hbase common add Class To List org apache hadoop hbase Compatibility Factory class List hbase hadoop compat add Class To List org jboss netty channel Channel Factory class List netty return Func Utils get Ship Files class List private void add Class To List String class Name List Class class List try Class klass Class for Name class Name class List add klass catch Class Not Found Exception e debug Skipping adding jar for class class Name private Job Conf initialize Local Job Config Job job Properties udf Props get Properties Configuration job Conf job get Configuration Job Conf local Conf new Job Conf job Conf if udf Props contains Key for Entry Object Object entry udf Props entry Set local Conf set String entry get Key String entry get Value else Configuration hbase Conf Base Configuration create for Entry String String entry hbase Conf Job Conf may have some conf overriding ones in hbase site xml So only copy hbase config not in job config to Context Also avoids copying core default xml and core site xml props in hbase Conf to Context which would be redundant if job Conf get entry get Key null udf Props set Property entry get Key entry get Value local Conf set entry get Key entry get Value udf Props set Property true return local Conf Get delegation token from hbase and add it to the Job Suppress Warnings rawtypes unchecked private void add Base Delegation Token Configuration hbase Conf Job job if Context get Context is Frontend return if kerberos equals Ignore Case hbase Conf get try User Group Information current User User Group Information get Current User if current User has Kerberos Credentials Token Util obtain Token For Job hbase Conf current User job else info Not fetching hbase delegation token as no Kerberos is available catch Runtime Exception re throw re catch Exception e throw new Undeclared Throwable Exception e Unexpected error calling Token Util obtain Token For Job Override public String relative To Absolute Path String location Path cur Dir throws Exception return location Set up the caster to use for reading values out of and writing to Base Override public Load Caster get Load Caster throws Exception return caster Store Func Methods see org apache pig Store Func Interface get Output Format Override public Output Format get Output Format throws Exception if output Format null if m conf null throw new Illegal State Exception set Store Location has not been called else this output Format new Table Output Format this output Format set Conf m conf return output Format Override public void check Schema Resource Schema s throws Exception if caster instanceof Load Store Caster error Caster must implement Load Store Caster for writing to Base throw new Exception Bad Caster caster get Class schema s get Properties set Property context Signature schema Object Serializer serialize schema Suppressing unchecked warnings for Record Writer which is not parameterized by Store Func Interface Override public void prepare To Write Suppress Warnings rawtypes Record Writer writer throws Exception this writer writer Suppressing unchecked warnings for Record Writer which is not parameterized by Store Func Interface Suppress Warnings unchecked Override public void put Next Tuple t throws Exception Resource Field Schema field Schemas schema null null schema get Fields byte type field Schemas null Data Type find Type t get field Schemas get Type long ts int start Index if include Timestamp byte timestamp Type field Schemas null Data Type find Type t get start Index field Schemas start Index get Type Load Store Caster caster Load Store Caster caster switch timestamp Type case Data Type ts caster bytes To Long Data Byte Array t get start Index get break case Data Type ts Long t get start Index long Value break case Data Type ts Date Time t get start Index get Millis break default throw new Exception Unable to find a converter for timestamp field t get start Index start Index else ts System current Time Millis check for deletes if include Tombstone if Boolean t get start Index boolean Value Delete delete create Delete t get type ts try this is a delete so there will be no put and we are done here writer write null delete return catch Interrupted Exception e throw new Exception e start Index Put put create Put t get type if is Debug Enabled debug put Next disabled no for Column Info column Info column Info debug put Next col column Info for int i start Index i t size i Column Info column Info column Info get i start Index if is Debug Enabled debug put Next tuple i value t get i cf column column Info if column Info is Column Map put add column Info get Column Family column Info get Column Name ts obj To Bytes t get i field Schemas null Data Type find Type t get i field Schemas i get Type else Map String Object cf Map Map String Object t get i if cf Map null for String col Name cf Map key Set if is Debug Enabled debug put Next col Name col Name class col Name get Class deal with the fact that maps can have types now Currently we detect types at runtime in the case of storing to a cf which is suboptimal put add column Info get Column Family Bytes to Bytes col Name to String ts obj To Bytes cf Map get col Name Data Type find Type cf Map get col Name try if put is Empty writer write null put catch Interrupted Exception e throw new Exception e Public method to initialize a Delete param key param type param timestamp return new delete throws Exception public Delete create Delete Object key byte type long timestamp throws Exception Delete delete new Delete obj To Bytes key type delete set Timestamp timestamp if no delete set Write To false return delete Public method to initialize a Put Used to allow assertions of how Puts are initialized by unit tests param key param type return new put throws Exception public Put create Put Object key byte type throws Exception Put put new Put obj To Bytes key type if no put set Write To false return put Suppress Warnings unchecked private byte obj To Bytes Object o byte type throws Exception Load Store Caster caster Load Store Caster caster if o null return null switch type case Data Type return Data Byte Array o get case Data Type return caster to Bytes Data Bag o case Data Type return caster to Bytes String o case Data Type return caster to Bytes Double o case Data Type return caster to Bytes Float o case Data Type return caster to Bytes Integer o case Data Type return caster to Bytes Long o case Data Type return caster to Bytes Big Integer o case Data Type return caster to Bytes Big Decimal o case Data Type return caster to Bytes Boolean o case Data Type return caster to Bytes Date Time o The type conversion here is unchecked Relying on Data Type find Type to do the right thing case Data Type return caster to Bytes Map String Object o case Data Type return null case Data Type return caster to Bytes Tuple o case Data Type throw new Exception Unable to determine type of o get Class default throw new Exception Unable to find a converter for tuple field o Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return location Override public void set Store Func Context Signature String signature this context Signature signature Override public void set Store Location String location Job job throws Exception if location starts With hbase job get Configuration set Table Output Format location substring else job get Configuration set Table Output Format location String serialized Schema get Properties get Property context Signature schema if serialized Schema null schema Resource Schema Object Serializer deserialize serialized Schema m conf initialize Local Job Config job Not setting a udf property and getting the hbase delegation token only once like in set Location as set Store Location gets different Job objects for each call and the last Job passed is the one that is launched So we end up getting multiple hbase delegation tokens add Base Delegation Token m conf job Override public void cleanup On Failure String location Job job throws Exception Override public void cleanup On Success String location Job job throws Exception Load Push Down Methods Override public List Operator Set get Features return Arrays as List Load Push Down Operator Set Override public Required Field Response push Projection Required Field List required Field List throws Frontend Exception List Required Field required Fields required Field List get Fields List Column Info new Columns Lists new Array List With Expected Size required Fields size if this required Field List null in addition to this is also called by this set Location debug projection is already set skipping return new Required Field Response true How projection is handled push Projection is invoked by on the front end push Projection here both stores serialized projection in the context and adjusts column Info set Location is invoked on the backend and it reads the projection from context set Location invokes this method again so that column Info is adjected col Offset is the offset in our column List that we need to apply to indexes we get from required Fields row key is not a real column int col Offset load Row Key proj Offset is the offset to the required Field List we need to apply when figuring out which columns to prune if key is pruned we should skip row key s element in this list when trimming col List int proj Offset col Offset this required Field List required Field List if required Field List null required Fields size column Info size col Offset throw new Frontend Exception The list of columns to project from Base required Fields size is larger than Base Storage is configured to load column Info size col Offset remember the projection store Projected Field Names required Field List if load Row Key required Fields size required Fields get get Index load Row Key false proj Offset for int i proj Offset i required Fields size i int field Index required Fields get i get Index new Columns add column Info get field Index col Offset if is Debug Enabled debug push Projection After Projection load Row Key is load Row Key for Column Info col Info new Columns debug push Projection col col Info set Column Info List new Columns return new Required Field Response true Override public void ensure All Key Instances In Same Split throws Exception no op because hbase keys are unique This will also work with things like Delimited Key Prefix Region Split Policy if you need a partial key match to be included in the split debug ensure All Key Instances In Same Split Override public Writable Comparable Table Split get Split Comparable Input Split split throws Exception if split instanceof Table Split return new Table Split Comparable Table Split split else throw new Runtime Exception Load Func expected split of type Table Split but was split get Class get Name Class to encapsulate logic around which column names were specified in each position of the column list Users can specify columns names in one of ways Foo Foo Foo bar or Foo bar The first result in a Map being added to the tuple while the last results in a scalar The rd form results in a prefix filtered Map public class Column Info final String original Column Name always set final byte column Family always set final byte column Name set if it exists and does n t contain final byte column Prefix set if contains a prefix followed by public Column Info String col Name original Column Name col Name String cf And Column col Name split Fs are byte and columns are byte column Family Bytes to Bytes cf And Column if cf And Column length cf And Column length equals cf And Column if cf And Column ends With column Prefix Bytes to Bytes cf And Column substring cf And Column length column Name null else column Name Bytes to Bytes cf And Column column Prefix null else column Prefix null column Name null public byte get Column Family return column Family public byte get Column Name return column Name public byte get Column Prefix return column Prefix public boolean is Column Map return column Name null public boolean has Prefix Match byte qualifier return Bytes starts With qualifier column Prefix Override public String to String return original Column Name Group the list of Column Info objects by their column family and returns a map of to its list of Column Info objects Using String as key since it implements Comparable param column Infos the column Info list to group return a Map of lists keyed by their column family static Map String List Column Info group By Family List Column Info column Infos Map String List Column Info grouped Map new Hash Map String List Column Info for Column Info column Info column Infos String cf Bytes to String column Info get Column Family List Column Info column Info List grouped Map get cf if column Info List null column Info List new Array List Column Info column Info List add column Info grouped Map put cf column Info List return grouped Map static String to String byte bytes if bytes null return null String Buffer sb new String Buffer for int i i bytes length i if i sb append sb append bytes i return sb to String Increments the byte array by one for use with setting stop Row If all bytes in the array are set to the maximum byte value then the original array will be returned with a byte appended to it This is because Base compares bytes from left to right If byte array is equal to byte array but with an extra byte appended will be For example code byte code increments to code byte code and code code param bytes array to increment bytes on return a copy of the byte array incremented by static byte increment byte bytes boolean all At Max true for int i i bytes length i if bytes bytes length i x ff all At Max false break if all At Max return Arrays copy Of bytes bytes length byte incremented bytes clone for int i bytes length i i boolean carry false int val bytes i x ff int total val if total carry true total else if total carry true incremented i byte total if carry return incremented return incremented 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine import java io File import java io Exception import java io Print Stream import java net import java util Iterator import java util Map import java util Map Entry import java util Properties import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop hdfs Distributed File System import org apache hadoop mapred Job Conf import org apache pig Pig Exception import org apache pig backend Backend Exception import org apache pig backend datastorage Data Storage import org apache pig backend executionengine Exec Exception import org apache pig backend executionengine Execution Engine import org apache pig backend hadoop Kerberos import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop datastorage Data Storage import org apache pig backend hadoop executionengine fetch Fetch Launcher import org apache pig backend hadoop executionengine fetch Fetch Optimizer import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig backend hadoop executionengine map Reduce Layer Phy Plan Setter import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine util Map Red Util import org apache pig backend hadoop streaming Hadoop Executable Manager import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl streaming Executable Manager import org apache pig impl util Utils import org apache pig newplan Operator import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Log To Phy Translation Visitor import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig tools pigstats Pig Stats import com google common collect Maps public abstract class Execution Engine implements Execution Engine private static final Log Log Factory get Log Execution Engine class public static final String hadoop site xml public static final String core site xml public static final String yarn site xml public static final String core default xml public static final String mapred default xml public static final String yarn default xml public static final String local protected Pig Context pig Context protected Data Storage ds protected Launcher launcher key the operator key from the logical plan that originated the physical plan val the operator key for the root of the phyisical plan protected Map Operator Key Operator Key logical To Physical Keys protected Map Operator Physical Operator new Log To Phy Map public Execution Engine Pig Context pig Context this pig Context pig Context this ds null this logical To Physical Keys Maps new Hash Map Deprecated public Job Conf get Job Conf Job Conf jc new Job Conf false Utils recompute Properties jc pig Context get Properties return jc Override public Data Storage get Data Storage return this ds Override public void init throws Exec Exception init this pig Context get Properties Loads properties from core site xml including aws keys that are needed for both local and non local mode public Job Conf get Conf throws Exec Exception Job Conf jc new Job Conf jc add Resource Job Conf s Jc new Job Conf false Iterator Entry String String i jc iterator while i has Next Entry String String e i next String key e get Key String value e get Value if key starts With fs s key starts With fs s n s Jc set key value return s Jc public Job Conf get Local Conf Job Conf jc new Job Conf false jc add Resource jc add Resource jc add Resource return jc public Job Conf get Exec Conf Properties properties throws Exec Exception Job Conf jc null Check existence of user provided configs String is Hadoop Configs Overriden properties get Property pig use overriden hadoop configs if is Hadoop Configs Overriden null is Hadoop Configs Overriden equals true jc new Job Conf Configuration Util to Configuration properties else Check existence of hadoop site xml or core site xml in classpath if user provided confs are not being used Configuration test Conf new Configuration Class Loader cl test Conf get Class Loader hadoop site cl get Resource core site cl get Resource if hadoop site null core site null throw new Exec Exception Can not find hadoop configurations in classpath neither hadoop site xml nor core site xml was found in the classpath If you plan to use local mode please put x local option in command line jc new Job Conf jc add Resource pig cluster hadoop site xml jc add Resource return jc Suppress Warnings resource private void init Properties properties throws Exec Exception String cluster null String name Node null We need to build a configuration object first in the manner described below and then get back a properties object to inspect the and The reason to do this is if we looked only at the existing properties object we may not get the right settings So we want to read the configurations in the order specified below and only then look for and Hadoop by default specifies two resources loaded in order from the classpath hadoop default xml Read only defaults for hadoop hadoop site xml Site specific configuration for a given hadoop installation Now add the settings from properties object to override any existing properties All of the above is accomplished in the method call below Job Conf jc if this pig Context get Exec Type is Local jc get Exec Conf properties Trick to invoke static initializer of Distributed File System to add hdfs default xml into configuration new Distributed File System else If we are running in local mode we dont read the hadoop conf file if properties get Property Configuration null properties set Property Configuration properties set Property Configuration properties remove fs default name Deprecated in Hadoop x properties set Property File System file jc get Local Conf Job Conf s Jc get Conf Configuration Util merge Conf jc s Jc the method below alters the properties object by overriding the hadoop properties with the values from properties and recomputing the properties Utils recompute Properties jc properties Ensure we have been logged in using the kerberos keytab if provided before continuing Kerberos try Kerberos Keytab Login jc cluster jc get Configuration name Node jc get File System info Connecting to hadoop file system at name Node null name Node constructor sets ds new Data Storage properties if cluster null cluster equals Ignore Case info Connecting to map reduce job tracker at jc get Configuration public Physical Plan compile Logical Plan plan Properties properties throws Frontend Exception if plan null int err Code String msg No Plan to compile throw new Frontend Exception msg err Code Pig Exception translate new logical plan to physical plan Log To Phy Translation Visitor translator new Log To Phy Translation Visitor plan translator set Pig Context pig Context translator visit new Log To Phy Map translator get Log To Phy Map return translator get Physical Plan public Map Operator Physical Operator get Log To Phy Map return new Log To Phy Map public Map For Each Map Logical Relational Operator Physical Operator get For Each Inner Log To Phy Map Logical Plan plan Map For Each Map Logical Relational Operator Physical Operator result Maps new Hash Map Iterator Operator outer Iter plan get Operators while outer Iter has Next Operator oper outer Iter next if oper instanceof For Each Logical Plan inner Plan For Each oper get Inner Plan Map Logical Relational Operator Physical Operator inner Op Map Maps new Hash Map Iterator Operator inner Iter inner Plan get Operators while inner Iter has Next Operator inner Oper inner Iter next inner Op Map put Logical Relational Operator inner Oper new Log To Phy Map get inner Oper result put For Each oper inner Op Map return result Override public Pig Stats launch Pig Logical Plan lp String grp Name Pig Context pc throws Frontend Exception Exec Exception try Physical Plan pp compile lp pc get Properties if the compiled physical plan fulfills the requirements of the fetch optimizer then further transformations jobs creations are skipped a Simple Fetch Pig Stats will be returned through which the result can be directly fetched from the underlying storage if Fetch Optimizer is Plan Fetchable pc pp new Phy Plan Setter pp visit return new Fetch Launcher pc launch Pig pp return launcher launch Pig pp grp Name pig Context catch Exec Exception e throw Exec Exception e catch Frontend Exception e throw Frontend Exception e catch Exception e throw new Exec Exception e finally launcher reset Override public void explain Logical Plan lp Pig Context pc Print Stream ps String format boolean verbose File file String suffix throws Plan Exception Visitor Exception Exception Frontend Exception Print Stream pps ps Print Stream eps ps boolean is Fetchable false try if file null pps new Print Stream new File file physical plan suffix eps new Print Stream new File file exec plan suffix Physical Plan pp compile lp pc get Properties pp explain pps format verbose Map Red Util check Leaf Is Store pp pig Context is Fetchable Fetch Optimizer is Plan Fetchable pc pp if is Fetchable new Fetch Launcher pig Context explain pp pc eps format return launcher explain pp pig Context eps format verbose finally launcher reset if is Fetchable pig Context get Properties remove Pig Impl Constants Only close the stream if we opened it if file null pps close eps close Override public Properties get Configuration Properties properties new Properties properties put All pig Context get Properties return properties Override public void set Configuration Properties new Configuration throws Exec Exception init new Configuration Override public void set Property String property String value Properties properties pig Context get Properties if Configuration is Deprecated property properties put All Configuration Util expand For Alternative Names property value else properties put property value Override public Executable Manager get Executable Manager return new Hadoop Executable Manager Override public void kill throws Backend Exception if launcher null launcher kill Override public void kill Job String job throws Backend Exception if launcher null launcher kill Job job get Job Conf Override public void destroy if launcher null launcher destroy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util List import org apache hadoop conf Configuration import org apache hadoop hive conf Hive Conf import org apache hadoop hive conf Hive Conf Conf Vars import org apache hadoop hive ql exec Function Info import org apache hadoop hive ql exec Function Registry import org apache hadoop hive ql exec Mapred Context import org apache hadoop hive ql exec import org apache hadoop hive ql exec import org apache hadoop hive ql parse Semantic Exception import org apache hadoop hive ql udf generic Collector import org apache hadoop hive ql udf generic Generic Resolver import org apache hadoop hive ql udf generic Generic import org apache hadoop hive ql udf generic Generic import org apache hadoop hive serde objectinspector Constant Object Inspector import org apache hadoop hive serde objectinspector Primitive Object Inspector import org apache hadoop hive serde objectinspector Struct Field import org apache hadoop hive serde objectinspector Struct Object Inspector import org apache hadoop hive shims Hadoop Shims import org apache hadoop hive shims Hadoop Shims Secure import org apache hadoop hive shims Shim Loader import org apache hadoop mapred Counters import org apache hadoop mapred Counters Counter import org apache hadoop mapred Input Split import org apache hadoop mapred Job Conf import org apache hadoop mapred Reporter import org apache pig Eval Func import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl util Context import org apache pig impl util Utils import org apache pig impl util hive Hive Utils import org apache pig tools pigstats Pig Status Reporter import com esotericsoftware kryo Serializer abstract class Hive Base extends Eval Func Object static protected class Constant Object Inspect Info Constant Object Inspector constants static Constant Object Inspect Info parse String params throws Exception Constant Object Inspect Info info new Constant Object Inspect Info params params replace All Object constant Utils parse Constant params if Data Type find Type constant Data Type Tuple t Tuple constant info constants new Constant Object Inspector t size for int i i t size i if t get i null info constants i Hive Utils get Constant Object Inspector t get i else info constants new Constant Object Inspector info constants Hive Utils get Constant Object Inspector constant return info boolean is Empty return constants null int size return constants length Constant Object Inspector get int i return constants i void inject Constant Object Inspector Struct Object Inspector input Object Inspector if is Empty for int i i size i if get i null Struct Field orig Field input Object Inspector get All Struct Field Refs get i Struct Field newfield new Hive Utils Field orig Field get Field Name get i i List Hive Utils Field input Object Inspector get All Struct Field Refs set i Hive Utils Field newfield static protected Class resolve Func String func Name throws Exception String class Name func Name Class udf Class if Function Registry get Function Names contains func Name Function Info func try func Function Registry get Function Info func Name catch Semantic Exception e throw new Exception e udf Class func get Function Class else udf Class Pig Context resolve Class Name class Name if udf Class null throw new Exception Can not find Hive func Name return udf Class constant of Reporter type that does nothing static protected class Hive Reporter implements Reporter Pig Status Reporter rep Hive Reporter Pig Status Reporter rep this rep rep public void set Status String s rep set Status s public void progress rep progress public Counter get Counter Enum name try Counters counters new Counters counters incr Counter name rep get Counter name get Value return counters find Counter name catch Exception e throw new Runtime Exception e public Counter get Counter String group String name try Counters counters new Counters counters incr Counter group name rep get Counter group name get Value return counters find Counter group name catch Exception e throw new Runtime Exception e public void incr Counter Enum key long amount rep incr Counter key amount public void incr Counter String group String counter long amount rep incr Counter group counter amount public Input Split get Input Split throws Unsupported Operation Exception throw new Unsupported Operation Exception reporter has no input public float get Progress return protected static Mapred Context instantiate Mapred Context Configuration conf Context get Context get Job Conf boolean is Map conf get Boolean Configuration false if conf get exectype starts With is Map true Hive Conf set Var conf Conf Vars tez Mapred Context context Mapred Context init is Map new Job Conf Context get Context get Job Conf context set Reporter new Hive Reporter Pig Status Reporter get Instance return context Override public List String get Ship Files List String files Func Utils get Ship Files new Class Generic class Primitive Object Inspector class Hive Conf class Serializer class Shim Loader class Hadoop Shims class Hadoop Shims Secure class Collector class return files static protected String get Error Message Class c String Buffer message new String Buffer Please declare c get Name as if class is Assignable From c Generic class is Assignable From c message append Hive class get Name else if Generic class is Assignable From c message append Hive class get Name else if class is Assignable From c Generic Resolver class is Assignable From c message append Hive class get Name else message new String Buffer c get Name is not Hive return message to String 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache hadoop hive ql metadata Hive Exception import org apache hadoop hive ql udf generic Collector import org apache hadoop hive ql udf generic Generic import org apache hadoop hive serde objectinspector Object Inspector import org apache hadoop hive serde objectinspector Struct Object Inspector import org apache hadoop hive serde typeinfo Type Info import org apache hadoop hive serde typeinfo Type Info Utils import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer schema Schema import org apache pig impl util hive Hive Utils Use Hive Generic Example define explode Hive explode load mydata as a b chararray foreach generate flatten explode a public class Hive extends Hive Base private boolean inited false private Generic udtf private boolean end Of All Input false static class Schema Info Struct Object Inspector input Object Inspector Object Inspector output Object Inspector private void init Schema input Schema Generic udtf Constant Object Inspect Info constants Info throws Exception Resource Schema rs new Resource Schema input Schema Resource Field Schema wrapped Tuple Field Schema new Resource Field Schema wrapped Tuple Field Schema set Type Data Type wrapped Tuple Field Schema set Schema rs Type Info ti Hive Utils get Type Info wrapped Tuple Field Schema input Object Inspector Struct Object Inspector Hive Utils create Object Inspector ti if constants Info null constants Info inject Constant Object Inspector input Object Inspector try output Object Inspector udtf initialize input Object Inspector catch Exception e throw new Exception e Schema Info schema Info new Schema Info Constant Object Inspect Info constants Info private static Bag Factory bf Bag Factory get Instance private Hive Collector collector null public Hive String func Name throws Instantiation Exception Illegal Access Exception Exception Class hive Class resolve Func func Name if Generic class is Assignable From hive Class udtf Generic hive Class new Instance else throw new Exception get Error Message hive Class public Hive String func Name String params throws Instantiation Exception Illegal Access Exception Exception this func Name constants Info Constant Object Inspect Info parse params Override public Object exec Tuple input throws Exception if inited udtf configure instantiate Mapred Context schema Info init get Input Schema udtf constants Info inited true if collector null collector new Hive Collector udtf set Collector collector else collector init try if end Of All Input udtf process input get All to Array else udtf close catch Exception e throw new Exception e return collector get Bag Override public Schema output Schema Schema input try if inited schema Info init get Input Schema udtf constants Info inited true Resource Field Schema rfs Hive Utils get Resource Field Schema Type Info Utils get Type Info From Object Inspector schema Info output Object Inspector Resource Schema tuple Schema new Resource Schema tuple Schema set Fields new Resource Field Schema rfs Resource Field Schema bag Field Schema new Resource Field Schema bag Field Schema set Type Data Type bag Field Schema set Schema tuple Schema Resource Schema bag Schema new Resource Schema bag Schema set Fields new Resource Field Schema bag Field Schema return Schema get Pig Schema bag Schema catch Exception e throw new Runtime Exception e class Hive Collector implements Collector Data Bag bag bf new Default Bag public void init bag clear Override public void collect Object input throws Hive Exception try Tuple output Tuple Tuple Hive Utils convert Hive To Pig input schema Info output Object Inspector null if output Tuple size output Tuple get instanceof Tuple bag add Tuple output Tuple get else bag add output Tuple catch Exception e throw new Hive Exception e public Data Bag get Bag return bag Override public boolean need End Of All Input Processing return true Override public void set End Of All Input boolean end Of All Input this end Of All Input end Of All Input 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl util hive import java io Exception import java math Big Decimal import java math Big Integer import java sql Timestamp import java util Array List import java util Hash Map import java util Iterator import java util List import java util Map import org apache hadoop hive common type Hive Char import org apache hadoop hive common type Hive Decimal import org apache hadoop hive common type Hive Varchar import org apache hadoop hive serde io Hive Decimal Writable import org apache hadoop hive serde io Timestamp Writable import org apache hadoop hive serde objectinspector Constant Object Inspector import org apache hadoop hive serde objectinspector List Object Inspector import org apache hadoop hive serde objectinspector Map Object Inspector import org apache hadoop hive serde objectinspector Object Inspector import org apache hadoop hive serde objectinspector Primitive Object Inspector import org apache hadoop hive serde objectinspector Struct Field import org apache hadoop hive serde objectinspector Struct Object Inspector import org apache hadoop hive serde objectinspector primitive Abstract Primitive Java Object Inspector import org apache hadoop hive serde objectinspector primitive Binary Object Inspector import org apache hadoop hive serde objectinspector primitive Hive Decimal Object Inspector import org apache hadoop hive serde objectinspector primitive Primitive Object Inspector Factory import org apache hadoop hive serde objectinspector primitive Java Constant Boolean Object Inspector import org apache hadoop hive serde objectinspector primitive Java Constant Double Object Inspector import org apache hadoop hive serde objectinspector primitive Java Constant Float Object Inspector import org apache hadoop hive serde objectinspector primitive Java Constant Int Object Inspector import org apache hadoop hive serde objectinspector primitive Java Constant Long Object Inspector import org apache hadoop hive serde objectinspector primitive Java Constant String Object Inspector import org apache hadoop hive serde objectinspector primitive Timestamp Object Inspector import org apache hadoop hive serde objectinspector primitive Writable Constant Float Object Inspector import org apache hadoop hive serde typeinfo List Type Info import org apache hadoop hive serde typeinfo Map Type Info import org apache hadoop hive serde typeinfo Primitive Type Info import org apache hadoop hive serde typeinfo Struct Type Info import org apache hadoop hive serde typeinfo Type Info import org apache hadoop hive serde typeinfo Type Info Factory import org apache hadoop io Bytes Writable import org apache hadoop io Float Writable import org apache pig Pig Warning import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig backend executionengine Exec Exception import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig tools pigstats Pig Status Reporter import org joda time Date Time public class Hive Utils static Tuple Factory tf Tuple Factory get Instance public static Object convert Hive To Pig Object obj Object Inspector oi boolean included Columns Object result null if obj null return result switch oi get Category case Primitive Object Inspector poi Primitive Object Inspector oi result get Primary From Hive obj poi break case Struct Object Inspector soi Struct Object Inspector oi List Struct Field element Fields List Struct Field soi get All Struct Field Refs List Object items soi get Struct Fields Data As List obj Tuple t tf new Tuple for int i i items size i if included Columns null included Columns i Object converted Item convert Hive To Pig items get i element Fields get i get Field Object Inspector null t append converted Item result t break case Map Object Inspector moi Map Object Inspector oi Object Inspector key Object Inspector moi get Map Key Object Inspector Object Inspector value Object Inspector moi get Map Value Object Inspector Map Object Object m Map Object Object obj result new Hash Map for Map Entry Object Object entry m entry Set Object converted Key convert Hive To Pig entry get Key key Object Inspector null Object converted Value convert Hive To Pig entry get Value value Object Inspector null if converted Key null Map result put converted Key to String converted Value else Pig Status Reporter reporter Pig Status Reporter get Instance if reporter null reporter incr Counter Pig Warning break case List Object Inspector loi List Object Inspector oi result Bag Factory get Instance new Default Bag Object Inspector item Object Inspector loi get List Element Object Inspector for Object item loi get List obj Object converted Item convert Hive To Pig item item Object Inspector null Tuple inner Tuple Hive array contains a single item of any type if it is not tuple need to wrap it in tuple if converted Item instanceof Tuple inner Tuple Tuple converted Item else inner Tuple tf new Tuple try inner Tuple set converted Item catch Exception e throw new Runtime Exception e Data Bag result add inner Tuple break default throw new Illegal Argument Exception Unknown type oi get Category return result public static Object get Primary From Hive Object obj Primitive Object Inspector poi Object result null if obj null return result switch poi get Primitive Category case case case case case case result poi get Primitive Java Object obj break case result Hive Char poi get Primitive Java Object obj get Value break case result Hive Varchar poi get Primitive Java Object obj get Value break case result int Byte poi get Primitive Java Object obj break case result int Short poi get Primitive Java Object obj break case byte b byte poi get Primitive Java Object obj Make a copy result new Data Byte Array b b length break case java sql Timestamp orig Time Stamp java sql Timestamp poi get Primitive Java Object obj result new Date Time orig Time Stamp get Time break case java sql Date orig Date java sql Date poi get Primitive Java Object obj result new Date Time orig Date get Time break case org apache hadoop hive common type Hive Decimal orig Decimal org apache hadoop hive common type Hive Decimal poi get Primitive Java Object obj result orig Decimal big Decimal Value break default throw new Illegal Argument Exception Unknown primitive type poi get Primitive Category return result public static Resource Field Schema get Resource Field Schema Type Info ti throws Exception Resource Field Schema field Schema new Resource Field Schema Resource Field Schema inner Fs Resource Schema inner Schema switch ti get Category case Struct Type Info sti Struct Type Info ti field Schema set Type Data Type List Type Info type Infos sti get All Struct Field Type Infos List String names sti get All Struct Field Names inner Fs new Resource Field Schema type Infos size for int i i type Infos size i inner Fs i get Resource Field Schema type Infos get i inner Fs i set Name names get i inner Schema new Resource Schema inner Schema set Fields inner Fs field Schema set Schema inner Schema break case List Type Info lti List Type Info ti field Schema set Type Data Type inner Fs new Resource Field Schema Resource Field Schema item Schema get Resource Field Schema lti get List Element Type Info if item Schema get Type Data Type inner Fs item Schema else If item is not tuple wrap it into tuple Resource Field Schema tuple Field Schema new Resource Field Schema tuple Field Schema set Type Data Type Resource Schema tuple Schema new Resource Schema tuple Schema set Fields new Resource Field Schema item Schema inner Fs tuple Field Schema inner Schema new Resource Schema inner Schema set Fields inner Fs field Schema set Schema inner Schema break case Map Type Info mti Map Type Info ti field Schema set Type Data Type inner Fs new Resource Field Schema inner Fs get Resource Field Schema mti get Map Value Type Info inner Schema new Resource Schema inner Schema set Fields inner Fs field Schema set Schema inner Schema break case switch Primitive Type Info ti get Primitive Category case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break case field Schema set Type Data Type break default throw new Illegal Argument Exception Unknown primitive type Primitive Type Info ti get Primitive Category break return field Schema public static Type Info get Type Info Resource Field Schema fs throws Exception Type Info ti switch fs get Type case Data Type ti new Struct Type Info Array List String names new Array List String Array List Type Info type Infos new Array List Type Info for Resource Field Schema sub Fs fs get Schema get Fields Type Info info get Type Info sub Fs names add sub Fs get Name type Infos add info Struct Type Info ti set All Struct Field Names names Struct Type Info ti set All Struct Field Type Infos type Infos break case Data Type ti new List Type Info if fs get Schema null fs get Schema get Fields length throw new Exception Wrong bag inner schema Resource Field Schema tuple Schema fs get Schema get Fields Resource Field Schema item Schema tuple Schema If single item tuple remove the tuple put the inner item into list directly if tuple Schema get Schema get Fields length item Schema tuple Schema get Schema get Fields Type Info element Field get Type Info item Schema List Type Info ti set List Element Type Info element Field break case Data Type ti new Map Type Info Type Info value Field if fs get Schema null fs get Schema get Fields length value Field Type Info Factory binary Type Info else value Field get Type Info fs get Schema get Fields Map Type Info ti set Map Key Type Info Type Info Factory string Type Info Map Type Info ti set Map Value Type Info value Field break case Data Type ti Type Info Factory boolean Type Info break case Data Type ti Type Info Factory int Type Info break case Data Type ti Type Info Factory long Type Info break case Data Type ti Type Info Factory float Type Info break case Data Type ti Type Info Factory double Type Info break case Data Type ti Type Info Factory string Type Info break case Data Type ti Type Info Factory timestamp Type Info break case Data Type ti Type Info Factory decimal Type Info break case Data Type ti Type Info Factory decimal Type Info break case Data Type ti Type Info Factory binary Type Info break default throw new Illegal Argument Exception Unknown data type Data Type find Type Name fs get Type return ti static public class Field implements Struct Field private final String name private final Object Inspector inspector private final int offset public Field String name Object Inspector inspector int offset this name name this inspector inspector this offset offset Override public String get Field Name return name Override public Object Inspector get Field Object Inspector return inspector Override public int get Field return offset Override public String get Field Comment return null static class Pig Struct Inspector extends Struct Object Inspector private List Struct Field fields Pig Struct Inspector Struct Type Info info Array List String field Names info get All Struct Field Names Array List Type Info field Types info get All Struct Field Type Infos fields new Array List Struct Field field Names size for int i i field Names size i fields add new Field field Names get i create Object Inspector field Types get i i Pig Struct Inspector List Struct Field fields this fields fields Override public List Struct Field get All Struct Field Refs return fields Override public Struct Field get Struct Field Ref String s for Struct Field field fields if field get Field Name equals s return field return null Override public Object get Struct Field Data Object object Struct Field field Object result null try result Tuple object get Field field offset catch Exec Exception e throw new Runtime Exception e return result Override public List Object get Struct Fields Data As List Object object return Tuple object get All Override public String get Type Name String Builder buffer new String Builder buffer append struct for int i i fields size i Struct Field field fields get i if i buffer append buffer append field get Field Name buffer append buffer append field get Field Object Inspector get Type Name buffer append return buffer to String Override public Category get Category return Category Override public boolean equals Object o if o null o get Class get Class return false else if o this return true else List Struct Field other Pig Struct Inspector o fields if other size fields size return false for int i i fields size i Struct Field left other get i Struct Field right fields get i if left get Field Name equals right get Field Name left get Field Object Inspector equals right get Field Object Inspector return false return true static class Pig Map Object Inspector implements Map Object Inspector private Object Inspector key private Object Inspector value Pig Map Object Inspector Map Type Info info key Primitive Object Inspector Factory java String Object Inspector value create Object Inspector info get Map Value Type Info Override public Object Inspector get Map Key Object Inspector return key Override public Object Inspector get Map Value Object Inspector return value Override public Object get Map Value Element Object map Object key return Map map get key Override public Map Object Object get Map Object map return Map map Override public int get Map Size Object map return Map map size Override public String get Type Name return map key get Type Name value get Type Name Override public Category get Category return Category Override public boolean equals Object o if o null o get Class get Class return false else if o this return true else Pig Map Object Inspector other Pig Map Object Inspector o return other key equals key other value equals value static class Pig List Object Inspector implements List Object Inspector private Object Inspector child private Object cached Object private int index private Iterator Tuple iter Pig List Object Inspector List Type Info info child create Object Inspector info get List Element Type Info Override public Object Inspector get List Element Object Inspector return child Override public Object get List Element Object list int i if i list cached Object cached Object list index Data Bag db Data Bag list iter db iterator if i index index try Tuple t iter next If single item tuple take the item directly from list if t size return t get else return t catch Exception e throw new Runtime Exception e else throw new Runtime Exception Only sequential read is supported Override public int get List Length Object list return int Data Bag list size Override Suppress Warnings unchecked public List get List Object list List Object result new Array List Object Data Bag bag Data Bag list for Tuple t bag if t size try result add t get catch Exec Exception e throw new Runtime Exception e else result add t return result Override public String get Type Name return array child get Type Name Override public Category get Category return Category Override public boolean equals Object o if o null o get Class get Class return false else if o this return true else Object Inspector other Pig List Object Inspector o child return other equals child static class Pig Data Byte Array Object Inspector extends Abstract Primitive Java Object Inspector implements Binary Object Inspector Pig Data Byte Array Object Inspector super Type Info Factory binary Type Info Override public Bytes Writable get Primitive Writable Object Object o return o null null o instanceof Data Byte Array new Bytes Writable Data Byte Array o get new Bytes Writable byte o Override public byte get Primitive Java Object Object o return Data Byte Array o get static class Pig Joda Time Stamp Object Inspector extends Abstract Primitive Java Object Inspector implements Timestamp Object Inspector protected Pig Joda Time Stamp Object Inspector super Type Info Factory timestamp Type Info Override public Timestamp Writable get Primitive Writable Object Object o return o null null new Timestamp Writable new Timestamp Date Time o get Millis Override public Timestamp get Primitive Java Object Object o return o null null new Timestamp Date Time o get Millis static class Pig Decimal Object Inspector extends Abstract Primitive Java Object Inspector implements Hive Decimal Object Inspector protected Pig Decimal Object Inspector super Type Info Factory decimal Type Info Override public Hive Decimal Writable get Primitive Writable Object Object o if o instanceof Big Decimal return o null null new Hive Decimal Writable Hive Decimal create Big Decimal o else Big Integer return o null null new Hive Decimal Writable Hive Decimal create Big Integer o Override public Hive Decimal get Primitive Java Object Object o if o instanceof Big Decimal return o null null Hive Decimal create Big Decimal o else Big Integer return o null null Hive Decimal create Big Integer o public static Object Inspector create Object Inspector Type Info info switch info get Category case switch Primitive Type Info info get Primitive Category case return Primitive Object Inspector Factory java Float Object Inspector case return Primitive Object Inspector Factory java Double Object Inspector case return Primitive Object Inspector Factory java Boolean Object Inspector case return Primitive Object Inspector Factory java Int Object Inspector case return Primitive Object Inspector Factory java Long Object Inspector case return Primitive Object Inspector Factory java String Object Inspector case return new Pig Joda Time Stamp Object Inspector case return new Pig Decimal Object Inspector case return new Pig Data Byte Array Object Inspector case case case case throw new Illegal Argument Exception Should never happen Primitive Type Info info get Primitive Category is not valid Pig primitive data type default throw new Illegal Argument Exception Unknown primitive type Primitive Type Info info get Primitive Category case return new Pig Struct Inspector Struct Type Info info case return new Pig Map Object Inspector Map Type Info info case return new Pig List Object Inspector List Type Info info default throw new Illegal Argument Exception Unknown type info get Category public static Constant Object Inspector get Constant Object Inspector Object obj switch Data Type find Type obj case Data Type return new Java Constant Float Object Inspector Float obj case Data Type return new Java Constant Double Object Inspector Double obj case Data Type return new Java Constant Boolean Object Inspector Boolean obj case Data Type return new Java Constant Int Object Inspector Integer obj case Data Type return new Java Constant Long Object Inspector Long obj case Data Type return new Java Constant String Object Inspector String obj default throw new Illegal Argument Exception Not implemented obj get Class get Name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine import java io Output Stream import java util Iterator import java util Properties import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Load Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend executionengine Exec Job import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl io Read To End Loader import org apache pig tools pigstats Pig Stats public class Job implements Exec Job private final Log log Log Factory get Log get Class protected status protected Pig Context pig Context protected File Spec out File Spec protected Exception backend Exception protected String alias protected Store po Store private Pig Stats stats public Job status Pig Context pig Context Store store String alias this status status this pig Context pig Context this po Store store this out File Spec po Store get File this alias alias public Job status Pig Context pig Context Store store String alias Pig Stats stats this status status this pig Context pig Context this po Store store this out File Spec po Store get File this alias alias this stats stats Override public get Status return status Override public boolean has Completed throws Exec Exception return true Override public Iterator Tuple get Results throws Exec Exception final Load Func p try Load Func original Load Func Load Func Pig Context instantiate Func From Spec out File Spec get Func Spec p Load Func new Read To End Loader original Load Func Configuration Util to Configuration pig Context get Properties out File Spec get File Name catch Exception e int err Code String msg Unable to get results for out File Spec throw new Exec Exception msg err Code Pig Exception e return new Iterator Tuple Tuple t boolean at End Override public boolean has Next if at End return false try if t null t p get Next if t null at End true catch Exception e log error e t null at End true throw new Error e return at End Override public Tuple next Tuple next t if next null t null return next try next p get Next catch Exception e log error e if next null at End true return next Override public void remove throw new Runtime Exception Removal not supported Override public Properties get Configuration return pig Context get Properties Override public Pig Stats get Statistics throw new Unsupported Operation Exception return stats Override public void completion Notification Object cookie throw new Unsupported Operation Exception Override public void kill throws Exec Exception throw new Unsupported Operation Exception Override public void get Logs Output Stream log throws Exec Exception throw new Unsupported Operation Exception Override public void get Out Output Stream out throws Exec Exception throw new Unsupported Operation Exception Override public void get Error Output Stream error throws Exec Exception throw new Unsupported Operation Exception public void set Exception Exception e backend Exception e Override public Exception get Exception return backend Exception Override public String get Alias throws Exec Exception return alias return the po Store Override public Store get Store return po Store 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop security User Group Information import java io Exception Support for logging in using a kerberos keytab file br Kerberos is a authentication system that uses tickets with a limited valitity time br As a consequence running a pig script on a kerberos secured hadoop cluster limits the running time to at most the remaining validity time of these kerberos tickets When doing really complex analytics this may become a problem as the job may need to run for a longer time than these ticket times allow br kerberos keytab file is essentially a Kerberos specific form of the password of a user br It is possible to enable a Hadoop job to request new tickets when they expire by creating a keytab file and make it part of the job that is running in the cluster This will extend the maximum job duration beyond the maximum renew time of the kerberos tickets br br Usage ol li Create a keytab file for the required principal br p Using the ktutil tool you can create a keytab using roughly these commands br i addent password p niels k e rc hmac br addent password p niels k e aes cts br wkt niels keytab i p li li Set the following properties either via the pigrc file or on the command line via file br ul li i java security krb conf i br The path to the local krb conf file br Usually this is etc krb conf li li i hadoop security krb principal i br The pricipal you want to login with br Usually this would look like this niels li li i hadoop security krb keytab i br The path to the local keytab file that must be used to authenticate with br Usually this would look like this home niels krb niels keytab li ul li ol All paths in these variables are local to the client system starting the actual pig script This can be run without any special access to the cluster nodes public class Kerberos private static final Log Log Factory get Log Kerberos class public static void try Kerberos Keytab Login Configuration conf Before we can actually connect we may need to login using the provided credentials if User Group Information is Security Enabled User Group Information login User try login User User Group Information get Login User catch Exception e error Unable to start attempt to login using Kerberos keytab e get Message return If we are logged in into Kerberos with a keytab we can skip this to avoid needless logins if login User has Kerberos Credentials login User is From Keytab String krb Conf conf get java security krb conf String krb Principal conf get hadoop security krb principal String krb Keytab conf get hadoop security krb keytab Only attempt login if we have all the required settings if krb Conf null krb Principal null krb Keytab null info Trying login using Kerberos Keytab info krb Conf krb Conf info krb Principal krb Principal info krb Keytab krb Keytab System set Property java security krb conf krb Conf try User Group Information login User From Keytab krb Principal krb Keytab catch Exception e error Unable to perform keytab based kerberos authentication e get Message 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig pen import java util Array List import java util Collection import java util Hash Map import java util Linked List import java util List import java util Map import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Greater Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Less Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Not Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators And import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Cast import org apache pig backend hadoop executionengine physical Layer expression Operators Is Null import org apache pig backend hadoop executionengine physical Layer expression Operators Map Look Up import org apache pig backend hadoop executionengine physical Layer expression Operators Negative import org apache pig backend hadoop executionengine physical Layer expression Operators Not import org apache pig backend hadoop executionengine physical Layer expression Operators Or import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators Regexp import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Lite Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Counter import org apache pig backend hadoop executionengine physical Layer relational Operators Demux import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Optimized For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig data Data Bag import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl plan Depth First Walker import org apache pig impl plan Plan Walker import org apache pig impl plan Visitor Exception import org apache pig impl util Identity Hash Set import org apache pig newplan logical relational Logical Schema import org apache pig pen util Lineage Tracer The class used to re attach illustrators to physical operators public class Illustrator Attacher extends Phy Plan Visitor Pig Context pig Context Lineage Tracer lineage Hash Map Physical Operator Collection Identity Hash Set Tuple po To Eqclasses Map private Hash Map Physical Operator Data Bag po To Data Map private int max Records private boolean revisit false private Array List Boolean sub Exp Results null private final Map Load Logical Schema poload To Schema Map public Illustrator Attacher Physical Plan plan Lineage Tracer lineage int max Records Map Load Logical Schema po Load To Schema Map Pig Context hadoop Pig Context throws Visitor Exception super plan new Depth First Walker Physical Operator Physical Plan plan pig Context hadoop Pig Context this lineage lineage po To Eqclasses Map new Hash Map Physical Operator Collection Identity Hash Set Tuple po To Data Map new Hash Map Physical Operator Data Bag this max Records max Records this poload To Schema Map po Load To Schema Map revisit an enhanced physical plan from compilation param plan a physical plan to be traversed public void revisit Physical Plan plan throws Visitor Exception push Walker new Depth First Walker Physical Operator Physical Plan plan revisit true Physical Plan ori Plan m Plan m Plan plan visit m Plan ori Plan pop Walker private void set Illustrator Physical Operator po int n Eq Classes if revisit po get Illustrator null return Linked List Identity Hash Set Tuple eq Classes new Linked List Identity Hash Set Tuple po To Eqclasses Map put po eq Classes for int i i n Eq Classes i Identity Hash Set Tuple eq Class new Identity Hash Set Tuple eq Classes add eq Class Illustrator illustrator new Illustrator lineage eq Classes this pig Context po set Illustrator illustrator po To Data Map put po illustrator get Data private void set Illustrator Physical Operator po Linked List Identity Hash Set Tuple eq Classes if revisit po get Illustrator null return Illustrator illustrator new Illustrator lineage eq Classes this pig Context po set Illustrator illustrator if eq Classes null po To Eqclasses Map put po eq Classes po To Data Map put po illustrator get Data void set Illustrator Physical Operator po if revisit po get Illustrator null return Linked List Identity Hash Set Tuple eq Classes new Linked List Identity Hash Set Tuple Identity Hash Set Tuple eq Class new Identity Hash Set Tuple eq Classes add eq Class Illustrator illustrator new Illustrator lineage eq Classes this pig Context po set Illustrator illustrator po To Eqclasses Map put po eq Classes po To Data Map put po illustrator get Data public Map Physical Operator Data Bag get Data Map return po To Data Map Override public void visit Load Load ld throws Visitor Exception from temporary files need no illustrator if revisit return Linked List Identity Hash Set Tuple eq Classes new Linked List Identity Hash Set Tuple po To Eqclasses Map put ld eq Classes Identity Hash Set Tuple eq Class new Identity Hash Set Tuple eq Classes add eq Class Illustrator illustrator illustrator new Illustrator lineage eq Classes max Records this poload To Schema Map get ld pig Context ld set Illustrator illustrator po To Data Map put ld illustrator get Data Override public void visit Store Store st throws Visitor Exception set Illustrator st Override public void visit Filter Filter fl throws Visitor Exception set Illustrator fl sub Exp Results fl get Illustrator get Sub Exp Results inner Plan Attach fl fl get Plan sub Exp Results null Override public void visit Local Rearrange Local Rearrange lr throws Visitor Exception super visit Local Rearrange lr set Illustrator lr Override public void visit Package Package pkg throws Visitor Exception set Illustrator pkg pkg number Of Equivalence Classes Override public void visit For Each For Each nfe throws Visitor Exception if revisit nfe get Illustrator null return List Physical Plan inner Plans nfe get Input Plans for Physical Plan inner Plan inner Plans inner Plan Attach nfe inner Plan List Physical Operator preds m Plan get Predecessors nfe if preds null preds size preds get instanceof Package Package preds get get Pkgr is Distinct equivalence class of Package for needs to be used instead of the succeeding For Each s equivalence class set Illustrator nfe preds get get Illustrator get Equivalence Classes nfe get Illustrator set Eq Classes Shared else set Illustrator nfe Override public void visit Union Union un throws Visitor Exception if revisit un get Illustrator null return set Illustrator un null Override public void visit Split Split spl throws Visitor Exception if revisit spl get Illustrator null return for Physical Plan po Plan spl get Plans inner Plan Attach spl po Plan set Illustrator spl Override public void visit Demux Demux demux throws Visitor Exception if revisit demux get Illustrator null return List Physical Plan inner Plans demux get Plans for Physical Plan inner Plan inner Plans inner Plan Attach demux inner Plan set Illustrator demux Override public void visit Distinct Distinct distinct throws Visitor Exception set Illustrator distinct Override public void visit Sort Sort sort throws Visitor Exception set Illustrator sort Override public void visit Rank Rank rank throws Visitor Exception set Illustrator rank Override public void visit Counter Counter counter throws Visitor Exception set Illustrator counter Override public void visit Project Project proj throws Visitor Exception Override public void visit Greater Than Greater Than Expr grt throws Visitor Exception set Illustrator grt if revisit sub Exp Results null sub Exp Results add grt get Illustrator get Sub Exp Result Override public void visit Less Than Less Than Expr lt throws Visitor Exception set Illustrator lt if revisit sub Exp Results null sub Exp Results add lt get Illustrator get Sub Exp Result Override public void visit Or Equal Or Equal To Expr gte throws Visitor Exception set Illustrator gte if revisit sub Exp Results null sub Exp Results add gte get Illustrator get Sub Exp Result Override public void visit Or Equal Or Equal To Expr lte throws Visitor Exception set Illustrator lte if revisit sub Exp Results null sub Exp Results add lte get Illustrator get Sub Exp Result Override public void visit Equal To Equal To Expr eq throws Visitor Exception set Illustrator eq if revisit sub Exp Results null sub Exp Results add eq get Illustrator get Sub Exp Result Override public void visit Not Equal To Not Equal To Expr eq throws Visitor Exception set Illustrator eq if revisit sub Exp Results null sub Exp Results add eq get Illustrator get Sub Exp Result Override public void visit Regexp Regexp re throws Visitor Exception set Illustrator re if revisit sub Exp Results null sub Exp Results add re get Illustrator get Sub Exp Result Override public void visit Is Null Is Null is Null throws Visitor Exception set Illustrator is Null if revisit sub Exp Results null sub Exp Results add is Null get Illustrator get Sub Exp Result Override public void visit And And and throws Visitor Exception set Illustrator and Override public void visit Or Or or throws Visitor Exception set Illustrator or Override public void visit Not Not not throws Visitor Exception set Illustrator not if revisit sub Exp Results null sub Exp Results add not get Illustrator get Sub Exp Result Override public void visit Bin Cond Bin Cond bin Cond Override public void visit Negative Negative negative set Illustrator negative Override public void visit User Func User Func user Func throws Visitor Exception Override public void visit Comparison Func User Comparison Func comp Func throws Visitor Exception one each for and set Illustrator comp Func Override public void visit Map Look Up Map Look Up map Look Up set Illustrator map Look Up Override public void visit Cast Cast cast Override public void visit Limit Limit lim throws Visitor Exception set Illustrator lim Override public void visit Stream Stream stream throws Visitor Exception set Illustrator stream param optimized For Each Override public void visit Optimized For Each Optimized For Each optimized For Each throws Visitor Exception visit For Each optimized For Each private void inner Plan Attach Physical Operator po Physical Plan plan throws Visitor Exception Plan Walker Physical Operator Physical Plan child Walker m Current Walker spawn Child Walker plan push Walker child Walker child Walker walk this pop Walker Linked List Identity Hash Set Tuple eq Classes new Linked List Identity Hash Set Tuple if sub Exp Results null revisit int size sub Exp Results size for int i i size i eq Classes add new Identity Hash Set Tuple po get Illustrator set Equivalence Classes eq Classes po 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache pig classification Interface Audience import org apache pig classification Interface Stability This class encapsulates the runtime statistics of a user specified input Interface Audience Public Interface Stability Evolving public final class Input Stats private String name private String location private long bytes private long records private boolean success public static enum regular sampler indexer side private type regular private Configuration conf public Input Stats String location long bytes long records boolean success this location location this bytes bytes this records records this success success try this name new Path location get Name catch Exception e location is a mal formatted this name location public String get Name return name public String get Location return location public long get Bytes return bytes public long get Number Records return records public boolean is Successful return success public Configuration get Conf return conf public get Input Type return type public String get Display String String Builder sb new String Builder if success sb append Successfully if type sampler sb append sampled else if type indexer sb append indexed else sb append read if records sb append records append records else sb append records if bytes sb append append bytes append bytes sb append from append location append if type side sb append as side file sb append n else sb append Failed to read data from append location append n return sb to String public void set Conf Configuration conf this conf conf public void mark Sample Input type sampler public void mark Indexer Input type indexer public void mark Side File Input type side 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Iterator import org apache pig Accumulator import org apache pig Algebraic import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer schema Schema This method should never be used directly use link public class Int Avg extends Eval Func Double implements Algebraic Accumulator Double private static Tuple Factory m Tuple Factory Tuple Factory get Instance Override public Double exec Tuple input throws Exception try Long sum sum input if sum null either we were handed an empty bag or a bag filled with nulls return null in this case return null double count count input Double avg null if count avg new Double sum count return avg catch Exec Exception ee throw ee Override public String get Initial return Initial class get Name Override public String get Intermed return Intermediate class get Name Override public String get Final return Final class get Name static public class Initial extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception try Tuple t m Tuple Factory new Tuple input is a bag with one tuple containing the column we are trying to avg on Data Bag bg Data Bag input get Integer i null if bg iterator has Next Tuple tp bg iterator next i Integer tp get t set i null Long value Of i null if i null t set else t set return t catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Intermediate extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception try Data Bag b Data Bag input get return combine b catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static public class Final extends Eval Func Double Override public Double exec Tuple input throws Exception try Data Bag b Data Bag input get Tuple combined combine b Long sum Long combined get if sum null return null double count Long combined get Double avg null if count avg new Double sum count return avg catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e static protected Tuple combine Data Bag values throws Exec Exception long sum long count combine is called from Intermediate and Final In either case Initial would have been called before and would have sent in valid tuples Hence we do n t need to check if incoming bag is empty Tuple output m Tuple Factory new Tuple boolean saw Non Null false for Iterator Tuple it values iterator it has Next Tuple t it next Long l Long t get we count nulls in avg as contributing a departure from for performance of which implemented by just inspecting size of the bag if l null l else saw Non Null true sum l count Long t get if saw Non Null output set Long value Of sum else output set null output set Long value Of count return output static protected long count Tuple input throws Exec Exception Data Bag values Data Bag input get Iterator it values iterator long cnt while it has Next Tuple t Tuple it next if t null t size t get null cnt return cnt static protected Long sum Tuple input throws Exec Exception Exception Data Bag values Data Bag input get if we were handed an empty bag return if values null values size return null long sum boolean saw Non Null false for Iterator Tuple it values iterator it has Next Tuple t it next try Integer i Integer t get we count nulls in avg as contributing a departure from for performance of which implemented by just inspecting size of the bag if i null continue saw Non Null true sum i catch Runtime Exception exp int err Code String msg Problem while computing sum of ints throw new Exec Exception msg err Code Pig Exception exp if saw Non Null return Long value Of sum else return null Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type Accumulator interface private Long intermediate Sum null private Double intermediate Count null Override public void accumulate Tuple b throws Exception try Long sum sum b if sum null return set default values if intermediate Sum null intermediate Count null intermediate Sum intermediate Count double count Long count b if count intermediate Count count intermediate Sum sum catch Exec Exception ee throw ee catch Exception e int err Code String msg Error while computing average in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public void cleanup intermediate Sum null intermediate Count null Override public Double get Value Double avg null if intermediate Count null intermediate Count avg new Double intermediate Sum intermediate Count return avg 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Buffered Input Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java io File import java io File Input Stream import java io File Not Found Exception import java io Exception import java util Array List import java util Iterator import java util No Such Element Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Counters import org apache pig classification Interface Audience import org apache pig classification Interface Stability Interface Audience Private Interface Stability Evolving public class Internal Cached Bag extends Self Spill Bag private static final long serial Version private static final Log log Log Factory get Log Internal Cached Bag class private transient Data Output Stream out private transient boolean add Done private transient Tuple Factory factory used to store number of tuples spilled until counter is incremented private transient int num Tuples Spilled private static final Inter Sedes Inter Sedes Factory get Inter Sedes Instance public Internal Cached Bag this f public Internal Cached Bag int bag Count this bag Count f public Internal Cached Bag int bag Count float percent super bag Count percent init private void init factory Tuple Factory get Instance m Contents new Array List Tuple add Done false Override public void add Tuple t if add Done throw new Illegal State Exception Internal Cached Bag is closed for adding new tuples if m Contents size mem Limit get Cache Limit m Contents add t if m Contents size mem Limit add New Obj Size t get Memory Size else above cache Limit spill to disk try if out null if log is Debug Enabled log debug Memory can hold m Contents size records put the rest in spill file out get Spill File inc Spill Count Pig Counters write Datum out t Data Type periodically update number of tuples spilled num Tuples Spilled if num Tuples Spilled update Spill Rec Counter catch Exception e throw new Runtime Exception e m Size private void update Spill Rec Counter inc Spill Count Pig Counters num Tuples Spilled num Tuples Spilled private void add Done if out null try out flush out close catch Exception e ignore if num Tuples Spilled update Spill Rec Counter add Done true Override public void clear if add Done add Done super clear add Done false out null Override public boolean is Distinct return false Override public boolean is Sorted return false Override public Iterator Tuple iterator if add Done close the spill file and mark adding is done so further adding is disallowed add Done return new Cached Bag Iterator Override public long spill throw new Runtime Exception Internal Cached Bag spill should not be called private class Cached Bag Iterator implements Iterator Tuple Iterator Tuple iter Data Input Stream in Tuple next long num Tuples Read public Cached Bag Iterator iter m Contents iterator if m Spill Files null m Spill Files size File file m Spill Files get try in new Data Input Stream new Buffered Input Stream new File Input Stream file catch File Not Found Exception fnfe String msg Unable to find our spill file throw new Runtime Exception msg fnfe Override public boolean has Next if next null return true if iter has Next next iter next return true if in null return false try Tuple t Tuple read Datum in next t return true catch Exception eof try in close catch Exception e in null return false catch Exception e String msg Unable to read our spill file throw new Runtime Exception msg e Override public Tuple next if next null if has Next throw new No Such Element Exception No more elements from iterator Tuple t next next null num Tuples Read This will report progress every records if num Tuples Read x fff report Progress return t Override public void remove throw new Unsupported Operation Exception remove is not supported for Cached Bag Iterator 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Buffered Input Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java io File import java io File Input Stream import java io File Not Found Exception import java io Exception import java util Array List import java util Collections import java util Comparator import java util Iterator import java util Linked List import java util List Iterator import java util Priority Queue import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Warning An ordered collection of Tuples possibly with multiples Data is stored unsorted as it comes in and only sorted when it is time to dump it to a file or when the first iterator is requested Experementation found this to be the faster than storing it sorted to begin with We allow a user defined comparator but provide a default comparator in cases where the user does n t specify one This bag is not registered with Spillable Memory Manager It calculates the number of tuples to hold in memory and spill pro actively into files public class Internal Sorted Bag extends Sorted Spill Bag private static final long serial Version private static Tuple Factory g Tuple Factory Tuple Factory get Instance private static final Log log Log Factory get Log Internal Sorted Bag class private transient Comparator Tuple m Comp private transient boolean m Read Started false static private class Default Comparator implements Comparator Tuple Override Suppress Warnings unchecked public int compare Tuple t Tuple t return t compare To t Override public boolean equals Object o return o this Override public int hash Code return public Internal Sorted Bag this null public Internal Sorted Bag Comparator Tuple comp this comp public Internal Sorted Bag int bag Count Comparator Tuple comp this bag Count f comp public Internal Sorted Bag int bag Count float percent Comparator Tuple comp super bag Count percent init bag Count percent comp param comp Comparator to use to do the sorting If null Default Comparator will be used private void init int bag Count double percent Comparator Tuple comp m Comp comp null new Default Comparator comp m Contents new Array List Tuple Override public void add Tuple t synchronized m Contents if m Read Started throw new Illegal State Exception Internal Sorted Bag is closed for adding new tuples if m Contents size mem Limit get Cache Limit proactive spill m Comp m Contents add t check how many tuples memory can hold by getting average size of first tuples if m Size m Spill Files null m Spill Files is Empty t null mem Limit add New Obj Size t get Memory Size m Size mark Spillable If Necessary Override public boolean is Sorted return true Override public boolean is Distinct return false Override public Iterator Tuple iterator return new Sorted Data Bag Iterator An iterator that handles getting the next tuple from the bag Data can be stored in a combination of in memory and on disk private class Sorted Data Bag Iterator implements Iterator Tuple container to hold tuples in a priority queue Stores the file number the tuple came from so that when the tuple is read out of the queue we know which file to read its replacement tuple from private class Container implements Comparable Container public Tuple tuple public int file Num Override public int compare To Container other return m Comp compare tuple other tuple Override public boolean equals Object obj if obj instanceof Container return compare To Container obj return false Override public int hash Code return tuple hash Code We have to buffer a tuple because there s no easy way for next to tell whether or not there s another tuple available other than to read it private Tuple m Buf null private int m Memory Ptr private Priority Queue Container m Merge null private Array List Data Input Stream m Streams null private int m Cntr Sorted Data Bag Iterator If this is the first read we need to sort the data synchronized m Contents if m Read Started pre Merge Collections sort Array List Tuple m Contents m Comp m Read Started true Override public boolean has Next See if we can find a tuple If so buffer it m Buf next return m Buf null Override public Tuple next This will report progress every times through next This should be much faster than using mod if m Cntr x ff report Progress If there s one in the buffer use that one if m Buf null Tuple t m Buf m Buf null return t if m Spill Files null m Spill Files size return read From Memory We have spill files so we need to read the next tuple from one of those files or from memory return read From Priority Not implemented Override public void remove private Tuple read From Priority if m Merge null First read we need to set up the queue and the array of file streams Add one to the size for the list in memory m Merge new Priority Queue Container m Spill Files size Add one to the size in case we spill later m Streams new Array List Data Input Stream m Spill Files size Iterator File i m Spill Files iterator while i has Next try Data Input Stream in new Data Input Stream new Buffered Input Stream new File Input Stream i next m Streams add in Add the first tuple from this file into the merge queue add To Queue null m Streams size catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Prime one from memory too if m Contents size add To Queue null Pop the top one off the queue Container c m Merge poll if c null return null Add the next tuple from whereever we read from into the queue Buffer the tuple we re returning as we ll be reusing c Tuple t c tuple add To Queue c c file Num return t private void add To Queue Container c int file Num if c null c new Container c file Num file Num if file Num Need to read from memory c tuple read From Memory if c tuple null m Merge add c return Read the next tuple from the indicated file Data Input Stream in m Streams get file Num if in null There s still data in this file c tuple g Tuple Factory new Tuple try c tuple read Fields in m Merge add c catch Exception eof Out of tuples in this file Set our slot in the array to null so we do n t keep trying to read from this file try in close catch Exception e log warn Failed to close spill file e m Streams set file Num null catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe Function assumes that the reader lock is already held before we enter this function private Tuple read From Memory if m Contents size return null if m Memory Ptr m Contents size return Array List Tuple m Contents get m Memory Ptr else return null Pre merge if there are too many spill files This avoids the issue of having too large a fan out in our merge Experimentation by the hadoop team has shown that is about the optimal number of spill files This function modifies the m Spill Files array and assumes the write lock is already held It will not unlock it Tuples are reconstituted as tuples evaluated and rewritten as tuples This is expensive but need to do this in order to use the sort spec that was provided to me private void pre Merge if m Spill Files null m Spill Files size return While there are more than max spill files gather max spill files together and merge them into one file Then remove the others from m Spill Files The new spill files are attached at the end of the list so can just keep going until get a small enough number without too much concern over uneven size merges Convert m Spill Files to a linked list since we ll be removing pieces from the middle and we want to do it efficiently try Linked List File ll new Linked List File m Spill Files Linked List File files To Delete new Linked List File while ll size List Iterator File i ll list Iterator m Streams new Array List Data Input Stream m Merge new Priority Queue Container for int j j j try File f i next Data Input Stream in new Data Input Stream new Buffered Input Stream new File Input Stream f m Streams add in add To Queue null m Streams size i remove files To Delete add f catch File Not Found Exception fnfe We ca n t find our own spill file That should neer happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Get a new spill file This adds one to the end of the spill files list So need to append it to my linked list as well so that it s still there when move my linked list back to the spill files Data Output Stream out null try out get Spill File ll add m Spill Files get m Spill Files size Tuple t while t read From Priority null t write out out flush catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe finally if out null try out close catch Exception e warn Error closing spill Pig Warning e delete files that have been merged into new files for File f files To Delete if f delete false log warn Failed to delete spill file f get Path clear the list so that finalize does not delete any files when m Spill Files is assigned a new value m Spill Files clear Now move our new list back to the spill files array m Spill Files new File List ll finally Reset m Streams and m Merge so that they ll be allocated properly for regular merging m Streams null m Merge null Override public long spill return proactive spill m Comp Override public long proactive spill Comparator Tuple comp synchronized m Contents if this m Read Started return return super proactive spill comp 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl io import java io Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Data Output Stream import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop io Text import org apache hadoop io Writable Comparable import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce Task Attempt Context import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib output File Output Format import org apache pig Expression import org apache pig File Input Load Func import org apache pig Load Func import org apache pig Load Metadata import org apache pig Resource Schema import org apache pig Resource Statistics import org apache pig Store Func import org apache pig Store Func Interface import org apache pig backend hadoop executionengine map Reduce Layer Pig File Input Format import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig classification Interface Audience import org apache pig data Tuple import org apache pig impl util Utils This load function is used for storing intermediate data between jobs of a pig query The serialization format of this load function can change in newer versions of pig so this should be used to store any persistent data Interface Audience Private public class Inter Storage extends File Input Load Func implements Store Func Interface Load Metadata private static final Log m Log Log Factory get Log Inter Storage class public static final String use Log Pig Internal storage in use private Inter Record Reader rec Reader null private Inter Record Writer rec Writer null Simple binary nested reader format public Inter Storage m Log debug use Log Override public Tuple get Next throws Exception if rec Reader next Key Value return rec Reader get Current Value else return null Override public void put Next Tuple t throws Exception try rec Writer write null t catch Interrupted Exception e throw new Exception e public static class Inter Input Format extends Pig File Input Format Text Tuple non Javadoc see org apache hadoop mapreduce Input Format create Record Reader org apache hadoop mapreduce Input Split org apache hadoop mapreduce Task Attempt Context Override public Record Reader Text Tuple create Record Reader Input Split split Task Attempt Context context throws Exception Interrupted Exception return new Inter Record Reader Override public Input Format get Input Format return new Inter Input Format Override public int hash Code return Override public void prepare To Read Record Reader reader Pig Split split rec Reader Inter Record Reader reader Override public void set Location String location Job job throws Exception File Input Format set Input Paths job location public static class Inter Output Format extends File Output Format org apache hadoop io Writable Comparable Tuple non Javadoc see org apache hadoop mapreduce lib output File Output Format get Record Writer org apache hadoop mapreduce Task Attempt Context Override public Record Writer Writable Comparable Tuple get Record Writer Task Attempt Context job throws Exception Interrupted Exception Configuration conf job get Configuration Path file get Default Work File job File System fs file get File System conf Data Output Stream file Out fs create file false return new Inter Record Writer file Out Override public Output Format get Output Format return new Inter Output Format Override public void prepare To Write Record Writer writer this rec Writer Inter Record Writer writer Override public void set Store Location String location Job job throws Exception File Output Format set Output Path job new Path location Override public void check Schema Resource Schema s throws Exception Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir Override public String get Partition Keys String location Job job throws Exception return null Override public Resource Schema get Schema String location Job job throws Exception return Utils get Schema this location true job Override public Resource Statistics get Statistics String location Job job throws Exception return null Override public void set Partition Filter Expression plan throws Exception throw new Unsupported Operation Exception Override public void set Store Func Context Signature String signature Override public void cleanup On Failure String location Job job throws Exception Store Func cleanup On Failure Impl location job Override public void cleanup On Success String location Job job throws Exception do nothing 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin This method should never be used directly use link public class Int Min extends Algebraic Int Math Base public Int Min set Op public static class Intermediate extends Algebraic Int Math Base Intermediate Override public get Op return public static class Final extends Algebraic Int Math Base Final Override public get Op return 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import org apache pig impl logical Layer Frontend Exception see Generic Invoker public class Invoke For Double extends Generic Invoker Double public Invoke For Double public Invoke For Double String full Name throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name public Invoke For Double String full Name String param Specs Str throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name param Specs Str public Invoke For Double String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception super full Name param Specs Str is Static 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import org apache pig impl logical Layer Frontend Exception see Generic Invoker public class Invoke For Float extends Generic Invoker Float public Invoke For Float public Invoke For Float String full Name throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name public Invoke For Float String full Name String param Specs Str throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name param Specs Str public Invoke For Float String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception super full Name param Specs Str is Static 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import org apache pig impl logical Layer Frontend Exception see Generic Invoker public class Invoke For Int extends Generic Invoker Integer public Invoke For Int public Invoke For Int String full Name String param Specs Str throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name param Specs Str public Invoke For Int String full Name throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name public Invoke For Int String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception super full Name param Specs Str is Static 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import org apache pig impl logical Layer Frontend Exception see Generic Invoker public class Invoke For Long extends Generic Invoker Long public Invoke For Long public Invoke For Long String full Name throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name public Invoke For Long String full Name String param Specs Str throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name param Specs Str public Invoke For Long String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception super full Name param Specs Str is Static 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import org apache pig impl logical Layer Frontend Exception see Generic Invoker public class Invoke For String extends Generic Invoker String public Invoke For String public Invoke For String String full Name String param Specs Str throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name param Specs Str public Invoke For String String full Name throws Frontend Exception Security Exception Class Not Found Exception No Such Method Exception super full Name public Invoke For String String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception super full Name param Specs Str is Static 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java lang reflect Invocation Target Exception import java lang reflect Method import java lang reflect Type import java util Arrays import java util List import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import com google common collect Lists import com google common collect Sets public class Invoker private static final Log Log Factory get Log Invoker class private static final Class new double get Class private static final Class new int get Class private static final Class new float get Class private static final Class new String get Class private static final Class new long get Class Suppress Warnings unchecked private static final Set Class Sets new Hash Set private Method method private Class param Classes private boolean is Static private Class self Class private Type return Type public Invoker String full Name String param Specs Str throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception this full Name param Specs Str true public Invoker String full Name String param Specs Str String is Static throws Class Not Found Exception Frontend Exception Security Exception No Such Method Exception String class Name full Name substring full Name last Index Of String method Name full Name substring full Name last Index Of Class klazz try klazz Pig Context resolve Class Name class Name catch Exception e the amusing part is that Pig Context throws this to wrap one of the exceptions we declare throw new Frontend Exception e String param Specs equals param Specs Str new String param Specs Str split is Static static equals Ignore Case is Static true equals is Static param Classes new Class param Specs length for int i i param Specs length i param Classes i string To Class param Specs i if is Static self Class param Classes method klazz get Method method Name is Static param Classes drop First Class param Classes return Type method get Generic Return Type Suppress Warnings rawtypes public Type get Return Type return un Primitivize Class return Type private static Class drop First Class Class original if original length return new Class else return Arrays copy Of Range original original length private static Object drop First Object Object original if original length return new Object else return Arrays copy Of Range original original length private static Class string To Class String klass throws Frontend Exception if string equals Ignore Case klass return String class else if int equals Ignore Case klass return Integer else if double equals Ignore Case klass return Double else if float equals Ignore Case klass return Float else if long equals Ignore Case klass return Long else if double equals Ignore Case klass return else if int equals Ignore Case klass return else if long equals Ignore Case klass return else if float equals Ignore Case klass return else if string equals Ignore Case klass return else throw new Frontend Exception unable to find matching class for klass private static Class un Primitivize Class klass if klass equals Integer return Integer class if klass equals Long return Long class else if klass equals Float return Float class else if klass equals Double return Double class else return klass private static convert To Expected Arg Class klass Object obj throws Exec Exception if contains klass Data Bag dbag Data Bag obj if equals klass List String data List Lists new Array List for Tuple t dbag data List add String t get String data Array new String data List size for int i i data List size i data Array i data List get i obj data Array else List Number data List bag To Number List dbag if equals klass double data Array new double data List size for int i i data List size i data Array i data List get i double Value obj data Array else if equals klass int data Array new int data List size for int i i data List size i data Array i data List get i int Value obj data Array else if equals klass float data Array new float data List size for int i i data List size i data Array i data List get i float Value obj data Array else if equals klass long data Array new long data List size for int i i data List size i data Array i data List get i long Value obj data Array try return klass cast obj catch Class Cast Exception e error Error in dynamic argument processing Casting to klass from obj get Class e throw new Exec Exception e private static List Number bag To Number List Data Bag dbag throws Exec Exception List Number data List Lists new Array List for Tuple t dbag data List add Number t get return data List private Object tuple To Args Tuple t throws Exec Exception if t null param Classes null param Classes length t null t size param Classes length throw new Exec Exception unable to match function arguments to declared signature if t null return null Object args new Object param Classes length for int i i param Classes length i args i convert To Expected Arg un Primitivize param Classes i t get i return args Suppress Warnings unchecked public invoke Tuple input throws Exception Object args tuple To Args input try if is Static return method invoke self Class cast args drop First Object args else return method invoke null args catch Illegal Argument Exception e throw new Exec Exception e catch Illegal Access Exception e throw new Exec Exception e catch Invocation Target Exception e throw new Exec Exception e 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Map import org apache pig Filter Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple Determine whether a bag or map is empty public class Is Empty extends Filter Func Override public Boolean exec Tuple input throws Exception try Object values input get if values instanceof Data Bag return Data Bag values size else if values instanceof Map return Map values size else int err Code String msg Can not test a Data Type find Type Name values for emptiness throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Is Null Expression extends Unary Expression public Is Null Expression Operator Plan plan Logical Expression exp super Is Null plan exp Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Is Null Expression Is Null Expression of Is Null Expression other return get Expression is Equal of get Expression else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Is Null Expression lg Exp Plan this get Expression deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl util import java io File import java io File Input Stream import java io File Output Stream import java io Exception import java io Input Stream import java io Output Stream import java net Malformed Exception import java net import java net import java net Class Loader import java net Decoder import java util Array List import java util Enumeration import java util Hash Map import java util Hash Set import java util List import java util Map import java util Set import java util jar Jar Entry import java util jar Jar Output Stream import org antlr runtime Common Token Stream import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop mapreduce Job import org apache hadoop util String Utils import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig impl Pig Context import org apache tools bzip r Zip Constants import org joda time Date Time import com google common collect Multimaps import dk brics automaton Automaton public class Jar Manager private static Log log Log Factory get Log Jar Manager class private static enum Default Pig Packages Pig Map Reduce class Zip Constants class Automaton class Common Token Stream class Date Time class private final Class pkg Class Default Pig Packages Class pkg Class this pkg Class pkg Class public Class get Pkg Class return pkg Class public static File create Pig Script Jar Pig Context pig Context throws Exception File script Jar File File create Temp File Pig Script jar ensure the script Jar File is deleted on exit script Jar File delete On Exit File Output Stream fos new File Output Stream script Jar File Hash Map String String contents new Hash Map String String create Pig Script Jar fos pig Context contents if contents is Empty File Input Stream fis null String md null try fis new File Input Stream script Jar File md org apache commons codec digest Digest Utils md Hex fis finally if fis null fis close File new Script Jar File new File script Jar File get Parent Pig Script md jar script Jar File rename To new Script Jar File return new Script Jar File return null private static void create Pig Script Jar Output Stream os Pig Context pig Context Hash Map String String contents throws Exception Jar Output Stream jar Output Stream new Jar Output Stream os for String path pig Context script Files log debug Adding entry path to job jar Input Stream stream null File input File new File path if input File exists stream new File Input Stream input File else stream Pig Context get Class Loader get Resource As Stream path if stream null throw new Exception Can not find path try add Stream jar Output Stream path stream contents input File last Modified finally stream close for Map Entry String File entry pig Context get Script Files entry Set log debug Adding entry entry get Key to job jar Input Stream stream null if entry get Value exists stream new File Input Stream entry get Value else stream Pig Context get Class Loader get Resource As Stream entry get Value get Path if stream null throw new Exception Can not find entry get Value get Path try add Stream jar Output Stream entry get Key stream contents entry get Value last Modified finally stream close if contents is Empty jar Output Stream close else os close Creates a Classloader based on the passed jar File and any extra jar files param jar File the jar file to be part of the newly created Classloader This jar file plus any jars in the extra Jars list will constitute the classpath return the new Classloader throws Malformed Exception static Class Loader create Cl String jar File Pig Context pig Context throws Malformed Exception int len pig Context extra Jars size int passed Jar jar File null urls new len passed Jar if jar File null urls new file jar File for int i i pig Context extra Jars size i urls i passed Jar new file pig Context extra Jars get i return new Class Loader urls Pig Map Reduce class get Class Loader Adds a stream to a Jar file param os the Output Stream of the Jar file to which the stream will be added param name the name of the stream param is the stream to add param contents the current contents of the Jar file We use this to avoid adding two streams with the same name param timestamp timestamp of the entry throws Exception private static void add Stream Jar Output Stream os String name Input Stream is Map String String contents long timestamp throws Exception if contents get name null return contents put name Jar Entry entry new Jar Entry name entry set Time timestamp os put Next Entry entry byte buffer new byte int rc while rc is read buffer os write buffer rc public static List String get Default Jars List String default Jars new Array List String for Default Pig Packages pkg To Send Default Pig Packages values String jar find Containing Jar pkg To Send get Pkg Class if jar null default Jars contains jar default Jars add jar return default Jars Find a jar that contains a class of the same name if any It will return a jar file even if that is not the first thing on the class path that has a class with the same name param my class the class to find return a jar file that contains the class or null throws Exception public static String find Containing Jar Class my class Class Loader loader Pig Context get Class Loader String class file my class get Name replace All class try Enumeration itr null Try to find the class in registered jars if loader instanceof Class Loader itr Class Loader loader find Resources class file Try system classloader if not Class Loader or no resources found in Class Loader if itr null itr has More Elements itr loader get Resources class file for itr has More Elements url itr next Element if jar equals url get Protocol String to Return url get Path if to Return starts With file to Return to Return substring file length Decoder is a misnamed class since it actually decodes x www form urlencoded type rather than actual encoding which the file path has Therefore it would decode s to s which is incorrect spaces are actually either unencoded or encoded as Replace s first so that they are kept sacred during the decoding process to Return to Return replace All to Return Decoder decode to Return return to Return replace All catch Exception e throw new Runtime Exception e return null Add the jars containing the given classes to the job s configuration such that Job Client will ship them to the cluster and add them to the Distributed Cache param job Job object param classes classes to find throws Exception public static void add Dependency Jars Job job Class classes throws Exception Configuration conf job get Configuration File System fs File System get Local conf Set String jars new Hash Set String jars add All conf get String Collection tmpjars add Qualified Jars Name fs jars classes if jars is Empty return conf set tmpjars String Utils array To String jars to Array new String Add the qualified path name of jars containing the given classes param fs File System object param jars the resolved path names to be added to this set param classes classes to find private static void add Qualified Jars Name File System fs Set String jars Class classes fs Uri fs get Uri Path working Dir fs get Working Directory for Class clazz classes String jar Name find Containing Jar clazz if jar Name null log warn Could not find jar for class clazz continue jars add new Path jar Name make Qualified fs Uri working Dir to String 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import static org apache pig Pig Configuration import static org apache pig Pig Configuration import java io File import java io Exception import java io Input Stream import java io Output Stream import java net import java net Syntax Exception import java net import java util Array List import java util Collection import java util Collections import java util Hash Map import java util Iterator import java util Linked List import java util List import java util Map import java util Map Entry import java util Properties import java util Tree Map import java util regex Matcher import java util regex Pattern import org apache commons codec digest Digest Utils import org apache commons io Filename Utils import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop filecache Distributed Cache import org apache hadoop fs File Status import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop io Utils import org apache hadoop io Writable Comparable import org apache hadoop io Writable Comparator import org apache hadoop mapred Counters import org apache hadoop mapred Counters Counter import org apache hadoop mapred Counters Group import org apache hadoop mapred Job Conf import org apache hadoop mapred Job Priority import org apache hadoop mapred jobcontrol Job import org apache hadoop mapred jobcontrol Job Control import org apache hadoop mapreduce lib output Lazy Output Format import org apache pig Comparison Func import org apache pig Exec Type import org apache pig Func Spec import org apache pig Load Func import org apache pig Overwritable Store Func import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig Store Func Interface import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop Date Time Writable import org apache pig backend hadoop Data Type import org apache pig backend hadoop Pig Job Control import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine Job Creation Exception import org apache pig backend hadoop executionengine map Reduce Layer partitioners Secondary Key Partitioner import org apache pig backend hadoop executionengine map Reduce Layer partitioners Skewed Partitioner import org apache pig backend hadoop executionengine map Reduce Layer partitioners Weighted Range Partitioner import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer plans Udf Cache Ship Files Visitor import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig backend hadoop executionengine util Map Red Util import org apache pig data Bag Factory import org apache pig data Data Type import org apache pig data Schema Tuple Frontend import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl builtin Cross import org apache pig impl io File Localizer import org apache pig impl io File Spec import org apache pig impl io Nullable Big Decimal Writable import org apache pig impl io Nullable Big Integer Writable import org apache pig impl io Nullable Boolean Writable import org apache pig impl io Nullable Bytes Writable import org apache pig impl io Nullable Date Time Writable import org apache pig impl io Nullable Double Writable import org apache pig impl io Nullable Float Writable import org apache pig impl io Nullable Int Writable import org apache pig impl io Nullable Long Writable import org apache pig impl io Nullable Partition Writable import org apache pig impl io Nullable Text import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl plan Depth First Walker import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig impl util Jar Manager import org apache pig impl util Object Serializer import org apache pig impl util Pair import org apache pig impl util Context import org apache pig impl util Utils import org apache pig tools pigstats mapreduce Job Stats import org apache pig tools pigstats mapreduce Pig Stats Util import org apache pig tools pigstats mapreduce Script State This is compiler class that takes an Oper Plan and converts it into a Job Control object with the relevant dependency info maintained The Job Control Object is made up of Jobs each of which has a Job Conf The Map Reduce Oper corresponds to a Job and the get Job Cong method returns the Job Conf that is configured as per the Map Reduce Oper h Comparator Design h p few words on how comparators are chosen In almost all cases we use raw comparators the one exception being when the user provides a comparison function for order by For order by queries the Pig Raw Comparator functions are used where is Int Long etc These comparators are null aware and asc desc aware The first byte of each of the Nullable Writable classes contains info on whether the value is null Asc desc is written as an array into the Job Conf with the key pig sort Order so that it can be read by each of the comparators as part of their set Conf call p For non order by queries Pig Writable Comparator classes are used These are all just type specific instances of Writable Comparator Suppress Warnings deprecation public class Job Control Compiler Oper Plan plan Configuration conf Configuration default Conf Pig Context pig Context private static final Matcher Pattern compile zip tgz tar gz tar matcher private static final Log log Log Factory get Log Job Control Compiler class public static final String logs public static final String pig invoke close in map public static final String pig counters counter public static final String pig rank public static final String public Hash Map String Array List Pair String Long global Counters new Hash Map String Array List Pair String Long public static final String This job was detected as a small job will run in process instead public static final String This job can not be converted run in process We will serialize the Store s present in map and reduce in lists in the Hadoop Conf In the case of Multi stores we could deduce these from the map plan and reduce plan but in the case of single store we remove the Store from the plan in either case we serialize the Store s so that Pig Output Format and Pig Output Commiter can get the Store s in the same way irrespective of whether it is multi store or single store public static final String pig map stores public static final String pig reduce stores mapping of job to pair of store locations and tmp locations for that job private Map Job Pair List Store Path job Store Map private Map Job Map Reduce Oper job Mro Map public Job Control Compiler Pig Context pig Context Configuration conf this pig Context conf null public Job Control Compiler Pig Context pig Context Configuration conf Configuration default Conf this pig Context pig Context this conf conf this default Conf default Conf job Store Map new Hash Map Job Pair List Store Path job Mro Map new Hash Map Job Map Reduce Oper Returns all store locations of a previously compiled job public List Store get Stores Job job Pair List Store Path pair job Store Map get job if pair null pair first null return pair first else return new Array List Store Resets the state public void reset job Store Map new Hash Map Job Pair List Store Path job Mro Map new Hash Map Job Map Reduce Oper Context get Context reset Gets the map of Job and the Operator public Map Job Map Reduce Oper get Job Mro Map return Collections unmodifiable Map job Mro Map Moves all the results of a collection of jobs to the final output directory Some of the results may have been put into a temp location to work around restrictions with multiple output from a single map reduce job This method should always be called after the job execution completes public void move Results List Job completed Jobs throws Exception for Job job completed Jobs Pair List Store Path pair job Store Map get job if pair null pair second null Path tmp pair second Path abs new Path tmp abs Path rel new Path tmp rel File System fs tmp get File System conf if fs exists abs move Results abs abs to Uri get Path fs if fs exists rel move Results rel rel to Uri get Path fs Walks the temporary directory structure to move rename files to their final location private void move Results Path p String rem File System fs throws Exception for File Status fstat fs list Status p Path src fstat get Path if fstat is Dir log info mkdir src fs mkdirs remove Part src rem move Results fstat get Path rem fs else Path dst remove Part src rem log info mv src dst fs rename src dst private Path remove Part Path src String part uri src to Uri String path Str uri get Path replace part return new Path path Str Compiles all jobs that have no dependencies removes them from the plan and returns Should be called with the same plan until exhausted param plan The Oper Plan to be compiled param grp Name The name given to the Job Control return Job Control object null if no more jobs in plan throws Job Creation Exception public Job Control compile Oper Plan plan String grp Name throws Job Creation Exception Assert plan size this plan plan int time To Sleep String default Pig Job Control Sleep pig Context get Exec Type is Local String pig Job Control Sleep conf get pig jobcontrol sleep default Pig Job Control Sleep if pig Job Control Sleep equals default Pig Job Control Sleep log info overriding default Job Control sleep default Pig Job Control Sleep to pig Job Control Sleep try time To Sleep Integer parse Int pig Job Control Sleep catch Number Format Exception e throw new Runtime Exception Invalid configuration pig jobcontrol sleep pig Job Control Sleep should be a time in ms default default Pig Job Control Sleep e Job Control job Ctrl new Pig Job Control grp Name time To Sleep try List Map Reduce Oper roots new Linked List Map Reduce Oper roots add All plan get Roots for Map Reduce Oper mro roots if mro instanceof Native Map Reduce Oper return null Print plan before launching if needed if conf get Boolean Pig Configuration false log info mro to String Job job get Job plan mro conf pig Context job Mro Map put job mro job Ctrl add Job job catch Job Creation Exception jce throw jce catch Exception e int err Code String msg Internal error creating job configuration throw new Job Creation Exception msg err Code Pig Exception e return job Ctrl Update Map Reduce plan with the execution status of the jobs If one job completely fail the job has only one store and that job fail then we remove all its dependent jobs This method will return the number of Map Reduce Oper removed from the Map Reduce plan public int update Op Plan List Job complete Failed Jobs int size Before plan size for Job job complete Failed Jobs remove all subsequent jobs Map Reduce Oper mr Oper job Mro Map get job plan trim Below mr Oper plan remove mr Oper Remove successful jobs from job Mro Map for Job job job Mro Map key Set if complete Failed Jobs contains job Map Reduce Oper mro job Mro Map get job if pig Context in Illustrator mro is Counter Operation save Counters job mro get Operation mro is Row Number plan remove mro job Mro Map clear int size After plan size return size Before size After Reads the global counters produced by a job on the group labeled with Then it is calculated the cumulative sum which consists on the sum of previous cumulative sum plus the previous global counter value param job with the global counters collected param operation After being collected on global counters Counter these values are passed via configuration file to Rank by using the unique operation identifier private void save Counters Job job String operation boolean is Row Number Counters counters Group group Counters int counter Size Long previous Value Long previous Sum Array List Pair String Long counter Pairs try counters Job Stats get Counters job String group Name get Group Name counters get Group Names In case that the counter group was not find we need to find out why Only acceptable state is that the relation has been empty if group Name null Counter output Records counters get Group Pig Stats Util get Counter For Name Pig Stats Util if output Records get Counter global Counters put operation new Array List Pair String Long return else throw new Runtime Exception Did not found counter group for operation Id operation group Counters counters get Group group Name Tree Map Integer Long counter List new Tree Map Integer Long Iterator Counter it group Counters iterator while it has Next try Counter c it next counter List put Integer value Of c get Display Name c get Value catch Exception ex ex print Stack Trace counter Size counter List size counter Pairs new Array List Pair String Long There could be empty tasks with no counters That is not an issue and we only need to calculate offsets for non empty task ids which will be accessed in Rank for Entry Integer Long entry counter List entry Set previous Sum previous Value previous Value entry get Value counter Pairs add new Pair String Long Job Control Compiler operation Job Control Compiler entry get Key previous Sum global Counters put operation counter Pairs catch Exception e String msg Error to read counters into Rank operation counter Size counter Size throw new Runtime Exception msg e private String get Group Name Collection String collection for String name collection if name contains return name return null private boolean ok To Run Local org apache hadoop mapreduce Job job Map Reduce Oper mro List Load lds throws Exception Configuration conf job get Configuration if conf get Boolean Pig Configuration false return false long input Byte Max conf get Long Pig Configuration l long total Input File Size Input Size Reducer Estimator get Total Input File Size conf lds job input Byte Max log info Size of input total Input File Size bytes Small job threshold input Byte Max if total Input File Size total Input File Size input Byte Max return false int reducers conf get Int Configuration log info No of reducers reducers if reducers return false return true The method that creates the Job corresponding to a Map Reduce Oper The assumption is that every Map Reduce Oper will have a load and a store The Job Conf removes the load operator and serializes the input filespec so that Pig Input Format can take over the creation of splits It also removes the store operator and serializes the output filespec so that Pig Output Format can take over record writing The remaining portion of the map plan and reduce plans are serialized and stored for the Pig Map Reduce or Pig Map Only objects to take over the actual running of the plans The Mapper amp Reducer classes and the required key value formats are set Checks if this is a map only job and uses Pig Map Only class as the mapper and uses Pig Map Reduce otherwise If it is a Map Reduce job it is bound to have a package operator Remove it from the reduce plan and serializes it so that the Pig Map Reduce class can use it to package the indexed tuples received by the reducer param mro The Map Reduce Oper for which the Job Conf is required param config the Configuration object from which Job Conf is built param pig Context The Pig Context passed on from execution engine return Job corresponding to mro throws Job Creation Exception Suppress Warnings unchecked private Job get Job Oper Plan plan Map Reduce Oper mro Configuration config Pig Context pig Context throws Job Creation Exception org apache hadoop mapreduce Job nw Job null try nw Job new org apache hadoop mapreduce Job config catch Exception e throw new Job Creation Exception e Configuration conf nw Job get Configuration Array List File Spec inp new Array List File Spec Array List List Operator Key inp Targets new Array List List Operator Key Array List String inp Signature Lists new Array List String Array List Long inp Limits new Array List Long Array List Store store Locations new Array List Store Path tmp Location null add settings for pig statistics String set Script Prop conf get Pig Configuration true if set Script Prop equals Ignore Case true Script State ss Script State get ss add Settings To Conf mro conf conf set Configuration true conf set Configuration true String buff Percent conf get Configuration if buff Percent null Double parse Double buff Percent log info Configuration is not set set to default conf set Configuration else log info Configuration is set to conf get Configuration Date Time Writable setup Available Zone Ids configure Compression conf try Process the Loads List Load lds Plan Helper get Physical Operators mro map Plan Load class if lds null lds size for Load ld lds Load Func lf ld get Load Func lf set Location ld get File get File Name nw Job Store the inp filespecs inp add ld get File if mro reduce Plan is Empty log info Reduce phase detected estimating of required reducers adjust Num Reducers plan mro nw Job else nw Job set Num Reduce Tasks if pig Context in Illustrator pig Context get Exec Type is Local if ok To Run Local nw Job mro lds log info override with the default conf to run in local mode for Entry String String entry default Conf String key entry get Key if key equals Configuration key equals Configuration this must not be set back to the default in case it has been set to for example continue if key starts With fs we do n t want to change fs settings back continue if key starts With io we do n t want to change io settings back continue String value entry get Value if conf get key null conf get key equals value conf set key value conf set Boolean Pig Impl Constants true else log info Search to see if we have any Load Func Store Func that need to pack things into the distributed cache List String cache Files new Array List String List String ship Files new Array List String Udf Cache Ship Files Visitor map Udf Cache File Visitor new Udf Cache Ship Files Visitor mro map Plan map Udf Cache File Visitor visit cache Files add All map Udf Cache File Visitor get Cache Files ship Files add All map Udf Cache File Visitor get Ship Files Udf Cache Ship Files Visitor reduce Udf Cache File Visitor new Udf Cache Ship Files Visitor mro reduce Plan reduce Udf Cache File Visitor visit cache Files add All reduce Udf Cache File Visitor get Cache Files ship Files add All reduce Udf Cache File Visitor get Ship Files setup Distributed Cache pig Context conf cache Files to Array new String false Setup the Distributed Cache for this job List all Jars new Array List for extra Jar pig Context extra Jars if all Jars contains extra Jar all Jars add extra Jar for String udf mro Fs Class clazz pig Context get Class For Alias udf if clazz null String jar Jar Manager find Containing Jar clazz if jar null jar new File jar to to if all Jars contains jar all Jars add jar for String script Jar pig Context script Jars jar new File script Jar to to if all Jars contains jar all Jars add jar for String ship File ship Files jar new File ship File to to if all Jars contains jar all Jars add jar for String default Jar Jar Manager get Default Jars jar new File default Jar to to if all Jars contains jar all Jars add jar for jar all Jars boolean predeployed false for String predeployed Jar pig Context predeployed Jars if predeployed Jar contains new File jar to get Name predeployed true if predeployed if jar get File to Lower Case ends With jar put Jar On Class Path Through Distributed Cache pig Context conf jar else setup Distributed Cache pig Context conf new String jar get Path true File script Jar File Jar Manager create Pig Script Jar pig Context if script Jar File null put Jar On Class Path Through Distributed Cache pig Context conf script Jar File to to for String udf mro Fs if udf contains Cross Object func Pig Context instantiate Func From Spec new Func Spec udf if func instanceof Cross String cross Key Cross func get Cross Key conf set Pig Impl Constants cross Key Integer to String mro get Requested Parallelism if lds null lds size for Load ld lds Store the target operators for tuples read from this input List Physical Operator ld Sucs mro map Plan get Successors ld List Operator Key ld Suc Keys new Array List Operator Key if ld Sucs null for Physical Operator operator ld Sucs ld Suc Keys add operator get Operator Key inp Targets add ld Suc Keys inp Signature Lists add ld get Signature inp Limits add ld get Limit Remove the Load from the plan if pig Context in Illustrator mro map Plan remove ld if Utils is Local pig Context conf Configuration Util replace Config For Local Mode conf conf set Pig Input Format Object Serializer serialize inp conf set Pig Input Format Object Serializer serialize inp Targets conf set Pig Input Format Object Serializer serialize inp Signature Lists conf set Pig Input Format Object Serializer serialize inp Limits Removing job credential entry before serializing pigcontext into jobconf since this path would be invalid for the new job being created pig Context get Properties remove mapreduce job credentials binary conf set Boolean Pig Impl Constants pig Context get Exec Type is Local conf set Pig Impl Constants Object Serializer serialize pig Context get Log j Properties conf set udf import list Object Serializer serialize Pig Context get Package Import List this is for unit tests since some do n t create Pig Server if user specified the job name using switch Pig wo n t reset the name then if System get Property Configuration null pig Context get Properties get Property Pig Context null nw Job set Job Name pig Context get Properties get Property Pig Context if pig Context get Properties get Property Pig Context null If the job priority was set attempt to get the corresponding enum value and set the hadoop job priority String job Priority pig Context get Properties get Property Pig Context to Upper Case try Allow arbitrary case the Hadoop job priorities are all upper case conf set Configuration Job Priority value Of job Priority to String catch Illegal Argument Exception e String Buffer sb new String Buffer The job priority must be one of Job Priority priorities Job Priority values for int i i priorities length i if i sb append sb append priorities i sb append You specified job Priority throw new Job Creation Exception sb to String setup Distributed Cache pig Context conf pig Context get Properties pig streaming ship files true setup Distributed Cache pig Context conf pig Context get Properties pig streaming cache files false nw Job set Input Format Class Pig Input Format class tmp file compression setups This must be done before set Store Location on Stores Utils set Tmp File Compression On Conf pig Context conf Process Store and remove it from the plan Linked List Store map Stores Plan Helper get Physical Operators mro map Plan Store class Linked List Store reduce Stores Plan Helper get Physical Operators mro reduce Plan Store class for Store st map Stores store Locations add st Store Func Interface s Func st get Store Func s Func set Store Location st get File get File Name nw Job if s Func instanceof Overwritable Store Func Overwritable Store Func osf Overwritable Store Func s Func if osf should Overwrite osf cleanup Output st nw Job for Store st reduce Stores store Locations add st Store Func Interface s Func st get Store Func s Func set Store Location st get File get File Name nw Job if s Func instanceof Overwritable Store Func Overwritable Store Func osf Overwritable Store Func s Func if osf should Overwrite osf cleanup Output st nw Job set Output Format nw Job if map Stores size reduce Stores size single store case log info Setting up single store job Store st if reduce Stores is Empty st map Stores get if pig Context in Illustrator mro map Plan remove st else st reduce Stores get if pig Context in Illustrator mro reduce Plan remove st Map Red Util setup Streaming Dirs Conf Single st pig Context conf else if map Stores size reduce Stores size multi store case log info Setting up multi store job Map Red Util setup Streaming Dirs Conf Multi pig Context conf boolean disable Counter conf get Boolean pig disable counter false if disable Counter log info Disable Pig custom output counters int idx for Store sto store Locations sto set Disable Counter disable Counter sto set Multi Store true sto set Index idx store map key type this is needed when the key is null to create an appropriate Nullable Writable object conf set pig map keytype Object Serializer serialize new byte mro map Key Type set parent plan in all operators in map and reduce plans currently the parent plan is really used only when Stream is present in the plan new Phy Plan Setter mro map Plan visit new Phy Plan Setter mro combine Plan visit new Phy Plan Setter mro reduce Plan visit this call modifies the Repl Files names of Join operators within the plans must be called before the plans are serialized setup Distributed Cache For Join mro pig Context conf Schema Tuple Frontend copy All Generated To Distributed Cache pig Context conf Package pack null if mro reduce Plan is Empty Map Only Job nw Job set Mapper Class Pig Map Only Map class if pig Context in Illustrator conf set pig map Plan Object Serializer serialize mro map Plan if mro is End Of All Input Set In Map this is used in Map close to decide whether the pipeline needs to be rerun one more time in the close The pipeline is rerun if there either was a stream or Merge Join conf set true else Map Reduce Job Process the Package operator and remove it from the reduce plan if mro combine Plan is Empty Package comb Pack Package mro combine Plan get Roots get mro combine Plan remove comb Pack nw Job set Combiner Class Pig Combiner Combine class conf set pig combine Plan Object Serializer serialize mro combine Plan conf set pig combine package Object Serializer serialize comb Pack else if mro needs Distinct Combiner nw Job set Combiner Class Distinct Combiner Combine class log info Setting identity combiner class pack Package mro reduce Plan get Roots get if pig Context in Illustrator mro reduce Plan remove pack nw Job set Mapper Class Pig Map Reduce Map class nw Job set Reducer Class Pig Map Reduce Reduce class if mro custom Partitioner null nw Job set Partitioner Class Pig Context resolve Class Name mro custom Partitioner if pig Context in Illustrator conf set pig map Plan Object Serializer serialize mro map Plan if mro is End Of All Input Set In Map this is used in Map close to decide whether the pipeline needs to be rerun one more time in the close The pipeline is rerun only if there was a stream or merge join conf set true if pig Context in Illustrator conf set pig reduce Plan Object Serializer serialize mro reduce Plan if mro is End Of All Input Set In Reduce this is used in Map close to decide whether the pipeline needs to be rerun one more time in the close The pipeline is rerun only if there was a stream conf set pig stream in reduce true if pig Context in Illustrator conf set pig reduce package Object Serializer serialize pack conf set pig reduce key type Byte to String pack get Pkgr get Key Type if mro get Use Secondary Key nw Job set Grouping Comparator Class Pig Secondary Key Group Comparator class nw Job set Partitioner Class Secondary Key Partitioner class nw Job set Sort Comparator Class Pig Secondary Key Comparator class nw Job set Output Key Class Nullable Tuple class conf set pig secondary Sort Order Object Serializer serialize mro get Secondary Sort Order else Class extends Writable Comparable key Class Data Type get Writable Comparable Types pack get Pkgr get Key Type get Class nw Job set Output Key Class key Class select Comparator mro pack get Pkgr get Key Type nw Job nw Job set Output Value Class Nullable Tuple class if mro is Global Sort mro is Limit After Sort if mro is Global Sort String symlink add Single File To Distributed Cache pig Context conf mro get Quant File pigsample conf set pig quantiles File symlink nw Job set Partitioner Class Weighted Range Partitioner class if mro is Comparator Used boolean usercomparator false for String comp Func Spec mro Fs Class comparator Pig Context resolve Class Name comp Func Spec if Comparison Func class is Assignable From comparator nw Job set Mapper Class Pig Map Reduce Map With Comparator class nw Job set Reducer Class Pig Map Reduce Reduce With Comparator class conf set pig reduce package Object Serializer serialize pack conf set pig usercomparator true nw Job set Output Key Class Nullable Tuple class nw Job set Sort Comparator Class comparator usercomparator true break if usercomparator String msg Internal error Ca n t find the comparator throw new Exception msg else conf set pig sort Order Object Serializer serialize mro get Sort Order if mro is Skewed Join String symlink add Single File To Distributed Cache pig Context conf mro get Skewed Join Partition File pigdistkey conf set pig key Dist File symlink nw Job set Partitioner Class Skewed Partitioner class nw Job set Mapper Class Pig Map Reduce Map With Partition Index class nw Job set Map Output Key Class Nullable Partition Writable class nw Job set Grouping Comparator Class Pig Grouping Partition Writable Comparator class if mro is Counter Operation if mro is Row Number nw Job set Mapper Class Pig Map Reduce Counter Pig Map Counter class else nw Job set Reducer Class Pig Map Reduce Counter Pig Reduce Counter class if mro is Rank Operation Iterator String operation Ds mro get Rank Operation Id iterator while operation Ds has Next String operation operation Ds next Iterator Pair String Long it Pairs global Counters get operation iterator Pair String Long pair null while it Pairs has Next pair it Pairs next conf set Long pair first pair second if pig Context in Illustrator unset inputs for Store otherwise map reduce plan will be unnecessarily deserialized for Store st map Stores st set Inputs null st set Parent Plan null for Store st reduce Stores st set Inputs null st set Parent Plan null conf set Object Serializer serialize map Stores conf set Object Serializer serialize reduce Stores String tmp long max Combined Split Size if mro combine Small Splits pig Context get Properties get Property pig split Combination true equals false conf set Boolean pig no Split Combination true else if tmp pig Context get Properties get Property pig max Combined Split Size null null try max Combined Split Size Long parse Long tmp catch Number Format Exception e log warn Invalid numeric format for pig max Combined Split Size use the default maximum combined split size if max Combined Split Size conf set Long pig max Combined Split Size max Combined Split Size It s a hack to set distributed cache file for hadoop Once Mini Cluster do not require local jar on fixed location this can be removed if pig Context get Exec Type Exec Type String newfiles conf get alternative mapreduce job cache files if newfiles null String files conf get Configuration conf set Configuration files null newfiles to String files newfiles Serialize the specific context info Context get Context serialize conf Job cjob new Job new Job Conf conf new Array List Job job Store Map put cjob new Pair List Store Path store Locations tmp Location return cjob catch Job Creation Exception jce throw jce catch Exception e int err Code String msg Internal error creating job configuration throw new Job Creation Exception msg err Code Pig Exception e public static void configure Compression Configuration conf Convert mapred output to output compression See if true equals conf get Configuration conf set output compression enabled true String codec conf get Configuration if codec null throw new Illegal Argument Exception Configuration is set but no value is specified for Configuration else conf set output compression codec codec Adjust the number of reducers based on the default parallel requested parallel and estimated parallel For sampler jobs we also adjust the next job in advance to get its runtime parallel as the number of partitions used in the sampler param plan the plan param mro the operator param nw Job the current job throws Exception public void adjust Num Reducers Oper Plan plan Map Reduce Oper mro org apache hadoop mapreduce Job nw Job throws Exception int job Parallelism calculate Runtime Reducers mro nw Job if mro is Sampler plan get Successors mro null We need to calculate the final number of reducers of the next job order by or skew join to generate the quantfile Map Reduce Oper next Mro plan get Successors mro get Here we use the same conf and Job to calculate the runtime reducers of the next job which is fine as the statistics comes from the next Mro s Loads int n Partitions calculate Runtime Reducers next Mro nw Job set the runtime reducer of the next job as the partition Parallel Constant Visitor visitor new Parallel Constant Visitor mro reduce Plan n Partitions visitor visit log info Setting Parallelism to job Parallelism Configuration conf nw Job get Configuration set various parallelism into the job conf for later analysis conf set Int Pig Impl Constants pig Context default Parallel conf set Int Pig Impl Constants mro requested Parallelism conf set Int Pig Impl Constants mro estimated Parallelism this is for backward compatibility and we encourage to use runtime Parallelism at runtime mro requested Parallelism job Parallelism finally set the number of reducers conf set Int Configuration job Parallelism Calculate the runtime reducers based on the default parallel requested parallel and estimated parallel and save it to Map Reduce Oper s runtime Parallelism return the runtime Parallelism throws Exception private int calculate Runtime Reducers Map Reduce Oper mro org apache hadoop mapreduce Job nw Job throws Exception we do n t recalculate for the same job if mro runtime Parallelism return mro runtime Parallelism int job Parallelism if mro requested Parallelism job Parallelism mro requested Parallelism else if pig Context default Parallel job Parallelism pig Context default Parallel else mro estimated Parallelism estimate Number Of Reducers nw Job mro if mro estimated Parallelism job Parallelism mro estimated Parallelism else reducer estimation could return if it could n t estimate log info Could not estimate number of reducers and no requested or default parallelism set Defaulting to reducer job Parallelism save it mro runtime Parallelism job Parallelism return job Parallelism Looks up the estimator from and invokes it to find the number of reducers to use If is n t set defaults to Input Size Reducer Estimator param job param map Reducer Oper throws Exception public static int estimate Number Of Reducers org apache hadoop mapreduce Job job Map Reduce Oper map Reducer Oper throws Exception Configuration conf job get Configuration Pig Reducer Estimator estimator conf get null new Input Size Reducer Estimator Pig Context instantiate Object From Params conf Pig Reducer Estimator class log info Using reducer estimator estimator get Class get Name int number Of Reducers estimator estimate Number Of Reducers job map Reducer Oper return number Of Reducers public static class Pig Secondary Key Group Comparator extends Writable Comparator public Pig Secondary Key Group Comparator super Nullable Tuple class true Suppress Warnings unchecked Override public int compare Writable Comparable a Writable Comparable b Pig Nullable Writable wa Pig Nullable Writable a Pig Nullable Writable wb Pig Nullable Writable b if wa get Index Pig Nullable Writable mq Flag this is a multi query index if wa get Index Pig Nullable Writable idx Space wb get Index Pig Nullable Writable idx Space return else if wa get Index Pig Nullable Writable idx Space wb get Index Pig Nullable Writable idx Space return If equal we fall through wa and wb are guaranteed to be not null Local Rearrange will create a tuple anyway even if main key and secondary key are both null however main key can be null we need to check for that using the same logic we have in Pig Nullable Writable Object valuea null Object valueb null try Get the main key from compound key valuea Tuple wa get Value As Pig Type get valueb Tuple wb get Value As Pig Type get catch Exec Exception e throw new Runtime Exception Unable to access tuple field e if wa is Null wb is Null int result Data Type compare valuea valueb If any of the field inside tuple is null then we do not merge keys See if result valuea instanceof Tuple valueb instanceof Tuple try for int i i Tuple valuea size i if Tuple valueb get i null return wa get Index Pig Nullable Writable idx Space wb get Index Pig Nullable Writable idx Space catch Exec Exception e throw new Runtime Exception Unable to access tuple field e return result else if valuea null valueb null If they re both null compare the indicies if wa get Index Pig Nullable Writable idx Space wb get Index Pig Nullable Writable idx Space return else if wa get Index Pig Nullable Writable idx Space wb get Index Pig Nullable Writable idx Space return else return else if valuea null return else return public static class Pig Writable Comparator extends Writable Comparator Suppress Warnings unchecked protected Pig Writable Comparator Class c super c Override public int compare byte b int s int l byte b int s int l return Writable Comparator compare Bytes b s l b s l public static class Pig Boolean Writable Comparator extends Pig Writable Comparator public Pig Boolean Writable Comparator super Nullable Boolean Writable class public static class Pig Int Writable Comparator extends Pig Writable Comparator public Pig Int Writable Comparator super Nullable Int Writable class public static class Pig Long Writable Comparator extends Pig Writable Comparator public Pig Long Writable Comparator super Nullable Long Writable class public static class Pig Float Writable Comparator extends Pig Writable Comparator public Pig Float Writable Comparator super Nullable Float Writable class public static class Pig Double Writable Comparator extends Pig Writable Comparator public Pig Double Writable Comparator super Nullable Double Writable class public static class Pig Big Integer Writable Comparator extends Pig Writable Comparator public Pig Big Integer Writable Comparator super Nullable Big Integer Writable class public static class Pig Big Decimal Writable Comparator extends Pig Writable Comparator public Pig Big Decimal Writable Comparator super Nullable Big Decimal Writable class public static class Pig Date Time Writable Comparator extends Pig Writable Comparator public Pig Date Time Writable Comparator super Nullable Date Time Writable class public static class Pig Char Array Writable Comparator extends Pig Writable Comparator public Pig Char Array Writable Comparator super Nullable Text class public static class Pig Writable Comparator extends Pig Writable Comparator public Pig Writable Comparator super Nullable Bytes Writable class public static class Pig Tuple Writable Comparator extends Pig Writable Comparator public Pig Tuple Writable Comparator super Tuple Factory get Instance tuple Class public static class Pig Bag Writable Comparator extends Pig Writable Comparator public Pig Bag Writable Comparator super Bag Factory get Instance new Default Bag get Class hadoop new integration we need to explicitly set the Grouping Comparator public static class Pig Grouping Boolean Writable Comparator extends Writable Comparator public Pig Grouping Boolean Writable Comparator super Nullable Boolean Writable class true public static class Pig Grouping Int Writable Comparator extends Writable Comparator public Pig Grouping Int Writable Comparator super Nullable Int Writable class true public static class Pig Grouping Long Writable Comparator extends Writable Comparator public Pig Grouping Long Writable Comparator super Nullable Long Writable class true public static class Pig Grouping Float Writable Comparator extends Writable Comparator public Pig Grouping Float Writable Comparator super Nullable Float Writable class true public static class Pig Grouping Double Writable Comparator extends Writable Comparator public Pig Grouping Double Writable Comparator super Nullable Double Writable class true public static class Pig Grouping Date Time Writable Comparator extends Writable Comparator public Pig Grouping Date Time Writable Comparator super Nullable Date Time Writable class true public static class Pig Grouping Char Array Writable Comparator extends Writable Comparator public Pig Grouping Char Array Writable Comparator super Nullable Text class true public static class Pig Grouping Writable Comparator extends Writable Comparator public Pig Grouping Writable Comparator super Nullable Bytes Writable class true public static class Pig Grouping Tuple Writable Comparator extends Writable Comparator public Pig Grouping Tuple Writable Comparator super Nullable Tuple class true public static class Pig Grouping Partition Writable Comparator extends Writable Comparator public Pig Grouping Partition Writable Comparator super Nullable Partition Writable class true public static class Pig Grouping Bag Writable Comparator extends Writable Comparator public Pig Grouping Bag Writable Comparator super Bag Factory get Instance new Default Bag get Class true public static class Pig Grouping Big Integer Writable Comparator extends Writable Comparator public Pig Grouping Big Integer Writable Comparator super Nullable Big Integer Writable class true public static class Pig Grouping Big Decimal Writable Comparator extends Writable Comparator public Pig Grouping Big Decimal Writable Comparator super Nullable Big Decimal Writable class true private void select Comparator Map Reduce Oper mro byte key Type org apache hadoop mapreduce Job job throws Job Creation Exception If this operator is involved in an order by use the pig specific raw comparators If it has a cogroup we need to set the comparator class to the raw comparator and the grouping comparator class to pig specific raw comparators which skip the index Otherwise use the hadoop provided raw comparator An operator has an order by if global sort is set or if it s successor has global sort set because in that case it s the sampling job or if it s a limit after a sort boolean has Order By false if mro is Global Sort mro is Limit After Sort mro using Typed Comparator has Order By true else List Map Reduce Oper succs plan get Successors mro if succs null Map Reduce Oper succ succs get if succ is Global Sort has Order By true if has Order By switch key Type case Data Type job set Sort Comparator Class Pig Boolean Raw Comparator class break case Data Type job set Sort Comparator Class Pig Int Raw Comparator class break case Data Type job set Sort Comparator Class Pig Long Raw Comparator class break case Data Type job set Sort Comparator Class Pig Float Raw Comparator class break case Data Type job set Sort Comparator Class Pig Double Raw Comparator class break case Data Type job set Sort Comparator Class Pig Date Time Raw Comparator class break case Data Type job set Sort Comparator Class Pig Text Raw Comparator class break case Data Type job set Sort Comparator Class Pig Bytes Raw Comparator class break case Data Type job set Sort Comparator Class Pig Big Integer Raw Comparator class break case Data Type job set Sort Comparator Class Pig Big Decimal Raw Comparator class break case Data Type int err Code String msg Using Map as key not supported throw new Job Creation Exception msg err Code Pig Exception case Data Type job set Sort Comparator Class Pig Tuple Sort Comparator class break case Data Type err Code msg Using Bag as key not supported throw new Job Creation Exception msg err Code Pig Exception default break return switch key Type case Data Type job set Sort Comparator Class Pig Boolean Writable Comparator class job set Grouping Comparator Class Pig Grouping Boolean Writable Comparator class break case Data Type job set Sort Comparator Class Pig Int Writable Comparator class job set Grouping Comparator Class Pig Grouping Int Writable Comparator class break case Data Type job set Sort Comparator Class Pig Big Integer Writable Comparator class job set Grouping Comparator Class Pig Grouping Big Integer Writable Comparator class break case Data Type job set Sort Comparator Class Pig Big Decimal Writable Comparator class job set Grouping Comparator Class Pig Grouping Big Decimal Writable Comparator class break case Data Type job set Sort Comparator Class Pig Long Writable Comparator class job set Grouping Comparator Class Pig Grouping Long Writable Comparator class break case Data Type job set Sort Comparator Class Pig Float Writable Comparator class job set Grouping Comparator Class Pig Grouping Float Writable Comparator class break case Data Type job set Sort Comparator Class Pig Double Writable Comparator class job set Grouping Comparator Class Pig Grouping Double Writable Comparator class break case Data Type job set Sort Comparator Class Pig Date Time Writable Comparator class job set Grouping Comparator Class Pig Grouping Date Time Writable Comparator class break case Data Type job set Sort Comparator Class Pig Char Array Writable Comparator class job set Grouping Comparator Class Pig Grouping Char Array Writable Comparator class break case Data Type job set Sort Comparator Class Pig Writable Comparator class job set Grouping Comparator Class Pig Grouping Writable Comparator class break case Data Type int err Code String msg Using Map as key not supported throw new Job Creation Exception msg err Code Pig Exception case Data Type job set Sort Comparator Class Pig Tuple Writable Comparator class job set Grouping Comparator Class Pig Grouping Tuple Writable Comparator class break case Data Type err Code msg Using Bag as key not supported throw new Job Creation Exception msg err Code Pig Exception default err Code msg Unhandled key type Data Type find Type Name key Type throw new Job Creation Exception msg err Code Pig Exception private void setup Distributed Cache For Join Map Reduce Oper mro Pig Context pig Context Configuration conf throws Exception new Join Distributed Cache Visitor mro map Plan pig Context conf visit new Join Distributed Cache Visitor mro reduce Plan pig Context conf visit private static void setup Distributed Cache Pig Context pig Context Configuration conf Properties properties String key boolean ship To Cluster throws Exception Set up the Distributed Cache for this job String file Names properties get Property key if file Names null String paths file Names split setup Distributed Cache pig Context conf paths ship To Cluster private static void add To Distributed Cache uri Configuration conf if reset uri to String find Distributed Cache add Cache Archive uri conf else Distributed Cache add Cache File uri conf private static void setup Distributed Cache Pig Context pig Context Configuration conf String paths boolean ship To Cluster throws Exception Turn on the symlink feature Distributed Cache create Symlink conf for String path paths path path trim if path length Path src new Path path Ensure that src is a valid src to src Ship it to the cluster if necessary and add to the Distributed Cache if ship To Cluster Path dst new Path File Localizer get Temporary Path pig Context to String File System fs dst get File System conf fs copy From Local File src dst fs set Replication dst short conf get Int Configuration Construct the dst src Name uri for Distributed Cache dst null try dst new dst to String src get Name catch Syntax Exception ue byte err Src pig Context get Error Source int err Code switch err Src case Pig Exception err Code break case Pig Exception err Code break default err Code break String msg Invalid ship specification File does n t exist dst throw new Exec Exception msg err Code err Src add To Distributed Cache dst conf else add To Distributed Cache src conf private static String add Single File To Distributed Cache Pig Context pig Context Configuration conf String filename String prefix throws Exception if pig Context in Illustrator File Localizer file Exists filename pig Context throw new Exception Internal error skew join partition file filename does not exist String symlink filename Hadoop currently does n t support distributed cache in local mode This line will be removed after the support is added by Hadoop team if Utils is Local pig Context conf symlink prefix Integer to String System identity Hash Code filename Long to String System current Time Millis filename filename symlink setup Distributed Cache pig Context conf new String filename false return symlink Ensure that src is a valid param src the source Path return a for this path throws Exec Exception private static to Path src throws Exec Exception String path In String src to String String fragment null if path In String contains fragment path In String substring path In String index Of path In String path In String substring path In String index Of Encode the path uri new Path path In String to Uri String uri Encoded uri to String if fragment null uri Encoded uri Encoded fragment try return new uri Encoded catch Syntax Exception ue int err Code String msg Invalid cache specification File does n t exist src throw new Exec Exception msg err Code Pig Exception if url is not in will copy the path to from local before adding to distributed cache param pig Context the pig Context param conf the job conf param url the url to be added to distributed cache return the path as seen on distributed cache throws Exception Suppress Warnings deprecation private static void put Jar On Class Path Through Distributed Cache Pig Context pig Context Configuration conf url throws Exception Turn on the symlink feature Distributed Cache create Symlink conf Path dist Cache Path get Existing Dist Cache File Path conf url if dist Cache Path null log info Jar file url already in Distributed Cache as dist Cache Path Not copying to hdfs and adding again else always copies locally the jar file see Pig Server register Jar Path path In ship To pig Context conf url Distributed Cache add File To Class Path path In conf File System get conf log info Added jar url to Distributed Cache through path In private static Path get Existing Dist Cache File Path Configuration conf url throws Exception cache File Uris Distributed Cache get Cache Files conf if cache File Uris null String file Name url get Ref null Filename Utils get Name url get Path url get Ref for cache File Uri cache File Uris Path path new Path cache File Uri String cache File Name cache File Uri get Fragment null path get Name cache File Uri get Fragment Match if both filenames are same and no symlinks or if both symlinks are same or symlink of existing cache file is same as the name of the new file to be added That would be the case when hbase jar hbase jar is configured via Oozie and register hbase jar is done in the pig script If two different files are symlinked to the same name then there is a conflict and hadoop itself does not guarantee which file will be symlinked to that name So we are good if file Name equals cache File Name return path return null public static Path get Cache Staging Dir Configuration conf throws Exception String pig Temp Dir conf get Pig Configuration conf get Pig Configuration tmp String current User System get Property user name Path staging Dir new Path pig Temp Dir current User pigcache File System fs File System get conf fs mkdirs staging Dir fs set Permission staging Dir File Localizer return staging Dir public static Path get From Cache Pig Context pig Context Configuration conf url throws Exception Input Stream is null Input Stream is null Output Stream os null try Path staging Dir get Cache Staging Dir conf String filename Filename Utils get Name url get Path is url open Stream String checksum Digest Utils sha Hex is File System fs File System get conf Path cache Dir new Path staging Dir checksum Path cache File new Path cache Dir filename if fs exists cache File log debug Found url in jar cache at cache Dir long cur Time System current Time Millis fs set Times cache File cur Time return cache File log info Url url was not found in jarcache at cache Dir attempt to copy to cache else return null fs mkdirs cache Dir File Localizer is url open Stream short replication short conf get Int Pig Configuration conf get Int mapred submit replication os fs create cache File replication fs set Permission cache File File Localizer Utils copy Bytes is os true return cache File catch Exception ioe log info Unable to retrieve jar from jar cache ioe return null finally org apache commons io Utils close Quietly is org apache commons io Utils close Quietly is Utils should not close stream to quietly if os null os close copy the file to hdfs in a temporary path param pig Context the pig context param conf the job conf param url the url to ship to hdfs return the location where it was shipped throws Exception private static Path ship To Pig Context pig Context Configuration conf url throws Exception short replication short conf get Int Configuration boolean cache Enabled conf get Boolean Pig Configuration false if cache Enabled Path path On Dfs get From Cache pig Context conf url if path On Dfs null return path On Dfs String suffix Filename Utils get Name url get Path Path dst new Path File Localizer get Temporary Path pig Context to Uri get Path suffix File System fs dst get File System conf Output Stream os null Input Stream is null try is url open Stream os fs create dst Utils copy Bytes is os true finally org apache commons io Utils close Quietly is Utils should not close stream to quietly if os null os close fs set Replication dst replication return dst private static class Join Distributed Cache Visitor extends Phy Plan Visitor private Pig Context pig Context null private Configuration conf null public Join Distributed Cache Visitor Physical Plan plan Pig Context pig Context Configuration conf super plan new Depth First Walker Physical Operator Physical Plan plan this pig Context pig Context this conf conf Override public void visit Join Join join throws Visitor Exception Hadoop currently does n t support distributed cache in local mode This line will be removed after the support is added if Utils is Local pig Context conf return set up distributed cache for the replicated files File Spec repl Files join get Repl Files Array List String replicated Path new Array List String File Spec new Repl Files new File Spec repl Files length long max Size Long value Of pig Context get Properties get Property Pig Configuration the first input is not replicated long size Of Replicated Inputs try for int i i repl Files length i ignore fragmented file String symlink if i join get Fragment symlink pigrepl join get Operator Key to String Integer to String System identity Hash Code repl Files i get File Name Long to String System current Time Millis i replicated Path add repl Files i get File Name symlink Path path new Path repl Files i get File Name File System fs path get File System conf size Of Replicated Inputs Map Red Util get Path Length fs fs get File Status path max Size new Repl Files i new File Spec symlink repl Files i null null repl Files i get Func Spec join set Repl Files new Repl Files if size Of Replicated Inputs max Size throw new Visitor Exception Replicated input files size size Of Replicated Inputs exceeds Pig Configuration max Size setup Distributed Cache pig Context conf replicated Path to Array new String false catch Exception e String msg Internal error Distributed cache could not be set up for the replicated files throw new Visitor Exception msg e Override public void visit Merge Join Merge Join join throws Visitor Exception Hadoop currently does n t support distributed cache in local mode This line will be removed after the support is added if Utils is Local pig Context conf return String index File join get Index File merge join may not use an index file if index File null return try String symlink add Single File To Distributed Cache pig Context conf index File indexfile join set Index File symlink catch Exception e String msg Internal error Distributed cache could not be set up for merge join index file throw new Visitor Exception msg e Override public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception Hadoop currently does n t support distributed cache in local mode This line will be removed after the support is added if Utils is Local pig Context conf return String index File merge Co Grp get Index File Name if index File null throw new Visitor Exception No index file try String symlink add Single File To Distributed Cache pig Context conf index File indexfile mergecogrp merge Co Grp set Index File Name symlink catch Exception e String msg Internal error Distributed cache could not be set up for merge cogrp index file throw new Visitor Exception msg e private static class Parallel Constant Visitor extends Phy Plan Visitor private int rp private boolean replaced false public Parallel Constant Visitor Physical Plan plan int rp super plan new Depth First Walker Physical Operator Physical Plan plan this rp rp Override public void visit Constant Constant Expression cnst throws Visitor Exception if cnst get Requested Parallelism Object obj cnst get Value if obj instanceof Integer if replaced sample job should have only one Constant Expression throw new Visitor Exception Invalid reduce plan more than one Constant Expression found in sampling job cnst set Value rp cnst set Requested Parallelism rp replaced true public static void set Output Format org apache hadoop mapreduce Job job the Output Format we report to Hadoop is always Pig Output Format which can be wrapped with Lazy Output Format provided if Pig Configuration is set if true equals Ignore Case job get Configuration get Pig Configuration Lazy Output Format set Output Format Class job Pig Output Format class else job set Output Format Class Pig Output Format class 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import java io File Not Found Exception import java io Exception import java util Array List import java util Collections import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop mapred Counters import org apache pig backend hadoop executionengine map Reduce Layer File Based Output Size Reader import org apache pig backend hadoop executionengine map Reduce Layer Pig Stats Output Size Reader import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig tools pigstats Pig Stats Job Graph This class encapsulates the runtime statistics of a Map Reduce job Job statistics is collected when job is completed Interface Audience Public Interface Stability Evolving public abstract class Job Stats extends Operator private static final Log Log Factory get Log Job Stats class public static final String Job Statistics alias public static final String Job Statistics alias location public static final String Job Statistics feature public static final String null public static final String null public static enum Job State protected Job State state Job State protected Array List Output Stats outputs protected Array List Input Stats inputs protected Configuration conf protected long hdfs Bytes Read protected long hdfs Bytes Written private String error Msg private Exception exception null protected Job Stats String name Job Graph plan super name plan outputs new Array List Output Stats inputs new Array List Input Stats public abstract String get Job Id public void set Conf Configuration conf if conf null return this conf conf public Job State get State return state public boolean is Successful return state Job State public void set Successful boolean is Successful this state is Successful Job State Job State public String get Error Message return error Msg public Exception get Exception return exception public List Output Stats get Outputs return Collections unmodifiable List outputs public List Input Stats get Inputs return Collections unmodifiable List inputs public String get Alias return String get Annotation public String get Alias Location return String get Annotation public String get Feature return String get Annotation public long get Hdfs Bytes Read return hdfs Bytes Read public long get Hdfs Bytes Written return hdfs Bytes Written Returns the total bytes written to user specified locations of this job public long get Bytes Written long count for Output Stats out outputs long n out get Bytes if n count n return count Returns the total number of records in user specified output locations of this job public long get Record Writtern long count for Output Stats out outputs long rec out get Number Records if rec count rec return count Override public abstract void accept Plan Visitor v throws Frontend Exception Override public boolean is Equal Operator operator if operator instanceof Job Stats return false return name equals Ignore Case operator get Name public void set Error Msg String error Msg this error Msg error Msg public void set Backend Exception Exception e exception e public abstract String get Display String Calculate the median value from the given array param durations return median value protected long calculate Median Value List Long durations long median figure out the median Collections sort durations int mid Point durations size if durations size odd median durations get mid Point else even median durations get mid Point durations get mid Point return median public boolean is Sampler return get Feature contains Script State name public boolean is Indexer return get Feature contains Script State name deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Number Maps instead Deprecated abstract public int get Number Maps deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Number Reduces instead Deprecated abstract public int get Number Reduces deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Max Map Time instead Deprecated abstract public long get Max Map Time deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Min Map Time instead Deprecated abstract public long get Min Map Time deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Avg Map Time instead Deprecated abstract public long get Avg Map Time deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Max Reduce Time instead Deprecated abstract public long get Max Reduce Time deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Min Reduce Time instead Deprecated abstract public long get Min Reduce Time deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Avg Educe Time instead Deprecated abstract public long get Avg Educe Time deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Map Input Records instead Deprecated abstract public long get Map Input Records deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Map Output Records instead Deprecated abstract public long get Map Output Records deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Reduce Input Records instead Deprecated abstract public long get Reduce Input Records deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Reduce Output Records instead Deprecated abstract public long get Reduce Output Records deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Spill Count instead Deprecated abstract public long get Spill Count deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Proactive Spill Count Objects instead Deprecated abstract public long get Proactive Spill Count Objects deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Proactive Spill Count Recs instead Deprecated abstract public long get Proactive Spill Count Recs deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Hadoop Counters instead Deprecated abstract public Counters get Hadoop Counters deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Multi Store Counters instead Deprecated abstract public Map String Long get Multi Store Counters deprecated If you are using mapreduce please cast Job Stats to org apache pig tools pigstats mapreduce Job Stats then use link org apache pig tools pigstats mapreduce Job Stats get Multi Input Counters instead Deprecated abstract public Map String Long get Multi Input Counters Looks up the output size reader from and invokes it to get the size of output If is not set defaults to File Based Output Size Reader param sto Store param conf configuration public static long get Output Size Store sto Configuration conf Pig Stats Output Size Reader reader null String reader Names conf get Pig Stats Output Size Reader File Based Output Size Reader class get Canonical Name for String class Name reader Names split reader Pig Stats Output Size Reader Pig Context instantiate Func From Spec class Name if reader supports sto conf info using output size reader class Name try return reader get Output Size sto conf catch File Not Found Exception e warn unable to find the output file e return catch Exception e warn unable to get byte written of the job e return warn unable to find an output size reader return 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Byte Array Input Stream import java io Exception import java math Big Decimal import java util Array List import java util Hash Map import java util List import java util Map import java util Properties import org apache hadoop io Text import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib input Text Input Format import org apache pig Expression import org apache pig Load Caster import org apache pig Load Func import org apache pig Load Metadata import org apache pig Pig Warning import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Resource Statistics import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl util Context import org apache pig impl util Utils import org codehaus jackson Json Factory import org codehaus jackson Json Parser import org codehaus jackson Json Token import org joda time format Date Time Formatter import org joda time format Date Time Format loader for data stored using link Json Storage This is not a generic loader It depends on the schema being stored with the data when conceivably you could write a loader that determines the schema from the public class Json Loader extends Load Func implements Load Metadata protected Record Reader reader null protected Resource Schema schema null private String udfc Signature null private Json Factory json Factory null private Tuple Factory tuple Factory Tuple Factory get Instance private Bag Factory bag Factory Bag Factory get Instance private static final String pig jsonloader schema public Json Loader public Json Loader String schema String throws Exception schema new Resource Schema Utils parse Schema schema String Override public void set Location String location Job job throws Exception Tell our input format where we will be reading from File Input Format set Input Paths job location Override Suppress Warnings unchecked public Input Format get Input Format throws Exception We will use Text Input Format the default Hadoop input format for text It has a Long Writable key that we will ignore and the value is a Text a string writable that the data is in return new Text Input Format Override public Load Caster get Load Caster throws Exception We do not expect to do casting of byte arrays because we will be returning typed data return null Override Suppress Warnings unchecked public void prepare To Read Record Reader reader Pig Split split throws Exception this reader reader Get the schema string from the Context object Context udfc Context get Context Properties p udfc get Properties this get Class new String udfc Signature String str Schema p get Property if str Schema null throw new Exception Could not find schema in context Parse the schema from the string stored in the properties object schema new Resource Schema Utils get Schema From String str Schema json Factory new Json Factory Override public Tuple get Next throws Exception Text val null try Read the next key value pair from the record reader If it s finished return null if reader next Key Value return null Get the current value We do n t use the key val Text reader get Current Value catch Interrupted Exception ie throw new Exception ie Create a parser specific for this input line This may not be the most efficient approach byte new Bytes new byte val get Length System arraycopy val get Bytes new Bytes val get Length Byte Array Input Stream bais new Byte Array Input Stream new Bytes Json Parser p json Factory create Json Parser bais Create the tuple we will be returning We create it with the right number of fields as the Tuple object is optimized for this case Resource Field Schema fields schema get Fields Tuple t tuple Factory new Tuple fields length Read the start object marker Throughout this file if the parsing is n t what we expect we return a tuple with null fields rather than throwing an exception That way a few mangled lines do n t fail the job try if p next Token Json Token warn Bad record could not find start of record val to String Pig Warning return t Read each field in the record for int i i fields length i t set i read Field p fields i i if p next Token Json Token warn Bad record could not find end of record val to String Pig Warning return t catch Exception jpe Throwable ex jpe get Cause null jpe jpe get Cause warn Encountered exception ex get Class get Name ex get Message Bad record returning null for val Pig Warning finally p close return t private Object read Primitive Json Parser p Json Token tok Resource Field Schema field throws Exception if tok Json Token return null switch field get Type Read based on our expected type case Data Type return p get Boolean Value case Data Type return p get Int Value case Data Type return p get Long Value case Data Type return p get Float Value case Data Type return p get Double Value case Data Type Date Time Formatter formatter Date Time Format date Time Parser return formatter with Offset Parsed parse Date Time p get Text case Data Type byte b p get Text get Bytes Use the constructor that copies the bytes so that we own the memory return new Data Byte Array b b length case Data Type return p get Text case Data Type return p get Big Integer Value case Data Type return new Big Decimal p get Text default throw new Exception Unknown type in input schema field get Type private Object read Field Json Parser p Resource Field Schema field int fieldnum throws Exception Read the next token Json Token tok p next Token if tok null warn Early termination of record expected schema get Fields length fields bug found fieldnum Pig Warning return null Check to see if this value was null if tok Json Token return null tok p next Token Read based on our expected type switch field get Type case Data Type Should be a start of the map object if tok Json Token warn Bad map field could not find start of object field fieldnum Pig Warning return null Map String String m new Hash Map String String while p next Token Json Token String k p get Current Name String v p get Current Token Json Token null p get Text m put k v return m case Data Type if tok Json Token warn Bad tuple field could not find start of object field fieldnum Pig Warning return null Resource Schema s field get Schema Resource Field Schema fs s get Fields Tuple t tuple Factory new Tuple fs length for int j j fs length j t set j read Field p fs j j if p next Token Json Token warn Bad tuple field could not find end of object field fieldnum Pig Warning return null return t case Data Type if tok Json Token warn Bad bag field could not find start of array field fieldnum Pig Warning return null s field get Schema fs s get Fields Drill down the next level to the tuple s schema s fs get Schema fs s get Fields Data Bag bag bag Factory new Default Bag Json Token inner Tok while inner Tok p next Token Json Token t tuple Factory new Tuple fs length if inner Tok Json Token for int j j fs length j t set j read Field p fs j j if p next Token Json Token warn Bad bag tuple field could not find end of object field fieldnum Pig Warning return null bag add t else handle array of kind primitive primitive t set read Primitive p inner Tok fs bag add t return bag default return read Primitive p tok field Override public void set Context Signature String signature udfc Signature signature Override public Resource Schema get Schema String location Job job throws Exception Resource Schema s if schema null s schema else Parse the schema s new Json Metadata get Schema location job true if s null throw new Exception Unable to parse schema found in file in location Now that we have determined the schema store it in our Context properties object so we have it when we need it on the backend Context udfc Context get Context Properties p udfc get Properties this get Class new String udfc Signature p set Property s to String return s Override public Resource Statistics get Statistics String location Job job throws Exception We do n t implement this one return null Override public String get Partition Keys String location Job job throws Exception We do n t have partitions return null Override public void set Partition Filter Expression partition Filter throws Exception We do n t have partitions Override public List String get Ship Files List String cache Files new Array List String Class class List new Class Json Factory class return Func Utils get Ship Files class List 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java io Output Stream import java util Hash Set import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache hadoop mapreduce Job import org apache pig Expression import org apache pig Load Func import org apache pig Load Metadata import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig Resource Statistics import org apache pig Store Metadata import org apache pig backend datastorage Container Descriptor import org apache pig backend datastorage Data Storage import org apache pig backend datastorage Element Descriptor import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop datastorage Data Storage import org apache pig backend hadoop datastorage Directory import org apache pig backend hadoop datastorage File import org apache pig backend hadoop datastorage Path import org apache pig impl io File Localizer import org apache pig impl logical Layer Frontend Exception import org codehaus jackson Json Generation Exception import org codehaus jackson Json Parse Exception import org codehaus jackson map Json Mapping Exception import org codehaus jackson map Object Mapper import org codehaus jackson map util Map Reads and Writes metadata using in metafiles next to the data public class Json Metadata implements Load Metadata Store Metadata private static final Log log Log Factory get Log Json Metadata class private final String schema File Name private final String header File Name private final String stat File Name private boolean print Headers true private byte field Del private byte record Del private transient Map Element Descriptor Boolean lookup Cache new Map Element Descriptor Boolean public Json Metadata this pig schema pig header pig stats public Json Metadata String schema File Name String header File Name String stat File Name this schema File Name schema File Name this header File Name header File Name this stat File Name stat File Name Given a path which may represent a glob pattern a directory comma separated files glob patterns or a file this method finds the set of relevant metadata files on the storage system The algorithm for finding the metadata file is as follows p For each object represented by the path either directly or via a glob If object is a directory and path metaname exists use that as the metadata file Else if parent Path metaname exists use that as the metadata file p Resolving conflicts merging the metadata etc is not handled by this method and should be taken care of by downstream code p param path Path as passed in to a Load Func may be a Hadoop glob param metaname Metadata file designation such as pig schema or pig stats param conf configuration object return Set of element descriptors for all metadata files associated with the files on the path protected Set Element Descriptor find Meta File String path String metaname Configuration conf throws Exception Set Element Descriptor meta File Set new Hash Set Element Descriptor String locations Load Func get Path Strings path for String loc locations Data Storage storage storage new Data Storage new Path loc to Uri Configuration Util to Properties conf String full Path File Localizer full Path loc storage if storage is Container full Path Element Descriptor meta File Path storage as Element full Path metaname if exists meta File Path meta File Set add meta File Path else Element Descriptor descriptors storage as Collection loc for Element Descriptor descriptor descriptors Container Descriptor container null if descriptor instanceof File Path descriptor Path Path descriptor get Path Path parent descriptor Path get Parent container new Directory Data Storage storage parent else descriptor instanceof Directory container Directory descriptor if no custom schema try the parent directory Element Descriptor meta File Path storage as Element container metaname if exists meta File Path meta File Set add meta File Path return meta File Set private boolean exists Element Descriptor e throws Exception if lookup Cache contains Key e return lookup Cache get e else boolean res e exists lookup Cache put e res return res Implementation of Load Meta Data interface Override public String get Partition Keys String location Job job return null Override public void set Partition Filter Expression partition Filter throws Exception For Json Metadata schema is considered optional This method suppresses and logs errors if they are encountered Override public Resource Schema get Schema String location Job job throws Exception return get Schema location job false Read the schema from json metadata file If is Schema On parameter is false the errors are suppressed and logged param location param job param is Schema On return schema throws Exception public Resource Schema get Schema String location Job job boolean is Schema On throws Exception Configuration conf job get Configuration Set Element Descriptor schema File Set null try schema File Set find Meta File location schema File Name conf catch Exception e String msg Could not find schema file for location return null Or Exception is Schema On msg e we assume that all schemas are the same The question of merging schemas is left open for now Element Descriptor schema File null if schema File Set is Empty schema File schema File Set iterator next else String msg Could not find schema file for location return null Or Exception is Schema On msg null log debug Found schema file schema File to String Resource Schema resource Schema null try resource Schema new Object Mapper read Value schema File open Resource Schema class catch Json Parse Exception e String msg Unable to load Resource Schema for location return null Or Exception is Schema On msg e catch Json Mapping Exception e String msg Unable to load Resource Schema for location return null Or Exception is Schema On msg e catch Exception e String msg Unable to load Resource Schema for location return null Or Exception is Schema On msg e return resource Schema private Resource Schema null Or Exception boolean is Schema On String msg Exception e throws Frontend Exception if is Schema On throw new Frontend Exception msg Pig Exception e a valid schema file was probably not expected so just log a debug message and return null log debug msg return null For Json Metadata stats are considered optional This method suppresses and logs errors if they are encountered see org apache pig Load Metadata get Statistics String Job Override public Resource Statistics get Statistics String location Job job throws Exception Configuration conf job get Configuration Set Element Descriptor stat File Set null try stat File Set find Meta File location stat File Name conf catch Exception e log warn could not fine stat file for location return null Element Descriptor stat File null if stat File Set is Empty stat File stat File Set iterator next else log warn Could not find stat file for location return null log debug Found stat file stat File to String Resource Statistics resource Stats null try resource Stats new Object Mapper read Value stat File open Resource Statistics class catch Json Parse Exception e log warn Unable to load Resource Statistics for location e print Stack Trace catch Json Mapping Exception e log warn Unable to load Resource Statistics for location e print Stack Trace catch Exception e log warn Unable to load Resource Statistics for location e print Stack Trace return resource Stats Implementation of Store Meta Data interface Override public void store Statistics Resource Statistics stats String location Job job throws Exception Configuration conf job get Configuration Data Storage storage new Data Storage new Path location to Uri Configuration Util to Properties conf Element Descriptor stat File Path storage as Element location stat File Name if stat File Path exists stats null try new Object Mapper write Value stat File Path create stats catch Json Generation Exception e log warn Unable to write Resource Statistics for location e print Stack Trace catch Json Mapping Exception e log warn Unable to write Resource Statistics for location e print Stack Trace Override public void store Schema Resource Schema schema String location Job job throws Exception Configuration conf job get Configuration Data Storage storage new Data Storage new Path location to Uri Configuration Util to Properties conf Element Descriptor schema File Path storage as Element location schema File Name if schema File Path exists schema null try new Object Mapper write Value schema File Path create schema catch Json Generation Exception e log warn Unable to write Resource Statistics for location e print Stack Trace catch Json Mapping Exception e log warn Unable to write Resource Statistics for location e print Stack Trace if print Headers Element Descriptor header File Path storage as Element location header File Name if header File Path exists Output Stream os header File Path create try String names schema field Names String fn for int i i names length i fn names i null i names i os write fn get Bytes if i names length os write field Del else os write record Del finally os close public void set Field Del byte field Del this field Del field Del public void set Record Del byte record Del this record Del record Del 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Byte Array Output Stream import java io Exception import java util List import java util Map import java util Properties import java math Big Decimal import java math Big Integer import org codehaus jackson Json Encoding import org codehaus jackson Json Factory import org codehaus jackson Json Generator import org apache hadoop fs Path import org apache hadoop io Long Writable import org apache hadoop io Text import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce lib output File Output Format import org apache hadoop mapreduce lib output Text Output Format import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Resource Statistics import org apache pig Store Metadata import org apache pig Store Func import org apache pig Store Resources import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Data Bag import org apache pig impl util Context import org apache pig impl util Utils Pig store function Each Pig tuple is stored on one line as one value for Text Output Format so that it can be read easily using Text Input Format Pig tuples are mapped to objects Pig bags are mapped to arrays Pig maps are also mapped to objects Maps are assumed to be string to string schema is stored in a side file to deal with mapping between and Pig types The schema file share the same format as the one we use in Pig Storage public class Json Storage extends Store Func implements Store Metadata Store Resources protected Record Writer writer null protected Resource Schema schema null private String udfc Signature null private Json Factory json Factory null Default size for the byte buffer should fit most tuples private static final int private static final String pig jsonstorage schema Methods called on the front end Override public Output Format get Output Format throws Exception We will use Text Output Format the default Hadoop output format for text The key is unused and the value will be a Text a string writable type that we store our data in return new Text Output Format Long Writable Text Override public void set Store Location String location Job job throws Exception File Output Format has a utility method for setting up the output location File Output Format set Output Path job new Path location Override public void set Store Func Context Signature String signature store the signature so we can use it later udfc Signature signature Override public void check Schema Resource Schema s throws Exception We wo n t really check the schema here we ll store it in our Context properties object so we have it when we need it on the backend Context udfc Context get Context Properties p udfc get Properties this get Class new String udfc Signature p set Property fix Schema s to String Methods called on the back end Override public void prepare To Write Record Writer writer throws Exception Store the record writer reference so we can use it when it s time to write tuples this writer writer Get the schema string from the Context object Context udfc Context get Context Properties p udfc get Properties this get Class new String udfc Signature String str Schema p get Property if str Schema null throw new Exception Could not find schema in context Parse the schema from the string stored in the properties object schema new Resource Schema Utils get Schema From String str Schema Build a Json factory json Factory new Json Factory Suppress Warnings unchecked public void put Next Tuple t throws Exception Build a Byte Array Output Stream to write the into Byte Array Output Stream baos new Byte Array Output Stream Build the generator Json Generator json json Factory create Json Generator baos Json Encoding Write the beginning of the top level tuple object json write Start Object Resource Field Schema fields schema get Fields for int i i fields length i int tuple Length t size write col if exists in tuple null otherwise if i tuple Length write Field json fields i t get i else write Field json fields i null json write End Object json close Hand a null key and our string to Hadoop try writer write null new Text baos to Byte Array catch Interrupted Exception ie throw new Exception ie Suppress Warnings unchecked private void write Field Json Generator json Resource Field Schema field Object d throws Exception If the field is missing or the value is null write a null if d null json write Null Field field get Name return Based on the field s type write it out switch field get Type case Data Type json write Boolean Field field get Name Boolean d return case Data Type json write Number Field field get Name Integer d return case Data Type json write Number Field field get Name Long d return case Data Type json write Number Field field get Name Float d return case Data Type json write Number Field field get Name Double d return case Data Type json write String Field field get Name d to String return case Data Type json write String Field field get Name d to String return case Data Type json write String Field field get Name String d return case Data Type Since Jackson doesnt have a write Number Field for Big Integer we have to do it manually here json write Field Name field get Name json write Number Big Integer d return case Data Type json write Number Field field get Name Big Decimal d return case Data Type json write Field Name field get Name json write Start Object for Map Entry String Object e Map String Object d entry Set json write String Field e get Key e get Value null null e get Value to String json write End Object return case Data Type json write Field Name field get Name json write Start Object Resource Schema s field get Schema if s null throw new Exception Schemas must be fully specified to use this storage function No schema found for field field get Name Resource Field Schema fs s get Fields for int j j fs length j write Field json fs j Tuple d get j json write End Object return case Data Type json write Field Name field get Name json write Start Array s field get Schema if s null throw new Exception Schemas must be fully specified to use this storage function No schema found for field field get Name fs s get Fields if fs length fs get Type Data Type throw new Exception Found a bag without a tuple inside Drill down the next level to the tuple s schema s fs get Schema if s null throw new Exception Schemas must be fully specified to use this storage function No schema found for field field get Name fs s get Fields for Tuple t Data Bag d json write Start Object for int j j fs length j write Field json fs j t get j json write End Object json write End Array return public void store Statistics Resource Statistics stats String location Job job throws Exception We do n t implement this method public void store Schema Resource Schema schema String location Job job throws Exception Store the schema in a side file in the same directory Map Reduce does not include files starting with when reading data for a job Json Metadata metadata Writer new Json Metadata byte record Del n byte field Del t metadata Writer set Field Del field Del metadata Writer set Record Del record Del metadata Writer store Schema schema location job public Resource Schema fix Schema Resource Schema s for Resource Field Schema filed s get Fields if filed get Type Data Type filed set Type Data Type return s Override public List String get Ship Files Class class List new Class Json Factory class return Func Utils get Ship Files class List Override public List String get Cache Files return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util Hash Map import java util List import java util Map import org apache pig Pig Exception import org apache pig backend hadoop executionengine map Reduce Layer plans Op Plan Visitor import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig data Data Type import org apache pig impl plan Depth First Walker import org apache pig impl plan Visitor Exception visitor to figure out the type of the key for the map plan this is needed when the key is null to create an appropriate Nullable Writable object public class Key Type Discovery Visitor extends Op Plan Visitor non Javadoc see org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor visit Local Rearrange org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange param plan The Oper Plan to visit to discover key Type public Key Type Discovery Visitor Oper Plan plan super plan new Depth First Walker Map Reduce Oper Oper Plan plan Override public void visit Op Map Reduce Oper mr throws Visitor Exception if mr instanceof Native Map Reduce Oper return if mr use Secondary Key mr map Key Type Data Type return boolean found Key Type false Phy Plan Key Type Visitor kvisitor new Phy Plan Key Type Visitor mr map Plan mr kvisitor visit if kvisitor found Key Type look for the key type from a Local Rearrange in the previous reduce List Map Reduce Oper preds m Plan get Predecessors mr if there are no predecessors then we probably are in a simple load store script there is no way to figure out the key type in this case which probably means we do n t need to figure it out if preds null Map Byte Integer seen new Hash Map Byte Integer for Map Reduce Oper pred preds Phy Plan Key Type Visitor visitor new Phy Plan Key Type Visitor pred reduce Plan mr visitor visit found Key Type visitor found Key Type byte type mr map Key Type seen put type if seen size throw exception since we should get the same key type from all predecessors int error Code String message Internal Error Found multiple data types for map key throw new Visitor Exception message error Code Pig Exception if we were not able to find the key and if there is a map and reduce phase then the map would need to send a valid key object and this can be an issue when the key is null so error out here if the reduce phase is empty then this is a map only job which will not need a key object Pig Map Only collect null collect if found Key Type mr reduce Plan is Empty throw exception since we were not able to determine key type int error Code String message Internal Error Unable to determine data type for map key throw new Visitor Exception message error Code Pig Exception static class Phy Plan Key Type Visitor extends Phy Plan Visitor private Map Reduce Oper mro private boolean found Key Type false public Phy Plan Key Type Visitor Physical Plan plan Map Reduce Oper mro super plan new Depth First Walker Physical Operator Physical Plan plan this mro mro Override public void visit Local Rearrange Local Rearrange lr throws Visitor Exception this mro map Key Type lr get Key Type found Key Type true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine import java io Exception import java io Print Stream import java util Array List import java util Arrays import java util Hash Set import java util Iterator import java util List import java util Map import java util Set import java util regex Matcher import java util regex Pattern import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop mapred Status import org apache hadoop mapreduce Task Report import org apache hadoop mapred jobcontrol Job import org apache hadoop mapred jobcontrol Job Control import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend Backend Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Log Utils import org apache pig impl util Utils import org apache pig tools pigstats Pig Stats import com google common collect Lists import com google common collect Maps Provides core processing implementation for the backend of Pig if Execution Engine chosen decides to delegate it s work to this class Also contains set of utility methods including ones centered around Hadoop public abstract class Launcher private static final Log log Log Factory get Log Launcher class private static final String Out Of Memory Error private boolean pig Exception false private boolean out Of Memory false private String new Line n Used to track the exception thrown by the job control which is run in a separate thread protected String job Control Exception Stack Trace null protected Exception job Control Exception null protected long total Hadoop Time Spent protected Map File Spec Exception failure Map protected Job Control jc null protected class Hanging Job Killer extends Thread public Hanging Job Killer Override public void run try kill catch Exception e log warn Error in killing Execution Engine e protected Launcher handle the windows portion of r if System get Property os name to Upper Case starts With new Line r n reset Resets the state after a launch public void reset failure Map Maps new Hash Map total Hadoop Time Spent Method to launch pig for hadoop either for a cluster s job tracker or for a local job runner He only difference between the two is the job client Depending on the pig context the job client will be initialize to one of the two Launchers for other frameworks can overide these methods Given an input Physical Plan it compiles it to get a Map Reduce Plan The Map Reduce plan which has multiple Map Reduce operators each one of which has to be run as a map reduce job with dependency information stored in the plan It compiles the Oper Plan into a Job Control object Each Map Reduce operator is converted into a Job and added to the Job Control object Each Job also has a set of dependent Jobs that are created using the Oper Plan The Job Control object is obtained from the Job Control Compiler Then a new thread is spawned that submits these jobs while respecting the dependency information The parent thread monitors the submitted jobs progress and after it is complete stops the Job Control thread param php param grp Name param pc throws Exception public abstract Pig Stats launch Pig Physical Plan php String grp Name Pig Context pc throws Exception Explain how a pig job will be executed on the underlying infrastructure param pp Physical Plan to explain param pc Pig Context to use for configuration param ps Print Stream to write output on param format Format to write in param verbose Amount of information to print throws Visitor Exception throws Exception public abstract void explain Physical Plan pp Pig Context pc Print Stream ps String format boolean verbose throws Plan Exception Visitor Exception Exception public abstract void kill throws Backend Exception public abstract void kill Job String job Configuration conf throws Backend Exception protected boolean is Complete double prog return int Math ceil prog protected long compute Time Spent Iterator Task Report task Reports long time Spent while task Reports has Next Task Report r task Reports next time Spent r get Finish Time r get Start Time return time Spent protected void get Error Messages Iterator Task Report reports String type boolean err Not Dbg Pig Context pig Context throws Exception while reports has Next Task Report report reports next String msgs report get Diagnostics Array List Exception exceptions new Array List Exception String exception Create Fail Msg null boolean job Failed false if msgs length if report get Current Status Status job Failed true Set String error Message Set new Hash Set String for int j j msgs length j if error Message Set contains msgs j error Message Set add msgs j if err Not Dbg err Not Dbg is used only for failed jobs keep track of all the unique exceptions try Log Utils write Log Backend error message msgs j pig Context get Properties get Property pig logfile log Exception e get Exception From String msgs j exceptions add e catch Exception e exception Create Fail Msg msgs j else log debug Error message from task type report get Task msgs j if there are no valid exception that could be created report if job Failed exceptions size exception Create Fail Msg null int err Code String msg Unable to recreate exception from backed error exception Create Fail Msg throw new Exec Exception msg err Code Pig Exception if its a failed job then check if there is more than one exception more than one exception implies possibly different kinds of failures log all the different failures and throw the exception corresponding to the first failure if job Failed if exceptions size for int j j exceptions size j String header Message Error message from task type report get Task Log Utils write Log exceptions get j pig Context get Properties get Property pig logfile log false header Message false false throw exceptions get else if exceptions size throw exceptions get else int err Code String msg Internal error Expected to throw exception from the backend Did not find any exception to throw throw new Exec Exception msg err Code Pig Exception Compute the progress of the current job submitted through the Job Control object jc to the Job Client job Client param jc The Job Control object that has been submitted param job Client The Job Client to which it has been submitted return The progress as a precentage in double format throws Exception protected double calculate Progress Job Control jc throws Exception double prog prog jc get Successful Jobs size List Job runn Jobs jc get Running Jobs for Job j runn Jobs prog progress Of Running Job j return prog Returns the progress of a Job j which is part of a submitted Job Control object The progress is for this Job So it has to be scaled down by the num of jobs that are present in the Job Control param j The Job for which progress is required return Returns the percentage progress of this Job throws Exception private static double progress Of Running Job Job j throws Exception org apache hadoop mapreduce Job mr Job j get Job try return mr Job map Progress mr Job reduce Progress catch Exception ir return public long get Total Hadoop Time Spent return total Hadoop Time Spent An exception handler class to handle exceptions thrown by the job controller thread Its a local class This is the only mechanism to catch unhandled thread exceptions Unhandled exceptions in threads are handled by the if the handler is not registered explicitly or if the default handler is null public class Job Control Thread Exception Handler implements Thread Uncaught Exception Handler Override public void uncaught Exception Thread thread Throwable throwable set Job Exception throwable protected void set Job Exception Throwable throwable job Control Exception Stack Trace Utils get Stack Strace Str throwable try job Control Exception get Exception From String job Control Exception Stack Trace catch Exception e String err Msg Could not resolve error that occurred when launching job job Control Exception Stack Trace job Control Exception new Runtime Exception err Msg throwable param stack Trace The string representation of link Throwable print Stack Trace print Stack Trace Handles internal Pig Exception and its subclasses that override the link Throwable to String to String method return An exception object whose string representation of print Stack Trace is the input stack Trace throws Exception public Exception get Exception From String String stack Trace throws Exception String lines stack Trace split new Line Throwable t get Exception From Strings lines if pig Exception int err Code String msg During execution encountered a Hadoop error Exec Exception ee new Exec Exception msg err Code Pig Exception t ee set Stack Trace t get Stack Trace return ee else pig Exception false if out Of Memory out Of Memory false int err Code String msg Out of memory Exec Exception ee new Exec Exception msg err Code Pig Exception t ee set Stack Trace t get Stack Trace return ee return Exception t param stack Trace Lines An array of strings that represent link Throwable print Stack Trace print Stack Trace output split by newline return An exception object whose string representation of print Stack Trace is the input stack Trace throws Exception private Throwable get Exception From Strings String stack Trace Lines int starting Line Num throws Exception parse the array of string and throw the appropriate exception first from the line starting Line Num extract the exception name extract the message if any fourth create the appropriate exception and return it An example of the stack trace org apache pig backend executionengine Exec Exception Received a bytearray from the Can not determine how to convert the bytearray to int at org apache pig backend hadoop executionengine physical Layer expression Operators Cast get Next Cast java at org apache pig backend hadoop executionengine physical Layer expression Operators Less Than Expr get Next Less Than Expr java at org apache pig backend hadoop executionengine physical Layer relational Operators Filter get Next Filter java at org apache pig backend hadoop executionengine map Reduce Layer Pig Map Base run Pipeline Pig Map Base java at org apache pig backend hadoop executionengine map Reduce Layer Pig Map Base map Pig Map Base java at org apache pig backend hadoop executionengine map Reduce Layer Pig Map Only Map map Pig Map Only java at org apache hadoop mapred Map Runner run Map Runner java at org apache hadoop mapred Map Task run Map Task java at org apache hadoop mapred Task Tracker Child main Task Tracker java if stack Trace Lines length starting Line Num stack Trace Lines length the regex for matching the exception class name note the use of the for matching nested classes String exception Name Delimiter w w w Pattern exception Name Pattern Pattern compile exception Name Delimiter from the first line extract the exception name and the exception message Matcher exception Name Matcher exception Name Pattern matcher stack Trace Lines starting Line Num String exception Name null String exception Message null if exception Name Matcher find exception Name exception Name Matcher group note that the substring is from end the regex matcher ends at one position beyond the match in this case it will end at colon the exception message will have a preceding space after the colon if exception Name contains out Of Memory true if stack Trace Lines starting Line Num length exception Name Matcher end exception Message stack Trace Lines starting Line Num substring exception Name Matcher end starting Line Num the exception Name should not be null if exception Name null Array List Stack Trace Element stack Trace Elements Lists new Array List Create stack trace elements for the remaining lines String stack Element Regex s at s w w w Pattern stack Element Pattern Pattern compile stack Element Regex String pig Exception Regex org apache pig Pattern pig Exception Pattern Pattern compile pig Exception Regex String more Element Regex s s d s more Pattern more Element Pattern Pattern compile more Element Regex int line Num starting Line Num for line Num stack Trace Lines length line Num Matcher stack Element Matcher stack Element Pattern matcher stack Trace Lines line Num if stack Element Matcher find Stack Trace Element ste get Stack Trace Element stack Trace Lines line Num stack Trace Elements add ste String class Name ste get Class Name Matcher pig Exception Matcher pig Exception Pattern matcher class Name if pig Exception Matcher find pig Exception true else Matcher more Element Matcher more Element Pattern matcher stack Trace Lines line Num if more Element Matcher find line Num break starting Line Num line Num create the appropriate exception setup the stack trace and message Object object Pig Context instantiate Func From Spec exception Name if object instanceof Pig Exception extract the error code and message the regex for matching the custom format of String err Message Regex s d Pattern err Message Pattern Pattern compile err Message Regex Matcher err Message Matcher err Message Pattern matcher exception Message if err Message Matcher find String err Message Stub err Message Matcher group extract the actual exception message sans the again note that the matcher ends at the space following the colon the exception message appears after the space and hence the end exception Message exception Message substring err Message Matcher end the regex to match the error code wich is a string of numerals String err Code Regex d Pattern err Code Pattern Pattern compile err Code Regex Matcher err Code Matcher err Code Pattern matcher err Message Stub String code null if err Code Matcher find code err Code Matcher group could receive a number format exception here but it will be propagated up the stack int err Code if code null err Code Integer parse Int code else err Code create the exception with the message and then set the error code and error source Func Spec func Spec new Func Spec exception Name exception Message object Pig Context instantiate Func From Spec func Spec Pig Exception object set Error Code err Code Pig Exception object set Error Source Pig Exception determine Error Source err Code else else for if err Message Matcher find did not find the error code which means that the Pig Exception or its subclass is not returning the error code highly unlikely should never be here Func Spec func Spec new Func Spec exception Name exception Message object Pig Context instantiate Func From Spec func Spec Pig Exception object set Error Code generic error code Pig Exception object set Error Source Pig Exception else else for if object instanceof Pig Exception its not Pig Exception create the exception with the message object Pig Context instantiate Func From Spec new Func Spec exception Name exception Message Stack Trace Element ste Arr new Stack Trace Element stack Trace Elements size Throwable object set Stack Trace stack Trace Elements to Array ste Arr if starting Line Num stack Trace Lines length Throwable e get Exception From Strings stack Trace Lines starting Line Num Throwable object init Cause e return Throwable object else else for if exception Name null int err Code String msg Did not find exception name to create exception from string Arrays to String stack Trace Lines throw new Exec Exception msg err Code Pig Exception else else for if lines length int err Code String msg Can not create exception from empty string throw new Exec Exception msg err Code Pig Exception param line the string representation of a stack trace returned by link Throwable print Stack Trace print Stack Trace return the Stack Trace Element object representing the stack trace throws Exception public Stack Trace Element get Stack Trace Element String line throws Exception the format of the line is something like at org apache pig backend hadoop executionengine map Reduce Layer Pig Map Only Map map Pig Map Only java note the white space before the at Its not of much importance but noted for posterity String items regex for matching the fully qualified method Name note the use of the for matching nested classes and the use of and for constructors String qualified Method Name Regex w w w Pattern qualified Method Name Pattern Pattern compile qualified Method Name Regex Matcher content Matcher qualified Method Name Pattern matcher line org apache pig backend hadoop executionengine map Reduce Layer Pig Map Only Map map Pig Map Only java String content null if content Matcher find content line substring content Matcher start else int err Code String msg Did not find fully qualified method name to reconstruct stack trace line throw new Exec Exception msg err Code Pig Exception Matcher qualified Method Name Matcher qualified Method Name Pattern matcher content org apache pig backend hadoop executionengine map Reduce Layer Pig Map Only Map map String qualified Method Name null Pig Map Only java String file Details null if qualified Method Name Matcher find qualified Method Name qualified Method Name Matcher group file Details content substring qualified Method Name Matcher end else int err Code String msg Did not find fully qualified method name to reconstruct stack trace line throw new Exec Exception msg err Code Pig Exception From the fully qualified method name extract the declaring class and method name items qualified Method Name split initialize the declaring Class to org in most cases String declaring Class items the last member is always the method name String method Name items items length String Builder sb new String Builder concatenate the names by adding the dot between the members till the penultimate member for int i i items length i sb append sb append items i declaring Class sb to String from the file details extract the file name and the line number Pig Map Only java file Details file Details substring file Details length items file Details split Pig Map Only java String file Name null int line Number if items length file Name items if items length line Number Integer parse Int items return new Stack Trace Element declaring Class method Name file Name line Number public void destroy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Less Than Equal Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Less Than Equal Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Less Than Equal plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Less Than Equal Expression Less Than Equal Expression eo Less Than Equal Expression other return eo get Lhs is Equal get Lhs eo get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Less Than Equal Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Less Than Expr extends Binary Comparison Operator private static final long serial Version transient private final Log log Log Factory get Log get Class public Less Than Expr Operator Key k this k public Less Than Expr Operator Key k int rp super k rp result Type Data Type Override public String name return Less Than Data Type find Type Name result Type m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Less Than this Override public Result get Next Boolean throws Exec Exception Result left right switch operand Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type Result r accum Child null operand Type if r null return r left lhs get Next operand Type right rhs get Next operand Type return do Comparison left right default int err Code String msg this get Class get Simple Name does not know how to handle type Data Type find Type Name operand Type throw new Exec Exception msg err Code Pig Exception Suppress Warnings unchecked private Result do Comparison Result left Result right if left return Status Status return left if right return Status Status return right if either operand is null the result should be null if left result null right result null left result null left return Status Status return left assert left result instanceof Comparable assert right result instanceof Comparable if Comparable left result compare To right result left result Boolean else left result Boolean illustrator Markup null left result Boolean left result return left Override public Less Than Expr clone throws Clone Not Supported Exception Less Than Expr clone new Less Than Expr new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Less Than Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Less Than Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Less Than plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Less Than Expression Less Than Expression eo Less Than Expression other return eo get Lhs is Equal get Lhs eo get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Less Than Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java util Array List import java util List import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend hadoop executionengine map Reduce Layer plans Op Plan Visitor import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig impl Pig Context import org apache pig impl io File Localizer import org apache pig impl io File Spec import org apache pig impl plan Depth First Walker import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Utils public class Limit Adjuster extends Op Plan Visitor Array List Map Reduce Oper ops To Adjust new Array List Map Reduce Oper Pig Context pig Context Node Id Generator nig private String scope public Limit Adjuster Oper Plan plan Pig Context pig Context super plan new Depth First Walker Map Reduce Oper Oper Plan plan this pig Context pig Context nig Node Id Generator get Generator List Map Reduce Oper roots plan get Roots scope roots get get Operator Key get Scope Override public void visit Op Map Reduce Oper mr throws Visitor Exception Look for map reduce operators which contains limit operator If so add one additional map reduce operator with reducer into the original plan This new job can be skipped if at runtime we discover that its parent only has a single reducer mr requested Parallelism This check happen at runtime since that s when reducer estimation happens if mr limit mr limit Plan null ops To Adjust add mr public void adjust throws Exception Plan Exception for Map Reduce Oper mr ops To Adjust if mr reduce Plan is Empty continue List Physical Operator mp Leaves mr reduce Plan get Leaves if mp Leaves size int err Code String msg Expected reduce to have single leaf Found mp Leaves size leaves throw new Compiler Exception msg err Code Pig Exception Physical Operator mp Leaf mp Leaves get if pig Context in Illustrator if mp Leaf instanceof Store int err Code String msg Expected leaf of reduce plan to always be Store Found mp Leaf get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception File Spec old Spec Store mp Leaf get File boolean old Is Tmp Store Store mp Leaf is Tmp Store File Spec f Spec new File Spec File Localizer get Temporary Path pig Context to String new Func Spec Utils get Tmp File Compressor Name pig Context Store store Op Store mp Leaf store Op set File f Spec store Op set Is Tmp Store true mr set Reduce Done true Map Reduce Oper limit Adjust Op new Map Reduce Oper new Operator Key scope nig get Next Node Id scope Load ld new Load new Operator Key scope nig get Next Node Id scope ld set Pc pig Context ld set File f Spec ld set Is Tmp Load true limit Adjust Op map Plan add ld if mr is Global Sort connect Map To Reduce Limited Sort limit Adjust Op mr else Util simple Connect Map To Reduce limit Adjust Op scope nig Need to split the original reduce plan into two mapreduce job st From the root Package to Limit nd From Limit to leaves Store duplicate Limit The reason for doing that We need to have two map reduce job otherwise we will end up with records is number of reducer is limit constant We need one extra mapreduce job with reducer We do n t want to move operator after Limit into the first mapreduce job because Foreach will shift the key type for second mapreduce job see Foreach flatten may generating more than records which get cut by Limit see split Reducer For Limit limit Adjust Op mr if mr is Global Sort limit Adjust Op set Limit After Sort true limit Adjust Op set Sort Order mr get Sort Order Store st new Store new Operator Key scope nig get Next Node Id scope st set File old Spec st set Is Tmp Store old Is Tmp Store st set Schema Store mp Leaf get Schema st set Signature Store mp Leaf get Signature st copy Alias From mp Leaf limit Adjust Op reduce Plan add As Leaf st limit Adjust Op requested Parallelism limit Adjust Op set Limit Only true List Map Reduce Oper successor List m Plan get Successors mr Map Reduce Oper successors null Save a snapshot for successors since we will modify Plan use the list directly will be problematic if successor List null successor List size successors new Map Reduce Oper successor List size int i for Map Reduce Oper op successor List successors i op Process Fs for String udf mr Fs if limit Adjust Op Fs contains udf limit Adjust Op Fs add udf m Plan add limit Adjust Op m Plan connect mr limit Adjust Op if successors null for int i i successors length i Map Reduce Oper next Mr successors i if next Mr null m Plan disconnect mr next Mr if next Mr null m Plan connect limit Adjust Op next Mr Move all operators between Limit and Store in reducer plan from first Op to the second Op private void split Reducer For Limit Map Reduce Oper second Op Map Reduce Oper first Op throws Plan Exception Visitor Exception Physical Operator op first Op reduce Plan get Roots get assert op instanceof Package while true List Physical Operator succs first Op reduce Plan get Successors op if succs null break op succs get if op instanceof Limit find operator after Limit op first Op reduce Plan get Successors op get break Limit p Limit new Limit new Operator Key scope nig get Next Node Id scope p Limit set Limit first Op limit p Limit set Limit Plan first Op limit Plan second Op reduce Plan add As Leaf p Limit while true if op instanceof Store break Physical Operator op To Move op List Physical Operator succs first Op reduce Plan get Successors op op succs get first Op reduce Plan remove And Reconnect op To Move second Op reduce Plan add As Leaf op To Move private void connect Map To Reduce Limited Sort Map Reduce Oper mro Map Reduce Oper sort Op throws Plan Exception Visitor Exception Local Rearrange slr Local Rearrange sort Op map Plan get Leaves get Local Rearrange lr null try lr slr clone catch Clone Not Supported Exception e int err Code String msg Error cloning Local Rearrange for limit after sort throw new Compiler Exception msg err Code Pig Exception e mro map Plan add As Leaf lr Package spkg Package sort Op reduce Plan get Roots get Package pkg null try pkg spkg clone catch Exception e int err Code String msg Error cloning Package Lite for limit after sort throw new Compiler Exception msg err Code Pig Exception e mro reduce Plan add pkg mro reduce Plan add As Leaf Util get Plain For Each scope nig 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical visitor import java io Exception import java util Array List import java util Hash Map import java util List import java util Map import java util Set import org apache pig Eval Func import org apache pig Func Spec import org apache pig Load Caster import org apache pig Load Func import org apache pig Pig Exception import org apache pig data Data Type import org apache pig impl Pig Context import org apache pig Stream To Pig import org apache pig impl builtin Identity Column import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Visitor Exception import org apache pig impl streaming Streaming Command import org apache pig impl streaming Streaming Command Handle Spec import org apache pig impl util Multi Map import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical expression Bin Cond Expression import org apache pig newplan logical expression Cast Expression import org apache pig newplan logical expression Constant Expression import org apache pig newplan logical expression Dereference Expression import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical expression Map Lookup Expression import org apache pig newplan logical expression Project Expression import org apache pig newplan logical expression Scalar Expression import org apache pig newplan logical expression User Func Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema Create mapping between uid and Load Func Spec when the Logical Expression associated with it is known to hold an unmodified element of data returned by the load function This information is used to find the Load Caster to be used to cast bytearrays into other other types public class Lineage Find Rel Visitor extends Logical Relational Nodes Visitor Map Long Func Spec uid Load Func Map new Hash Map Long Func Spec if Logical Relational Operator is associated with a single load func spec then the mapping is stored here Map Logical Relational Operator Func Spec rel Input Func Map new Hash Map Logical Relational Operator Func Spec Map Func Spec Class func caster Map new Hash Map Func Spec Class public Lineage Find Rel Visitor Operator Plan plan throws Frontend Exception super plan new Dependency Order Walker plan public Map Long Func Spec get Uid Load Func Map return uid Load Func Map Override public void visit Load load throws Frontend Exception Logical Schema schema load get Schema set Load Func For Uids schema load get File Spec get Func Spec rel Input Func Map put load load get File Spec get Func Spec Override public void visit Stream stream throws Frontend Exception map To Pred Load Func stream Streaming Command command Stream stream get Streaming Command Handle Spec stream Output Spec command get Output Spec Func Spec stream Loader Spec new Func Spec stream Output Spec get Spec set Load Func For Uids stream get Schema stream Loader Spec rel Input Func Map put stream stream Loader Spec Override public void visit Inner Load inner Load throws Frontend Exception For Each foreach inner Load get For Each Logical Relational Operator pred Logical Relational Operator foreach get Plan get Predecessors foreach get Logical Schema pred Schema pred get Schema if this has a schema the lineage can be tracked using the uid in input schema if inner Load get Schema null if inner Load get Schema size inner Load get Schema get Field type Data Type uid Load Func Map get inner Load get Schema get Field uid null pred Schema null long inp Uid pred Schema get Field inner Load get Projection get Col Num uid if uid Load Func Map get inp Uid null add Uid Load Func To Map inner Load get Schema get Field uid uid Load Func Map get inp Uid return associated load func could not be found using uid use the single load func associated with input relation if any if get Associated Load Func pred null map Rel To Pred Load Func inner Load pred map all uids in schema to func Spec param schema param func Spec throws Visitor Exception private void set Load Func For Uids Logical Schema schema Func Spec func Spec throws Visitor Exception if schema null return for Logical Field Schema fs schema get Fields add Uid Load Func To Map Long fs uid func Spec set Load Func For Uids fs schema func Spec Override public void visit Filter filter throws Frontend Exception map To Pred Load Func filter visit Expression filter get Filter Plan If all predecessors of rel Op are associated with same load func then map re Op to it param rel Op throws Frontend Exception private void map To Pred Load Func Logical Relational Operator rel Op throws Frontend Exception Operator Plan lp rel Op get Plan List Operator preds lp get Predecessors rel Op if lp get Predecessors rel Op null lp get Predecessors rel Op size no predecessors nothing to do return Func Spec load Func Spec get Associated Load Func Logical Relational Operator preds get if load Func Spec null return ensure that all predecessors are mapped to same load func spec for int i i preds size i if have Identical Casters load Func Spec get Associated Load Func Logical Relational Operator preds get i return rel Input Func Map put rel Op load Func Spec Find single load func spec associated with this relation If the relation has schema all uids in schema should be associated with same load func spec if it does not have schema check the existing mapping param rel Op return throws Frontend Exception private Func Spec get Associated Load Func Logical Relational Operator rel Op throws Frontend Exception Logical Schema schema rel Op get Schema Func Spec func Spec null if schema null if schema size return null func Spec uid Load Func Map get schema get Field uid if func Spec null for int i i schema size i Logical Field Schema fs schema get Field i if have Identical Casters func Spec uid Load Func Map get fs uid all uid are not associated with same func spec there is no single func spec that represents all the fields func Spec null break if func Spec null If rel Op is For Each and contains byte field could come from We do n t assume it share the Load Caster with predecessor if rel Op instanceof For Each Finder udf Finder new Finder For Each rel Op get Inner Plan udf Finder visit if udf Finder get List size return null func Spec rel Input Func Map get rel Op return func Spec private void map Rel To Pred Load Func Logical Relational Operator rel Op Operator pred if rel Input Func Map get pred null rel Input Func Map put rel Op rel Input Func Map get pred private void visit Expression Logical Expression Plan exp Plan throws Frontend Exception Lineage Find Exp Visitor find Lineage Exp new Lineage Find Exp Visitor exp Plan uid Load Func Map find Lineage Exp visit Override public void visit Cogroup group throws Frontend Exception map To Pred Load Func group List Operator inputs group get Inputs Logical Plan plan list of field schemas of group plans List Logical Field Schema group Plan Schemas new Array List Logical Field Schema Multi Map Integer Logical Expression Plan plans group get Expression Plans for Logical Expression Plan exp Plan plans values visit Expression exp Plan if exp Plan get Sources size throw new Assertion Error Group plans should have only one output group Plan Schemas add Logical Expression exp Plan get Sources get get Field Schema Logical Schema sch group get Schema if the group plans are associated with same load function associate same load fucntion with group column schema if get Associated Load Func group null add Uid Load Func To Map sch get Field uid rel Input Func Map get group if sch get Field schema null set Load Func For Uids sch get Field schema rel Input Func Map get group else map Match Load Func To Uid sch get Field group Plan Schemas set the load func spec for the bags in the schema this helps if the input schemas are not set group schema has a group column followed by bags corresponding to each input if sch size inputs size throw new Assertion Error cogroup schema size not same as number of inputs for int i i sch size i long uid sch get Field i uid Logical Relational Operator input Logical Relational Operator inputs get i if get Associated Load Func input null add Uid Load Func To Map uid rel Input Func Map get input Override public void visit Join join throws Frontend Exception map To Pred Load Func join Multi Map Integer Logical Expression Plan plans join get Expression Plans for Logical Expression Plan exp Plan plans values visit Expression exp Plan Override public void visit For Each fe throws Frontend Exception map To Pred Load Func fe Logical Plan inner Plan fe get Inner Plan Plan Walker new Walker current Walker spawn Child Walker inner Plan push Walker new Walker current Walker walk this pop Walker Override public void visit Generate gen throws Frontend Exception map To Pred Load Func gen List Logical Expression Plan exp Plans gen get Output Plans for Logical Expression Plan exp Plan exp Plans visit Expression exp Plan associate flatten output to the load func associated with input expression of flatten boolean flattens gen get Flatten Flags for int i i flattens length i if flattens i true get the output schema corresponding to this exp plan gen get Schema if gen get Output Plan Schemas null gen get Output Plan Schemas size i return Logical Schema sch gen get Output Plan Schemas get i if sch null continue get the only output exp of the ith plan Logical Expression exp Logical Expression gen get Output Plans get i get Sources get get its funcspec and associate it with uid of all fields in the schema Func Spec func Spec uid Load Func Map get exp get Field Schema uid for Logical Field Schema fs sch get Fields add Uid Load Func To Map fs uid func Spec Override public void visit Sort sort throws Frontend Exception map To Pred Load Func sort List Logical Expression Plan exp Plans sort get Sort Col Plans for Logical Expression Plan exp Plan exp Plans visit Expression exp Plan Override public void visit Rank rank throws Frontend Exception map To Pred Load Func rank List Logical Expression Plan exp Plans rank get Rank Col Plans for Logical Expression Plan exp Plan exp Plans visit Expression exp Plan Override public void visit Distinct rel Op throws Frontend Exception map To Pred Load Func rel Op Override public void visit Limit lo Limit throws Frontend Exception map To Pred Load Func lo Limit if lo Limit get Limit Plan null visit Expression lo Limit get Limit Plan Override public void visit Split rel Op throws Frontend Exception map To Pred Load Func rel Op Override public void visit Store rel Op throws Frontend Exception map To Pred Load Func rel Op Override public void visit Cross rel Op throws Frontend Exception map To Pred Load Func rel Op Override public void visit Union rel Op throws Frontend Exception map To Pred Load Func rel Op Since the uid changes for Union add mappings for new uids to funcspec Logical Schema schema rel Op get Schema if schema null For each output field checking all the fields being union ed bundled together and only set the funcspec when of them come from the same caster i j i j Checking if i and i have the same caster Same for j and j i and j may come from the different casters for Logical Field Schema logical Field Schema schema get Fields Set Long inputs rel Op get Input Uids logical Field Schema uid if inputs size uid was not changed funcspec should be already set skipping continue Func Spec prev Load Func Spec null cur Load Func Spec null boolean all Same Loader true for Long input Uid inputs cur Load Func Spec uid Load Func Map get input Uid if prev Load Func Spec null have Identical Casters prev Load Func Spec cur Load Func Spec all Same Loader false break prev Load Func Spec cur Load Func Spec if all Same Loader add Uid Load Func To Map logical Field Schema uid cur Load Func Spec Override public void visit Split Output split throws Frontend Exception map To Pred Load Func split visit Expression split get Filter Plan The uid changes across the input and output of a split In this visitor the uids for the fields in output are mapped to ones in input based on position uid Load Func Map is updated with output schema uid and load func mapped to corresponding uid in inputschema Logical Schema input Sch Logical Relational Operator split get Plan get Predecessors split get get Schema Logical Schema out Schema split get Schema if input Sch null out Schema null return if input Sch null out Schema null String msg Bug in split only one of input output schema is null split throw new Visitor Exception split msg Pig Exception if input Sch size out Schema size String msg Bug input and output schema size of split differ split throw new Visitor Exception split msg Pig Exception for int i i input Sch size i Logical Field Schema in Field input Sch get Field i Logical Field Schema out Field out Schema get Field i if uid Load Func Map get in Field uid null add Uid Load Func To Map out Field uid uid Load Func Map get in Field uid Add given uid load Func Spec to mapping param uid param load Func Spec throws Visitor Exception private void add Uid Load Func To Map long uid Func Spec load Func Spec throws Visitor Exception if load Func Spec null return ensure that uid always matches to same load func Func Spec cur Func Spec uid Load Func Map get uid if cur Func Spec null uid Load Func Map put uid load Func Spec else if have Identical Casters cur Func Spec load Func Spec String msg Bug uid mapped to two different load functions cur Func Spec and load Func Spec throw new Visitor Exception msg Pig Exception if uid in input field schemas or their inner schemas map to same load function then map the new uid in bincond also to same load function in uid Load Func Map param out param input Field Schemas throws Visitor Exception void map Match Load Func To Uid Logical Field Schema out List Logical Field Schema input Field Schemas throws Visitor Exception if input Field Schemas size return If schema of any input is null we skip output schema for Logical Field Schema fs input Field Schemas if fs null return if same non null load func is associated with all fieldschemas asssociate that with the uid of out Logical Field Schema inp input Field Schemas get Func Spec func Spec uid Load Func Map get inp uid boolean all Inner Schema Match false if func Spec null boolean all Match true all Inner Schema Match true for Logical Field Schema fs input Field Schemas check if all func spec match if have Identical Casters func Spec uid Load Func Map get fs uid all Match false check if all inner schema match for use later if out schema null out schema is Equal fs schema all Inner Schema Match false if all Match add Uid Load Func To Map out uid func Spec recursively call the function for corresponding files in inner schemas if all Inner Schema Match List Logical Field Schema out Fields out schema get Fields for int i i out Fields size i List Logical Field Schema in Fs List new Array List Logical Field Schema for Logical Field Schema fs input Field Schemas in Fs List add fs schema get Field i map Match Load Func To Uid out Fields get i in Fs List If a input of dereference or map lookup has associated load function the same load function should be associated with the dereference or map lookup class Lineage Find Exp Visitor extends Logical Expression Visitor private Map Long Func Spec uid Load Func Map public Lineage Find Exp Visitor Logical Expression Plan plan Map Long Func Spec uid Load Func Map throws Frontend Exception super plan new Reverse Dependency Order Walker plan this uid Load Func Map uid Load Func Map Override public void visit Project Expression proj throws Frontend Exception proj should have the same uid as input if input has schema if uid does not have associated func spec and input relation has null schema or it is inner load use the load func associated with the relation Logical Relational Operator input Rel proj find Referent if proj get Field Schema null return long uid proj get Field Schema uid if uid Load Func Map get uid null input Rel get Schema null input Rel instanceof Inner Load Func Spec func Spec rel Input Func Map get input Rel if func Spec null add Uid Load Func To Map uid func Spec Override public void visit Dereference Expression deref throws Frontend Exception update Uid Map deref deref get Referred Expression Override public void visit Map Lookup Expression map Lookup throws Frontend Exception update Uid Map map Lookup map Lookup get Map private void update Uid Map Logical Expression exp Logical Expression inp throws Frontend Exception find input uid and corresponding load Func Spec long inp Uid inp get Field Schema uid Func Spec inp Load Func Spec uid Load Func Map get inp Uid add Uid Load Func To Map exp get Field Schema uid inp Load Func Spec Override public void visit Bin Cond Expression bin Cond throws Frontend Exception if either side is a null constant we can safely associate this bincond with the load func spec on other side Logical Expression lhs bin Cond get Lhs Logical Expression rhs bin Cond get Rhs if get Constant In Cast lhs null Func Spec func Spec uid Load Func Map get rhs get Field Schema uid uid Load Func Map put bin Cond get Field Schema uid func Spec else if get Constant In Cast rhs null Func Spec func Spec uid Load Func Map get lhs get Field Schema uid uid Load Func Map put bin Cond get Field Schema uid func Spec else List Logical Field Schema in Field Schemas new Array List Logical Field Schema in Field Schemas add lhs get Field Schema in Field Schemas add rhs get Field Schema map Match Load Func To Uid bin Cond get Field Schema in Field Schemas Override public void visit Scalar Expression scalar Exp throws Frontend Exception track lineage to input load function Logical Relational Operator input Rel Logical Relational Operator scalar Exp get Attached Logical Operator Logical Plan rel Plan Logical Plan input Rel get Plan List Operator soft Preds rel Plan get Soft Link Predecessors input Rel st argument is the column number in input nd arg is the input file name Integer input Col Num Integer Constant Expression scalar Exp get Arguments get get Value String input File String Constant Expression scalar Exp get Arguments get get Value long output Uid scalar Exp get Field Schema uid boolean found Input false a variable to do sanity check on num of input relations find the input relation and use it to get lineage for Operator soft Pred soft Preds Store input Store Store soft Pred if input Store get File Spec get File Name equals input File if found Input true throw new Frontend Exception More than one input found for scalar expression Pig Exception found Input true found the store corresponding to this scalar expression Logical Schema sch input Store get Schema if sch null see if there is a load function associated with the store Func Spec func Spec rel Input Func Map get input Store add Uid Load Func To Map output Uid func Spec else find input uid and corresponding load func Logical Field Schema fs sch get Field input Col Num Func Spec func Spec uid Load Func Map get fs uid add Uid Load Func To Map output Uid func Spec if found Input false throw new Frontend Exception No input found for scalar expression Pig Exception Override public void visit User Func Expression op throws Frontend Exception if op get Field Schema null return Func Spec func Spec null Class loader instantiate Caster op get Func Spec List Logical Expression arguments op get Arguments if loader null if eval Func get Load Caster returns simply use that func Spec op get Func Spec else if arguments size Func Spec base Func Spec null Logical Field Schema fs arguments get get Field Schema if fs null base Func Spec uid Load Func Map get fs uid if base Func Spec null func Spec base Func Spec for int i i arguments size i fs arguments get i get Field Schema if fs null have Identical Casters base Func Spec uid Load Func Map get fs uid func Spec null break if func Spec null add Uid Load Func To Map op get Field Schema uid func Spec in case schema is nested set func Spec for all set Load Func For Uids op get Field Schema schema func Spec if there is a null constant under casts return it param rel return throws Frontend Exception private Object get Constant In Cast Logical Expression rel throws Frontend Exception if rel instanceof Cast Expression if Cast Expression rel get Expression instanceof Cast Expression return get Constant In Cast Cast Expression rel get Expression else if Cast Expression rel get Expression instanceof Constant Expression Constant Expression const Exp Constant Expression Cast Expression rel get Expression if const Exp get Value null return const Exp else return null return null Copied from Cast instantiate Func private Class instantiate Caster Func Spec func Spec throws Visitor Exception if func Spec null return null if func caster Map contains Key func Spec return func caster Map get func Spec Load Caster caster null Object obj Pig Context instantiate Func From Spec func Spec try if obj instanceof Load Func caster Load Func obj get Load Caster else if obj instanceof Stream To Pig caster Stream To Pig obj get Load Caster else if obj instanceof Eval Func caster Eval Func obj get Load Caster else throw new Visitor Exception Invalid class type func Spec get Class Name Pig Exception catch Exception e throw new Visitor Exception Invalid class type func Spec get Class Name e Class retval caster null null caster get Class func caster Map put func Spec retval return retval private boolean have Identical Casters Func Spec f Func Spec f throws Visitor Exception if f null f null return false if f equals f return true Class caster instantiate Caster f Class caster instantiate Caster f if caster null caster null return false From If the class name for Load Caster are the same and Load Caster only has default constructor then two Load Casters are considered equal if caster get Canonical Name equals caster get Canonical Name caster get Constructors length caster get Constructors get Generic Parameter Types length caster get Constructors length caster get Constructors get Generic Parameter Types length return true return false 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Exception import java net import java util Abstract Collection import java util Array List import java util Iterator import java util List import org apache hadoop fs Path import org apache hadoop mapreduce Counter import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Record Reader import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig Load Push Down Required Field List import org apache pig backend hadoop executionengine map Reduce Layer Pig Hadoop Logger import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig builtin Utf Storage Converter import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Context import org apache pig tools pigstats Pig Status Reporter Load Func loads data into Pig It can read from an file or other source Load Func is tightly coupled to Hadoop s link org apache hadoop mapreduce Input Format Load Func s sit atop an Input Format and translate from the keys and values of Hadoop to Pig s tuples p Load Func contains the basic features needed by the majority of load functions For more advanced functionality there are separate interfaces that a load function can implement See link Load Caster link Load Metadata link Load Push Down link Ordered Load Func link Collectable Load Func and link Indexable Load Func Interface Audience Public Interface Stability Stable public abstract class Load Func This method is called by the Pig runtime in the front end to convert the input location to an absolute path if the location is relative The load Func implementation is free to choose how it converts a relative location to an absolute location since this may depend on what the location string represent hdfs path or some other data source param location location as provided in the load statement of the script param cur Dir the current working direction based on any cd statements in the script before the load statement If there are no cd statements in the script this would be the home directory pre user username pre return the absolute location based on the arguments passed throws Exception if the conversion is not possible public String relative To Absolute Path String location Path cur Dir throws Exception return get Absolute Path location cur Dir Communicate to the loader the location of the object s being loaded The location string passed to the Load Func here is the return value of link Load Func relative To Absolute Path String Path Implementations should use this method to communicate the location and any other information to its underlying Input Format through the Job object This method will be called in the frontend and backend multiple times Implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls param location Location as returned by link Load Func relative To Absolute Path String Path param job the link Job object store or retrieve earlier stored information from the link Context throws Exception if the location is not valid public abstract void set Location String location Job job throws Exception This will be called during planning on the front end This is the instance of Input Format rather than the class name because the load function may need to instantiate the Input Format in order to control how it is constructed return the Input Format associated with this loader throws Exception if there is an exception during Input Format construction Suppress Warnings unchecked public abstract Input Format get Input Format throws Exception This will be called on both the front end and the back end during execution return the link Load Caster associated with this loader Returning null indicates that casts from byte array are not supported for this loader construction throws Exception if there is an exception during Load Caster public Load Caster get Load Caster throws Exception return new Utf Storage Converter Initializes Load Func for reading data This will be called during execution before any calls to get Next The Record Reader needs to be passed here because it has been instantiated for a particular Input Split param reader link Record Reader to be used by this instance of the Load Func param split The input link Pig Split to process throws Exception if there is an exception during initialization Suppress Warnings unchecked public abstract void prepare To Read Record Reader reader Pig Split split throws Exception Retrieves the next tuple to be processed Implementations should reuse tuple objects or inner member objects they return across calls and should return a different tuple object in each call return the next tuple to be processed or null if there are no more tuples to be processed throws Exception if there is an exception while retrieving the next tuple public abstract Tuple get Next throws Exception Join multiple strings into a string delimited by the given delimiter param s a collection of strings param delimiter the delimiter return a delimiter separated string public static String join Abstract Collection String s String delimiter if s is Empty return Iterator String iter s iterator String Buffer buffer new String Buffer iter next while iter has Next buffer append delimiter buffer append iter next return buffer to String Parse comma separated path strings into a string array This method escapes commas in the Hadoop glob pattern of the given paths This method is borrowed from link org apache hadoop mapreduce lib input File Input Format jira is opened to make the same name method there accessible We ll use that method directly when the jira is fixed param comma Separated Paths a comma separated string return an array of path strings public static String get Path Strings String comma Separated Paths int length comma Separated Paths length int curly Open int path Start boolean glob Pattern false List String path Strings new Array List String for int i i length i char ch comma Separated Paths char At i switch ch case curly Open if glob Pattern glob Pattern true break case curly Open if curly Open glob Pattern glob Pattern false break case if glob Pattern path Strings add comma Separated Paths substring path Start i path Start i break path Strings add comma Separated Paths substring path Start length return path Strings to Array new String Construct the absolute path from the file location and the current directory The current directory is either of the form code hdfs nodename nodeport directory code in Hadoop Map Reduce mode or of the form code file directory code in Hadoop local mode param location the location string specified in the load statement param cur Dir the current file system directory return the absolute path of file in the file system throws Frontend Exception if the scheme of the location is incompatible with the scheme of the file system public static String get Absolute Path String location Path cur Dir throws Frontend Exception if location null cur Dir null throw new Frontend Exception location location cur Dir cur Dir fs Uri cur Dir to Uri String fs Scheme fs Uri get Scheme if fs Scheme null throw new Frontend Exception cur Dir cur Dir fs Scheme fs Scheme to Lower Case String authority fs Uri get Authority if authority null authority Path root Dir new Path fs Scheme authority Array List String path Strings new Array List String String fnames get Path Strings location for String fname fnames remove leading trailing whitespace s fname fname trim Path p new Path fname uri p to Uri if the supplied location has a scheme i e uri is absolute or an absolute path just use it if uri is Absolute p is Absolute String scheme uri get Scheme if scheme null scheme scheme to Lower Case if scheme null scheme equals fs Scheme throw new Frontend Exception Incompatible file scheme scheme fs Scheme String path uri get Path fname p is Absolute new Path root Dir path to String new Path cur Dir path to String fname fname replace First file file remove the trailing fname fname replace First path Strings add fname return join path Strings This method will be called by Pig both in the front end and back end to pass a unique signature to the link Load Func The signature can be used to store into the link Context any information which the link Load Func needs to store between various method invocations in the front end and back end use case is to store link Required Field List passed to it in link Load Push Down push Projection Required Field List for use in the back end before returning tuples in link Load Func get Next This method will be call before other methods in link Load Func param signature a unique signature to identify this Load Func public void set Context Signature String signature default implementation is a no op Issue a warning Warning messages are aggregated and reported to the user param msg String message of the warning param warning Enum type of warning public final void warn String msg Enum warning Enum Pig Hadoop Logger get Instance warn this msg warning Enum Allow a Load Func to specify a list of files it would like placed in the distributed cache The default implementation returns null return list of files public List String get Cache Files return null Allow a Load Func to specify a list of files located locally and would like to ship to backend through distributed cache Check for link Func Utils for utility function to facilitate it The default implementation returns null return list of files public List String get Ship Files return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util Properties import org apache pig Exec Type import org apache pig backend executionengine Execution Engine import org apache pig impl Pig Context Local Exec Type is the Exec Type for local mode in Hadoop Mapreduce public class Local Exec Type implements Exec Type private static final String modes Override public boolean accepts Properties properties String exec Type Specified properties get Property exectype to Upper Case for String mode modes if exec Type Specified equals mode return true return false Override public Execution Engine get Execution Engine Pig Context pig Context return new Execution Engine pig Context Override public Class get Execution Engine Class return Execution Engine class Override public boolean is Local return true Override public String name return public String to String return name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig pen import java io Exception import java util Array List import java util Hash Map import java util Linked List import java util List import java util Map import org apache hadoop conf Configuration import org apache hadoop io Text import org apache hadoop io Writable import org apache hadoop mapred Task import org apache hadoop mapred jobcontrol Job import org apache hadoop mapred jobcontrol Job Control import org apache hadoop mapreduce Mapper import org apache hadoop mapreduce Reducer import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine map Reduce Layer Job Control Compiler import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Launcher import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Base import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Only import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce Counter import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig data Data Bag import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl builtin Read Scalars import org apache pig impl io File Localizer import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl plan Depth First Walker import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig impl util Configuration Validator import org apache pig impl util Object Serializer import org apache pig impl util Pair import org apache pig newplan logical relational Load import org apache pig newplan logical relational Logical Relational Operator import org apache pig pen util Lineage Tracer Main class that launches pig for Map Reduce public class Local Map Reduce Simulator private Map Reduce Launcher launcher new Map Reduce Launcher private Map Physical Operator Physical Operator phy To Map new Hash Map Physical Operator Physical Operator Suppress Warnings unchecked public void launch Pig Physical Plan php Map Load Data Bag base Data Lineage Tracer lineage Illustrator Attacher attacher Example Generator eg Pig Context pc throws Pig Exception Exception Interrupted Exception phy To Map clear Oper Plan mrp launcher compile php pc Configuration Validator validate Pig Properties pc get Properties Configuration conf Configuration Util to Configuration pc get Properties Job Control Compiler jcc new Job Control Compiler pc conf Job Control jc int num Jobs Compl Data Bag input List Pair Pig Nullable Writable Writable intermediate Data new Array List Pair Pig Nullable Writable Writable Map Job Map Reduce Oper job To Mro Map jcc get Job Mro Map Hash Map String Data Bag output new Hash Map String Data Bag Configuration job Conf jc is null only when mrp size boolean need File Input final Array List Operator Key empty Inp Targets new Array List Operator Key pc get Properties set Property pig illustrating true String jt Identifier System current Time Millis int job Id while mrp size jc jcc compile mrp Illustrator if jc null throw new Exec Exception Native execution is not supported List Job jobs jc get Waiting Jobs for Job job jobs job Id job Conf job get Job Conf File Localizer set Initialized false Array List Array List Operator Key inp Targets Array List Array List Operator Key Object Serializer deserialize job Conf get pig inp Targets intermediate Data clear Map Reduce Oper mro job To Mro Map get job Pig Split split null List Store stores null Physical Operator pack null revisit as there are new physical operators from compilation if mro map Plan is Empty attacher revisit mro map Plan if mro reduce Plan is Empty attacher revisit mro reduce Plan pack mro reduce Plan get Roots get List Load lds Plan Helper get Physical Operators mro map Plan Load class if mro map Plan is Empty stores Plan Helper get Physical Operators mro map Plan Store class if mro reduce Plan is Empty if stores null stores Plan Helper get Physical Operators mro reduce Plan Store class else stores add All Plan Helper get Physical Operators mro reduce Plan Store class for Store store stores output put store get File get File Name attacher get Data Map get store Output Attacher oa new Output Attacher mro map Plan output oa visit if mro reduce Plan is Empty oa new Output Attacher mro reduce Plan output oa visit int index for Load ld lds input output get ld get File get File Name if input null base Data null for Logical Relational Operator lo base Data key Set if Load lo get Schema File equals ld get File get File Name input base Data get lo break if input null mro map Plan remove ld int map Task Id for Load ld lds check newly generated data first input output get ld get File get File Name if input null base Data null if input null base Data null for Logical Relational Operator lo base Data key Set if Load lo get Schema File equals ld get File get File Name input base Data get lo break need File Input input null split new Pig Split null index need File Input empty Inp Targets inp Targets get index index Mapper Text Tuple Pig Nullable Writable Writable map if mro reduce Plan is Empty map only map new Pig Map Only Map Mapper Text Tuple Pig Nullable Writable Writable Context context Pig Map Only Map map get Illustrator Context job Conf input intermediate Data split if mro is Counter Operation if mro is Row Number map new Pig Map Reduce Counter Pig Map Counter context Pig Map Reduce Counter Pig Map Counter map get Illustrator Context job Conf input intermediate Data split Pig Map Base map set Map Plan mro map Plan context get Configuration set Configuration new Task jt Identifier job Id true map Task Id to String map run context else if true equals job Conf get pig usercomparator map new Pig Map Reduce Map With Comparator else if equals job Conf get pig key Dist File map new Pig Map Reduce Map With Partition Index else map new Pig Map Reduce Map Mapper Text Tuple Pig Nullable Writable Writable Context context Pig Map Base map get Illustrator Context job Conf input intermediate Data split Pig Map Base map set Map Plan mro map Plan context get Configuration set Configuration new Task jt Identifier job Id true map Task Id to String map run context if mro reduce Plan is Empty if pack instanceof Package mro reduce Plan remove pack reducer run Pig Map Reduce Reduce reduce if true equals job Conf get pig usercomparator reduce new Pig Map Reduce Reduce With Comparator else reduce new Pig Map Reduce Reduce Reducer Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable Context context reduce get Illustrator Context job intermediate Data Package pack if mro is Counter Operation reduce new Pig Map Reduce Counter Pig Reduce Counter context Pig Map Reduce Counter Pig Reduce Counter reduce get Illustrator Context job intermediate Data Package pack Pig Map Reduce Reduce reduce set Reduce Plan mro reduce Plan context get Configuration set Configuration new Task jt Identifier job Id false to String reduce run context for Physical Operator key mro phy To Map key Set for Physical Operator value mro phy To Map get key phy To Map put key value int removed Op jcc update Op Plan new Linked List Job num Jobs Compl removed Op jcc reset private class Output Attacher extends Phy Plan Visitor private Map String Data Bag output Buffer Output Attacher Physical Plan plan Map String Data Bag output super plan new Depth First Walker Physical Operator Physical Plan plan this output Buffer output Override public void visit User Func User Func user Func throws Visitor Exception if user Func get Func null user Func get Func instanceof Read Scalars Read Scalars user Func get Func set Output Buffer output Buffer public Map Physical Operator Physical Operator get Phy To Map return phy To Map 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util Arrays import java util Collection import java util Collections import java util Hash Map import java util List import java util Map import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Multi Map import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical relational Logical Schema Logical Field Schema public class Cogroup extends Logical Relational Operator List of booleans specifying if any of the cogroups is inner private boolean m Is Inner List of expression Plans according to input private Multi Map Integer Logical Expression Plan m Expression Plans Enum for the type of group public static enum Regular co group Collected group Map side Co Group on sorted data private m Group Type private Logical Field Schema group Key Uid Only Schema This is a map storing Uids which have been generated for an input This map is required to make the uids persistant between calls of reset Schema and get Schema private Map Integer Long generated Input Uids new Hash Map Integer Long final static String group static constant to refer to the option of selecting a group type public final static Integer Constructor for use in defining rule patterns param plan public Cogroup Logical Plan plan super Cogroup plan public Cogroup Operator Plan plan Multi Map Integer Logical Expression Plan expression Plans boolean is Inner this plan expression Plans is Inner public Cogroup Operator Plan plan Multi Map Integer Logical Expression Plan expression Plans group Type boolean is Inner super Cogroup plan this m Expression Plans expression Plans if is Inner null m Is Inner Arrays copy Of is Inner is Inner length this m Group Type group Type Given an expression plan this function returns a Logical Field Schema that can be generated using this expression plan param expr Plan Expression Plan which generates this field return private Logical Field Schema get Plan Schema Logical Expression Plan expr Plan throws Frontend Exception Logical Expression source Exp Logical Expression expr Plan get Sources get Logical Field Schema plan Schema null if source Exp get Field Schema null plan Schema source Exp get Field Schema deep Copy return plan Schema Override public Logical Schema get Schema throws Frontend Exception if schema is calculated before just return if schema null return schema List Operator inputs null inputs plan get Predecessors this if inputs null throw new Frontend Exception this Can not get predecessor for this List Logical Field Schema field Schema List new Array List Logical Field Schema See if we have more than one expression plans if so the schema of the group column will be a tuple boolean has Multiple Keys false for Integer key m Expression Plans key Set if m Expression Plans get key size has Multiple Keys true break Logical Field Schema group Key Schema null Generate the group Field Schema if has Multiple Keys Logical Schema key Schema new Logical Schema We sort here to maintain the correct order of inputs for Integer key m Expression Plans key Set Collection Logical Expression Plan plans m Expression Plans get key for Logical Expression Plan plan plans Logical Field Schema field Schema get Plan Schema plan if any plan schema is null that means we ca n t calculate further schemas so we bail out if field Schema null schema null return schema field Schema new Logical Field Schema field Schema key Schema add Field field Schema We only need fields from one input and not all break group Key Schema new Logical Field Schema key Schema Data Type else We sort here to maintain the correct order of inputs for Integer key m Expression Plans key Set Collection Logical Expression Plan plans m Expression Plans get key for Logical Expression Plan plan plans group Key Schema get Plan Schema plan if any plan schema is null that means we can not figure out the arity of keys just give an empty tuple if group Key Schema null group Key Schema new Logical Schema Logical Field Schema group null Data Type break group Key Schema new Logical Schema Logical Field Schema group Key Schema Change the uid of this field group Key Schema alias break break if m Expression Plans size reset the uid because the group column is associated with more than one input group Key Schema reset Uid if group Key Schema null throw new Frontend Exception this Can not get group key schema for this group Key Uid Only Schema group Key Schema merge Uid group Key Uid Only Schema field Schema List add group Key Schema Generate the Bag Schema int counter for Operator op inputs Logical Schema input Schema Logical Relational Operator op get Schema Check if we already have calculated Uid for this bag for given input operator long bag Uid if generated Input Uids get counter null bag Uid generated Input Uids get counter else bag Uid Logical Expression get Next Uid generated Input Uids put counter bag Uid Logical Field Schema new Tuple Field Schema new Logical Field Schema null input Schema Data Type Logical Expression get Next Uid Logical Schema bag Schema new Logical Schema bag Schema add Field new Tuple Field Schema Logical Field Schema new Bag Field Schema new Logical Field Schema Logical Relational Operator op get Alias bag Schema Data Type bag Uid field Schema List add new Bag Field Schema counter schema new Logical Schema for Logical Field Schema field Schema field Schema List schema add Field field Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Cogroup Cogroup oc Cogroup other if m Group Type oc m Group Type m Is Inner length oc m Is Inner length m Expression Plans size oc m Expression Plans size for int i i m Is Inner length i if m Is Inner i oc m Is Inner i return false for Integer key m Expression Plans key Set if oc m Expression Plans contains Key key return false Collection Logical Expression Plan exp m Expression Plans get key Collection Logical Expression Plan exp oc m Expression Plans get key if exp instanceof Array List exp instanceof Array List throw new Frontend Exception Expected an Array List of Expression Plans Array List Logical Expression Plan exp List Array List Logical Expression Plan exp Array List Logical Expression Plan exp List Array List Logical Expression Plan exp for int i i exp List size i if exp List get i is Equal exp List get i return false return check Equality Logical Relational Operator other return false public get Group Type return m Group Type public void reset Group Type m Group Type Returns an Unmodifiable Map of Input Number to Uid return Unmodifiable Map Integer Long public Map Integer Long get Generated Input Uids return Collections unmodifiable Map generated Input Uids public Multi Map Integer Logical Expression Plan get Expression Plans return m Expression Plans public void set Expression Plans Multi Map Integer Logical Expression Plan plans this m Expression Plans plans public void set Group Type gt m Group Type gt public void set Inner Flags boolean flags if flags null m Is Inner Arrays copy Of flags flags length public boolean get Inner return m Is Inner Override public void reset Uid group Key Uid Only Schema null generated Input Uids new Hash Map Integer Long public List Operator get Inputs Logical Plan plan return plan get Predecessors this 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Plan Visitor public class Cross extends Logical Relational Operator private static final long serial Version private static Log log Log Factory get Log Filter class protected boolean nested false public Cross Logical Plan plan super Cross plan public boolean is Nested return nested public void set Nested boolean nested this nested nested Override public Logical Schema get Schema throws Frontend Exception if schema is calculated before just return if schema null return schema List Operator inputs null inputs plan get Predecessors this if inputs null return null List Logical Schema Logical Field Schema fss new Array List Logical Schema Logical Field Schema for Operator op inputs Logical Schema input Schema Logical Relational Operator op get Schema the schema of one input is unknown so the join schema is unknown just return if input Schema null schema null return schema for int i i input Schema size i Logical Schema Logical Field Schema fs input Schema get Field i Logical Schema Logical Field Schema new null if fs alias null new new Logical Schema Logical Field Schema Logical Relational Operator op get Alias fs alias fs schema fs type fs uid else new new Logical Schema Logical Field Schema fs alias fs schema fs type fs uid fss add new if nested Logical Relational Operator fix Duplicate Uids fss schema new Logical Schema for Logical Schema Logical Field Schema field Schema fss schema add Field field Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Cross return check Equality Logical Relational Operator other else return false public List Operator get Inputs return plan get Predecessors this 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Plan Visitor public class Distinct extends Logical Relational Operator private static final long serial Version private static Log log Log Factory get Log Filter class public Distinct Logical Plan plan super Distinct plan Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Relational Operator input null input Logical Relational Operator plan get Predecessors this get schema input get Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Distinct return check Equality Logical Relational Operator other else return false public Operator get Input Logical Plan plan return plan get Predecessors this get 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression Plan public class Filter extends Logical Relational Operator private static final long serial Version private Logical Expression Plan filter Plan private boolean is Sample public Filter Logical Plan plan super Filter plan public Filter Logical Plan plan Logical Expression Plan filter Plan super Filter plan this filter Plan filter Plan public Filter Logical Plan plan boolean sample this plan is Sample sample public Filter Logical Plan plan Logical Expression Plan filter Plan boolean sample this plan filter Plan is Sample sample public Logical Expression Plan get Filter Plan return filter Plan public void set Filter Plan Logical Expression Plan filter Plan this filter Plan filter Plan public boolean is Sample return is Sample Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Relational Operator input null input Logical Relational Operator plan get Predecessors this get schema input get Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Filter Filter of Filter other return filter Plan is Equal of filter Plan check Equality of else return false public Operator get Input Logical Plan plan return plan get Predecessors this get 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util Deque import java util Linked List import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical expression Project Expression import org apache pig newplan logical optimizer All Same Ralational Nodes Visitor public class For Each extends Logical Relational Operator private static final long serial Version private Logical Plan inner Plan public For Each Operator Plan plan super For Each plan public Logical Plan get Inner Plan return inner Plan public void set Inner Plan Logical Plan p inner Plan p Override public boolean is Equal Operator other throws Frontend Exception if other instanceof For Each return false return inner Plan is Equal For Each other inner Plan Override public Logical Schema get Schema throws Frontend Exception List Operator ll inner Plan get Sinks Logical Relational Operator generate null We can assume Generate is the only sink of the inner plan but only after Dangling Nested Node Remover For Each get Schema will be run before Dangling Nested Node Remover so need to make sure we do get Generate for Operator op ll if op instanceof Generate generate Logical Relational Operator op break if generate null schema generate get Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Find the Inner Load of the inner plan corresponding to the project and also find whether there is a relational operator in inner plan along the way public static Pair List Inner Load Boolean find Reacheable Inner Load From Boundary Project Project Expression project throws Frontend Exception boolean need New Uid false List Inner Load inner Loads new Array List Inner Load Logical Relational Operator referred project find Referent Deque Operator stack new Linked List Operator stack add referred while stack is Empty Operator op stack pop if op instanceof Inner Load inner Loads add Inner Load op else if op instanceof Generate need New Uid true List Operator ops referred get Plan get Predecessors op if ops null for Operator o ops stack push o return new Pair List Inner Load Boolean inner Loads need New Uid public Logical Schema dump Nested Schema String alias String nested Alias throws Frontend Exception Nested Relational Operator Finder op Finder new Nested Relational Operator Finder inner Plan nested Alias op Finder visit if op Finder get Matched Operator null Logical Schema nested Sc op Finder get Matched Operator get Schema return nested Sc return null private static class Nested Relational Operator Finder extends All Same Ralational Nodes Visitor String alias Of Operator Logical Relational Operator op Found null public Nested Relational Operator Finder Logical Plan plan String alias throws Frontend Exception super plan new Reverse Dependency Order Walker plan alias Of Operator alias public Logical Relational Operator get Matched Operator return op Found Override public void execute Logical Relational Operator op throws Frontend Exception if op get Alias null op get Alias equals alias Of Operator op Found op 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util List import java util Map import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical relational Logical Schema Logical Field Schema public class Generate extends Logical Relational Operator private List Logical Expression Plan output Plans private boolean flatten Flags m User Defined Schema is the original input from the user we do n t suppose to store uid in m User Defined Schema private List Logical Schema m User Defined Schema null private List Logical Schema output Plan Schemas null private List Logical Schema exp Schemas null If Generate generate new uid cache it here This happens when expression plan does not have complete schema however user give complete schema in For Each statement in script private List Logical Schema uid Only Schemas null public Generate Operator Plan plan List Logical Expression Plan ps boolean flatten this plan output Plans ps flatten Flags flatten public void set Output Plans List Logical Expression Plan plans this output Plans plans public Generate Operator Plan plan super Generate plan Override public Logical Schema get Schema throws Frontend Exception if schema null return schema if uid Only Schemas null uid Only Schemas new Array List Logical Schema for int i i output Plans size i uid Only Schemas add null schema new Logical Schema output Plan Schemas new Array List Logical Schema exp Schemas new Array List Logical Schema for int i i output Plans size i Logical Expression exp Logical Expression output Plans get i get Sources get Logical Schema m User Defined Schema Copy null if m User Defined Schema null m User Defined Schema get i null m User Defined Schema Copy new Logical Schema for Logical Schema Logical Field Schema fs m User Defined Schema get i get Fields m User Defined Schema Copy add Field fs deep Copy Logical Field Schema field Schema null schema of the expression after flatten Logical Schema exp Schema null if exp get Field Schema null field Schema exp get Field Schema deep Copy exp Schema new Logical Schema if field Schema type Data Type field Schema type Data Type field Schema type Data Type flatten Flags i if type is primitive just add to schema if field Schema null exp Schema add Field field Schema else if bag tuple map do n t have inner schema after flatten we do n t have schema for the entire operator if field Schema schema null exp Schema null else if we come here we get a Tuple Map with flatten extract inner schema of the tuple as exp Schema List Logical Schema Logical Field Schema inner Field Schemas new Array List Logical Schema Logical Field Schema if flatten Flags i if field Schema type Data Type if it is bag get the schema of tuples if field Schema schema null if field Schema schema get Field schema null inner Field Schemas field Schema schema get Field schema get Fields for Logical Schema Logical Field Schema fs inner Field Schemas fs alias fs alias null null field Schema alias fs alias else if field Schema type Data Type should only contain schemafield for Map s value inner Field Schemas field Schema schema get Fields Logical Schema Logical Field Schema fs For Value inner Field Schemas get fs For Value alias field Schema alias value Logical Schema Logical Field Schema fs For Key new Logical Field Schema field Schema alias key null Data Type field Schema uid exp Schema add Field fs For Key else Data Type inner Field Schemas field Schema schema get Fields for Logical Schema Logical Field Schema fs inner Field Schemas fs alias fs alias null null field Schema alias fs alias for Logical Schema Logical Field Schema fs inner Field Schemas exp Schema add Field fs else exp Schema add Field field Schema Merge with user defined schema if exp Schema null exp Schema size exp Schema null Logical Schema plan Schema new Logical Schema exp Schemas add exp Schema if m User Defined Schema Copy null Logical Schema merged Schema new Logical Schema merge with user Defined Schema if exp Schema null Use user defined schema for Logical Field Schema fs m User Defined Schema Copy get Fields fs stamp Field Schema merged Schema add Field new Logical Field Schema fs else Merge uid with the exp field schema merged Schema Logical Schema merge m User Defined Schema Copy exp Schema Logical Schema Merge Mode Load For Each if merged Schema null throw new Frontend Exception this Can not merge exp Schema to String false with user defined schema m User Defined Schema Copy to String false merged Schema merge Uid exp Schema set Null Type To Byte Array Type merged Schema for Logical Field Schema fs merged Schema get Fields plan Schema add Field fs else if any plan do not have schema the whole Generate do not have schema if exp Schema null plan Schema null else Merge schema for the plan for Logical Field Schema fs exp Schema get Fields plan Schema add Field fs if plan Schema null schema null break for Logical Field Schema fs plan Schema get Fields schema add Field fs If the schema is generated by user defined schema keep uid if exp Schema null Logical Schema uid Only Schema plan Schema merge Uid uid Only Schemas get i uid Only Schemas set i uid Only Schema output Plan Schemas add plan Schema if schema null schema size schema null output Plan Schemas null return schema public List Logical Expression Plan get Output Plans return output Plans public boolean get Flatten Flags return flatten Flags public void set Flatten Flags boolean flatten flatten Flags flatten Override public boolean is Equal Operator other throws Frontend Exception if other instanceof Generate return false List Logical Expression Plan other Plan Generate other get Output Plans boolean fs Generate other get Flatten Flags if output Plans size other Plan size return false for int i i output Plans size i if flatten Flags i fs i return false if output Plans get i is Equal other Plan get i return false return true Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public String to String String Builder msg new String Builder if alias null msg append alias msg append Name name for int i i flatten Flags length i msg append flatten Flags i if i flatten Flags length msg append msg append Schema if schema null msg append schema else msg append null msg append if annotations null for Map Entry String Object entry annotations entry Set msg append entry return msg to String public List Logical Schema get User Defined Schema return m User Defined Schema public void set User Defined Schema List Logical Schema user Defined Schema m User Defined Schema user Defined Schema Get the output schema corresponding to each input expression plan return list of output schemas public List Logical Schema get Output Plan Schemas return output Plan Schemas public void set Output Plan Schemas List Logical Schema output Plan Schemas this output Plan Schemas output Plan Schemas public List Logical Schema get Uid Only Schemas return uid Only Schemas public void set Uid Only Schemas List Logical Schema uid Only Schemas this uid Only Schemas uid Only Schemas Override public void reset Uid this uid Only Schemas null Override public void reset Schema super reset Schema output Plan Schemas null public List Logical Schema get Exp Schemas return exp Schemas private void set Null Type To Byte Array Type Logical Schema s if s null for Logical Field Schema fs s get Fields if Data Type is Schema Type fs type set Null Type To Byte Array Type fs schema else if fs type Data Type fs type Data Type 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema Logical representation of expression operators Expression operators have a data type and a uid Uid is a unique id for each expression public abstract class Logical Expression extends Operator static long next Uid protected Logical Schema Logical Field Schema field Schema protected Logical Schema Logical Field Schema uid Only Field Schema static public long get Next Uid return next Uid used for junit test should not be called elsewhere static public void reset Next Uid next Uid param name of the operator param plan Logical Expression Plan this is part of public Logical Expression String name Operator Plan plan super name plan This is a convenience method to avoid the side effectful nature of get Field Schema It simply returns whether or not field Schema is currently null public boolean has Field Schema return field Schema null Get the field schema for the output of this expression operator This does not merely return the field schema variable If schema is not yet set this will attempt to construct it Therefore it is abstract since each operator will need to construct its field schema differently return the Field Schema throws Frontend Exception abstract public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception public void reset Field Schema field Schema null Get the data type for this expression return data type one of the static bytes of Data Type public byte get Type throws Frontend Exception if get Field Schema null get Field Schema type Data Type return get Field Schema type return Data Type public String to String String Builder msg new String Builder msg append Name name Type if field Schema null msg append Data Type find Type Name field Schema type else msg append null msg append Uid if field Schema null msg append field Schema uid else msg append null msg append return msg to String public void never Use For Real Set Field Schema Logical Field Schema fs throws Frontend Exception field Schema fs uid Only Field Schema field Schema merge Uid uid Only Field Schema Create the deep copy of this expression and add that into the passed Logical Expression Plan Return the copy of this expression with updated logical expression plan param lg Exp Plan Logical Expression Plan in which this expression will be added return Logical Expression with its own logical expression plan throws Exception abstract public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Erase all cached uid regenerate uid when we regenerating schema This process currently only used in Implicit Split Insert which will insert split and invalidate some uids in plan public void reset Uid uid Only Field Schema null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java io Exception import java io Print Stream import java util Array List import java util Hash Set import java util Iterator import java util List import org apache pig Pig Constants import org apache pig Pig Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig data Schema Tuple Backend import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Compilation Message Collector import org apache pig impl plan Compilation Message Collector Message Type import org apache pig impl util Hash Output Stream import org apache pig impl util Object Serializer import org apache pig newplan Base Operator Plan import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical Dot Printer import org apache pig newplan logical optimizer Logical Plan Optimizer import org apache pig newplan logical optimizer Logical Plan Printer import org apache pig newplan logical optimizer Schema Resetter import org apache pig newplan logical optimizer Uid Resetter import org apache pig newplan logical visitor Cast Lineage Setter import org apache pig newplan logical visitor Column Alias Conversion Visitor import org apache pig newplan logical visitor Dangling Nested Node Remover import org apache pig newplan logical visitor Duplicate For Each Column Rewrite Visitor import org apache pig newplan logical visitor For Each User Schema Visitor import org apache pig newplan logical visitor Implicit Split Insert Visitor import org apache pig newplan logical visitor Input Output File Validator Visitor import org apache pig newplan logical visitor Scalar Variable Validator import org apache pig newplan logical visitor Scalar Visitor import org apache pig newplan logical visitor Schema Alias Visitor import org apache pig newplan logical visitor Sort Info Setter import org apache pig newplan logical visitor Store Alias Setter import org apache pig newplan logical visitor Type Checking Rel Visitor import org apache pig newplan logical visitor Union On Schema Setter import org apache pig pen Optimize Disabler import org apache pig validator Black And Whitelist Validator import com google common base Splitter import com google common collect Lists import com google common hash Hash Function import com google common hash Hashing Logical Plan is the logical view of relational operations Pig will execute for a given script Note that it contains only relational operations All expressions will be contained in Logical Expression Plans inside each relational operator public class Logical Plan extends Base Operator Plan public Logical Plan Logical Plan other shallow copy constructor super other public Logical Plan super Equality is checked by calling equals on every leaf in the plan This assumes that plans are always connected graphs It is somewhat inefficient since every leaf will test equality all the way to every root But it is only intended for use in testing so that should be ok Checking predecessors as opposed to successors was chosen because splits which have multiple successors do not depend on order of outputs for correctness whereas joins with multiple predecessors do That is reversing the outputs of split in the graph has no correctness implications whereas reversing the inputs of join can This method of doing equals will detect predecessors in different orders but not successors in different orders It will return false if either plan has non deterministic Eval Func Override public boolean is Equal Operator Plan other throws Frontend Exception if other null other instanceof Logical Plan return false return super is Equal other Override public void explain Print Stream ps String format boolean verbose throws Frontend Exception if format equals xml ps println logical Plan Not Supported logical Plan return ps println ps println New Logical Plan ps println if this size ps println Logical plan is empty else if format equals dot Dot Printer lpp new Dot Printer this ps lpp dump else Logical Plan Printer npp new Logical Plan Printer this ps npp visit public Operator find By Alias String alias Iterator Operator it get Operators List Operator ops new Array List Operator while it has Next Logical Relational Operator op Logical Relational Operator it next if op get Alias null continue if op get Alias equals alias ops add op if ops is Empty return null else return ops get ops size Last one Returns the signature of the Logical Plan The signature is a unique identifier for a given plan generated by a Pig script The same script run multiple times with the same version of Pig is guaranteed to produce the same signature even if the input or output locations differ return a unique identifier for the logical plan throws Frontend Exception if signature ca n t be computed public String get Signature throws Frontend Exception Use a streaming hash function We use a murmur function with a constant seed Hash Function hf Hashing murmur Hash Output Stream hos new Hash Output Stream hf Print Stream ps new Print Stream hos Logical Plan Printer printer new Logical Plan Printer this ps printer visit return Integer to String hos get Hash Code as Int public void validate Pig Context pig Context String scope boolean skip Input Output Validation throws Frontend Exception new Dangling Nested Node Remover this visit new Column Alias Conversion Visitor this visit new Schema Alias Visitor this visit new Scalar Visitor this pig Context scope visit new For Each User Schema Visitor this visit Implicit Split Insert Visitor has to be called before Duplicate For Each Column Rewrite Visitor Detail at pig new Implicit Split Insert Visitor this visit Duplicate For Each Column Rewrite Visitor should be before Type Checking Rel Visitor which does reset Schema get Schema heavily new Duplicate For Each Column Rewrite Visitor this visit Compilation Message Collector collector new Compilation Message Collector new Type Checking Rel Visitor this collector visit new Union On Schema Setter this visit new Cast Lineage Setter this collector visit new Scalar Variable Validator this visit new Store Alias Setter this visit compute whether output data is sorted or not new Sort Info Setter this visit boolean aggregate Warning true equals Ignore Case pig Context get Properties get Property aggregate warning if aggregate Warning Compilation Message Collector log Messages collector Message Type Warning aggregate Warning log else for Enum type Message Type values Compilation Message Collector log All Messages collector log if skip Input Output Validation pig Context in Explain pig Context in Dump Schema Validate input output file new Input Output File Validator Visitor this pig Context visit Black And Whitelist Validator validator new Black And Whitelist Validator pig Context this validator validate Now make sure the plan is consistent Uid Resetter uid Resetter new Uid Resetter this uid Resetter visit Schema Resetter schema Resetter new Schema Resetter this true skip duplicate uid check schema Resetter visit public void optimize Pig Context pig Context throws Frontend Exception if pig Context in Illustrator disable all specific optimizations Optimize Disabler pod new Optimize Disabler this pod visit Hash Set String disabled Optimizer Rules try disabled Optimizer Rules Hash Set String Object Serializer deserialize pig Context get Properties get Property Pig Impl Constants catch Exception ioe int err Code String msg Unable to deserialize optimizer rules throw new Frontend Exception msg err Code Pig Exception ioe if disabled Optimizer Rules null disabled Optimizer Rules new Hash Set String String pig Optimizer Rules Disabled pig Context get Properties get Property Pig Constants if pig Optimizer Rules Disabled null disabled Optimizer Rules add All Lists new Array List Splitter on split pig Optimizer Rules Disabled if pig Context in Illustrator disabled Optimizer Rules add Merge For Each disabled Optimizer Rules add Partition Filter Optimizer disabled Optimizer Rules add Limit Optimizer disabled Optimizer Rules add Nested Limit Optimizer disabled Optimizer Rules add Split Filter disabled Optimizer Rules add Push Up Filter disabled Optimizer Rules add Merge Filter disabled Optimizer Rules add Push Down For Each Flatten disabled Optimizer Rules add Column Map Key Prune disabled Optimizer Rules add Add For Each disabled Optimizer Rules add Group By Const Parallel Setter try Schema Tuple Backend initialize Configuration Util to Configuration pig Context get Properties true pig Context catch Exception e throw new Frontend Exception e run optimizer Logical Plan Optimizer optimizer new Logical Plan Optimizer this disabled Optimizer Rules pig Context optimizer optimize 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig parser import java io Exception import java math Big Decimal import java math Big Integer import java net Malformed Exception import java util Array List import java util Hash Map import java util Iterator import java util List import java util Map import java util Properties import org antlr runtime Int Stream import org antlr runtime Recognition Exception import org apache pig Exec Type import org apache pig Func Spec import org apache pig Load Func import org apache pig Non Load Func import org apache pig Pig Configuration import org apache pig Store Func Interface import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig builtin Assert import org apache pig builtin Cube Dimensions import org apache pig builtin Invoker Generator import org apache pig builtin Pig Storage import org apache pig builtin import org apache pig builtin Rollup Dimensions import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl streaming Streaming Command import org apache pig impl streaming Streaming Command Handle import org apache pig impl streaming Streaming Command Handle Spec import org apache pig impl util Multi Map import org apache pig impl util String Utils import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical expression Bin Cond Expression import org apache pig newplan logical expression Constant Expression import org apache pig newplan logical expression Is Null Expression import org apache pig newplan logical expression Less Than Expression import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Not Expression import org apache pig newplan logical expression Or Expression import org apache pig newplan logical expression Project Expression import org apache pig newplan logical expression User Func Expression import org apache pig newplan logical optimizer Schema Resetter import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Cube import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Native import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig newplan logical rules Optimizer Utils import org apache pig newplan logical visitor Proj Star In Udf Expander import org apache pig newplan logical visitor Project Star Expander import org apache pig newplan logical visitor Reset Projection Attached Relational Op Visitor import org apache pig validator Black And Whitelist Filter import org apache pig validator Pig Command Filter public class Logical Plan Builder private Logical Plan plan new Logical Plan private String last Rel null private Map String Operator operators new Hash Map String Operator Override public Operator put String k Operator v last Rel k return super put k v Map String String file Name Map private Pig Context pig Context null private String scope null private Int Stream int Stream private int store Index private int load Index private final Black And Whitelist Filter filter private static Node Id Generator node Id Gen Node Id Generator get Generator public static long get Next Id String scope return node Id Gen get Next Node Id scope Logical Plan Builder Pig Context pig Context String scope Map String String file Name Map Int Stream input this pig Context pig Context this scope scope this file Name Map file Name Map this int Stream input this filter new Black And Whitelist Filter this pig Context Logical Plan Builder Int Stream input throws Exec Exception pig Context new Pig Context Exec Type new Properties pig Context connect this scope test this file Name Map new Hash Map String String this int Stream input this filter new Black And Whitelist Filter this pig Context Operator lookup Operator String alias return operators get alias Func Spec lookup Function String alias return pig Context get Func Spec From Alias alias Streaming Command lookup Command String alias return pig Context get Command For Alias alias void define Command String alias Streaming Command command try filter validate Pig Command Filter Command catch Frontend Exception e throw new Runtime Exception e get Message pig Context register Stream Cmd alias command void define Function String alias Func Spec fs try filter validate Pig Command Filter Command catch Frontend Exception e throw new Runtime Exception e pig Context register Function alias fs Logical Plan get Plan return plan Map String Operator get Operators return operators Filter create Filter Op return new Filter plan Limit create Limit Op return new Limit plan Filter create Sample Op return new Filter plan true String build Filter Op Source Location loc Filter op String alias String input Alias Logical Expression Plan expr throws Parser Validation Exception op set Filter Plan expr alias build Op loc op alias input Alias null it should actually return same alias try new Proj Star In Udf Expander op get Plan visit op new Schema Resetter op get Plan true visit op catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias String build Distinct Op Source Location loc String alias String input Alias String partitioner throws Parser Validation Exception Distinct op new Distinct plan return build Op loc op alias input Alias partitioner String build Limit Op Source Location loc String alias String input Alias long limit throws Parser Validation Exception Limit op new Limit plan limit return build Op loc op alias input Alias null String build Limit Op Source Location loc Limit op String alias String input Alias Logical Expression Plan expr throws Parser Validation Exception op set Limit Plan expr return build Op loc op alias input Alias null String build Sample Op Source Location loc String alias String input Alias double value Source Location val Loc throws Parser Validation Exception Logical Expression Plan filter Plan new Logical Expression Plan Generate a filter condition Logical Expression konst new Constant Expression filter Plan value konst set Location val Loc User Func Expression udf new User Func Expression filter Plan new Func Spec class get Name new Less Than Expression filter Plan udf konst Filter filter new Filter plan true return build Filter Op loc filter alias input Alias filter Plan String build Sample Op Source Location loc Filter filter String alias String input Alias Logical Expression Plan sample Plan Logical Expression expr throws Parser Validation Exception User Func Expression udf new User Func Expression sample Plan new Func Spec class get Name new Less Than Expression sample Plan udf expr return build Filter Op loc filter alias input Alias sample Plan String build Union Op Source Location loc String alias List String input Aliases boolean on Schema throws Parser Validation Exception check Duplicate Aliases input Aliases loc Union op new Union plan on Schema return build Op loc op alias input Aliases null String build Split Op Source Location loc String input Alias throws Parser Validation Exception Split op new Split plan return build Op loc op null input Alias null Split Output create Split Output Op return new Split Output plan String build Split Output Op Source Location loc Split Output op String alias String input Alias Logical Expression Plan filter Plan throws Parser Validation Exception op set Filter Plan filter Plan return build Op loc op alias input Alias null String build Split Otherwise Op Source Location loc Split Output op String alias String input Alias boolean allow Nulls throws Parser Validation Exception Plan Generation Failure Exception Logical Expression Plan split Plan new Logical Expression Plan Operator losplit lookup Operator input Alias Logical Expression current Expr null for Operator losplitoutput plan get Successors losplit take all the Split Output and negate their filter plans Logical Expression Plan fragment Split Output losplitoutput get Filter Plan try if Optimizer Utils plan Has Non Deterministic Udf fragment throw new Parser Validation Exception int Stream loc new Frontend Exception op Can not use Otherwise in Split with an expression containing a Nondeterministic catch Frontend Exception e e print Stack Trace throw new Plan Generation Failure Exception int Stream loc e Logical Expression root null try get the root expression of the filter plan in Split Output and copy it root Logical Expression fragment get Sources get deep Copy split Plan catch Frontend Exception e e print Stack Trace throw new Plan Generation Failure Exception int Stream loc e if root null throw new Plan Generation Failure Exception int Stream loc new Frontend Exception op Could not retrieve Logical Expression for Split Output losplitoutput if current Expr null current Expr root else current Expr new Or Expression split Plan current Expr root using De Morgan s law current Expr new Not Expression split Plan current Expr if allow Nulls add Is Null expression to the condition i e this is needed to catch null values in otherwise branch Logical Expression is Null new Is Null Expression split Plan current Expr current Expr new Or Expression split Plan is Null current Expr try Going through all the Project Expressions that were cloned and updating the attached operators from its original Split Output to to the otherwise Split Output new Reset Projection Attached Relational Op Visitor split Plan op visit catch Frontend Exception e e print Stack Trace throw new Plan Generation Failure Exception int Stream loc e op set Filter Plan split Plan return build Op loc op alias input Alias null String build Cross Op Source Location loc String alias List String input Aliases String partitioner throws Parser Validation Exception Cross op new Cross plan return build Op loc op alias input Aliases partitioner Sort create Sort Op return new Sort plan String build Sort Op Source Location loc Sort sort String alias String input Alias List Logical Expression Plan plans List Boolean asc Flags Func Spec fs throws Parser Validation Exception sort set Sort Col Plans plans sort set User Func fs if asc Flags is Empty for int i i plans size i asc Flags add true sort set Ascending Cols asc Flags alias build Op loc sort alias input Alias null try new Project Star Expander sort get Plan visit sort new Proj Star In Udf Expander sort get Plan visit sort new Schema Resetter sort get Plan true visit sort catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias Rank create Rank Op return new Rank plan String build Rank Op Source Location loc Rank rank String alias String input Alias List Logical Expression Plan plans List Boolean asc Flags throws Parser Validation Exception Rank rank set Rank Col Plan plans if asc Flags is Empty for int i i plans size i asc Flags add true rank set Ascending Col asc Flags build Op loc rank alias input Alias null try new Project Star Expander rank get Plan visit rank new Proj Star In Udf Expander rank get Plan visit rank new Schema Resetter rank get Plan true visit rank catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias Join create Join Op return new Join plan String build Join Op Source Location loc Join op String alias List String input Aliases Multi Map Integer Logical Expression Plan join Plans jt List Boolean inner Flags String partitioner throws Parser Validation Exception check Duplicate Aliases input Aliases loc if jt null jt else op pin Option Join int input Count input Aliases size if jt if partitioner null throw new Parser Validation Exception int Stream loc Custom Partitioner is not supported for skewed join if input Count throw new Parser Validation Exception int Stream loc Skewed join can only be applied for way joins else if jt jt input Count throw new Parser Validation Exception int Stream loc Merge join can only be applied for way joins else if jt if inner Flags size inner Flags get false throw new Parser Validation Exception int Stream loc Replicated join does not support right full outer joins boolean flags new boolean join Plans size if inner Flags size for int i i join Plans size i flags i inner Flags get i else for int i i join Plans size i flags i true op set Join Type jt op set Inner Flags flags op set Join Plans join Plans alias build Op loc op alias input Aliases partitioner try new Project Star Expander op get Plan visit op new Proj Star In Udf Expander op get Plan visit op new Schema Resetter op get Plan true visit op catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias Cube create Cube Op return new Cube plan String build Cube Op Source Location loc Cube op String alias String input Alias List String operations Multi Map Integer Logical Expression Plan expression Plans throws Parser Validation Exception check if continuously occurring cube operations be combined combine Cube Operations Array List String operations expression Plans set the expression plans for cube operator and build cube operator op set Expression Plans expression Plans op set Operations operations build Op loc op alias input Alias null try new Project Star Expander op get Plan visit op new Proj Star In Udf Expander op get Plan visit op new Schema Resetter op get Plan true visit op catch Frontend Exception e throw new Parser Validation Exception int Stream loc e try alias convert Cube To Plan loc op input Alias operations expression Plans catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias if multiple operations occur continuously then it can be combined together rel a b c d rel a b c d private void combine Cube Operations Array List String operations Multi Map Integer Logical Expression Plan expression Plans int start Idx int end Idx int i boolean is Merged false scan and perform merge of column projections for i i operations size i if start Idx operations get i equals true start Idx i else if operations get i equals true end Idx i else if end Idx start Idx merge And Mark For Delete operations expression Plans start Idx end Idx is Merged true start Idx end Idx else start Idx end Idx this check is required for the case when the sequence of operations occurs at the end like in which case end Idx will be greater than start Idx if end Idx start Idx is Merged true merge And Mark For Delete operations expression Plans start Idx end Idx if merged then remove the column projections that were marked for deletion if is Merged perform Deletion expression Plans operations private void perform Deletion Multi Map Integer Logical Expression Plan expression Plans Array List String operations Multi Map Integer Logical Expression Plan ep new Multi Map Integer Logical Expression Plan List String op new Array List String int idx rearranging indices for int i i operations size i if operations get i null op add idx operations get i if expression Plans get i null ep put idx expression Plans get i idx performing deletions operations clear operations add All op expression Plans clear for Integer i ep key Set expression Plans put i ep get i performs merging of dimensions of merged cube operation Ex a b c d a b c d in the above example operator and dimensions are merged private void merge And Mark For Delete Array List String operations Multi Map Integer Logical Expression Plan expression Plans int start Idx int end Idx mark for delete for int i start Idx i end Idx i expression Plans put start Idx expression Plans get i expression Plans remove Key i operations remove i operations add i null This function creates logical plan for foreach and groupby operators It connects the predecessors of cube operator with foreach plan and disconnects cube operator from the logical plan It also connects foreach plan with groupby plan private String convert Cube To Plan Source Location loc Cube op String input Alias List String operations Multi Map Integer Logical Expression Plan expression Plans throws Frontend Exception For Each foreach new For Each plan Cogroup groupby new Cogroup plan Logical Plan inner Plan new Logical Plan Logical Relational Operator gen new Generate inner Plan inject Foreach Operator loc op foreach Get all column attributes from the input relation Create Project Expression for all columns Based on the dimensions specified by the user specified columns will be attached to Cube Dimension Rollup Dimension and rest will be pushed down List Operator inp Opers foreach get Plan get Predecessors foreach List Logical Expression Plan all Expr Plan new Array List Logical Expression Plan for Operator oper inp Opers Logical Schema schema new Logical Schema schema Logical Relational Operator oper get Schema if schema null Array List Logical Field Schema fields Array List Logical Field Schema schema get Fields for int i i fields size i Logical Expression Plan l Eplan new Logical Expression Plan new Project Expression l Eplan i fields get i alias null gen all Expr Plan add l Eplan iterate over all operations and generate corresponding Fs for int oper Idx oper Idx operations size oper Idx List Logical Expression Plan lexp Plan List new Array List Logical Expression Plan List Logical Expression lexp List new Array List Logical Expression lexp Plan List add All expression Plans get oper Idx If duplicates exists in the dimension list then exception is thrown check Duplicate Project lexp Plan List Construct Project Expression from the Logical Expression Plans lexp List get Project Exp List lexp Plan List gen for int i i lexp List size i Retain the columns that needs to be pushed down Remove the dimension columns from the input column list as it will be attached to Cube Dimension for int j j all Expr Plan size j Logical Expression lexp Logical Expression all Expr Plan get j get Sources get String col Alias Project Expression lexp List get i get Col Alias if col Alias null col Alias Project Expression lexp List get i get Field Schema alias String proj Exp Alias null try proj Exp Alias Project Expression lexp get Col Alias catch Class Cast Exception e if it is not projection then it should be User Func Expr ignore and continue till next Proj Expr is encountered continue if col Alias equals proj Exp Alias true all Expr Plan remove j else if projected exp alias is a namespaced alias if proj Exp Alias last Index Of proj Exp Alias proj Exp Alias substring proj Exp Alias last Index Of proj Exp Alias length if col Alias equals proj Exp Alias true all Expr Plan remove j Create with user specified dimensions Logical Expression Plan uexp Plan new Logical Expression Plan if operations get oper Idx equals new User Func Expression uexp Plan new Func Spec Cube Dimensions class get Name lexp List else new User Func Expression uexp Plan new Func Spec Rollup Dimensions class get Name lexp List for Logical Expression Plan lexp lexp Plan List Iterator Operator it lexp get Operators while it has Next uexp Plan add it next Add the to logical expression plan that contains dependent attributes pushed down from input columns all Expr Plan add oper Idx uexp Plan If the operator is a User Func Expression then set the flatten flags List Boolean flatten Flags new Array List Boolean for int idx idx all Expr Plan size idx List Operator opers all Expr Plan get idx get Sources for Operator oper opers if oper instanceof Project Expression flatten Flags add false else if oper instanceof User Func Expression flatten Flags add true Generate and Foreach operator creation String falias null try build Generate Op loc For Each foreach Generate gen all Expr Plan flatten Flags get User Defined Schema all Expr Plan falias build Foreach Op loc For Each foreach cube input Alias inner Plan catch Parser Validation Exception pve throw new Frontend Exception pve List Boolean inner Flags new Array List Boolean List String inp Aliases new Array List String inp Aliases add falias inner Flags add false Get the output schema of foreach operator and reconstruct the Logical Expression Plan for each dimensional attributes Multi Map Integer Logical Expression Plan expr Plans Copy new Multi Map Integer Logical Expression Plan for Logical Expression Plan exp expression Plans values Logical Expression lexp Logical Expression exp get Sources get Logical Expression Plan ep Grp new Logical Expression Plan new Project Expression ep Grp lexp get Field Schema alias null groupby expr Plans Copy put ep Grp build group by operator try return build Group Op loc Cogroup groupby op get Alias inp Aliases expr Plans Copy inner Flags null catch Parser Validation Exception pve throw new Frontend Exception pve User defined schema for generate operator If not specified output schema of will be used which will prefix dimensions namespace to all fields private List Logical Schema get User Defined Schema List Logical Expression Plan all Expr Plan throws Frontend Exception List Logical Schema gen Output Schema new Array List Logical Schema for int i i all Expr Plan size i List Operator opers all Expr Plan get i get Sources for Operator oper opers add a logical schema for dimensions that are pushed from predecessor of cube rollup if oper instanceof Project Expression Logical Schema output new Logical Schema output add Field new Logical Field Schema Project Expression oper get Col Alias null Data Type gen Output Schema add output else if oper instanceof User Func Expression add logical schema for dimensions specified in cube rollup operator Logical Schema output new Logical Schema for Operator op User Func Expression oper get Plan get Sinks output add Field new Logical Field Schema Project Expression op get Field Schema gen Output Schema add output return gen Output Schema private List Logical Expression get Project Exp List List Logical Expression Plan lexp Plan List Logical Relational Operator lro throws Frontend Exception List Logical Expression le List new Array List Logical Expression for int i i lexp Plan List size i Logical Expression Plan lexp lexp Plan List get i Logical Expression lex Logical Expression lexp get Sources get Iterator Operator opers lexp get Operators Proj Expr are initially attached to Cube Op So re attach it to specified operator while opers has Next Operator oper opers next try Project Expression oper set Attached Relational Op lro catch Class Cast Exception cce throw new Frontend Exception Column project expected cce le List add lex return le List This method connects the predecessors of cube operator with foreach operator and disconnects the cube operator from its predecessors private void inject Foreach Operator Source Location loc Cube op For Each foreach throws Frontend Exception connect the foreach operator with predecessors of cube operator List Operator opers op get Plan get Predecessors op for Operator oper opers Operator Plan foreach Plan foreach get Plan foreach Plan connect oper Operator foreach disconnect the cube operator from the plan opers foreach get Plan get Predecessors foreach for Operator lop opers List Operator succs lop get Plan get Successors lop for Operator succ succs if succ instanceof Cube succ get Plan disconnect lop succ succ get Plan remove succ This methods if the dimensions specified by the user has duplicates private void check Duplicate Project List Logical Expression Plan l Expr Plan throws Frontend Exception for int i i l Expr Plan size i for int j i j l Expr Plan size j Logical Expression outer Logical Expression l Expr Plan get i get Sources get Logical Expression inner Logical Expression l Expr Plan get j get Sources get String out Col Alias Project Expression outer get Col Alias String in Col Alias Project Expression inner get Col Alias if out Col Alias null out Col Alias outer get Field Schema alias if in Col Alias null in Col Alias inner get Field Schema alias if out Col Alias equals in Col Alias true l Expr Plan remove j throw new Frontend Exception Duplicate dimensions detected Dimension name in Col Alias Cogroup create Group Op return new Cogroup plan String build Group Op Source Location loc Cogroup op String alias List String input Aliases Multi Map Integer Logical Expression Plan expression Plans gt List Boolean inner Flags String partitioner throws Parser Validation Exception if gt if input Aliases size throw new Parser Validation Exception int Stream loc Collected group is only supported for single input List Logical Expression Plan expr Plans expression Plans get for Logical Expression Plan expr Plan expr Plans Iterator Operator it expr Plan get Operators while it has Next if it next instanceof Project Expression throw new Parser Validation Exception int Stream loc Collected group is only supported for columns or star projection boolean flags new boolean inner Flags size for int i i inner Flags size i flags i inner Flags get i op set Expression Plans expression Plans op set Group Type gt op set Inner Flags flags alias build Op loc op alias input Aliases partitioner try new Project Star Expander op get Plan visit op new Proj Star In Udf Expander op get Plan visit op new Schema Resetter op get Plan true visit op catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias String build Load Op Source Location loc String alias String filename Func Spec func Spec Logical Schema schema throws Parser Validation Exception String absolute Path Load Func lo Func try Load Load Func class from default properties if func Spec is null Fallback on Pig Storage if Load Func is not specified in properties func Spec func Spec null new Func Spec pig Context get Properties get Property Pig Configuration Pig Storage class get Name func Spec lo Func Load Func Pig Context instantiate Func From Spec func Spec String file Name Key Query Parser Utils construct File Name Signature filename func Spec load Index absolute Path file Name Map get file Name Key if absolute Path null absolute Path lo Func relative To Absolute Path filename Query Parser Utils get Current Dir pig Context if absolute Path null lo Func instanceof Non Load Func Query Parser Utils set Hdfs Servers absolute Path pig Context file Name Map put file Name Key absolute Path catch Exception ex throw new Parser Validation Exception int Stream loc ex File Spec loader new File Spec absolute Path func Spec Load op new Load loader schema plan Configuration Util to Configuration pig Context get Properties lo Func alias new Operator Key op set Tmp Load false Check if there s a store in the plan already that this load depends on If so add it as an input alias List String input Aliases new Array List String Get list of stores The stores are not all sinks in the plan if they ve already got successors Iterator Operator itr plan get Operators List Store stores new Array List Store while itr has Next Operator lop itr next if lop instanceof Store stores add Store lop for Store store stores String ifile op get File Spec get File Name String ofile store get File Spec get File Name if ofile equals ifile input Aliases add store get Alias return build Op loc op alias input Aliases null private String build Op Source Location loc Logical Relational Operator op String alias String input Alias String partitioner throws Parser Validation Exception List String input Aliases new Array List String if input Alias null input Aliases add input Alias return build Op loc op alias input Aliases partitioner private void check Duplicate Aliases List String input Aliases Source Location loc String op Name throws Parser Validation Exception Keep the count of the number of times the same Alias is used Map Operator Integer input Aliases Map new Hash Map Operator Integer for String a input Aliases Operator pred operators get a if pred null throw new Parser Validation Exception int Stream loc Unrecognized alias a if input Aliases Map contains Key pred throw new Parser Validation Exception int Stream loc Pig does not accept same alias as input for op Name operation a else input Aliases Map put pred private String build Op Source Location loc Logical Relational Operator op String alias List String input Aliases String partitioner throws Parser Validation Exception set Alias op alias set Partitioner op partitioner op set Location loc plan add op for String a input Aliases Operator pred operators get a if pred null throw new Parser Validation Exception int Stream loc Unrecognized alias a plan connect pred op operators put op get Alias op pig Context set Last Alias op get Alias return op get Alias String build Store Op Source Location loc String alias String input Alias String filename Func Spec func Spec throws Parser Validation Exception try Load Store Func class from default properties if func Spec is null Fallback on Pig Storage if Store Func is not specified in properties func Spec func Spec null new Func Spec pig Context get Properties get Property Pig Configuration Pig Storage class get Name func Spec Store Func Interface sto Func Store Func Interface Pig Context instantiate Func From Spec func Spec String file Name Key input Alias store Index String signature input Alias new Operator Key sto Func set Store Func Context Signature signature String absolute Path file Name Map get file Name Key if absolute Path null absolute Path sto Func rel To Abs Path For Store Location filename Query Parser Utils get Current Dir pig Context if absolute Path null Query Parser Utils set Hdfs Servers absolute Path pig Context file Name Map put file Name Key absolute Path File Spec file Spec new File Spec absolute Path func Spec boolean disambiguation Enabled Boolean parse Boolean pig Context get Properties get Property Pig Configuration Pig Configuration Store op new Store plan file Spec sto Func signature disambiguation Enabled return build Op loc op alias input Alias null catch Exception ex throw new Parser Validation Exception int Stream loc ex String build Assert Op Source Location loc Filter filter Op String alias String input Alias Logical Expression expr String comment Logical Expression Plan expr Plan throws Parser Validation Exception try filter Op set Alias input Alias List Logical Expression args new Array List Logical Expression Constant Expression lhs new Constant Expression expr Plan new Boolean true Constant Expression rhs new Constant Expression expr Plan new Boolean false Bin Cond Expression bin Cond new Bin Cond Expression expr Plan expr lhs rhs args add bin Cond Constant Expression const Expr new Constant Expression expr Plan comment null comment args add const Expr User Func Expression udf new User Func Expression expr Plan new Func Spec Assert class get Name args expr Plan add udf filter Op set Filter Plan expr Plan pass the input Alias to alias return build Filter Op loc filter Op input Alias input Alias expr Plan catch Exception ex throw new Parser Validation Exception int Stream loc ex private String new Operator Key return new Operator Key scope get Next Id to String public static String new Operator Key String scope return new Operator Key scope get Next Id scope to String For Each create Foreach Op return new For Each plan String build Foreach Op Source Location loc For Each op String alias String input Alias Logical Plan inner Plan throws Parser Validation Exception op set Inner Plan inner Plan alias build Op loc op alias input Alias null try new Project Star Expander op get Plan visit op new Proj Star In Udf Expander op get Plan visit op new Schema Resetter op get Plan true visit op catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return alias Generate create Generate Op Logical Plan plan return new Generate plan void build Generate Op Source Location loc For Each foreach Generate gen List Logical Expression Plan expr Plans List Boolean flatten Flags List Logical Schema schemas throws Parser Validation Exception boolean flags new boolean flatten Flags size for int i i flatten Flags size i flags i flatten Flags get i Logical Plan inner Plan Logical Plan gen get Plan Array List Operator inputs new Array List Operator int idx for Logical Expression Plan expr Plan expr Plans Logical Expression expr Logical Expression expr Plan get Sources get Logical Schema user Schema schemas get idx if user Schema null expr has Field Schema Logical Schema ls new Logical Schema try ls add Field expr get Field Schema schemas set idx ls catch Frontend Exception e if we get an exception then we have no schema to set idx try process Expression Plan foreach inner Plan expr Plan inputs catch Frontend Exception e throw new Parser Validation Exception int Stream loc e gen set Output Plans expr Plans gen set Flatten Flags flags gen set User Defined Schema schemas inner Plan add gen gen set Location loc for Operator input inputs inner Plan connect input gen Process expression plans of Generate and set inputs relation for the Project Expression param foreach param lp Logical plan in which the Generate is in param plan One of the output expression of the Generate param inputs inputs of the Generate throws Frontend Exception private static void process Expression Plan For Each foreach Logical Plan lp Logical Expression Plan plan Array List Operator inputs throws Frontend Exception Iterator Operator it plan get Operators while it has Next Operator sink it next check all Project Expression if sink instanceof Project Expression Project Expression proj Expr Project Expression sink String col Alias proj Expr get Col Alias if proj Expr is Range Project Inner Load inner Load new Inner Load lp foreach new Project Expression proj Expr new Logical Expression Plan setup Inner Load And Proj inner Load proj Expr lp inputs else if col Alias null the project is using a column alias Operator op proj Expr get Projected Operator if op null this means the project expression refers to a relation in the nested foreach add the relation to inputs of Generate and set projection input int index inputs index Of op if index index inputs size inputs add op proj Expr set Input Num index proj Expr set Col Num else this means the project expression refers to a column in the input of foreach Add a Inner Load and use that as input Inner Load inner Load new Inner Load lp foreach col Alias setup Inner Load And Proj inner Load proj Expr lp inputs else the project expression is referring to column in For Each input using position eg Inner Load inner Load new Inner Load lp foreach proj Expr get Col Num setup Inner Load And Proj inner Load proj Expr lp inputs private static void setup Inner Load And Proj Inner Load inner Load Project Expression proj Expr Logical Plan lp Array List Operator inputs inner Load set Location proj Expr get Location proj Expr set Input Num inputs size proj Expr set Col Num Projection Expression on Inner Load is always lp add inner Load inputs add inner Load Operator build Nested Operator Input Source Location loc Logical Plan inner Plan For Each foreach Map String Operator operators Logical Expression expr throws Non Project Expression Exception Parser Validation Exception Operator Plan plan expr get Plan Iterator Operator it plan get Operators if it next instanceof Project Expression it has Next throw new Non Project Expression Exception int Stream loc expr Operator op null Project Expression proj Expr Project Expression expr String col Alias proj Expr get Col Alias if col Alias null op operators get col Alias if op null op create Inner Load loc inner Plan foreach col Alias op set Location proj Expr get Location inner Plan add op else op new Inner Load inner Plan foreach proj Expr get Col Num op set Location proj Expr get Location inner Plan add op return op private Inner Load create Inner Load Source Location loc Logical Plan inner Plan For Each foreach String col Alias throws Parser Validation Exception try return new Inner Load inner Plan foreach col Alias catch Frontend Exception e throw new Parser Validation Exception int Stream loc e Streaming Command build Command Source Location loc String cmd List String ship Paths List String cache Paths List Handle Spec input Handle Specs List Handle Spec output Handle Specs String log Dir Integer limit throws Recognition Exception Streaming Command command null try command build Command loc cmd Process ship paths if ship Paths null if ship Paths size command set Ship Files false else for String path ship Paths command add Path To Ship path Process cache paths if cache Paths null for String path cache Paths command add Path To Cache path Process input handle specs if input Handle Specs null for Handle Spec spec input Handle Specs command add Handle Spec Handle spec Process output handle specs if output Handle Specs null for Handle Spec spec output Handle Specs command add Handle Spec Handle spec error handling if log Dir null command set Log Dir log Dir if limit null command set Log Files Limit limit catch Exception e throw new Plan Generation Failure Exception int Stream loc e return command Streaming Command build Command Source Location loc String cmd throws Recognition Exception try String args Streaming Command Utils split Args cmd Streaming Command command new Streaming Command pig Context args Streaming Command Utils validator new Streaming Command Utils pig Context validator check Auto Ship Specs command args return command catch Parser Exception e throw new Invalid Command Exception int Stream loc cmd String build Stream Op Source Location loc String alias String input Alias Streaming Command command Logical Schema schema Int Stream input throws Recognition Exception try Stream op new Stream plan pig Context create Executable Manager command schema return build Op loc op alias input Alias null catch Exec Exception ex throw new Plan Generation Failure Exception input loc ex String build Native Op Source Location loc String input Jar String cmd List String paths String store Alias String load Alias Int Stream input throws Recognition Exception Native op try op new Native plan input Jar Streaming Command Utils split Args cmd pig Context add Jar input Jar for String path paths pig Context add Jar path build Op loc op null new Array List String null Store operators get store Alias set Tmp Store true plan connect operators get store Alias op Load load Load operators get load Alias plan connect op load return load get Alias catch Parser Exception e throw new Invalid Command Exception input loc cmd catch Malformed Exception e throw new Invalid Path Exception input loc e void set Alias Logical Relational Operator op String alias if alias null alias new Operator Key op set Alias alias void set Parallel Logical Relational Operator op Integer parallel if parallel null op set Requested Parallelism parallel static void set Partitioner Logical Relational Operator op String partitioner if partitioner null op set Custom Partitioner partitioner Func Spec build Func Spec Source Location loc String func Name List String args byte ft throws Recognition Exception String arg Array new String args size Func Spec func Spec new Func Spec func Name args size null args to Array arg Array validate Func Spec loc func Spec ft return func Spec private void validate Func Spec Source Location loc Func Spec func Spec byte ft throws Recognition Exception switch ft case Function Type case Function Type case Function Type case Function Type case Function Type try Class func Pig Context resolve Class Name func Spec get Class Name Function Type try Casting func ft catch Exception ex throw new Parser Validation Exception int Stream loc ex static String unquote String s return String Utils unescape Input String s substring s length static int undollar String s return Integer parse Int s substring s length Parse the long given as a string such as static long parse Long String s String num s substring s length return Long parse Long num Parse big integer formatted string e g into Big Integer object static Big Integer parse Big Integer String s String num s substring s length return new Big Integer num Parse big decimal formatted string e g into Big Decimal object static Big Decimal parse Big Decimal String s String num s substring s length return new Big Decimal num static Tuple build Tuple List Object obj List Tuple Factory tf Tuple Factory get Instance return tf new Tuple obj List static Data Bag create Data Bag Bag Factory bag Factory Bag Factory get Instance return bag Factory new Default Bag Build a project expression in foreach inner plan The only difference here is that the projection can be for an expression alias for which we will return whatever the expression alias represents throws Recognition Exception Logical Expression build Project Expr Source Location loc Logical Expression Plan plan Logical Relational Operator op Map String Operator operators Map String Logical Expression Plan expr Plans String col Alias int col throws Recognition Exception Project Expression result null if col Alias null Logical Expression Plan expr Plan expr Plans get col Alias if expr Plan null Logical Expression Plan plan Copy null try plan Copy expr Plan deep Copy plan merge plan Copy catch Frontend Exception ex throw new Plan Generation Failure Exception int Stream loc ex The projected alias is actually expression alias so the projections in the represented expression does n t have any operator associated with it We need to set it when we substitute the expression alias with the its expression if op null Iterator Operator it plan get Operators while it has Next Operator o it next if o instanceof Project Expression Project Expression proj Expr Project Expression o proj Expr set Attached Relational Op op Logical Expression root Logical Expression plan Copy get Sources get get the root of the plan Logical Field Schema schema try schema root get Field Schema if schema alias null schema alias col Alias catch Frontend Exception e Sometimes it can throw an exception If it does then there is no schema to get return root else result new Project Expression plan col Alias operators get col Alias op result set Location loc return result result new Project Expression plan col op result set Location loc return result Build a project expression for a projection present in global plan not in nested foreach plan throws Parser Validation Exception Logical Expression build Project Expr Source Location loc Logical Expression Plan plan Logical Relational Operator rel Op int input String col Alias int col throws Parser Validation Exception Project Expression result null result col Alias null new Project Expression plan input col Alias null rel Op new Project Expression plan input col rel Op result set Location loc return result Build a project expression that projects a range of columns param loc param plan param rel Op param input param start Expr the first expression to be projected null if everything from first is to be projected param end Expr the last expression to be projected null if everything to the end is to be projected return project expression throws Parser Validation Exception Logical Expression build Range Project Expr Source Location loc Logical Expression Plan plan Logical Relational Operator rel Op int input Logical Expression start Expr Logical Expression end Expr throws Parser Validation Exception if start Expr null end Expr null should not reach here as the parser is enforcing this condition String msg in range project at least one of start or end has to be specified Use project star instead throw new Parser Validation Exception int Stream loc msg Project Expression proj new Project Expression plan input rel Op set first column to be projected if start Expr null check Range Project Expr loc start Expr Project Expression start Proj Project Expression start Expr if start Proj get Col Alias null try proj set Start Alias start Proj get Col Alias catch Frontend Exception e throw new Parser Validation Exception int Stream loc e else proj set Start Col start Proj get Col Num else proj set Start Col project from first column set last column to be projected if end Expr null check Range Project Expr loc end Expr Project Expression end Proj Project Expression end Expr if end Proj get Col Alias null try proj set End Alias end Proj get Col Alias catch Frontend Exception e throw new Parser Validation Exception int Stream loc e else proj set End Col end Proj get Col Num else proj set End Col project to last column try if start Expr null plan remove And Reconnect start Expr if end Expr null plan remove And Reconnect end Expr catch Frontend Exception e throw new Parser Validation Exception int Stream loc e return proj private void check Range Project Expr Source Location loc Logical Expression start Expr throws Parser Validation Exception if start Expr instanceof Project Expression should not reach here as the parser is enforcing this condition String msg range project can have only a simple column Found start Expr throw new Parser Validation Exception int Stream loc msg Logical Expression build Invoker Source Location loc Logical Expression Plan plan String package Name String func Name boolean is Static List Logical Expression args throws Recognition Exception Logical Expression le new User Func Expression plan new Func Spec Invoker Generator class get Name args false true is Static package Name func Name le set Location loc return le public static Class type To Class Class clazz if clazz Integer return Integer class else if clazz Long return Long class else if clazz Float return Long class else if clazz Double return Long class else if clazz Boolean return Long class else if clazz Short return Short class else if clazz Byte return Byte class else if clazz Character return Character class else throw new Runtime Exception Was not given a primitive class clazz Logical Expression build Source Location loc Logical Expression Plan plan String func Name List Logical Expression args throws Recognition Exception Class func try func pig Context get Class For Alias func Name Function Type try Casting func Function Type catch Exception e throw new Plan Generation Failure Exception int Stream loc e Func Spec func Spec pig Context get Func Spec From Alias func Name Logical Expression le if func Spec null func Name func get Name func Spec new Func Spec func Name this point is only reached if there was no statement for func Name in which case we pass that information along le new User Func Expression plan func Spec args false else le new User Func Expression plan func Spec args true le set Location loc return le private long get Next Id return get Next Id scope static Filter create Nested Filter Op Logical Plan plan return new Filter plan static Limit create Nested Limit Op Logical Plan plan return new Limit plan Build operator for foreach inner plan Operator build Nested Filter Op Source Location loc Filter op Logical Plan plan String alias Operator input Op Logical Expression Plan expr op set Filter Plan expr build Nested Op loc plan op alias input Op return op Operator build Nested Distinct Op Source Location loc Logical Plan plan String alias Operator input Op Distinct op new Distinct plan build Nested Op loc plan op alias input Op return op Operator build Nested Limit Op Source Location loc Logical Plan plan String alias Operator input Op long limit Limit op new Limit plan limit build Nested Op loc plan op alias input Op return op Operator build Nested Limit Op Source Location loc Limit op Logical Plan plan String alias Operator input Op Logical Expression Plan expr op set Limit Plan expr build Nested Op loc plan op alias input Op return op Operator build Nested Cross Op Source Location loc Logical Plan plan String alias List Operator input Op List Cross op new Cross plan op set Nested true build Nested Op loc plan op alias input Op List return op private void build Nested Op Source Location loc Logical Plan plan Logical Relational Operator op String alias Operator input Op op set Location loc set Alias op alias plan add op plan connect input Op op private void build Nested Op Source Location loc Logical Plan plan Logical Relational Operator op String alias List Operator input Op List op set Location loc set Alias op alias plan add op for Operator input Op input Op List plan connect input Op op static Sort create Nested Sort Op Logical Plan plan return new Sort plan For any type in the schema fields set the type to param sch static void set Bytearray For Type Logical Schema sch for Logical Field Schema fs sch get Fields if fs type Data Type fs type Data Type if fs schema null set Bytearray For Type fs schema static For Each create Nested Foreach Op Logical Plan plan return new For Each plan Operator build Nested Sort Op Source Location loc Sort op Logical Plan plan String alias Operator input Op List Logical Expression Plan plans List Boolean asc Flags Func Spec fs op set Sort Col Plans plans if asc Flags is Empty for int i i plans size i asc Flags add true op set Ascending Cols asc Flags op set User Func fs build Nested Op loc plan op alias input Op return op Operator build Nested Foreach Op Source Location loc For Each op Logical Plan plan String alias Operator input Op Logical Plan inner Plan throws Parser Validation Exception op set Inner Plan inner Plan build Nested Op loc plan op alias input Op return op Operator build Nested Project Op Source Location loc Logical Plan inner Plan For Each foreach Map String Operator operators String alias Project Expression proj Expr List Logical Expression Plan expr Plans throws Parser Validation Exception Operator input null String col Alias proj Expr get Col Alias if col Alias null Proj Expr refers to a name which can be an alias for another operator or col name Operator op operators get col Alias if op null Proj Expr refers to an operator alias input op else Assuming that Proj Expr refers to a column by name Create an Inner Load input create Inner Load loc inner Plan foreach col Alias input set Location proj Expr get Location else Proj Expr refers to a column by number input new Inner Load inner Plan foreach proj Expr get Col Num input set Location proj Expr get Location Logical Plan lp new Logical Plan f s inner plan For Each f new For Each inner Plan f set Inner Plan lp f set Location loc Generate gen new Generate lp boolean flatten new boolean expr Plans size List Operator inner Loads new Array List Operator expr Plans size for Logical Expression Plan plan expr Plans Project Expression pe Project Expression plan get Sinks get String al pe get Col Alias Inner Load iload al null new Inner Load lp f pe get Col Num create Inner Load loc lp f al iload set Location pe get Location pe set Col Num pe set Input Num inner Loads size pe set Attached Relational Op gen inner Loads add iload gen set Output Plans expr Plans gen set Flatten Flags flatten lp add gen for Operator il inner Loads lp add il lp connect il gen Connect the inner load operators to gen set Alias f alias inner Plan add input inner Plan add f inner Plan connect input f return f parse Group Type String hint Source Location loc throws Parser Validation Exception String modifier unquote hint if modifier equals Ignore Case collected return else if modifier equals Ignore Case regular return else if modifier equals Ignore Case merge return else throw new Parser Validation Exception int Stream loc Only or are valid modifiers parse Join Type String hint Source Location loc throws Parser Validation Exception String modifier unquote hint if modifier equals Ignore Case repl modifier equals Ignore Case replicated return else if modifier equals Ignore Case hash modifier equals Ignore Case default return Join else if modifier equals Ignore Case bloom return Join else if modifier equals Ignore Case skewed return else if modifier equals Ignore Case merge return else if modifier equals Ignore Case merge sparse return else throw new Parser Validation Exception int Stream loc Only and are vaild modifiers void put Operator String alias Operator op operators put alias op public String get Last Rel Source Location loc throws Parser Validation Exception if last Rel null throw new Parser Validation Exception int Stream loc Asked for last relation no relations have been defined return last Rel public String get Last Rel return last Rel 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical optimizer import java util Array List import java util Hash Set import java util List import java util Set import com google common base Preconditions import com google common collect Set Multimap import com google common collect Tree Multimap import org apache commons logging Log import org apache commons logging Log Factory import org apache pig impl Pig Context import org apache pig newplan Operator Plan import org apache pig newplan logical rules Add For Each import org apache pig newplan logical rules Column Map Key Prune import org apache pig newplan logical rules Filter Above Foreach import org apache pig newplan logical rules Filter Constant Calculator import org apache pig newplan logical rules For Each Constant Calculator import org apache pig newplan logical rules Group By Const Parallel Setter import org apache pig newplan logical rules Limit Optimizer import org apache pig newplan logical rules Load Type Cast Inserter import org apache pig newplan logical rules Merge Filter import org apache pig newplan logical rules Merge For Each import org apache pig newplan logical rules Nested Limit Optimizer import org apache pig newplan logical rules Partition Filter Optimizer import org apache pig newplan logical rules Predicate Pushdown Optimizer import org apache pig newplan logical rules Push Down For Each Flatten import org apache pig newplan logical rules Push Up Filter import org apache pig newplan logical rules Split Filter import org apache pig newplan logical rules Stream Type Cast Inserter import org apache pig newplan optimizer Plan Optimizer import org apache pig newplan optimizer Rule public class Logical Plan Optimizer extends Plan Optimizer private static final Log Log Factory get Log Logical Plan Optimizer class private static enum Rules Report Key private Set String m Rules Off null private boolean all Rules Disabled false private Set Multimap Rules Report Key String rules Report Tree Multimap create private Pig Context pc null public Logical Plan Optimizer Operator Plan p int iterations Set String turn Off Rules this p iterations turn Off Rules null Create a new Logical Plan Optimizer param p Plan to optimize param iterations Maximum number of optimizer iterations param turn Off Rules Optimization rules to disable all disables all non mandatory rules null enables all rules param pc Pig Context object public Logical Plan Optimizer Operator Plan p int iterations Set String turn Off Rules Pig Context pc super p null iterations this pc pc m Rules Off turn Off Rules null new Hash Set String turn Off Rules if m Rules Off contains all all Rules Disabled true rule Sets build Rule Sets info rules Report add Listeners protected List Set Rule build Rule Sets List Set Rule ls new Array List Set Rule Logical expression simplifier Set Rule s new Hash Set Rule add constant calculator rule Rule r new Filter Constant Calculator Constant Calculator pc check And Add Rule s r ls add s r new For Each Constant Calculator Constant Calculator pc check And Add Rule s r ls add s Type Cast Inserter set This set of rules Insert Foreach dedicated for casting after load s new Hash Set Rule add split filter rule r new Load Type Cast Inserter Load Type Cast Inserter check And Add Rule s r r new Stream Type Cast Inserter Stream Type Cast Inserter check And Add Rule s r if s is Empty ls add s Split Set This set of rules does splitting of operators only It does not move operators s new Hash Set Rule add split filter rule r new Split Filter Split Filter check And Add Rule s r if s is Empty ls add s Push Set This set does moving of operators only s new Hash Set Rule r new Push Up Filter Push Up Filter check And Add Rule s r r new Filter Above Foreach Push Up Filter check And Add Rule s r if s is Empty ls add s Merge Set This Set merges operators but does not move them s new Hash Set Rule check And Add Rule s r add merge filter rule r new Merge Filter Merge Filter check And Add Rule s r if s is Empty ls add s Partition filter set This set of rules push partition filter to Load Func s new Hash Set Rule Optimize partition filter r new Partition Filter Optimizer Partition Filter Optimizer check And Add Rule s r if s is Empty ls add s Predicate pushdown set This set of rules push filter conditions to Load Func s new Hash Set Rule Optimize partition filter r new Predicate Pushdown Optimizer Predicate Pushdown Optimizer check And Add Rule s r if s is Empty ls add s Push Down For Each Flatten set s new Hash Set Rule Add the Push Down For Each Flatten r new Push Down For Each Flatten Push Down For Each Flatten check And Add Rule s r if s is Empty ls add s Prune Set This set is used for pruning columns and maps s new Hash Set Rule Add the Prune Map Filter r new Column Map Key Prune Column Map Key Prune check And Add Rule s r if s is Empty ls add s Add For Each set s new Hash Set Rule Add the Add For Each r new Add For Each Add For Each check And Add Rule s r if s is Empty ls add s Add Merge For Each set s new Hash Set Rule Add the Add For Each r new Merge For Each Merge For Each check And Add Rule s r if s is Empty ls add s set parallism to for cogroup group by on constant s new Hash Set Rule r new Group By Const Parallel Setter Group By Const Parallel Setter check And Add Rule s r if s is Empty ls add s Limit Set This set of rules push up limit s new Hash Set Rule Optimize limit r new Limit Optimizer Limit Optimizer check And Add Rule s r if s is Empty ls add s Nested Limit Set This set of rules push up nested limit s new Hash Set Rule Optimize limit r new Nested Limit Optimizer Nested Limit Optimizer check And Add Rule s r if s is Empty ls add s return ls Add rule to rule Set if its mandatory or has not been disabled param rule Set Set rule will be added to if not disabled param rule Rule to potentially add private void check And Add Rule Set Rule rule Set Rule rule Preconditions check Argument rule Set null Preconditions check Argument rule null rule get Name null if rule is Mandatory rule Set add rule rules Report put Rules Report Key rule get Name else if all Rules Disabled m Rules Off contains rule get Name rule Set add rule rules Report put Rules Report Key rule get Name else rules Report put Rules Report Key rule get Name private void add Listeners add Plan Transform Listener new Schema Patcher add Plan Transform Listener new Projection Patcher 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical optimizer import java io Exception import java io Print Stream import java util Array List import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Logical Plan visitor mechanism printing out the logical plan public class Logical Plan Printer extends Plan Visitor private Print Stream m Stream null private byte get Bytes private byte More get Bytes private byte Bar n get Bytes private byte Sep get Bytes private byte Sep n get Bytes static public String t protected Array List byte tabs protected boolean reverse false param ps Print Stream to output plan information to param plan Logical plan to print public Logical Plan Printer Operator Plan plan Print Stream ps throws Frontend Exception this plan ps new Array List byte private Logical Plan Printer Operator Plan plan Print Stream ps Array List byte tabs throws Frontend Exception super plan null m Stream ps this tabs tabs if plan instanceof Logical Plan reverse false else reverse true Override public void visit throws Frontend Exception try depth First catch Exception e throw new Frontend Exception e protected void depth First throws Frontend Exception Exception List Operator leaves if reverse leaves plan get Sources else leaves plan get Sinks for Operator leaf leaves write With Tabs leaf to String n get Bytes depth First leaf private void write With Tabs byte data throws Exception for byte tab tabs m Stream write tab m Stream write data private void depth First Operator node throws Frontend Exception Exception print Node Plan node List Operator operators if reverse operators plan get Successors node else operators plan get Predecessors node if operators null return List Operator predecessors new Array List Operator operators int i for Operator pred predecessors i write With Tabs Bar write With Tabs Sep m Stream write pred to String n get Bytes if i predecessors size tabs add More else tabs add depth First pred tabs remove tabs size private void print Plan Operator Plan lp throws Visitor Exception Exception write With Tabs Sep tabs add More if lp null Logical Plan Printer printer new Logical Plan Printer lp m Stream tabs printer visit tabs remove tabs size private void print Node Plan Operator node throws Frontend Exception Exception if node instanceof Filter print Plan Filter node get Filter Plan else if node instanceof Limit print Plan Limit node get Limit Plan else if node instanceof For Each print Plan For Each node get Inner Plan else if node instanceof Cogroup Multi Map Integer Logical Expression Plan plans Cogroup node get Expression Plans for int i plans key Set Visit the associated plans for Operator Plan plan plans get i print Plan plan else if node instanceof Join Multi Map Integer Logical Expression Plan plans Join node get Expression Plans for int i plans key Set Visit the associated plans for Operator Plan plan plans get i print Plan plan else if node instanceof Rank Visit fields for rank for Operator Plan plan Rank node get Rank Col Plans print Plan plan else if node instanceof Sort for Operator Plan plan Sort node get Sort Col Plans print Plan plan else if node instanceof Split Output print Plan Split Output node get Filter Plan else if node instanceof Generate for Operator Plan plan Generate node get Output Plans print Plan plan 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Iterator import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan Plan Walker visitor for logical plans public abstract class Logical Relational Nodes Visitor extends Plan Visitor protected Logical Relational Nodes Visitor Operator Plan plan Plan Walker walker throws Frontend Exception super plan walker Iterator Operator iter plan get Operators while iter has Next if iter next instanceof Logical Relational Operator throw new Frontend Exception Logical Plan Visitor can only visit logical plan public void visit Load load throws Frontend Exception public void visit Filter filter throws Frontend Exception public void visit Store store throws Frontend Exception public void visit Join join throws Frontend Exception public void visit For Each foreach throws Frontend Exception public void visit Generate gen throws Frontend Exception public void visit Inner Load load throws Frontend Exception public void visit Cube cube throws Frontend Exception public void visit Cogroup lo Cogroup throws Frontend Exception public void visit Split lo Split throws Frontend Exception public void visit Split Output lo Split Output throws Frontend Exception public void visit Union lo Union throws Frontend Exception public void visit Sort lo Sort throws Frontend Exception public void visit Rank lo Rank throws Frontend Exception public void visit Distinct lo Distinct throws Frontend Exception public void visit Limit lo Limit throws Frontend Exception public void visit Cross lo Cross throws Frontend Exception public void visit Stream lo Stream throws Frontend Exception public void visit Native native throws Frontend Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Hash Set import java util List import java util Map import java util Set import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical relational Logical Schema Logical Field Schema import com google common collect Sets Logical representation of relational operators Relational operators have a schema abstract public class Logical Relational Operator extends Operator protected Logical Schema schema protected int requested Parallelism protected String alias protected int line Num Name of the custom Partitioner if one is used this is set to null otherwise protected String m Custom Partitioner null Hash Set to indicate whether an option such a Join Type was pinned by the user or can be chosen at runtime by the optimizer protected Hash Set Integer m Pinned Options new Hash Set Integer param name of this operator param plan this operator is in public Logical Relational Operator String name Operator Plan plan this name plan param name of this operator param plan this operator is in param rp requested parallelism public Logical Relational Operator String name Operator Plan plan int rp super name plan requested Parallelism rp Get the schema for the output of this relational operator This does not merely return the schema variable If schema is not yet set this will attempt to construct it Therefore it is abstract since each operator will need to construct its schema differently return the schema throws Frontend Exception abstract public Logical Schema get Schema throws Frontend Exception public void set Schema Logical Schema schema this schema schema Reset the schema to null so that the next time get Schema is called the schema will be regenerated from scratch public void reset Schema schema null Erase all cached uid regenerate uid when we regenerating schema This process currently only used in Implicit Split Insert which will insert split and invalidate some uids in plan public void reset Uid Get the requested Parallelism for this operator return requested Parallelsim public int get Requested Parallelism return requested Parallelism Get the alias of this operator That is if the Pig Latin for this operator was sort by then the alias will be For store and split it will be the alias being stored or split Note that because of this this alias is not guaranteed to be unique to a single operator return alias public String get Alias return alias public void set Alias String alias this alias alias public void set Requested Parallelism int parallel this requested Parallelism parallel Get the line number in the submitted Pig Latin script where this operator occurred return line number public int get Line Number return line Num Only to be used by unit tests This is a back door cheat to set the schema without having to calculate it This should never be called by production code only by tests param schema to set public void never Use For Real Set Schema Logical Schema schema this schema schema Do some basic equality checks on two relational operators Equality is defined here as having equal schemas and predecessors that are equal This is intended to be used by operators equals methods param other Logical Relational Operator to compare predecessors against return true if the is Equals methods of this node s predecessor s returns true when invoked with other s predecessor s throws Frontend Exception protected boolean check Equality Logical Relational Operator other throws Frontend Exception if other null return false Logical Schema s get Schema Logical Schema os other get Schema if s null os null intentionally blank else if s null os null one of them is null and one is n t return false else if s is Equal os return false return true public String to String String Builder msg new String Builder if alias null msg append alias msg append Name get Name Schema if schema null msg append schema else msg append null msg append if annotations null for Map Entry String Object entry annotations entry Set msg append entry return msg to String public String get Custom Partitioner return m Custom Partitioner public void set Custom Partitioner String custom Partitioner m Custom Partitioner custom Partitioner public void pin Option Integer opt m Pinned Options add opt public boolean is Pinned Option Integer opt return m Pinned Options contains opt private static void add Field Schema Uids To Set Set Long uids Logical Field Schema lfs while uids add lfs uid lfs uid Logical Expression get Next Uid Logical Schema ls lfs schema if ls null for Logical Field Schema lfs ls get Fields add Field Schema Uids To Set uids lfs In the case of an operation which manipualtes columns such as a foreach or a join it is possible for multiple columns to have been derived from the same column and thus have duplicate s This detects that case and resets the uid See and for more information param fss a list of Logical Field Schemas to check the uids of public static void fix Duplicate Uids List Logical Field Schema fss Set Long uids Sets new Hash Set for Logical Field Schema lfs fss Logical Relational Operator add Field Schema Uids To Set uids lfs 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util Hash Set import java util Iterator import java util List import org apache pig Pig Exception import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan logical expression Logical Expression Schema from a logical perspective public class Logical Schema public static class Logical Field Schema public String alias public byte type public long uid public Logical Schema schema public Logical Field Schema String alias Logical Schema schema byte type this alias schema type public Logical Field Schema Logical Field Schema fs this fs alias fs schema fs type fs uid public Logical Field Schema String alias Logical Schema schema byte type long uid this alias alias this type type this schema schema this uid uid Equality is defined as having the same type and either the same schema or both null schema Alias and uid are not checked public boolean is Equal Object other return is Equal other false Equality is defined as having the same type and either the same schema or both null schema if compare Alias argument is set to true alias is also compared param other param compare Alias return true if equal public boolean is Equal Object other boolean compare Alias if other instanceof Logical Field Schema Logical Field Schema ofs Logical Field Schema other if compare Alias if alias null alias equals ofs alias return false if type ofs type return false if schema null ofs schema null return true if schema null return false else return schema is Equal ofs schema compare Alias else return false public String to String boolean verbose String uid String if verbose uid String uid String alias To Print if alias null alias To Print alias if type Data Type if schema null return alias To Print uid String bag return alias To Print uid String bag schema to String verbose else if type Data Type if schema null return alias To Print uid String tuple return alias To Print uid String tuple schema to String verbose else if type Data Type if schema null return alias To Print uid String map else return alias To Print uid String map schema to String verbose return alias To Print uid String Data Type find Type Name type public String to String return to String true Give new value for uid if uid of field schema or those in fields schema of inner schema are public void stamp Field Schema if uid uid Logical Expression get Next Uid if schema null for Logical Field Schema fs schema get Fields fs stamp Field Schema private boolean compatible Logical Field Schema uid Only Field Schema if uid Only Field Schema null return false if this schema null uid Only Field Schema schema null this schema null uid Only Field Schema schema null return false if this schema null if this schema size uid Only Field Schema schema size return false for int i i this schema size i boolean comp schema get Field i compatible uid Only Field Schema schema get Field i if comp return false return true Check if fs is equal to fs with regard to type public static boolean type Match Logical Field Schema fs Logical Field Schema fs if fs null fs null return true if fs null fs null return false if fs type fs type return false if Data Type is Complex fs type Logical Schema s fs schema Logical Schema s fs schema if s null s null return true if s null s null return false if s size s size return false for int i i s size i if type Match s get Field i s get Field i return false return true Adds the uid from Field Schema argument to this Field Schema If the argument is null it stamps this Field Schema with uid param uid Only Field Schema return Field Schema throws Frontend Exception public Logical Schema Logical Field Schema merge Uid Logical Field Schema uid Only Field Schema throws Frontend Exception if uid Only Field Schema null compatible uid Only Field Schema this uid uid Only Field Schema uid if this schema null for int i i this schema size i schema get Field i merge Uid uid Only Field Schema schema get Field i return uid Only Field Schema else if uid Only Field Schema null stamp Field Schema else this uid uid Only Field Schema uid if this schema null for int i i this schema size i schema get Field i stamp Field Schema Logical Field Schema cloned Uid Only Copy clone Uid return cloned Uid Only Copy Rest uid of this fieldschema and inner schema public void reset Uid uid if schema null schema reset Uid public Logical Field Schema clone Uid Logical Field Schema result Fs null if schema null result Fs new Logical Field Schema null null type uid else Logical Schema new Schema new Logical Schema result Fs new Logical Field Schema null new Schema type uid for int i i schema size i Logical Field Schema fs schema get Field i clone Uid new Schema add Field fs return result Fs public Logical Field Schema deep Copy Logical Field Schema new Fs new Logical Field Schema alias null alias null schema null schema deep Copy null type uid return new Fs Compare two field schema for equality param relax Inner If true we do n t check inner tuple schemas param relax Alias If true we do n t check aliases return true if Field Schemas are equal false otherwise public static boolean equals Logical Field Schema fschema Logical Field Schema fother boolean relax Inner boolean relax Alias if fschema null fother null return false if fschema type fother type return false if relax Alias if fschema alias null fother alias null good else if fschema alias null return false else if fschema alias equals fother alias return false if relax Inner Data Type is Schema Type fschema type Do n t do the comparison if both embedded schemas are null That will cause Schema equals to return false even though we want to view that as true if fschema schema null fother schema null compare recursively using schema if Logical Schema equals fschema schema fother schema false relax Alias return false return true Check if Field Schema in Fs is castable to out Fs param in Fs param out Fs return true if it is castable public static boolean castable Logical Field Schema in Fs Logical Field Schema out Fs if out Fs null in Fs null return false if out Fs null return false if in Fs null return false byte in Type in Fs type byte out Type out Fs type if Data Type is Schema Type out Fs type if in Type Data Type good else if in Type out Type Do n t do the comparison if either input inner schema is null empty or both inner schemas are null That will cause Schema equals to return false even though we want to view that as true if in Fs schema null in Fs schema size out Fs schema null in Fs schema null compare recursively using schema if Logical Schema castable in Fs schema out Fs schema return false else return false else if in Type out Type good else if in Type Data Type out Type Data Type out Type Data Type Data Type is Number Type out Type good else if Data Type is Number Type in Type out Type Data Type out Type Data Type Data Type is Number Type out Type out Type Data Type good else if in Type Data Type out Type Data Type Data Type is Number Type out Type out Type Data Type good else if in Type Data Type good else return false return true Merge two Logical Field Schema the behavior of merge depends on mode If mode Merge Type Load For Each or Merge Type Load For Each Inner take left side if compatible otherwise throw exception If mode Merge Type Union Inner if not same type throw exception end up with null inner schema If mode Merge Type Union take more specific type param fs In Load Foreach fs is user declared schema in Union fs is left side param fs In Load Foreach fs is inferred schema in Union fs is left side param mode merge mode public static Logical Field Schema merge Logical Field Schema fs Logical Field Schema fs Merge Mode mode throws Frontend Exception deal with null schema if mode Merge Mode Load For Each if fs null throw new Frontend Exception We can not cast into null if fs null return fs deep Copy else if mode Merge Mode Load For Each Inner if fs null return null if fs null return fs deep Copy else Union Union Inner if fs null fs null return null String merged Alias byte merged Type Data Type Logical Schema merged Sub Schema null Infer merged data type if mode Merge Mode Union Inner if fs type fs type We do n t merge inner schema of different type for union throw exception throw new Frontend Exception Incompatible field schema left is fs to String false right is fs to String false else merged Type fs type else if mode Merge Mode Load For Each mode Merge Mode Load For Each Inner if fs type Data Type fs type Data Type If declared schema does not have type part merged Type fs type else if Data Type castable fs type fs type throw new Frontend Exception Incompatible field schema declared is fs to String false infered is fs to String false else merged Type fs type If compatible type we take the declared type else Union schema if fs type Data Type merged Type fs type else if fs type Data Type merged Type fs type else Take the more specific type merged Type Data Type merge Type fs type fs type if merged Type Data Type True incompatible set to bytearray merged Type Data Type if fs alias null merged Alias fs alias else if fs alias null merged Alias fs alias else merged Alias merge Name Spaced Alias fs alias fs alias if merged Alias null merged Alias fs alias if Data Type is Schema Type merged Type if mode Merge Mode Union try if fs type Data Type if fs schema null merged Sub Schema fs schema deep Copy else if fs type Data Type if fs schema null merged Sub Schema fs schema deep Copy else merged Sub Schema Logical Schema merge fs schema fs schema Merge Mode Union Inner catch Frontend Exception e if fs type Data Type fs type Data Type create an empty tuple as subschema merged Sub Schema new Logical Schema merged Sub Schema add Field new Logical Field Schema null new Logical Schema Data Type else if fs type Data Type fs type Data Type merged Sub Schema new Logical Schema If inner schema is not compatible merged Sub Schema set to null else if mode Merge Mode Union Inner merged Sub Schema Logical Schema merge fs schema fs schema Merge Mode Union Inner else Load For Each Load For Each Inner if fs type Data Type merged Sub Schema fs schema else try Only check compatibility merged Sub Schema Logical Schema merge fs schema fs schema Merge Mode Load For Each Inner catch Frontend Exception e throw new Frontend Exception Incompatible field schema left is fs to String false right is fs to String false Logical Field Schema merged new Logical Field Schema merged Alias merged Sub Schema merged Type return merged public static boolean is Equal Unless Unknown Logical Field Schema fs Logical Field Schema fs throws Frontend Exception if fs type Data Type return true else if fs type Data Type return true else if fs type fs type if Data Type is Complex fs type return Logical Schema is Equal Unless Unknown fs schema fs schema else return true else return false Old Pig field schema does not require a tuple schema inside a bag Now it is required to have that this method is to fill the gap public void normalize if type Data Type if schema null Check if the has a tuple field if schema size schema get Field type Data Type Logical Schema tuple Schema new Logical Schema for Logical Field Schema inner Fs schema get Fields tuple Schema add Field inner Fs schema new Logical Schema schema add Field new Logical Field Schema null tuple Schema Data Type if schema null for Logical Field Schema fs schema get Fields fs normalize private List Logical Field Schema fields public Logical Schema fields new Array List Logical Field Schema Reset uids of all fieldschema that the schema contains public void reset Uid for Logical Field Schema fs fields fs reset Uid Recursively compare two schemas to check if the input schema can be cast to the cast schema param in Sch schema of the cast input param out Sch schema of the cast operator return true if castable public static boolean castable Logical Schema in Sch Logical Schema out Sch If both of them are null they are castable if out Sch null in Sch null return false otherwise if out Sch null return false Cast to a more specific type is good if in Sch null return true if out Sch size in Sch size return false Iterator Logical Field Schema i out Sch fields iterator Iterator Logical Field Schema j in Sch fields iterator while i has Next iterate only for the number of fields in cast Logical Field Schema out Fs i next Logical Field Schema in Fs j next Compare recursively using field schema if Logical Field Schema castable in Fs out Fs return false return true Add a field to this schema param field to be added to the schema public void add Field Logical Field Schema field fields add field Fetch a field by alias param alias return field associated with alias or null if no such field throws Frontend Exception public Logical Field Schema get Field String alias throws Frontend Exception Logical Field Schema result null first look for an exact match for Logical Field Schema fs fields if fs alias null fs alias equals alias if result null result fs else String Builder sb new String Builder Found more than one match result alias fs alias throw new Frontend Exception sb to String if result null return result if no exact match is found look for matches for scoped aliases for Logical Field Schema fs fields if fs alias null fs alias matches alias if result null result fs else String Builder sb new String Builder Found more than one match result alias fs alias throw new Frontend Exception sb to String return result Given an alias name find the associated Logical Field Schema If exact name is not found see if any field matches the part of the namespaced alias eg if given alias is nm a and schema is a b It will return Field Schema of a if given alias is nm a and schema is nm a b it will return null param alias Alias to look up return Logical Field Schema or null if no such alias is in this tuple public Logical Field Schema get Field Sub Name Match String alias throws Frontend Exception if alias null return null Logical Field Schema fs get Field alias if fs null return fs fs is null final String sep Array List Logical Field Schema matched Field Schemas new Array List Logical Field Schema if alias contains sep for Logical Field Schema field fields if alias ends With sep field alias matched Field Schemas add field if matched Field Schemas size boolean has Next false String Builder sb new String Builder Found more than one sub alias name match for Logical Field Schema match Fs matched Field Schemas if has Next sb append else has Next true sb append match Fs alias int err Code throw new Frontend Exception sb to String err Code Pig Exception else if matched Field Schemas size fs matched Field Schemas get return fs public int get Field Position String alias Logical Field Schema fs null try fs get Field alias catch Frontend Exception e if fs null return return fields index Of fs Fetch a field by field number param field Num field number to fetch return field public Logical Field Schema get Field int field Num return fields get field Num Get all fields return list of all fields public List Logical Field Schema get Fields return fields Get the size of the schema return size public int size return fields size Two schemas are equal if they are of equal size and their fields schemas considered in order are equal This function does not compare the alias of the fields public boolean is Equal Object other return is Equal other false Two schemas are equal if they are of equal size and their fields schemas considered in order are equal If compare Alias argument is set to true the alias of the fields are also compared param other param compare Alias return true if equal public boolean is Equal Object other boolean compare Alias if other null other instanceof Logical Schema Logical Schema os Logical Schema other if size os size return false for int i i size i if get Field i is Equal os get Field i compare Alias return false return true else return false Look for the index of the field that contains the specified uid param uid the uid to look for return the index of the field if not found public int find Field long uid for int i i size i Logical Field Schema f get Field i if this field has the same uid then return this field if f uid uid return i if this field has a schema check its schema if f schema null if f schema find Field uid return i return public static enum Merge Mode Load For Each Load For Each Inner Union Union Inner Merge two schemas param s In Load For Each s is user declared schema In Union s is left side param s In Load For Each s is infered schema In Union s is right side param mode We merge schema in Load Foreach Union In Load Foreach we always take s if compatible s is set to be user defined schema In union we take more specific type between numeric and string we take string In the case type mismatch in s s we expect Type Checking Visitor will fill the gap later return a merged schema or null if the merge fails public static Logical Schema merge Logical Schema s Logical Schema s Merge Mode mode throws Frontend Exception If any of the schema is null take the other party if s null s null if mode Merge Mode Load For Each mode Merge Mode Load For Each Inner if s null return s deep Copy else if s null return s deep Copy else return null else Union Union Inner return null return null if s size s size if mode Merge Mode Union In union incompatible type result a null schema return null else throw new Frontend Exception Incompatible schema left is s to String false right is s to String false Logical Schema merged Schema new Logical Schema for int i i s size i Logical Field Schema fs s get Field i Logical Field Schema fs s get Field i Logical Field Schema merged Logical Field Schema merge fs fs mode merged Schema add Field merged return merged Schema public static boolean is Equal Unless Unknown Logical Schema s Logical Schema s throws Frontend Exception if s null return true else if s null return true else if s size s size return false else for int i i s size i if Logical Field Schema is Equal Unless Unknown s get Field i s get Field i return false return true public String to String boolean verbose String Builder str new String Builder for Logical Field Schema field fields str append field to String verbose if fields size str delete Char At str length return str to String public String to String return to String true public Logical Schema merge Uid Logical Schema uid Only Schema throws Frontend Exception if uid Only Schema null if size uid Only Schema size throw new Frontend Exception Structure of schema change Original uid Only Schema Now this for int i i size i get Field i merge Uid uid Only Schema get Field i return uid Only Schema else Logical Schema cloned Uid Only Copy new Logical Schema for int i i size i get Field i stamp Field Schema cloned Uid Only Copy add Field get Field i clone Uid return cloned Uid Only Copy public Logical Schema deep Copy Logical Schema new Schema new Logical Schema for int i i size i new Schema add Field get Field i deep Copy return new Schema Merges collection of schemas using their column aliases unlike merge Schema functions which merge using positions Schema will not be merged if types are incompatible as per Data Type merge Type For Tuples and Bags Sub Schemas have to be equal be considered compatible param schemas list of schemas to be merged using their column alias return merged schema public static Logical Schema merge Schemas By Alias List Logical Schema schemas throws Frontend Exception Logical Schema merged Schema null list of schemas that have currently been merged used in error message Array List Logical Schema merged Schemas new Array List Logical Schema schemas size for Logical Schema sch schemas if merged Schema null merged Schema sch deep Copy merged Schemas add sch continue try merged Schema merge Schema By Alias merged Schema sch merged Schemas add sch catch Frontend Exception e String msg Error merging schema sch with merged schema merged Schema of schemas merged Schemas throw new Frontend Exception msg e return merged Schema Merges two schemas using their column aliases unlike merge Schema functions which merge using positions Schema will not be merged if types are incompatible as per Data Type merge Type For Tuples and Bags Sub Schemas have to be equal be considered compatible public static Logical Schema merge Schema By Alias Logical Schema schema Logical Schema schema throws Frontend Exception Logical Schema merged Schema new Logical Schema Hash Set Logical Field Schema schema cols Added new Hash Set Logical Field Schema add merge fields present in first schema for Logical Field Schema fs schema get Fields check Null Alias fs schema Logical Field Schema fs schema get Field Sub Name Match fs alias if fs null if schema cols Added contains fs alias corresponds to multiple fields in schema just do a lookup on schema that will throw the appropriate error schema get Field Sub Name Match fs alias schema cols Added add fs Logical Field Schema merged Fs Logical Field Schema merge fs fs Merge Mode Union merged Fs alias merge Name Spaced Alias fs alias fs alias if merged Fs alias null merged Fs alias fs alias merged Schema add Field merged Fs else merged Schema add Field new Logical Field Schema fs add schemas from nd schema that are not already present in merged schema for Logical Field Schema fs schema get Fields check Null Alias fs schema if schema cols Added contains fs merged Schema add Field new Logical Field Schema fs return merged Schema private static void check Null Alias Logical Field Schema fs Logical Schema schema throws Frontend Exception if fs alias null throw new Frontend Exception Schema having field with null alias can not be merged using alias Schema schema If one of the aliases is of form nm str and other is of the form str this returns str private static String merge Name Spaced Alias String alias String alias throws Frontend Exception if alias equals alias return alias if alias ends With alias return alias if alias ends With alias return alias the aliases are different alias can not be merged return null Recursively compare two schemas for equality param schema param other param relax Inner if true inner schemas will not be checked param relax Alias if true aliases will not be checked return true if schemas are equal false otherwise public static boolean equals Logical Schema schema Logical Schema other boolean relax Inner boolean relax Alias If both of them are null they are equal if schema null other null return true otherwise if schema null other null return false if schema size other size return false Iterator Logical Field Schema i schema fields iterator Iterator Logical Field Schema j other fields iterator while i has Next Logical Field Schema my Fs i next Logical Field Schema other Fs j next if relax Alias if my Fs alias null other Fs alias null good else if my Fs alias null return false else if my Fs alias equals other Fs alias return false if my Fs type other Fs type return false if relax Inner Logical Field Schema equals my Fs other Fs false relax Alias Compare recursively using field schema return false return true Old Pig schema does not require a tuple schema inside a bag Now it is required to have that this method is to fill the gap public void normalize for Logical Field Schema fs get Fields fs normalize 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java io Exception import java util Array List import java util Arrays import java util Deque import java util Hash Map import java util Iterator import java util Linked List import java util List import java util Map import java util Random import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Func Spec import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig Store Resources import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Logical To Physical Translator Exception import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Counter import org apache pig backend hadoop executionengine physical Layer relational Operators Cross import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Native import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig backend hadoop executionengine physical Layer relational Operators Packager Package Type import org apache pig data Data Type import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data Schema Tuple Frontend import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl builtin Cross import org apache pig impl io File Localizer import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Compiler Utils import org apache pig impl util Linked Multi Map import org apache pig impl util Multi Map import org apache pig impl util Utils import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan Reverse Dependency Order Walker Seen Chk import org apache pig newplan Subtree Dependency Order Walker import org apache pig newplan logical Util import org apache pig newplan logical expression Exp To Phy Translation Visitor import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig parser Source Location public class Log To Phy Translation Visitor extends Logical Relational Nodes Visitor private static final Log Log Factory get Log Log To Phy Translation Visitor class public Log To Phy Translation Visitor Operator Plan plan throws Frontend Exception super plan new Dependency Order Walker plan current Plan new Physical Plan log To Phy Map new Hash Map Operator Physical Operator current Plans new Linked List Physical Plan protected Map Operator Physical Operator log To Phy Map protected Deque Physical Plan current Plans protected Physical Plan current Plan protected Node Id Generator node Gen Node Id Generator get Generator protected Pig Context pc public void set Pig Context Pig Context pc this pc pc public Map Operator Physical Operator get Log To Phy Map return log To Phy Map public Physical Plan get Physical Plan return current Plan Override public void visit Load lo Load throws Frontend Exception String scope The last parameter here is set to true as we assume all files are splittable due to Load Store Refactor Load load new Load new Operator Key scope node Gen get Next Node Id scope lo Load get Load Func load add Original Location lo Load get Alias lo Load get Location load set File lo Load get File Spec load set Pc pc load set Result Type Data Type load set Signature lo Load get Signature load set Limit lo Load get Limit load set Is Tmp Load lo Load is Tmp Load load set Cache Files lo Load get Load Func get Cache Files load set Ship Files lo Load get Load Func get Ship Files current Plan add load log To Phy Map put lo Load load Load is typically a root operator but in the multiquery case it might have a store as a predecessor List Operator op lo Load get Plan get Predecessors lo Load Physical Operator from if op null from log To Phy Map get op get try current Plan connect from load catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Native lo Native throws Frontend Exception String scope Native po Native new Native new Operator Key scope node Gen get Next Node Id scope po Native add Original Location lo Native get Alias lo Native get Location po Native set Native Rjar lo Native get Native Jar po Native set Params lo Native get Params po Native set Result Type Data Type log To Phy Map put lo Native po Native current Plan add po Native List Operator op lo Native get Plan get Predecessors lo Native Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Native throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from po Native catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Filter filter throws Frontend Exception String scope System err println Entering Filter Filter po Filter new Filter new Operator Key scope node Gen get Next Node Id scope filter get Requested Parallelism po Filter add Original Location filter get Alias filter get Location po Filter set Result Type Data Type current Plan add po Filter log To Phy Map put filter po Filter current Plans push current Plan current Plan new Physical Plan Plan Walker child Walker current Walker spawn Child Walker filter get Filter Plan Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk filter get Filter Plan push Walker child Walker current Walker walk this current Walker walk new Exp To Phy Translation Visitor current Walker get Plan child Walker filter current Plan log To Phy Map pop Walker po Filter set Plan current Plan current Plan current Plans pop List Operator op filter get Plan get Predecessors filter Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Filter throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from po Filter catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e translate Soft Links filter System err println Exiting Filter Override public void visit Sort sort throws Frontend Exception String scope List Logical Expression Plan log Plans sort get Sort Col Plans List Physical Plan sort Plans new Array List Physical Plan log Plans size convert all the logical expression plans to physical expression plans current Plans push current Plan for Logical Expression Plan plan log Plans current Plan new Physical Plan Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk plan push Walker child Walker child Walker walk new Exp To Phy Translation Visitor current Walker get Plan child Walker sort current Plan log To Phy Map sort Plans add current Plan pop Walker current Plan current Plans pop get the physical operator for sort Sort po Sort if sort get User Func null po Sort new Sort new Operator Key scope node Gen get Next Node Id scope sort get Requested Parallelism null sort Plans sort get Ascending Cols null else User Comparison Func comparator new User Comparison Func new Operator Key scope node Gen get Next Node Id scope sort get Requested Parallelism null sort get User Func po Sort new Sort new Operator Key scope node Gen get Next Node Id scope sort get Requested Parallelism null sort Plans sort get Ascending Cols comparator po Sort add Original Location sort get Alias sort get Location po Sort set Limit sort get Limit sort set Requested Parallelism s get Type log To Phy Map put sort po Sort current Plan add po Sort List Operator op sort get Plan get Predecessors sort Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Sort throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from po Sort catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e po Sort set Result Type Data Type Transformation from Logical to Physical Plan involves the following steps First it is generated a random number which will link a Counter within a Rank On this way avoiding possible collisions on parallel rank operations Then if it is row number mode pre In case of a operation row number mode are used two steps Each tuple is counted sequentially on each mapper and are produced global counters Global counters are gathered and summed each tuple calls to the respective counter value in order to calculate the corresponding rank value pre or not pre In case of a operation then are necessary five steps Group by the fields involved on the rank operation Package In case of multi fields the key group field is flatten For Each Sort operation by the fields available after flattening Sort Each group is sequentially counted on each mapper through a global counter Counter Global counters are summed and passed to the rank operation Rank pre param lo Rank describe if the rank operation is on a row number mode or is rank by dense or not Override public void visit Rank lo Rank throws Frontend Exception String scope Rank po Rank Counter po Counter Random random Generator new Random Long operation Math abs random Generator next Long try Physical operations for operator In case of a operation then are necessary five steps Group by the fields involved on the rank operation Package In case of multi fields the key group field is flatten For Each Sort operation by the fields available after flattening Sort Each group is sequentially counted on each mapper through a global counter Counter Global counters are summed and passed to the rank operation Rank if lo Rank is Row Number boolean flags false Multi Map Integer Logical Expression Plan expression Plans new Multi Map Integer Logical Expression Plan for int i i lo Rank get Rank Col Plans size i expression Plans put i lo Rank get Rank Col Plans Package po Package compile To Pack Trio lo Rank null flags expression Plans po Package get Pkgr set Package Type Package Type translate Soft Links lo Rank List Boolean flatten Lst Arrays as List true false Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope feproj add Original Location lo Rank get Alias lo Rank get Location feproj set Column feproj set Result Type po Package get Pkgr get Key Type feproj set Star false feproj set Overloaded false fep add feproj Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope feproj add Original Location lo Rank get Alias lo Rank get Location feproj set Column feproj set Result Type Data Type feproj set Star false feproj set Overloaded false fep add feproj List Physical Plan fe Plans Arrays as List fep fep For Each po For Each new For Each new Operator Key scope node Gen get Next Node Id scope fe Plans flatten Lst List Logical Expression Plan rank Plans lo Rank get Rank Col Plans byte new Types new byte rank Plans size for int i i rank Plans size i Logical Expression Plan loep rank Plans get i Iterator Operator inp Opers loep get Operators while inp Opers has Next Operator oper inp Opers next new Types i Project Expression oper get Type List Physical Plan new Physical Plan new Array List Physical Plan List Boolean new Order Plan new Array List Boolean for int i i lo Rank get Rank Col Plans size i Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope feproj add Original Location lo Rank get Alias lo Rank get Location feproj set Column i feproj set Result Type new Types i feproj set Star false feproj set Overloaded false fep add feproj new Physical Plan add fep new Order Plan add lo Rank get Ascending Col get i Sort po Sort po Sort new Sort new Operator Key scope node Gen get Next Node Id scope null new Physical Plan new Order Plan null po Sort set Requested Parallelism lo Rank get Requested Parallelism po Sort add Original Location lo Rank get Alias lo Rank get Location po Counter new Counter new Operator Key scope node Gen get Next Node Id scope null new Physical Plan new Order Plan po Counter add Original Location lo Rank get Alias lo Rank get Location po Counter set Result Type Data Type po Counter set Is Row Number lo Rank is Row Number po Counter set Is Dense Rank lo Rank is Dense Rank po Counter set Operation String value Of operation po Rank new Rank new Operator Key scope node Gen get Next Node Id scope null new Physical Plan new Order Plan po Rank add Original Location lo Rank get Alias lo Rank get Location po Rank set Result Type Data Type po Rank set Operation String value Of operation List Boolean flatten Lst Arrays as List false true Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope feproj add Original Location lo Rank get Alias lo Rank get Location feproj set Column feproj set Result Type Data Type feproj set Star false feproj set Overloaded false fep add feproj Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope feproj add Original Location lo Rank get Alias lo Rank get Location feproj set Column lo Rank get Rank Col Plans size feproj set Result Type Data Type feproj set Star false feproj set Overloaded false fep add feproj List Physical Plan fe Plans Arrays as List fep fep For Each po For Each new For Each new Operator Key scope node Gen get Next Node Id scope fe Plans flatten Lst current Plan add po For Each current Plan add po Sort current Plan add po Counter current Plan add po Rank current Plan add po For Each try current Plan connect po Package po For Each current Plan connect po For Each po Sort current Plan connect po Sort po Counter current Plan connect po Counter po Rank current Plan connect po Rank po For Each catch Plan Exception e throw new Logical To Physical Translator Exception e get Message e get Error Code e get Error Source e log To Phy Map put lo Rank po For Each In case of a operation are used two steps Each tuple is counted sequentially on each mapper and are produced global counters Global counters are gathered and summed each tuple calls to the respective counter value in order to calculate the corresponding rank value else List Logical Expression Plan log Plans lo Rank get Rank Col Plans List Physical Plan rank Plans new Array List Physical Plan log Plans size convert all the logical expression plans to physical expression plans current Plans push current Plan for Logical Expression Plan plan log Plans current Plan new Physical Plan Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk plan push Walker child Walker child Walker walk new Exp To Phy Translation Visitor current Walker get Plan child Walker lo Rank current Plan log To Phy Map rank Plans add current Plan pop Walker current Plan current Plans pop po Counter new Counter new Operator Key scope node Gen get Next Node Id scope null rank Plans lo Rank get Ascending Col po Counter add Original Location lo Rank get Alias lo Rank get Location po Counter set Result Type Data Type po Counter set Is Row Number lo Rank is Row Number po Counter set Is Dense Rank lo Rank is Dense Rank po Counter set Operation String value Of operation po Rank new Rank new Operator Key scope node Gen get Next Node Id scope null rank Plans lo Rank get Ascending Col po Rank add Original Location lo Rank get Alias lo Rank get Location po Rank set Result Type Data Type po Rank set Operation String value Of operation current Plan add po Counter current Plan add po Rank List Operator op lo Rank get Plan get Predecessors lo Rank Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Rank throw new Logical To Physical Translator Exception msg err Code Pig Exception current Plan connect from po Counter current Plan connect po Counter po Rank log To Phy Map put lo Rank po Rank catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Cross cross throws Frontend Exception String scope List Operator inputs cross get Plan get Predecessors cross if cross is Nested Cross phys Op new Cross new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism phys Op add Original Location phys Op get Alias phys Op get Original Locations current Plan add phys Op phys Op set Result Type Data Type log To Phy Map put cross phys Op for Operator op cross get Plan get Predecessors cross Physical Operator from log To Phy Map get op try current Plan connect from phys Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e else Global Rearrange po Global new Global Rearrange new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism po Global add Original Location cross get Alias cross get Location Package po Package new Package new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism po Global add Original Location cross get Alias cross get Location po Global set Cross true current Plan add po Global current Plan add po Package int count try current Plan connect po Global po Package List Boolean flatten Lst Arrays as List true true for Operator op inputs Physical Plan fep new Physical Plan Constant Expression ce new Constant Expression new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism ce set Value inputs size ce set Result Type Data Type fep add ce Constant Expression ce new Constant Expression new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism ce set Value count ce set Result Type Data Type fep add ce Tuple ce val Tuple Factory get Instance new Tuple ce val set inputs size ce val set count ce set Value ce val ce set Result Type Data Type User Func gfc new User Func new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism Arrays as List Physical Operator ce Physical Operator ce new Func Spec Cross class get Name po Global get Operator Key to String gfc add Original Location cross get Alias cross get Location gfc set Result Type Data Type fep add As Leaf gfc gfc set Inputs Arrays as List Physical Operator ce Physical Operator ce fep add gfc fep connect ce gfc fep connect ce gfc Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism feproj add Original Location cross get Alias cross get Location feproj set Result Type Data Type feproj set Star true feproj set Overloaded false fep add feproj List Physical Plan fe Plans Arrays as List fep fep For Each fe new For Each new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism fe Plans flatten Lst fe set Map Side Only true fe add Original Location cross get Alias cross get Location current Plan add fe current Plan connect log To Phy Map get op fe Local Rearrange phys Op new Local Rearrange new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism phys Op add Original Location cross get Alias cross get Location List Physical Plan lr Plans new Array List Physical Plan for int i i inputs size i Physical Plan lrp new Physical Plan Project lrproj new Project new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism i lrproj add Original Location cross get Alias cross get Location lrproj set Overloaded false lrproj set Result Type Data Type lrp add lrproj lr Plans add lrp phys Op set Cross true phys Op set Index count phys Op set Key Type Data Type phys Op set Plans lr Plans phys Op set Result Type Data Type current Plan add phys Op current Plan connect fe phys Op current Plan connect phys Op po Global catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e catch Exec Exception e int err Code String msg Unable to set index on newly create Local Rearrange throw new Visitor Exception msg err Code Pig Exception e po Package get Pkgr set Key Type Data Type po Package set Result Type Data Type po Package set Num Inps count boolean inner new boolean count for int i i count i inner i true po Package get Pkgr set Inner inner List Physical Plan fe Plans new Array List Physical Plan List Boolean flatten Lst new Array List Boolean for int i i count i Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism i feproj add Original Location cross get Alias cross get Location feproj set Result Type Data Type feproj set Overloaded false fep add feproj fe Plans add fep flatten Lst add true For Each fe new For Each new Operator Key scope node Gen get Next Node Id scope cross get Requested Parallelism fe Plans flatten Lst fe add Original Location cross get Alias cross get Location current Plan add fe try current Plan connect po Package fe catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put cross fe Override public void visit Stream stream throws Frontend Exception String scope Stream po Stream new Stream new Operator Key scope node Gen get Next Node Id scope stream get Executable Manager stream get Streaming Command this pc get Properties po Stream add Original Location stream get Alias stream get Location current Plan add po Stream log To Phy Map put stream po Stream List Operator op stream get Plan get Predecessors stream Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Stream throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from po Stream catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Inner Load load throws Frontend Exception String scope Project expr Op new Project new Operator Key scope node Gen get Next Node Id scope Logical Schema s load get Schema if load source Is Bag expr Op set Result Type Data Type expr Op set Overloaded true else if s null expr Op set Result Type s get Field type else expr Op set Result Type Data Type Project Expression proj load get Projection if proj is Project Star expr Op set Star proj is Project Star else if proj is Range Project if proj get End Col all other project range should have been expanded by project star expander throw new Assertion Error project range that is not a project to end seen in translation to physical plan expr Op set Project To End proj get Start Col else expr Op set Column load get Col Num set input to Project to the predecessor of foreach log To Phy Map put load expr Op current Plan add expr Op Override public void visit For Each foreach throws Frontend Exception String scope List Physical Plan inner Plans new Array List Physical Plan org apache pig newplan logical relational Logical Plan inner foreach get Inner Plan Generate gen Generate inner get Sinks get List Logical Expression Plan exps gen get Output Plans List Operator preds inner get Predecessors gen current Plans push current Plan we need to translate each predecessor of Generate into a physical plan The physical plan should contain the expression plan for this predecessor plus the subtree starting with this predecessor for int i i exps size i current Plan new Physical Plan translate the expression plan Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk exps get i push Walker child Walker child Walker walk new Exp To Phy Translation Visitor exps get i child Walker gen current Plan log To Phy Map pop Walker List Operator leaves exps get i get Sinks for Operator l leaves Physical Operator op log To Phy Map get l if l instanceof Project Expression int input Project Expression l get Input Num for each sink projection get its input logical plan and translate it Operator pred preds get input child Walker new Subtree Dependency Order Walker inner pred push Walker child Walker child Walker walk this pop Walker get the physical operator of the leaf of input logical plan Physical Operator leaf log To Phy Map get pred if pred instanceof Inner Load if predecessor is only an Inner Load remove the project that comes from Inner Load and change the column of project that comes from expression plan current Plan remove leaf log To Phy Map remove pred Project leaf Proj Project leaf try if leaf Proj is Star Project op set Star true else if leaf Proj is Project To End Project op set Project To End leaf Proj get Start Col else Project op set Column leaf Proj get Column catch Exec Exception e throw new Frontend Exception foreach Can not get column from leaf e else current Plan connect leaf op inner Plans add current Plan current Plan current Plans pop Physical Operator po Gen new Generate new Operator Key r next Long inputs to Be Flattened boolean flatten gen get Flatten Flags List Boolean flatten List new Array List Boolean for boolean fl flatten flatten List add fl Logical Schema log Schema foreach get Schema Schema schema null if log Schema null try schema Schema get Pig Schema new Resource Schema log Schema catch Frontend Exception e throw new Runtime Exception Logical Schema in foreach unable to be converted to Schema log Schema e if schema null Schema Tuple Frontend register To Generate If Possible schema false Gen Context may need to be appendable For Each po new For Each new Operator Key scope node Gen get Next Node Id scope foreach get Requested Parallelism inner Plans flatten List schema po add Original Location foreach get Alias foreach get Location po set Result Type Data Type log To Phy Map put foreach po current Plan add po generate can not have multiple inputs List Operator op foreach get Plan get Predecessors foreach generate may not have any predecessors if op null return Physical Operator from log To Phy Map get op get try current Plan connect from po catch Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e translate Soft Links foreach This function takes in a List of Logical Expression Plan and converts them to a list of Physical Plans param plans return throws Frontend Exception private List Physical Plan translate Expression Plans Logical Relational Operator loj List Logical Expression Plan plans throws Frontend Exception List Physical Plan expr Plans new Array List Physical Plan if plans null plans size return expr Plans Save the current plan onto stack current Plans push current Plan for Logical Expression Plan lp plans current Plan new Physical Plan We spawn a new Dependency Walker and use it Plan Walker child Walker current Walker spawn Child Walker lp Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk lp Save the old walker and use child Walker as current Walker push Walker child Walker We create a new Exp To Phy Translation Visitor to walk the Expression Plan current Walker walk new Exp To Phy Translation Visitor current Walker get Plan child Walker loj current Plan log To Phy Map expr Plans add current Plan pop Walker Pop the current plan back out current Plan current Plans pop return expr Plans Override public void visit Store lo Store throws Frontend Exception String scope System err println Entering Store Store store new Store new Operator Key scope node Gen get Next Node Id scope store add Original Location lo Store get Alias lo Store get Location store set File lo Store get Output Spec store set Input Spec lo Store get Input Spec store set Signature lo Store get Signature store set Sort Info lo Store get Sort Info store set Is Tmp Store lo Store is Tmp Store store set Store Func lo Store get Store Func store set Schema Util translate Schema lo Store get Schema if lo Store get Store Func instanceof Store Resources store set Cache Files Store Resources lo Store get Store Func get Cache Files store set Ship Files Store Resources lo Store get Store Func get Ship Files current Plan add store List Operator op lo Store get Plan get Predecessors lo Store Physical Operator from null if op null from log To Phy Map get op get Implement sorting when we have a Sort new and Limit new operator ready Sort Info sort Info null if store s predecessor is limit check limit s predecessor if op get instanceof Limit op lo Store get Plan get Predecessors op get Physical Operator sort Phy Op log To Phy Map get op get if this predecessor is a sort get the sort info if op get instanceof Sort sort Info Sort sort Phy Op get Sort Info store set Sort Info sort Info else int err Code String msg Did not find a predecessor for Store throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from store catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put lo Store store System err println Exiting Store Override public void visit Cogroup cg throws Frontend Exception switch cg get Group Type case translate Collected Cogroup cg break case Package po Package compile To Pack Trio cg cg get Custom Partitioner cg get Inner cg get Expression Plans po Package get Pkgr set Package Type Package Type log To Phy Map put cg po Package break case translate Merge Cogroup cg break default throw new Logical To Physical Translator Exception Unknown Co Group Modifier Pig Exception translate Soft Links cg private void translate Collected Cogroup Cogroup cg throws Frontend Exception can have only one input Logical Relational Operator pred Logical Relational Operator plan get Predecessors cg get List Logical Expression Plan expr Plans cg get Expression Plans get Collected Group phys Op new Collected Group new Operator Key node Gen get Next Node Id phys Op add Original Location cg get Alias cg get Location List Physical Plan p Expr Plans translate Expression Plans cg expr Plans try phys Op set Plans p Expr Plans catch Plan Exception pe int err Code String msg Problem with setting up map group s plans throw new Logical To Physical Translator Exception msg err Code Pig Exception pe Byte type null if expr Plans size type Data Type phys Op set Key Type type else type p Expr Plans get get Leaves get get Result Type phys Op set Key Type type phys Op set Result Type Data Type current Plan add phys Op try current Plan connect log To Phy Map get pred phys Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put cg phys Op private Merge Cogroup compile To Merge Cogrp Logical Relational Operator relational Op Multi Map Integer Logical Expression Plan inner Plans throws Frontend Exception List Operator inputs relational Op get Plan get Predecessors relational Op Local Rearrange corresponding to each of input is needed to extract keys out of the tuples Local Rearrange inner Rs new Local Rearrange inputs size int count List Physical Operator inp Os new Array List Physical Operator inputs size for int i i inputs size i Operator op inputs get i Physical Operator phys Op log To Phy Map get op inp Os add phys Op List Logical Expression Plan plans inner Plans get i Local Rearrange po Inner new Local Rearrange new Operator Key node Gen get Next Node Id po Inner add Original Location relational Op get Alias relational Op get Location will contain list of physical plans because there could be multiple keys and each key can be an expression List Physical Plan expr Plans translate Expression Plans relational Op plans try po Inner set Plans expr Plans catch Plan Exception pe int err Code String msg Problem with setting up local rearrange s plans throw new Logical To Physical Translator Exception msg err Code Pig Exception pe inner Rs count po Inner try po Inner set Index count catch Exec Exception e int err Code String msg Unable to set index on newly create Local Rearrange throw new Visitor Exception msg err Code Pig Exception e po Inner set Key Type plans size Data Type expr Plans get get Leaves get get Result Type po Inner set Result Type Data Type Merge Cogroup po Cogrp new Merge Cogroup new Operator Key node Gen get Next Node Id inp Os inner Rs relational Op get Requested Parallelism return po Cogrp private void translate Merge Cogroup Cogroup cg throws Frontend Exception if validate Merge Cogrp cg get Inner throw new Logical To Physical Translator Exception Inner is not supported for any relation on Merge Cogroup List Operator inputs cg get Plan get Predecessors cg Map Side Merge Validator validator new Map Side Merge Validator validator validate Map Side Merge inputs cg get Plan Merge Cogroup po Cogrp compile To Merge Cogrp cg cg get Expression Plans po Cogrp set Result Type Data Type po Cogrp add Original Location cg get Alias cg get Location current Plan add po Cogrp for Operator op inputs try current Plan connect log To Phy Map get op po Cogrp catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put cg po Cogrp private boolean validate Merge Cogrp boolean inner Flags for boolean flag inner Flags if flag return false return true Override public void visit Join loj throws Frontend Exception String scope List of join predicates List Operator inputs loj get Plan get Predecessors loj mapping of inner join physical plans corresponding to inner physical operators Multi Map Physical Operator Physical Plan join Plans new Linked Multi Map Physical Operator Physical Plan Outer list corresponds to join predicates Inner list is inner physical plan of each predicate List List Physical Plan pp Lists new Array List List Physical Plan List of physical operator corresponding to join predicates List Physical Operator inp new Array List Physical Operator Outer list corresponds to join predicates and inner list corresponds to type of keys for each predicate List List Byte key Types new Array List List Byte boolean inner Flags loj get Inner Flags String alias loj get Alias Source Location location loj get Location int parallel loj get Requested Parallelism for int i i inputs size i Operator op inputs get i Physical Operator phys Op log To Phy Map get op inp add phys Op List Logical Expression Plan plans List Logical Expression Plan loj get Join Plan i List Physical Plan expr Plans translate Expression Plans loj plans pp Lists add expr Plans join Plans put phys Op expr Plans Key could potentially be a tuple So we visit all expr Plans to get types of members of tuples List Byte tuple Key Member Types new Array List Byte for Physical Plan expr Plan expr Plans tuple Key Member Types add expr Plan get Leaves get get Result Type key Types add tuple Key Member Types if loj get Join Type Join Skewed Join skj try skj new Skewed Join new Operator Key scope node Gen get Next Node Id scope parallel inp inner Flags skj add Original Location alias location skj set Join Plans join Plans catch Exception e int err Code String msg Skewed Join creation failed throw new Logical To Physical Translator Exception msg err Code Pig Exception e skj set Result Type Data Type for int i i inputs size i Operator op inputs get i if inner Flags i try Logical Schema s Logical Relational Operator op get Schema if the schema can not be determined if s null throw new Frontend Exception loj Can not determine skewed join schema skj add Schema Util translate Schema s catch Frontend Exception e int err Code String msg Could n t set the schema for outer join throw new Logical To Physical Translator Exception msg err Code Pig Exception e else This will never be retrieved It just guarantees that the index will be valid when Compiler is trying to read the schema skj add Schema null current Plan add skj for Operator op inputs try current Plan connect log To Phy Map get op skj catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put loj skj else if loj get Join Type Join Schema input Schemas new Schema inputs size Schema key Schemas new Schema inputs size outer for int i i inputs size i Logical Schema logical Schema Logical Relational Operator inputs get i get Schema if logical Schema null continue Schema to Gen Schema get Pig Schema new Resource Schema logical Schema This registers the value piece Schema Tuple Frontend register To Generate If Possible to Gen false Gen Context input Schemas i to Gen Schema key To Gen new Schema for Byte byt key Types get i We can not generate any nested code because that information is thrown away if byt null Data Type is Complex byt byte Value continue outer key To Gen add new Field Schema null byt Schema Tuple Frontend register To Generate If Possible key To Gen false Gen Context key Schemas i key To Gen int fragment Join pfrj try boolean is Left Outer false We dont check for bounds issue as we assume that a join involves atleast two inputs is Left Outer inner Flags Tuple null Tuple null if is Left Outer try We know that in a Left outer join its only a two way join so we assume index of for the right input Logical Schema input Schema Logical Relational Operator inputs get get Schema We check if we have a schema before the join if input Schema null int err Code String msg Input Logical Relational Operator inputs get get Alias on which outer join is desired should have a valid schema throw new Logical To Physical Translator Exception msg err Code Pig Exception Using the schema we decide the number of columns fields in the null Tuple null Tuple Tuple Factory get Instance new Tuple input Schema size for int j j input Schema size j null Tuple set j null catch Frontend Exception e int err Code String msg Error while determining the schema of input throw new Logical To Physical Translator Exception msg err Code Pig Exception e pfrj new Join new Operator Key scope node Gen get Next Node Id scope parallel inp pp Lists key Types null fragment is Left Outer null Tuple input Schemas key Schemas pfrj add Original Location alias location catch Exec Exception e int err Code String msg Unable to set index on newly create Local Rearrange throw new Visitor Exception msg err Code Pig Exception e pfrj set Result Type Data Type current Plan add pfrj for Operator op inputs try current Plan connect log To Phy Map get op pfrj catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put loj pfrj else if loj get Join Type Join loj get Join Type Join new Map Side Merge Validator validate Map Side Merge inputs loj get Plan Physical Operator smj boolean use Merge Join inputs size inner Flags inner Flags if use Merge Join We register the merge join schema information for code generation Logical Schema logical Schema Logical Relational Operator inputs get get Schema Schema left Schema null if logical Schema null left Schema Schema get Pig Schema new Resource Schema logical Schema logical Schema Logical Relational Operator inputs get get Schema Schema right Schema null if logical Schema null right Schema Schema get Pig Schema new Resource Schema logical Schema logical Schema loj get Schema Schema merged Schema null if logical Schema null merged Schema Schema get Pig Schema new Resource Schema logical Schema if left Schema null Schema Tuple Frontend register To Generate If Possible left Schema false Gen Context if right Schema null Schema Tuple Frontend register To Generate If Possible right Schema false Gen Context if merged Schema null Schema Tuple Frontend register To Generate If Possible merged Schema false Gen Context inner join on two sorted inputs We have less restrictive implementation here in a form of Merge Join which does n t require loaders to implement collectable interface try smj new Merge Join new Operator Key scope node Gen get Next Node Id scope parallel inp join Plans key Types loj get Join Type left Schema right Schema merged Schema catch Plan Exception e int err Code String msg Merge Join creation failed throw new Logical To Physical Translator Exception msg err Code Pig Exception e log To Phy Map put loj smj else in all other cases we fall back to Merge Cogroup Flattening Es smj compile To Merge Cogrp loj loj get Expression Plans smj set Result Type Data Type current Plan add smj smj add Original Location alias location for Operator op inputs try current Plan connect log To Phy Map get op smj catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e if use Merge Join Now create and configure foreach which will flatten the output of cogroup For Each fe compile Flattening inner Flags scope parallel alias location inputs current Plan add fe try current Plan connect smj fe catch Plan Exception e throw new Logical To Physical Translator Exception e get Message e get Error Code e get Error Source e log To Phy Map put loj fe return else if loj get Join Type Join loj get Join Type Join Package po Package compile To Pack Trio loj loj get Custom Partitioner inner Flags loj get Expression Plans For Each fe compile Flattening inner Flags scope parallel alias location inputs current Plan add fe try current Plan connect po Package fe catch Plan Exception e throw new Logical To Physical Translator Exception e get Detailed Message e get Error Code e get Error Source e log To Phy Map put loj fe if loj get Join Type Join if inner Flags length if inner Flags false inner Flags false throw new Logical To Physical Translator Exception Error at loj get Location with alias loj get Alias Bloom join can not be used with a join Pig Exception po Package get Pkgr set Package Type Package Type else po Package get Pkgr set Package Type Package Type translate Soft Links loj private Package compile To Pack Trio Logical Relational Operator relational Op String custom Partitioner boolean inner Flags Multi Map Integer Logical Expression Plan inner Plans throws Frontend Exception Global Rearrange po Global new Global Rearrange new Operator Key node Gen get Next Node Id relational Op get Requested Parallelism po Global add Original Location relational Op get Alias relational Op get Location po Global set Custom Partitioner custom Partitioner Package po Package new Package new Operator Key node Gen get Next Node Id relational Op get Requested Parallelism po Package add Original Location relational Op get Alias relational Op get Location current Plan add po Global current Plan add po Package try current Plan connect po Global po Package catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e int count Byte type null List Operator inputs relational Op get Plan get Predecessors relational Op for int i i inputs size i Operator op inputs get i List Logical Expression Plan plans inner Plans get i Local Rearrange phys Op new Local Rearrange new Operator Key node Gen get Next Node Id relational Op get Requested Parallelism phys Op add Original Location relational Op get Alias relational Op get Location List Physical Plan expr Plans translate Expression Plans relational Op plans try phys Op set Plans expr Plans catch Plan Exception pe int err Code String msg Problem with setting up local rearrange s plans throw new Logical To Physical Translator Exception msg err Code Pig Exception pe try phys Op set Index count catch Exec Exception e int err Code String msg Unable to set index on newly create Local Rearrange throw new Visitor Exception msg err Code Pig Exception e if plans size type Data Type phys Op set Key Type type else type expr Plans get get Leaves get get Result Type phys Op set Key Type type phys Op set Result Type Data Type current Plan add phys Op try current Plan connect log To Phy Map get op phys Op current Plan connect phys Op po Global catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e po Package get Pkgr set Key Type type po Package set Result Type Data Type po Package set Num Inps count po Package get Pkgr set Inner inner Flags return po Package private For Each compile Flattening boolean inner Flags String scope int parallel String alias Source Location location List Operator inputs throws Frontend Exception List Physical Plan fe Plans new Array List Physical Plan List Boolean flatten Lst new Array List Boolean For Each fe try for int i i inputs size i Physical Plan fep new Physical Plan Project feproj new Project new Operator Key scope node Gen get Next Node Id scope parallel i i since the first column is the group field feproj add Original Location alias location feproj set Result Type Data Type feproj set Overloaded false fep add feproj fe Plans add fep the parser would have marked the side where we need to keep empty bags on non matched as outer inner Flags i would be false if inner Flags i Operator join Input inputs get i for outer join add a bincond which will project nulls when bag is empty update With Empty Bag Check fep join Input flatten Lst add true fe new For Each new Operator Key scope node Gen get Next Node Id scope parallel fe Plans flatten Lst fe add Original Location alias location catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e return fe Override public void visit Union lo Union throws Frontend Exception String scope Union phys Op new Union new Operator Key scope node Gen get Next Node Id scope lo Union get Requested Parallelism phys Op add Original Location lo Union get Alias lo Union get Location current Plan add phys Op phys Op set Result Type Data Type log To Phy Map put lo Union phys Op List Operator ops lo Union get Plan get Predecessors lo Union for Operator l ops Physical Operator from log To Phy Map get l try current Plan connect from phys Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Distinct lo Distinct throws Frontend Exception String scope Distinct phys Op new Distinct new Operator Key scope node Gen get Next Node Id scope lo Distinct get Requested Parallelism phys Op set Custom Partitioner lo Distinct get Custom Partitioner phys Op add Original Location lo Distinct get Alias lo Distinct get Location current Plan add phys Op phys Op set Result Type Data Type log To Phy Map put lo Distinct phys Op Operator op lo Distinct get Plan get Predecessors lo Distinct get Physical Operator from log To Phy Map get op try current Plan connect from phys Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Limit lo Limit throws Frontend Exception String scope Limit po Limit new Limit new Operator Key scope node Gen get Next Node Id scope lo Limit get Requested Parallelism po Limit set Limit lo Limit get Limit po Limit add Original Location lo Limit get Alias lo Limit get Location po Limit set Result Type Data Type current Plan add po Limit log To Phy Map put lo Limit po Limit if lo Limit get Limit Plan null add expression plan to Limit current Plans push current Plan current Plan new Physical Plan Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk lo Limit get Limit Plan push Walker child Walker current Walker walk new Exp To Phy Translation Visitor current Walker get Plan child Walker lo Limit current Plan log To Phy Map po Limit set Limit Plan current Plan pop Walker current Plan current Plans pop Operator op lo Limit get Plan get Predecessors lo Limit get Physical Operator from log To Phy Map get op try current Plan connect from po Limit catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e translate Soft Links lo Limit Override public void visit Split lo Split throws Frontend Exception String scope Split phys Op new Split new Operator Key scope node Gen get Next Node Id scope lo Split get Requested Parallelism phys Op add Original Location lo Split get Alias lo Split get Location File Spec spl Str File try spl Str File new File Spec File Localizer get Temporary Path pc to String new Func Spec Utils get Tmp File Compressor Name pc catch Exception e byte err Src pc get Error Source int err Code switch err Src case Pig Exception err Code break case Pig Exception err Code break case Pig Exception err Code break String msg Unable to obtain a temporary path throw new Logical To Physical Translator Exception msg err Code err Src e phys Op set Split Store spl Str File log To Phy Map put lo Split phys Op current Plan add phys Op List Operator op lo Split get Plan get Predecessors lo Split Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Split throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from phys Op catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e Override public void visit Split Output lo Split Output throws Frontend Exception String scope System err println Entering Filter Filter po Filter new Filter new Operator Key scope node Gen get Next Node Id scope lo Split Output get Requested Parallelism po Filter add Original Location lo Split Output get Alias lo Split Output get Location po Filter set Result Type Data Type current Plan add po Filter log To Phy Map put lo Split Output po Filter current Plans push current Plan current Plan new Physical Plan Plan Walker child Walker current Walker spawn Child Walker filter get Filter Plan Plan Walker child Walker new Reverse Dependency Order Walker Seen Chk lo Split Output get Filter Plan push Walker child Walker current Walker walk this current Walker walk new Exp To Phy Translation Visitor current Walker get Plan child Walker lo Split Output current Plan log To Phy Map pop Walker po Filter set Plan current Plan current Plan current Plans pop List Operator op lo Split Output get Plan get Predecessors lo Split Output Physical Operator from if op null from log To Phy Map get op get else int err Code String msg Did not find a predecessor for Filter throw new Logical To Physical Translator Exception msg err Code Pig Exception try current Plan connect from po Filter catch Plan Exception e int err Code String msg Invalid physical operators in the physical plan throw new Logical To Physical Translator Exception msg err Code Pig Exception e translate Soft Links lo Split Output System err println Exiting Filter updates plan with check for empty bag and if bag is empty to flatten a bag with as many null s as dictated by the schema param fe Plan the plan to update param join Input the relation for which the corresponding bag is being checked throws Frontend Exception public static void update With Empty Bag Check Physical Plan fe Plan Operator join Input throws Frontend Exception Logical Schema input Schema null try input Schema Logical Relational Operator join Input get Schema if input Schema null int err Code String msg Input Logical Relational Operator join Input get Alias on which outer join is desired should have a valid schema throw new Logical To Physical Translator Exception msg err Code Pig Exception catch Frontend Exception e int err Code String msg Error while determining the schema of input throw new Logical To Physical Translator Exception msg err Code Pig Exception e Compiler Utils add Empty Bag Outer Join fe Plan Util translate Schema input Schema false null private void translate Soft Links Operator op throws Frontend Exception List Operator preds op get Plan get Soft Link Predecessors op if preds null return for Operator pred preds Physical Operator from log To Phy Map get pred current Plan create Soft Link from log To Phy Map get op 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Map import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Source Location Operator to map the data into the inner plan of For Each It can only be used in the inner plan of For Each public class Inner Load extends Logical Relational Operator private Project Expression prj private For Each foreach private boolean source Is Bag false public Inner Load Operator Plan plan For Each foreach int col Num super Inner Load plan store column number as a Project Expression in a plan to be able to dynamically adjust column number during optimization Logical Expression Plan exp new Logical Expression Plan we do n t care about type so set to prj new Project Expression exp col Num foreach this foreach foreach public Inner Load Operator Plan plan For Each foreach String col Alias throws Frontend Exception super Inner Load plan store column number as a Project Expression in a plan to be able to dynamically adjust column number during optimization Logical Expression Plan exp new Logical Expression Plan this prj new Project Expression exp col Alias null foreach this foreach foreach public Inner Load Logical Plan plan For Each foreach Project Expression project Expression super Inner Load plan this prj project Expression this prj set Input Num this prj set Attached Relational Op foreach this foreach foreach Override public Logical Schema get Schema throws Frontend Exception if schema null return schema if prj find Referent get Schema null prj get Field Schema null if prj get Field Schema type Data Type source Is Bag true alias prj get Field Schema alias if prj get Field Schema schema null Logical Field Schema tuple Schema prj get Field Schema schema get Field if tuple Schema null tuple Schema schema null schema new Logical Schema for int i i tuple Schema schema size i schema add Field tuple Schema schema get Field i else schema new Logical Schema schema add Field prj get Field Schema else if prj is Range Or Star Project schema new Logical Schema schema add Field new Logical Field Schema null null Data Type return schema Override public void reset Schema super reset Schema prj reset Field Schema public Project Expression get Projection return prj Override public boolean is Equal Operator other throws Frontend Exception if other instanceof Inner Load return false return get Col Num Inner Load other get Col Num Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this public int get Col Num return prj get Col Num Get the For Each operator that contains this operator as part of inner plan return the For Each operator public For Each get For Each return foreach public boolean source Is Bag return source Is Bag public String to String String Builder msg new String Builder if alias null msg append alias msg append Name name msg append if get Projection get Col Alias null msg append get Projection get Col Alias else if get Projection is Project Star msg append else if get Projection is Range Project msg append get Projection get Start Col append append get Projection get End Col else msg append get Projection get Col Num msg append msg append Schema if schema null msg append schema else msg append null msg append if annotations null for Map Entry String Object entry annotations entry Set msg append entry return msg to String Override public void set Location Source Location loc super set Location loc prj set Location loc 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util Collection import java util Iterator import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Multi Map import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression Plan public class Join extends Logical Relational Operator private static final long serial Version Enum for the type of join public static enum Hash Join Bloom Join Fragment Replicated join Skewed Join Sort Merge Join Sort Merge Index Join Join contains a list of logical operators corresponding to the relational operators and a list of generates for each relational operator Each generate operator in turn contains a list of expressions for the columns that are projected private static Log log Log Factory get Log Join class expression plans for each input private Multi Map Integer Logical Expression Plan m Join Plans indicator for each input whether it is inner private boolean m Inner Flags private m Join Type Retains the type of the join static constant to refer to the option of selecting a join type public final static Integer public Join Logical Plan plan super Join plan public Join Logical Plan plan Multi Map Integer Logical Expression Plan join Plans jt boolean is Inner super Join plan m Join Plans join Plans m Join Type jt m Inner Flags is Inner public void set Join Plans Multi Map Integer Logical Expression Plan join Plans this m Join Plans join Plans public void set Inner Flags boolean is Inner this m Inner Flags is Inner public void set Join Type jt this m Join Type jt public boolean is Inner int input Index return m Inner Flags input Index public boolean get Inner Flags return m Inner Flags public get Join Type return m Join Type public void reset Join Type m Join Type public Collection Logical Expression Plan get Join Plan int input Index return m Join Plans get input Index Get all of the expressions plans that are in this join return collection of all expression plans public Multi Map Integer Logical Expression Plan get Expression Plans return m Join Plans public Collection Logical Expression Plan get Expression Plan Values return m Join Plans values Override public Logical Schema get Schema throws Frontend Exception if schema is calculated before just return if schema null return schema List Operator inputs null inputs plan get Predecessors this if inputs null return null List Logical Schema Logical Field Schema fss new Array List Logical Schema Logical Field Schema for Operator op inputs Logical Schema input Schema Logical Relational Operator op get Schema the schema of one input is unknown so the join schema is unknown just return if input Schema null schema null return schema for int i i input Schema size i Logical Schema Logical Field Schema fs input Schema get Field i Logical Schema Logical Field Schema new null if fs alias null new new Logical Schema Logical Field Schema Logical Relational Operator op get Alias fs alias fs schema fs type fs uid else new new Logical Schema Logical Field Schema fs alias fs schema fs type fs uid fss add new schema new Logical Schema for Logical Schema Logical Field Schema field Schema fss schema add Field field Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Join Join oj Join other if m Join Type oj m Join Type return false if m Inner Flags length oj m Inner Flags length return false for int i i m Inner Flags length i if m Inner Flags i oj m Inner Flags i return false if check Equality oj return false if m Join Plans size oj m Join Plans size return false Now we need to make sure that for each input we are projecting the same columns This is slightly complicated since Multi Map does n t return any particular order so we have to find the matching input in each case for Integer p m Join Plans key Set Iterator Integer iter oj m Join Plans key Set iterator int op while iter has Next op iter next if p equals op break if op Collection Logical Expression Plan c m Join Plans get p Collection Logical Expression Plan oc oj m Join Plans get op if c size oc size return false if c instanceof List oc instanceof List throw new Frontend Exception Expected list of expression plans List Logical Expression Plan elist List Logical Expression Plan c List Logical Expression Plan oelist List Logical Expression Plan oc for int i i elist size i if elist get i is Equal oelist get i return false else return false return true else return false Override public String get Name return name m Join Type to String public List Operator get Inputs Logical Plan plan return plan get Predecessors this 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression Plan public class Limit extends Logical Relational Operator private static final long serial Version private static final long private long m Limit private Logical Expression Plan mlimit Plan public Limit Logical Plan plan super Limit plan public Limit Logical Plan plan long limit super Limit plan this set Limit limit public Limit Logical Plan plan Logical Expression Plan limit Plan super Limit plan this set Limit Plan limit Plan public long get Limit return m Limit public void set Limit long limit this m Limit limit public Logical Expression Plan get Limit Plan return mlimit Plan public void set Limit Plan Logical Expression Plan mlimit Plan this mlimit Plan mlimit Plan Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Relational Operator input null input Logical Relational Operator plan get Predecessors this get schema input get Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Limit Limit other Limit Limit other if this get Limit this get Limit other Limit get Limit this get Limit Plan null this get Limit Plan is Equal other Limit get Limit Plan return check Equality other Limit return false public Operator get Input Logical Plan plan return plan get Predecessors this get 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java io Exception import java util List import org apache hadoop conf Configuration import org apache hadoop mapreduce Job import org apache pig Load Func import org apache pig Load Metadata import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig data Data Type import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl util Object Serializer import org apache pig impl util Utils import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig newplan logical Util public class Load extends Logical Relational Operator public enum Cast State private Logical Schema script Schema private final File Spec fs private transient Load Func load Func transient private Configuration conf private final Logical Schema determined Schema private List Integer required Fields null private Cast State cast State Cast State private Logical Schema uid Only Schema private final String schema File private final String signature private long limit private boolean is Tmp Load used for pattern matching param schema schema user specified in script or null if not specified param plan logical plan this load is part of public Load Logical Schema schema Logical Plan plan this null schema plan null null null Used from the Logical Plan Builder param loader Func Spec for load function to use for this load param schema schema user specified in script or null if not specified param plan logical plan this load is part of param conf param load Func the Load Func that was instantiated from loader param signature the signature that will be passed to the Load Func public Load File Spec loader Logical Schema schema Logical Plan plan Configuration conf Load Func load Func String signature super Load plan this script Schema schema this fs loader this schema File loader null null loader get File Name this conf conf this load Func load Func this signature signature store Script Schema conf script Schema signature if load Func null this load Func set Context Signature signature try this determined Schema get Schema From Meta Data catch Frontend Exception e throw new Runtime Exception Can not retrieve schema from loader load Func e else this determined Schema null public String get Schema File return schema File public Load Func get Load Func throws Frontend Exception try if load Func null fs null load Func Load Func Pig Context instantiate Func From Spec fs get Func Spec load Func set Context Signature signature return load Func catch Class Cast Exception cce throw new Frontend Exception this fs get Func Spec should implement the Load Func interface public void set Script Schema Logical Schema schema script Schema schema public void set Required Fields List Integer required Fields this required Fields required Fields Get the schema for this load The schema will be either be what was given by the user in the script or what the load functions get Schema call returned Otherwise null will be returned indicating that the schema is unknown return schema or null if unknown Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Schema original Schema null if script Schema null determined Schema null original Schema Logical Schema merge script Schema determined Schema Logical Schema Merge Mode Load For Each else if script Schema null original Schema script Schema else if determined Schema null original Schema determined Schema if is Cast Adjusted for int i i original Schema size i Logical Schema Logical Field Schema fs original Schema get Field i if determined Schema null Reset the loads field schema to byte array so that it will reflect reality fs type Data Type else Reset the type to what determined Schema says it is fs type determined Schema get Field i type if original Schema null uid Only Schema original Schema merge Uid uid Only Schema if required Fields null schema new Logical Schema for int i i original Schema size i if required Fields contains i schema add Field original Schema get Field i else schema original Schema return schema private Logical Schema get Schema From Meta Data throws Frontend Exception if get Load Func null get Load Func instanceof Load Metadata try Resource Schema resource Schema Load Metadata load Func get Schema get File Spec get File Name new Job conf Schema old Schema Schema get Pig Schema resource Schema return Util translate Schema old Schema catch Exception e throw new Frontend Exception this Can not get schema from load Func load Func get Class get Name e return null This method will store the script Schema Schema using Object Serializer to the current configuration br The schema can be retrieved by load functions or Fs to know the schema the user entered in the as clause br The name format is br pre Signature script Schema Object Serializer serialize script Schema pre p Note that this is not the schema the load function returns but will always be the as clause schema br That is a input as a chararray b chararray br The schema wil lbe a chararray b chararray br p Find better solution to make script schema available to Load Func see https issues apache org jira browse private void store Script Schema Configuration conf Logical Schema script Schema String signature if conf null script Schema null signature null try conf set Utils get Script Schema Key signature Object Serializer serialize Util translate Schema script Schema catch Exception ioe int err Code String msg Problem serializing script schema Frontend Exception fee new Frontend Exception this msg err Code Pig Exception false null ioe throw new Runtime Exception fee public File Spec get File Spec return fs Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this public Logical Schema get Determined Schema return determined Schema Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Load Load ol Load other if check Equality ol return false if fs null if ol fs null return true else return false return fs equals ol fs else return false public void set Cast State Cast State state cast State state public Cast State get Cast State return cast State public boolean is Cast Adjusted return cast State Cast State public Configuration get Configuration return conf Override public void reset Uid uid Only Schema null Override public String to String String str super to String return str Required Fields required Fields public String get Signature return signature public boolean is Tmp Load return is Tmp Load public void set Tmp Load boolean is Tmp Load this is Tmp Load is Tmp Load public Logical Schema get Script Schema return script Schema public long get Limit return limit public void set Limit long limit this limit limit 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Arrays import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor public class Native extends Logical Relational Operator private String native Jar private String params null private Load load private Store store public Native Operator Plan plan String native Jar String parameters super Native plan this store lo Store this load lo Load this native Jar native Jar this params parameters Override public Logical Schema get Schema throws Frontend Exception return load get Schema return null Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator obj throws Frontend Exception if this obj return true if obj null return false if get Class obj get Class return false Native other Native obj if load null if other load null return false else if load equals other load return false if native Jar null if other native Jar null return false else if native Jar equals other native Jar return false if Arrays equals params other params return false if store null if other store null return false else if store equals other store return false check predecessors and schema if check Equality other return false return true return the native Jar public String get Native Jar return native Jar param native Jar the native Jar to set public void set Native Jar String native Jar this native Jar native Jar return the params public String get Params return params param params the params to set public void set Params String params this params params return the load public Load get Load return load param load the load to set public void set Load Load load this load load return the store public Store get Store return store param store the store to set public void set Store Store store this store store 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin This method should never be used directly use link public class Long Sum extends Algebraic Long Math Base public Long Sum set Op public static class Intermediate extends Algebraic Long Math Base Intermediate Override public get Op return public static class Final extends Algebraic Long Math Base Final Override public get Op return 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util List import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan operator implementation Operator Syntax pre code alias rel col ref alias output alias operator rel input relation operator col ref or Column References or a range in the schema of rel dense rank means a sequential value without gasp among different tuple values pre public class Rank extends Logical Relational Operator private final static String rank private final static String List within logical expression plans in case of private List Logical Expression Plan rank Col Plans List within ascending columns on a private List Boolean asc Cols In case of it could by dense or not Being a dense rank means to assign consecutive ranking to different tuples private boolean is Dense Rank false In case of simple namely row number mode which is a consecutive number assigned to each tuple private boolean is Row Number false This is a uid which has been generated for the rank column It is important to keep this so that the uid will be persistent between calls of reset Schema and get Schema private long rank Column Uid public Rank Operator Plan plan super Rank plan this rank Column Uid public Rank Operator Plan plan List Logical Expression Plan rank Col Plans List Boolean asc Cols this plan this rank Col Plans rank Col Plans this asc Cols asc Cols this rank Column Uid public List Logical Expression Plan get Rank Col Plans return rank Col Plans public void set Rank Col Plan List Logical Expression Plan rank Col Plans this rank Col Plans rank Col Plans public List Boolean get Ascending Col return asc Cols public void set Ascending Col List Boolean asc Cols this asc Cols asc Cols Get the schema for the output of Rank Composed by long value prepended to the rest of the input schema return the schema throws Frontend Exception Override public Logical Schema get Schema throws Frontend Exception if schema is calculated before just return if schema null return schema Logical Relational Operator input null Same schema of previous predecessor input Logical Relational Operator plan get Predecessors this get if input null return null Logical Schema input Schema input get Schema the schema of one input is unknown so the rank schema is unknown just return if input Schema null schema null return schema Complete copy from previous schema for each Logical Field Schema List Logical Schema Logical Field Schema fss new Array List Logical Schema Logical Field Schema for int i i input Schema size i Logical Schema Logical Field Schema fs input Schema get Field i Logical Schema Logical Field Schema new null new new Logical Schema Logical Field Schema fs alias fs schema fs type fs uid fss add new schema new Logical Schema rank Column Uid rank Column Uid Logical Expression get Next Uid rank Column Uid schema add Field new Logical Schema Logical Field Schema input get Alias null Data Type rank Column Uid for Logical Schema Logical Field Schema field Schema fss schema add Field field Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Rank Rank o Rank other if rank Col Plans equals o rank Col Plans return false else return false return check Equality Logical Relational Operator other Get if it is a dense return boolean public boolean is Dense Rank return is Dense Rank Set if it is a dense param is Dense Rank if is dense rank or not public void set Is Dense Rank boolean is Dense Rank this is Dense Rank is Dense Rank Get if it is a simple operation Which means a row number attached to each tuple return boolean public boolean is Row Number return is Row Number Set if it is a simple operation param row Number if is a row number operation public void set Is Row Number boolean row Number this is Row Number row Number 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util Iterator import java util List import org apache pig Func Spec import org apache pig Sort Col Info import org apache pig Sort Col Info Order import org apache pig Sort Info import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical visitor Reset Projection Attached Relational Op Visitor public class Sort extends Logical Relational Operator private List Boolean m Asc Cols private Func Spec m Sort Func private boolean m Is Star false private long limit private List Logical Expression Plan m Sort Col Plans public Sort Operator Plan plan super Sort plan public Sort Operator Plan plan List Logical Expression Plan sort Col Plans List Boolean asc Cols Func Spec sort Func this plan m Sort Col Plans sort Col Plans m Asc Cols asc Cols m Sort Func sort Func public List Logical Expression Plan get Sort Col Plans return m Sort Col Plans public void set Sort Col Plans List Logical Expression Plan sort Plans m Sort Col Plans sort Plans public List Boolean get Ascending Cols return m Asc Cols public void set Ascending Cols List Boolean asc Cols m Asc Cols asc Cols public Func Spec get User Func return m Sort Func public void set User Func Func Spec func m Sort Func func public boolean is Star return m Is Star public void set Star boolean b m Is Star b public void set Limit long l limit l public long get Limit return limit public boolean is Limited return limit Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Relational Operator input null input Logical Relational Operator plan get Predecessors this get schema input get Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this public Sort Info get Sort Info throws Frontend Exception Logical Schema schema this get Schema List Sort Col Info sort Col Info List new Array List Sort Col Info for int i i m Sort Col Plans size i get the single project from the sort plans Logical Expression Plan lp m Sort Col Plans get i Iterator Operator ops Iterator lp get Operators List Operator ops List new Array List Operator while ops Iterator has Next ops List add ops Iterator next if ops List size ops List get instanceof Project Expression throw new Frontend Exception this Unsupported operator in inner plan ops List get Project Expression project Project Expression ops List get create Sort Col Info from the project if project is Project Star there is no input schema that is why project star is still here we do n t know how many columns are represented by this so do n t add further columns to sort list return new Sort Info sort Col Info List if project is Range Project if project get End Col stop here for same reason as project star condition above unkown number of columns this represents return new Sort Info sort Col Info List expand the project range into multiple Sort Col Infos for int cnum project get Start Col cnum project get End Col cnum sort Col Info List add new Sort Col Info null cnum get Order m Asc Cols i else int sort Col Index project get Col Num String sort Col Name schema null null schema get Field sort Col Index alias sort Col Info List add new Sort Col Info sort Col Name sort Col Index get Order m Asc Cols i return new Sort Info sort Col Info List private Order get Order List Boolean m Asc Cols int i return m Asc Cols get i Sort Col Info Order Sort Col Info Order Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Sort Sort other Sort Sort other if m Asc Cols equals other Sort get Ascending Cols return false if m Sort Func equals other Sort get User Func return false if m Is Star other Sort is Star return false if limit other Sort get Limit return false if m Sort Col Plans equals other Sort get Sort Col Plans return false return check Equality Logical Relational Operator other public Operator get Input Logical Plan plan return plan get Predecessors this get public static Sort create Copy Sort sort throws Frontend Exception Sort new Sort new Sort sort get Plan null sort get Ascending Cols sort get User Func List Logical Expression Plan new Sort Col Plans new Array List Logical Expression Plan sort get Sort Col Plans size for Logical Expression Plan lep sort get Sort Col Plans Logical Expression Plan new lep lep deep Copy Resetting the attached Sort operator of the Project Expression to the new Sort new Reset Projection Attached Relational Op Visitor new lep new Sort visit new Sort Col Plans add new lep new Sort set Sort Col Plans new Sort Col Plans return new Sort 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor public class Split extends Logical Relational Operator public Split Operator Plan plan super Split plan Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Relational Operator input null input Logical Relational Operator plan get Predecessors this get schema input get Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Split return check Equality Split other else return false 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Hash Map import java util Hash Set import java util Map import java util Set import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical relational Logical Schema Logical Field Schema public class Split Output extends Logical Relational Operator private Logical Expression Plan filter Plan private Map Long Long uid Mapping new Hash Map Long Long public Split Output Logical Plan plan super Split Output plan public Split Output Logical Plan plan Logical Expression Plan filter Plan super Split Output plan this filter Plan filter Plan public Logical Expression Plan get Filter Plan return filter Plan public void set Filter Plan Logical Expression Plan filter Plan this filter Plan filter Plan private void reassign Uid Rec Logical Schema schema for Logical Field Schema fs schema get Fields If one of the field contains a nested schema we need to reassign Uids on the nested fields too if fs schema null fs type Data Type fs type Data Type fs type Data Type reassign Uid Rec fs schema if uid Mapping contains Key fs uid fs uid uid Mapping get fs uid else long pred Uid fs uid fs uid Logical Expression get Next Uid uid Mapping put pred Uid fs uid Override public Logical Schema get Schema throws Frontend Exception if schema null return schema Logical Relational Operator input null input Logical Relational Operator plan get Predecessors this get if input get Schema null schema input get Schema deep Copy reassign Uid Rec schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Split Output Split Output os Split Output other return filter Plan is Equal os filter Plan check Equality os else return false Override public void reset Uid uid Mapping new Hash Map Long Long public long get Input Uids long uid for Map Entry Long Long pair uid Mapping entry Set if pair get Value uid return pair get Key return 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import org apache pig Sort Info import org apache pig Store Func Interface import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Plan Visitor public class Store extends Logical Relational Operator private final File Spec output If we know how to reload the store here s how The l File File Spec is set in Pig Server post Process It can be used to reload this store if the optimizer has the need private File Spec m Input Spec private final String signature private boolean is Tmp Store private Sort Info sort Info private final Store Func Interface store Func private boolean disambiguation Enabled true public Store Logical Plan plan File Spec output File Spec Store Func Interface store Func String signature super Store plan this output output File Spec this store Func store Func this signature signature public Store Logical Plan plan File Spec output File Spec Store Func Interface store Func String signature boolean disambiguation Enabled this plan output File Spec store Func signature this disambiguation Enabled disambiguation Enabled public File Spec get Output Spec return output public Store Func Interface get Store Func return store Func Override public Logical Schema get Schema throws Frontend Exception schema Logical Relational Operator plan get Predecessors this get get Schema if disambiguation Enabled schema null schema get Fields null If requested try and remove parent alias substring including colon s for Logical Schema Logical Field Schema field schema get Fields if field alias null field alias contains continue field alias field alias substring field alias last Index Of return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Store Store os Store other if check Equality os return false No need to test that store Func is equal since it s being instantiated from output if output null os output null return true else if output null os output null return false else return output equals os output else return false public Sort Info get Sort Info return sort Info public void set Sort Info Sort Info sort Info this sort Info sort Info public boolean is Tmp Store return is Tmp Store public void set Tmp Store boolean is Tmp Store this is Tmp Store is Tmp Store public void set Input Spec File Spec in m Input Spec in public File Spec get Input Spec return m Input Spec public String get Signature return signature public File Spec get File Spec return output 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl streaming Executable Manager import org apache pig impl streaming Streaming Command import org apache pig newplan Operator import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Load Cast State public class Stream extends Logical Relational Operator public enum Cast State private Logical Schema script Schema private static final long serial Version private static Log log Log Factory get Log Filter class the Streaming Command object for the Stream Operator this operator represents private Streaming Command command transient private Executable Manager executable Manager private Logical Schema uid Only Schema private Cast State cast State Cast State public Stream Logical Plan plan Executable Manager exe Manager Streaming Command cmd Logical Schema schema super Stream plan command cmd executable Manager exe Manager script Schema schema Get the Streaming Command object associated with this operator return the Streaming Command object public Streaming Command get Streaming Command return command return the Executable Manager public Executable Manager get Executable Manager return executable Manager Override public Logical Schema get Schema throws Frontend Exception if schema null return schema if is Cast Adjusted schema new Logical Schema for int i i script Schema size i Logical Schema Logical Field Schema fs script Schema get Field i deep Copy fs type Data Type schema add Field fs else if script Schema null schema script Schema deep Copy if schema null uid Only Schema schema merge Uid uid Only Schema return schema Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Stream return check Equality Logical Relational Operator other else return false public void set Cast State Cast State state cast State state public Cast State get Cast State return cast State public boolean is Cast Adjusted return cast State Cast State Override public void reset Uid uid Only Schema null public Logical Schema get Script Schema return script Schema 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util Array List import java util Hash Set import java util List import java util Set import org apache pig Pig Exception import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical relational Logical Schema Logical Field Schema public class Union extends Logical Relational Operator private boolean on Schema uid mapping from output uid to input uid private List Pair Long Long uid Mapping new Array List Pair Long Long public Union Operator Plan plan super Union plan public Union Operator Plan plan boolean on Schema this plan this on Schema on Schema public boolean is On Schema return on Schema Override public Logical Schema get Schema throws Frontend Exception if schema null return schema List Operator inputs plan get Predecessors this If any predecessor s schema is null then the schema for union is null for Operator input inputs Logical Relational Operator op Logical Relational Operator input if op get Schema null if is On Schema String msg Schema of relation op get Alias is null can not be used with relations that have null schema throw new Frontend Exception this msg Pig Exception else return null Logical Schema merged Schema null if inputs size return schema Logical Relational Operator inputs get get Schema List String input Aliases new Array List String inputs size List Logical Schema input Schemas new Array List Logical Schema inputs size for Operator input inputs Logical Relational Operator lop Logical Relational Operator input input Aliases add lop get Alias input Schemas add lop get Schema if is On Schema merged Schema create Merged Schema On Alias input Schemas input Aliases else Logical Schema s input Schemas get Logical Schema s input Schemas get merged Schema Logical Schema merge s s Logical Schema Merge Mode Union if merged Schema null return null Merge schema for int i i input Schemas size i Logical Schema other Schema input Schemas get i if merged Schema null other Schema null return null merged Schema Logical Schema merge merged Schema other Schema Logical Schema Merge Mode Union if merged Schema null return null Bring back cached uid if any otherwise cache uid generated for int i i merged Schema size i Logical Schema Logical Field Schema output Field Schema merged Schema get Field i long uid Search all the cached uid mappings by input field to see if we ve cached an output uid for this output field for Logical Schema input Schema input Schemas Logical Schema Logical Field Schema input Field Schema if on Schema input Field Schema input Schema get Field Sub Name Match output Field Schema alias else input Field Schema input Schema get Field i if input Field Schema null uid get Cached Ouput Uid input Field Schema uid if uid break No cached uid Allocate one and locate and cache all inputs if uid uid Logical Expression get Next Uid for Logical Schema input Schema input Schemas long input Uid Logical Field Schema matched Input Field Schema if on Schema matched Input Field Schema input Schema get Field Sub Name Match merged Schema get Field i alias if matched Input Field Schema null input Uid matched Input Field Schema uid uid Mapping add new Pair Long Long uid input Uid else matched Input Field Schema merged Schema get Field i input Uid input Schema get Field i uid uid Mapping add new Pair Long Long uid input Uid output Field Schema uid uid return schema merged Schema create schema for union onschema private Logical Schema create Merged Schema On Alias List Logical Schema input Schemas List String input Aliases throws Frontend Exception Array List Logical Schema schemas new Array List Logical Schema for int i i input Schemas size i Logical Schema sch input Schemas get i for Logical Field Schema fs sch get Fields if fs alias null String msg Schema of relation input Aliases get i has a null fieldschema for column s Schema sch to String false throw new Frontend Exception this msg Pig Exception schemas add sch create the merged schema Logical Schema merged Schema null try merged Schema Logical Schema merge Schemas By Alias schemas catch Frontend Exception e String msg Error merging schemas for union operator e get Message throw new Frontend Exception this msg Pig Exception e return merged Schema private long get Cached Ouput Uid long input Uid long uid for Pair Long Long pair uid Mapping if pair second input Uid uid pair first break return uid Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Relational Nodes Visitor throw new Frontend Exception Expected Logical Plan Visitor Logical Relational Nodes Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Union return check Equality Union other else return false Get input uids mapping to the output uid public Set Long get Input Uids long uid Set Long result new Hash Set Long for Pair Long Long pair uid Mapping if pair first uid result add pair second return result Override public void reset Uid uid Mapping new Array List Pair Long Long public List Operator get Inputs return plan get Predecessors this public List Operator get Inputs Logical Plan plan return plan get Predecessors this public void set Union On Schema boolean flag on Schema flag 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Or Equal To Expr extends Binary Comparison Operator private static final long serial Version transient private final Log log Log Factory get Log get Class public Or Equal To Expr Operator Key k this k public Or Equal To Expr Operator Key k int rp super k rp result Type Data Type Override public String name return Less Than or Equal Data Type find Type Name result Type m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Or Equal this Override public Result get Next Boolean throws Exec Exception Result left right switch operand Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type Result r accum Child null operand Type if r null return r left lhs get Next operand Type right rhs get Next operand Type return do Comparison left right default int err Code String msg this get Class get Simple Name does not know how to handle type Data Type find Type Name operand Type throw new Exec Exception msg err Code Pig Exception Suppress Warnings unchecked private Result do Comparison Result left Result right if left return Status Status return left if right return Status Status return right if either operand is null the result should be null if left result null right result null left result null left return Status Status return left assert left result instanceof Comparable assert right result instanceof Comparable if Comparable left result compare To right result left result Boolean else left result Boolean illustrator Markup null left result Boolean left result return left Override public Or Equal To Expr clone throws Clone Not Supported Exception Or Equal To Expr clone new Or Equal To Expr new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Buffered Input Stream import java io Buffered Reader import java io Buffered Writer import java io File import java io File Input Stream import java io File Reader import java io File Writer import java io Exception import java io Input Stream import java io Input Stream Reader import java io Reader import java io String Reader import java net import java nio charset Charset import java text Parse Exception import java util Abstract List import java util Array List import java util Arrays import java util Date import java util Hash Set import java util List import java util Map import java util Properties import java util jar Attributes import java util jar Jar File import java util jar Manifest import jline console Console Reader import jline console history File History import org antlr runtime Recognition Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop security User Group Information import org apache hadoop util Generic Options Parser import org apache log j Level import org apache log j Logger import org apache log j Property Configurator import org apache pig Pig Runner Return Code import org apache pig backend Backend Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl io File Localizer import org apache pig impl util Jar Manager import org apache pig impl util Log Utils import org apache pig impl util Object Serializer import org apache pig impl util Properties Util import org apache pig impl util Context import org apache pig impl util Utils import org apache pig parser Dry Run Grunt Parser import org apache pig scripting Script Engine import org apache pig scripting Script Engine Supported Script Lang import org apache pig tools cmdline Cmd Line Parser import org apache pig tools grunt Console Reader Input Stream import org apache pig tools grunt Grunt import org apache pig tools pigstats Pig Progress Notification Listener import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Pig Stats Util import org apache pig tools pigstats Script State import org apache pig tools timer Performance Timer Factory import org joda time Date Time import org joda time Duration import org joda time Period import org joda time Period Type import org joda time format Period Format import com google common annotations Visible For Testing import com google common base Strings import com google common io Closeables Main class for Pig engine Interface Audience Limited Private Oozie Interface Stability Stable public class Main static Utils add Shutdown Hook With Priority new Runnable Override public void run File Localizer delete Temp Resource Files Pig Impl Constants private final static Log log Log Factory get Log Main class private static final String log jconf private static final String brief private static final String debug private static final String verbose private static final String version private static final String major Version private static final String minor Version private static final String patch Version private static final String svn Revision private static final String build Time private enum Exec Mode protected static final String pig notification listener protected static final String pig notification listener arg static Attributes attr null try String find Containing Jar Jar Manager find Containing Jar Main class if find Containing Jar null Jar File jar new Jar File find Containing Jar final Manifest manifest jar get Manifest final Map String Attributes attrs manifest get Entries attr attrs get org apache pig else log info Unable to read pigs manifest file as we are not running from a jar version information unavailable catch Exception e log warn Unable to read pigs manifest file version information unavailable e if attr null version attr get Value Implementation Version svn Revision attr get Value Svn Revision build Time attr get Value Build Time Stamp String split version split major Version split minor Version split patch Version split else version null major Version null minor Version null patch Version null svn Revision null build Time null The Main Class for the Pig Jar that will provide a shell and setup a classpath appropriate for executing Jar files Warning this method calls System exit param args jar can be used to add additional jar files colon separated will start a shell e will execute the rest of the command line as if it was input to the shell throws Exception public static void main String args System exit run args null static int run String args Pig Progress Notification Listener listener Date Time start Time new Date Time int rc boolean verbose false boolean grunt Called false boolean delete Temp Files true String log File Name null boolean print Script Run Time true Pig Context pig Context null try Configuration conf new Configuration false Generic Options Parser parser new Generic Options Parser conf args conf parser get Configuration Properties properties new Properties Properties Util load Default Properties properties properties put All Configuration Util to Properties conf if listener null listener make Listener properties String pig Args parser get Remaining Args boolean user Specified Log false boolean check Script Only false Buffered Reader pin null boolean debug false boolean dryrun false boolean embedded false List String params new Array List String List String param Files new Array List String Hash Set String disabled Optimizer Rules new Hash Set String Cmd Line Parser opts new Cmd Line Parser pig Args opts register Opt log jconf Cmd Line Parser Value Expected opts register Opt b brief Cmd Line Parser Value Expected opts register Opt c check Cmd Line Parser Value Expected opts register Opt d debug Cmd Line Parser Value Expected opts register Opt e execute Cmd Line Parser Value Expected opts register Opt f file Cmd Line Parser Value Expected opts register Opt g embedded Cmd Line Parser Value Expected opts register Opt h help Cmd Line Parser Value Expected opts register Opt i version Cmd Line Parser Value Expected opts register Opt l logfile Cmd Line Parser Value Expected opts register Opt m param file Cmd Line Parser Value Expected opts register Opt p param Cmd Line Parser Value Expected opts register Opt r dryrun Cmd Line Parser Value Expected opts register Opt t optimizer off Cmd Line Parser Value Expected opts register Opt v verbose Cmd Line Parser Value Expected opts register Opt w warning Cmd Line Parser Value Expected opts register Opt x exectype Cmd Line Parser Value Expected opts register Opt stop on failure Cmd Line Parser Value Expected opts register Opt no multiquery Cmd Line Parser Value Expected opts register Opt no fetch Cmd Line Parser Value Expected opts register Opt property File Cmd Line Parser Value Expected Exec Mode mode Exec Mode String file null String engine null set up client side system properties in context Context get Context set Client System Props properties char opt while opt opts get Next Opt Cmd Line Parser End Of Opts switch opt case String log jconf opts get Val Str if log jconf null properties set Property log jconf break case b properties set Property true break case c check Script Only true break case d String log Level opts get Val Str if log Level null properties set Property log Level debug true break case e mode Exec Mode break case f mode Exec Mode file opts get Val Str break case g embedded true engine opts get Val Str break case properties set Property stop on failure true break case h String topic opts get Val Str if topic null if topic equals Ignore Case properties print Properties else System out println Invalide help topic topic usage else usage return Return Code case i print Script Run Time false System out println get Version String return Return Code case l call to method that validates the path to the log file and sets up the file to store the client side log file String log File Parameter opts get Val Str if log File Parameter null log File Parameter length log File Name validate Log File log File Parameter null else log File Name validate Log File log File Name null user Specified Log true properties set Property pig logfile log File Name null log File Name break case m param Files add opts get Val Str break case turns off multiquery optimization properties set Property Pig Configuration false break case properties set Property Pig Configuration false break case p params add opts get Val Str break case r currently only used for parameter substitution will be extended in the future dryrun true break case t disabled Optimizer Rules add opts get Val Str break case v properties set Property true verbose true break case w properties set Property aggregate warning false break case x properties set Property exectype opts get Val Str if opts get Val Str to Lower Case contains local User Group Information set Configuration new Configuration false break case Input Stream input Stream null try File Localizer Fetch File Ret local File Ret File Localizer fetch File properties opts get Val Str input Stream new Buffered Input Stream new File Input Stream local File Ret file properties load input Stream catch Exception e throw new Runtime Exception Unable to parse properties file opts get Val Str finally if input Stream null try input Stream close catch Exception e break default Character cc Character value Of opt throw new Assertion Error Unhandled option cc to String create the context with the parameter pig Context new Pig Context properties create the static script state object Script State script State pig Context get Execution Engine instantiate Script State String command Line Load Func join Abstract List String Arrays as List args script State set Command Line command Line if listener null script State register Listener listener Script State start script State pig Context get Properties set Property pig cmd args command Line if log File Name null user Specified Log log File Name validate Log File properties get Property pig logfile null pig Context get Properties set Property pig logfile log File Name null log File Name configure logging configure Log properties pig Context log info get Version String replace n if log File Name null log info Logging error messages to log File Name delete Temp Files Boolean value Of properties get Property Pig Configuration true pig Context get Properties set Property Pig Impl Constants Object Serializer serialize disabled Optimizer Rules Pig Context set Class Loader pig Context create Cl null construct the parameter substitution preprocessor Grunt grunt null Buffered Reader in String subst File null param Files fetch Remote Param Files param Files properties pig Context set Params params pig Context set Param Files param Files switch mode case String remainders opts get Remaining Args if remainders null pig Context get Properties set Property Pig Context Object Serializer serialize remainders File Localizer Fetch File Ret local File Ret File Localizer fetch File properties file if local File Ret did Fetch properties set Property pig jars relative to dfs true script State set File Name file if embedded return run Embedded Script pig Context local File Ret file get Path engine else Supported Script Lang type determine Script Type local File Ret file get Path if type null return run Embedded Script pig Context local File Ret file get Path type name to Lower Case Reader is created by first loading pig load default statements or pigbootup file if available in new Buffered Reader new Input Stream Reader Utils get Composite Stream new File Input Stream local File Ret file properties run parameter substitution preprocessor first subst File file substituted pin run Param Preprocessor pig Context in subst File debug dryrun check Script Only if dryrun if dryrun subst File pig Context log info Dry run completed Substituted pig script is at subst File Expanded pig script is at file expanded else log info Dry run completed Substituted pig script is at subst File return Return Code log File Name validate Log File log File Name local File Ret file pig Context get Properties set Property pig logfile log File Name null log File Name Set job name based on name of the script pig Context get Properties set Property Pig Context Pig Latin new File file get Name if debug new File subst File delete On Exit script State set Script local File Ret file grunt new Grunt pin pig Context grunt Called true if check Script Only grunt check Script subst File System err println file syntax rc Return Code else int results grunt exec rc get Return Code For Stats results return rc case if check Script Only System err println c check option is only valid when executing pig with a pig script file return Return Code Gather up all the remaining arguments into a string and pass them into grunt String Buffer sb new String Buffer String remainders opts get Remaining Args for int i i remainders length i if i sb append sb append remainders i sb append n script State set Script sb to String in new Buffered Reader new String Reader sb to String grunt new Grunt in pig Context grunt Called true int results grunt exec return get Return Code For Stats results default break If we re here we do n t know yet what they want They may have just given us a jar to execute they might have given us a pig script to execute or they might have given us a dash or nothing which means to run grunt interactive String remainders opts get Remaining Args if remainders null if check Script Only System err println c check option is only valid when executing pig with a pig script file return Return Code Interactive mode Exec Mode Reader is created by first loading pig load default statements or pigbootup file if available Console Reader reader new Console Reader Utils get Composite Stream System in properties System out reader set Expand Events false reader set Prompt grunt final String pig history String history File System get Property user home File separator reader set History new File History new File history File Console Reader Input Stream input Stream new Console Reader Input Stream reader grunt new Grunt new Buffered Reader new Input Stream Reader input Stream pig Context grunt set Console Reader reader grunt Called true grunt run return Return Code else pig Context get Properties set Property Pig Context Object Serializer serialize remainders They have a pig script they want us to run mode Exec Mode File Localizer Fetch File Ret local File Ret File Localizer fetch File properties remainders if local File Ret did Fetch properties set Property pig jars relative to dfs true script State set File Name remainders if embedded return run Embedded Script pig Context local File Ret file get Path engine else Supported Script Lang type determine Script Type local File Ret file get Path if type null return run Embedded Script pig Context local File Ret file get Path type name to Lower Case Reader is created by first loading pig load default statements or pigbootup file if available Input Stream seq Input Stream Utils get Composite Stream new File Input Stream local File Ret file properties in new Buffered Reader new Input Stream Reader seq Input Stream run parameter substitution preprocessor first subst File remainders substituted pin run Param Preprocessor pig Context in subst File debug dryrun check Script Only if dryrun if dryrun subst File pig Context log info Dry run completed Substituted pig script is at subst File Expanded pig script is at remainders expanded else log info Dry run completed Substituted pig script is at subst File return Return Code log File Name validate Log File log File Name local File Ret file pig Context get Properties set Property pig logfile log File Name null log File Name if debug new File subst File delete On Exit Set job name based on name of the script pig Context get Properties set Property Pig Context Pig Latin new File remainders get Name script State set Script local File Ret file grunt new Grunt pin pig Context grunt Called true if check Script Only grunt check Script subst File System err println remainders syntax rc Return Code else int results grunt exec rc get Return Code For Stats results return rc Per Utkarsh and Chris invocation of jar file via pig depricated catch Parse Exception e usage rc Return Code Pig Stats Util set Error Message e get Message Pig Stats Util set Error Throwable e catch org apache pig tools parameters Parse Exception e usage rc Return Code Pig Stats Util set Error Message e get Message Pig Stats Util set Error Throwable e catch Exception e if e instanceof Pig Exception Pig Exception pe Pig Exception e rc pe retriable Return Code Return Code Pig Stats Util set Error Message pe get Message Pig Stats Util set Error Code pe get Error Code else rc Return Code Pig Stats Util set Error Message e get Message Pig Stats Util set Error Throwable e if grunt Called Log Utils write Log e log File Name log verbose Error before Pig is launched kill Running Jobs If Interrupted e pig Context catch Throwable e rc Return Code Pig Stats Util set Error Message e get Message Pig Stats Util set Error Throwable e if grunt Called Log Utils write Log e log File Name log verbose Error before Pig is launched kill Running Jobs If Interrupted e pig Context finally if print Script Run Time print Script Run Time start Time if delete Temp Files clear temp files File Localizer delete Temp Files if pig Context null pig Context get Execution Engine destroy Performance Timer Factory get Perf Timer Factory dump Timers return rc private static void print Script Run Time Date Time start Time Date Time end Time new Date Time Duration duration new Duration start Time end Time Period period duration to Period normalized Standard Period Type time log info Pig script completed in Period Format get Default print period duration get Millis ms private static void kill Running Jobs If Interrupted Throwable e Pig Context pig Context Throwable cause e get Cause Kill running job when we get Interrupted Exception Pig thread is interrupted by mapreduce when Oozie launcher job is killed Shutdown hook kills running jobs but sometimes Node Manager can issue a after unregisters and before shutdown hook gets to execute causing orphaned jobs that continue to run if e instanceof Interrupted Exception cause null cause instanceof Interrupted Exception try pig Context get Execution Engine kill catch Backend Exception be log error Error while killing running jobs be protected static Pig Progress Notification Listener make Listener Properties properties try return Pig Context instantiate Object From Params Configuration Util to Configuration properties Pig Progress Notification Listener class catch Exec Exception e throw new Runtime Exception e private static int get Return Code For Stats int stats return stats Return Code no failed jobs stats Return Code no succeeded jobs Return Code some jobs have failed public static boolean dryrun String script File Pig Context pig Context throws Recognition Exception Exception Buffered Reader rd new Buffered Reader new File Reader script File Dry Run Grunt Parser dryrun new Dry Run Grunt Parser rd script File pig Context boolean has Macro dryrun parse Stop On Error if has Macro String expanded File script File replace substituted expanded Buffered Writer fw new Buffered Writer new File Writer expanded File fw append dryrun get Result fw close return has Macro jz log j properties should be used instead private static void configure Log Properties properties Pig Context pig Context Add a file appender for the logs Need to create a property in the properties file for it sgroschupf Feb this method will be obsolete with String log jconf properties get Property String true String true boolean brief true String equals Ignore Case properties get Property Level log Level Level String log Level String properties get Property if log Level String null log Level Level to Level log Level String Level final Properties props log j Conf As Properties log jconf if props size props set Property log j logger org apache pig log Level to String if log Level String System get Property pig logfile level null props set Property log j root Logger else log Level Level to Level log Level String Level props set Property log j logger org apache pig log Level to String props set Property log j root Logger props set Property log j appender org apache log j Rolling File Appender props set Property log j appender File properties get Property pig logfile props set Property log j appender layout org apache log j Pattern Layout props set Property log j appender layout Conversion Pattern brief m n d t p c m n props set Property log j appender org apache log j Console Appender props set Property log j appender target System err props set Property log j appender layout org apache log j Pattern Layout props set Property log j appender layout Conversion Pattern brief m n d t p c m n Property Configurator configure props log Level Logger get Logger org apache pig get Level if log Level null log Level Logger get Logger org apache pig get Effective Level Properties backend Props pig Context get Log j Properties backend Props set Property log j logger org apache pig log Level to String pig Context set Log j Properties backend Props pig Context set Default Log Level log Level Visible For Testing static Properties log j Conf As Properties String log jconf final Properties properties new Properties if Strings is Null Or Empty log jconf Reader property Reader null try final File file new File log jconf if file exists property Reader new File Reader file properties load property Reader log info Loaded log j properties from file file else final resource Main class get Class Loader get Resource log jconf if resource null property Reader new Input Stream Reader resource open Stream Charset for Name properties load property Reader log info Loaded log j properties from resource resource else log warn No file or resource found by the name log jconf catch Exception e log warn Can not open log j properties file log jconf using default finally Closeables close Quietly property Reader return properties private static List String fetch Remote Param Files List String param Files Properties properties throws Exception List String param Files new Array List String for String param param Files File Localizer Fetch File Ret local File Ret File Localizer fetch File properties param param Files add local File Ret file get Absolute Path return param Files returns the stream of final pig script to be passed to Grunt private static Buffered Reader run Param Preprocessor Pig Context context Buffered Reader orig Pig Script String script File boolean create File throws org apache pig tools parameters Parse Exception Exception if create File return context do Param Substitution Output To File orig Pig Script script File else String substituted context do Param Substitution orig Pig Script return new Buffered Reader new String Reader substituted Returns the major version of Pig being run public static String get Major Version return major Version Returns the major version of the Pig build being run public static String get Minor Version return minor Version Returns the patch version of the Pig build being run public static String get Patch Version return patch Version Returns the svn revision number of the Pig build being run public static String get Svn Revision return svn Revision Returns the built time of the Pig build being run public static String get Build Time return build Time Returns a version string formatted similarly to that of svn pre Apache Pig version r compiled Nov pre public static String get Version String return Apache Pig version version r svn Revision ncompiled build Time Print usage string public static void usage System out println n get Version String n System out println Pig options Run interactively in grunt shell System out println Pig options e xecute cmd cmd Run cmd s System out println Pig options f ile file Run cmds found in file System out println options include System out println log jconf Log j configuration file overrides log conf System out println b brief Brief logging no timestamps System out println c check Syntax check System out println d debug Debug level is default System out println e execute Commands to execute within quotes System out println f file Path to the script to execute System out println g embedded Script Engine classname or keyword for the Script Engine System out println h help Display this message You can specify topic to get help for that topic System out println properties is the only topic currently supported h properties System out println i version Display version information System out println l logfile Path to client side log file default is current working directory System out println m param file Path to the parameter file System out println p param Key value pair of the form param val System out println r dryrun Produces script with substituted parameters Script is not executed System out println t optimizer off Turn optimizations off The following values are supported System out println Constant Calculator Calculate constants at compile time System out println Split Filter Split filter conditions System out println Push Up Filter Filter as early as possible System out println Merge Filter Merge filter conditions System out println Push Down Foreach Flatten Join or explode as late as possible System out println Limit Optimizer Limit as early as possible System out println Column Map Key Prune Remove unused data System out println Add For Each Add For Each to remove unneeded columns System out println Merge For Each Merge adjacent For Each System out println Group By Const Parallel Setter Force parallel for group all statement System out println Partition Filter Optimizer Pushdown partition filter conditions to loader implementing Load Meta Data System out println Predicate Pushdown Optimizer Pushdown filter predicates to loader implementing Load Predicate Push Down System out println All Disable all optimizations System out println All optimizations listed here are enabled by default Optimization values are case insensitive System out println v verbose Print all error messages to screen System out println w warning Turn warning logging on also turns warning aggregation off System out println x exectype Set execution mode local mapreduce tez default is mapreduce System out println stop on failure Aborts execution on the first failed job default is off System out println no multiquery Turn multiquery optimization off default is on System out println no fetch Turn fetch optimization off default is on System out println property File Path to property file System out println print Cmd Debug Overrides anything else and prints the actual command used to run Pig including System out println any environment variables that are set by the pig command public static void print Properties System out println The following properties are supported System out println Logging System out println verbose true false default is false This property is the same as v switch System out println brief true false default is false This property is the same as b switch System out println debug default is This property is the same as d switch System out println aggregate warning true false default is true If true prints count of warnings System out println of each type rather than logging each warning System out println Performance tuning System out println pig cachedbag memusage mem fraction default is of all memory System out println Note that this memory is shared across all large bags used by the application System out println pig skewedjoin reduce memusagea mem fraction default is of all memory System out println Specifies the fraction of heap available for the reducer to perform the join System out println pig exec nocombiner true false default is false System out println Only disable combiner as a temporary workaround for problems System out println opt multiquery true false multiquery is on by default System out println Only disable multiquery as a temporary workaround for problems System out println opt fetch true false fetch is on by default System out println Scripts containing Filter Foreach Limit Stream and Union can be dumped without jobs System out println pig tmpfilecompression true false compression is off by default System out println Determines whether output of intermediate jobs is compressed System out println pig tmpfilecompression codec lzo gzip default is gzip System out println Used in conjunction with pig tmpfilecompression Defines compression type System out println pig no Split Combination true false Split combination is on by default System out println Determines if multiple small files are combined into a single map System out println pig exec map Part Agg true false Default is false System out println Determines if partial aggregation is done within map phase System out println before records are sent to combiner System out println pig exec map Part Agg min Reduction min aggregation factor Default is System out println If the in map partial aggregation does not reduce the output num records System out println by this factor it gets disabled System out println Miscellaneous System out println exectype mapreduce tez local default is mapreduce This property is the same as x switch System out println pig additional jars uris comma seperated list of jars Used in place of register command System out println udf import list Colon seperated list of imports Used to avoid package names in System out println stop on failure true false default is false Set to true to terminate on the first error System out println pig datetime default tz time offset e g Default is the default timezone of the host System out println Determines the timezone used to handle datetime datatype and Fs System out println pig artifacts download location path to download artifacts default is groovy grapes System out println Determines the location to download the artifacts when registering jars using ivy coordinates System out println Additionally any Hadoop property can be specified private static String validate Log File String log File Name File script File String stripped Down Script Name null if script File null if script File is Directory String script File Abs Path try script File Abs Path script File get Canonical Path stripped Down Script Name get File From Canonical Path script File Abs Path catch Exception ioe log warn Could not compute canonical path to the script file ioe get Message stripped Down Script Name null String default Log File Name stripped Down Script Name null pig stripped Down Script Name new Date get Time log File log File if log File Name null log File new File log File Name Check if the file name is a directory append the default file name to the file if log File is Directory if log File can Write try log File Name log File get Canonical Path File separator default Log File Name catch Exception ioe log warn Could not compute canonical path to the log file ioe get Message return null return log File Name else log warn Need write permission in the directory log File Name to create log file return null else we have a relative path or an absolute path to the log file check if we can write to the directory where this file is will be stored if log File exists if log File can Write try log File Name new File log File Name get Canonical Path catch Exception ioe log warn Could not compute canonical path to the log file ioe get Message return null return log File Name else do not have write permissions for the log file bail out with an error message log warn Can not write to file log File Name Need write permission return log File Name else log File log File get Parent File if log File null if the directory is writable we are good to go if log File can Write try log File Name new File log File Name get Canonical Path catch Exception ioe log warn Could not compute canonical path to the log file ioe get Message return null return log File Name else log warn Need write permission in the directory log File to create log file return log File Name end if log File null else is the default in fall through end else part of log File exists end else part of log File is Directory end if log File Name null file name is null or its in the current working directory revert to the current working directory String curr Dir System get Property user dir log File new File curr Dir log File Name curr Dir File separator log File Name null default Log File Name log File Name if log File can Write return log File Name log warn Can not write to log file log File Name return null private static String get File From Canonical Path String canonical Path return canonical Path substring canonical Path last Index Of File separator private static Supported Script Lang determine Script Type String file throws Exception return Script Engine get Supported Script Lang file private static int run Embedded Script Pig Context pig Context String file String engine throws Exception log info Run embedded script engine pig Context connect Script Engine script Engine Script Engine get Instance engine Map String List Pig Stats stats Map script Engine run pig Context file Pig Stats Util set Stats Map stats Map int fail Count int total Count for List Pig Stats lst stats Map values if lst null lst is Empty for Pig Stats stats lst if stats is Successful fail Count total Count return total Count fail Count total Count Return Code fail Count Return Code Return Code 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import java util List import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Source Location public class Map Lookup Expression extends Column Expression The key to lookup along with the type and schema corresponding to the type and schema of the value linked to the key private String m Map Key public Map Lookup Expression Operator Plan plan String map Key super Map plan m Map Key map Key plan add this link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Map Lookup Expression Map Lookup Expression po Map Lookup Expression other if po m Map Key compare To m Map Key return false else check the nested map equality if plan get Successors this null if other get Plan get Successors other null return false else return plan get Successors this get is Equal other get Plan get Successors other get else if other get Plan get Successors other null return false else return true else return false public Logical Expression get Map throws Frontend Exception List Operator preds plan get Successors this if preds null return null return Logical Expression preds get public String get Lookup Key return m Map Key public Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema Logical Expression successor Logical Expression plan get Successors this get Logical Field Schema pred successor get Field Schema if pred null if pred type Data Type pred schema null field Schema pred schema get Field return field Schema else field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema return null public String to String String Builder msg new String Builder msg append Name name Type if field Schema null msg append Data Type find Type Name field Schema type else msg append null msg append Uid if field Schema null msg append field Schema uid else msg append null msg append Key m Map Key msg append return msg to String Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy null copy new Map Lookup Expression lg Exp Plan this get Lookup Key Only one input is expected Logical Expression input Logical Expression plan get Successors this get Logical Expression input Copy input deep Copy lg Exp Plan lg Exp Plan add input Copy lg Exp Plan connect copy input Copy copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java io Print Stream import java text Simple Date Format import java util Array List import java util Calendar import java util Hash Map import java util Iterator import java util Linked List import java util List import java util Map import java util Properties import javax xml parsers Parser Configuration Exception import javax xml transform Transformer Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop mapred Counters import org apache hadoop mapred Job Client import org apache hadoop mapred Job Conf import org apache hadoop mapred Job import org apache hadoop mapred Running Job import org apache hadoop mapreduce Cluster import org apache hadoop mapreduce Task Report import org apache hadoop mapred jobcontrol Job import org apache hadoop mapreduce Task Type import org apache hadoop mapreduce lib jobcontrol Controlled Job import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig Pig Runner Return Code import org apache pig Pig Warning import org apache pig backend Backend Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine Job Creation Exception import org apache pig backend hadoop executionengine Launcher import org apache pig backend hadoop executionengine map Reduce Layer Compiler Last Input Streaming Optimizer import org apache pig backend hadoop executionengine map Reduce Layer plans Dot Printer import org apache pig backend hadoop executionengine map Reduce Layer plans End Of All Input Setter import org apache pig backend hadoop executionengine map Reduce Layer plans Intermediate Data Visitor import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine map Reduce Layer plans Printer import org apache pig backend hadoop executionengine map Reduce Layer plans Package Annotator import org apache pig backend hadoop executionengine map Reduce Layer plans Printer import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Join Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine shims Hadoop Shims import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl io File Localizer import org apache pig impl io File Spec import org apache pig impl plan Compilation Message Collector import org apache pig impl plan Compilation Message Collector Message Type import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Configuration Validator import org apache pig impl util Log Utils import org apache pig impl util Context import org apache pig impl util Utils import org apache pig tools pigstats Output Stats import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Pig Stats Util import org apache pig tools pigstats mapreduce Job Stats import org apache pig tools pigstats mapreduce Pig Stats Util import org apache pig tools pigstats mapreduce Script State import org python google common collect Lists Main class that launches pig for Map Reduce public class Map Reduce Launcher extends Launcher public static final String private static final Log log Log Factory get Log Map Reduce Launcher class private boolean aggregate Warning false public Map Reduce Launcher super Utils add Shutdown Hook With Priority new Hanging Job Killer Pig Impl Constants Override public void kill try if jc null jc get Running Jobs size log info Received kill signal for Job job jc get Running Jobs org apache hadoop mapreduce Job mr Job job get Job try if mr Job null mr Job kill Job catch Exception ir throw new Exception ir log info Job job get Assigned Job killed String time Stamp new Simple Date Format yyyy dd mm ss format Calendar get Instance get Time System err println time Stamp Job job get Assigned Job killed catch Exception e log warn Encounter exception on cleanup e Override public void kill Job String job Configuration conf throws Backend Exception try if conf null Job Conf job Conf new Job Conf conf Job Client jc new Job Client job Conf Job id Job for Name job Running Job job jc get Job id if job null System out println Job with id job is not active else job kill Job log info Kill id submitted catch Exception e throw new Backend Exception e Get the exception that caused a failure on the backend for a store location if any public Exception get Error File Spec spec return failure Map get spec Override public Pig Stats launch Pig Physical Plan php String grp Name Pig Context pc throws Plan Exception Visitor Exception Exception Exec Exception Job Creation Exception Exception long sleep Time aggregate Warning Boolean value Of pc get Properties get Property aggregate warning Oper Plan mrp compile php pc Configuration Validator validate Pig Properties pc get Properties Configuration conf Configuration Util to Configuration pc get Properties Execution Engine exe Execution Engine pc get Execution Engine Properties default Properties new Properties Job Conf default Job Conf exe get Local Conf Utils recompute Properties default Job Conf default Properties This is a generic Job Client for checking progress of the jobs Job Client stats Job Client new Job Client exe get Job Conf Job Control Compiler jcc new Job Control Compiler pc conf Configuration Util to Configuration default Properties Script State get add Workflow Adjacencies To Conf mrp conf start collecting statistics Pig Stats start pc get Execution Engine instantiate Pig Stats Pig Stats Util start Collection pc stats Job Client jcc mrp Find all the intermediate data stores The plan will be destroyed during compile execution so this needs to be done before Intermediate Data Visitor intermediate Visitor new Intermediate Data Visitor mrp intermediate Visitor visit List Job failed Jobs new Linked List Job List Native Map Reduce Oper failed Native new Linked List Native Map Reduce Oper List Job complete Failed Jobs In This Run new Linked List Job List Job succ Jobs new Linked List Job int total Jobs mrp size int num Jobs Compl double last Prog long script Submitted Timestamp System current Time Millis create the exception handler for the job control thread and register the handler with the job control thread Job Control Thread Exception Handler jct Exception Handler new Job Control Thread Exception Handler boolean stop on failure Boolean value Of pc get Properties get Property stop on failure false boolean stopped On Failure false jc is null only when mrp size while mrp size stopped On Failure jc jcc compile mrp grp Name if jc null List Map Reduce Oper roots new Linked List Map Reduce Oper roots add All mrp get Roots run the native mapreduce roots first then run the rest of the roots for Map Reduce Oper mro roots if mro instanceof Native Map Reduce Oper Native Map Reduce Oper nat Op Native Map Reduce Oper mro try Script State get emit Jobs Submitted Notification nat Op run Job num Jobs Compl catch Exception e mrp trim Below nat Op failed Native add nat Op String msg Error running native mapreduce operator job nat Op get Job Id e get Message String stack Trace Utils get Stack Strace Str e Log Utils write Log msg stack Trace pc get Properties get Property pig logfile log log info msg if stop on failure int err Code throw new Exec Exception msg err Code Pig Exception double prog double num Jobs Compl total Jobs notify Progress prog last Prog last Prog prog mrp remove nat Op continue Initially all jobs are in wait state List Job jobs Without Ids jc get Waiting Jobs log info jobs Without Ids size map reduce job s waiting for submission notify listeners about jobs submitted Script State get emit Jobs Submitted Notification jobs Without Ids size update Pig stats job with just compiled jobs Pig Stats Util update Job Mro Map jcc get Job Mro Map determine job tracker url String job Tracker Loc Job Conf job Conf jobs Without Ids get get Job Conf try String port job Conf get Configuration String job Tracker Add job Conf get Configuration job Tracker Loc job Tracker Add substring job Tracker Add index Of port substring port index Of catch Exception e Could not get the job tracker location most probably we are running in local mode If it is the case we do n t print out job tracker location because it is meaningless for local mode job Tracker Loc null log debug Failed to get job tracker location complete Failed Jobs In This Run clear Set the thread Context so registered classes are available final Context udf Context Context get Context Thread jc Thread new Thread jc Job Control Override public void run Context set Udf Context udf Context clone super run jc Thread set Uncaught Exception Handler jct Exception Handler jc Thread set Context Class Loader Pig Context get Class Loader mark the times that the jobs were submitted so it s reflected in job history props for Job job jc get Waiting Jobs Job Conf job Conf Copy job get Job Conf job Conf Copy set pig script submitted timestamp Long to String script Submitted Timestamp job Conf Copy set pig job submitted timestamp Long to String System current Time Millis job set Job Conf job Conf Copy All the setup done now lets launch the jobs jc Thread start try a flag whether to warn failure during the loop below so users can notice failure earlier boolean warn failure true Now wait till we are finished while jc all Finished jc Thread join sleep Time List Job jobs Assigned Id In This Run new Array List Job for Job job jobs Without Ids if job get Assigned Job null jobs Assigned Id In This Run add job log info Hadoop Job Id job get Assigned Job display the aliases being processed Map Reduce Oper mro jcc get Job Mro Map get job if mro null String alias Script State get get Alias mro log info Processing aliases alias String alias Location Script State get get Alias Location mro log info detailed locations alias Location update statistics for this job so job Id is set Pig Stats Util add Job Stats job Script State get emit Job Started Notification job get Assigned Job to String else This job is not assigned an id yet jobs Without Ids remove All jobs Assigned Id In This Run double prog num Jobs Compl calculate Progress jc total Jobs if notify Progress prog last Prog List Job runn Jobs jc get Running Jobs if runn Jobs null String Builder msg new String Builder for Object object runn Jobs Job j Job object if j null msg append j get Assigned Job append if msg length msg set Char At msg length log info Running jobs are msg last Prog prog collect job stats by frequently polling of completed jobs Pig Stats Util accumulate Stats jc if stop on failure is enabled we need to stop immediately when any job has failed stopped On Failure stop Jobs On Failure stop on failure otherwise we just display a warning message if there s any failure if stop on failure warn failure jc get Failed Jobs is Empty we do n t warn again for this group of jobs warn failure false log warn Ooops Some job has failed Specify stop on failure if you want Pig to stop immediately on failure check for the job Control Exception first if the job controller fails before launching the jobs then there are no jobs to check for failure if job Control Exception null if job Control Exception instanceof Pig Exception if job Control Exception Stack Trace null Log Utils write Log Error message from job controller job Control Exception Stack Trace pc get Properties get Property pig logfile log throw job Control Exception else int err Code String msg Unexpected error when launching map reduce job throw new Exec Exception msg err Code Pig Exception job Control Exception if jc get Failed Jobs is Empty stop if stop on failure is enabled stopped On Failure stop Jobs On Failure stop on failure if stopped On Failure If we only have one store and that job fail then we sure that the job completely fail and we shall stop dependent jobs for Job job jc get Failed Jobs complete Failed Jobs In This Run add job log info job job get Assigned Job has failed Stop running all dependent jobs failed Jobs add All jc get Failed Jobs int removed Op jcc update Op Plan complete Failed Jobs In This Run num Jobs Compl removed Op List Job jobs jc get Successful Jobs jcc move Results jobs succ Jobs add All jobs collecting final statistics Pig Stats Util accumulate Stats jc catch Exception e throw e finally jc stop Script State get emit Progress Updated Notification log info complete boolean failed false if failed Native size failed true if Boolean value Of pc get Properties get Property Pig Configuration true Clean up all the intermediate data for String path intermediate Visitor get Intermediate Skip non file system paths such as hbase see if Hadoop Shims has File System Impl new Path path conf File Localizer delete path pc Look to see if any jobs failed If so we need to report that if failed Jobs null failed Jobs size Exception backend Exception null for Job fj failed Jobs try get Stats fj true pc catch Exception e backend Exception e List Store sts jcc get Stores fj for Store st sts failure Map put st get File backend Exception Pig Stats Util set Backend Exception fj backend Exception failed true stats collection is done log the results Pig Stats Util stop Collection true Pig Stats Util stop Collection also computes the return code based on total jobs to run jobs successful and jobs failed failed failed Pig Stats get is Successful Map Enum Long warning Agg Map new Hash Map Enum Long if succ Jobs null for Job job succ Jobs List Store sts jcc get Stores job for Store st sts if st is Tmp Store create an file in output location if output location is a filesystem dir create Success File job st else log debug Successfully stored result in st get File get File Name get Stats job false pc if aggregate Warning compute Warning Aggregate job warning Agg Map if aggregate Warning Compilation Message Collector log Aggregate warning Agg Map Message Type Warning log if failed log info Success else if succ Jobs null succ Jobs size log info Some jobs have failed Stop running all dependent jobs else log info Failed jcc reset int ret failed succ Jobs null succ Jobs size Return Code Return Code Return Code Pig Stats pig Stats Pig Stats Util get Pig Stats ret run cleanup for all of the stores for Output Stats output pig Stats get Output Stats Store store output get Store try if output is Successful store get Store Func cleanup On Failure store get File get File Name new org apache hadoop mapreduce Job output get Conf else store get Store Func cleanup On Success store get File get File Name new org apache hadoop mapreduce Job output get Conf catch Exception e throw new Exec Exception e catch Abstract Method Error nsme Just swallow it This means we re running against an older instance of a Store Func that does n t implement this method if stopped On Failure throw new Exec Exception Stopping execution on job failure with stop on failure option Pig Exception return pig Stats If stop on failure is enabled and any job has failed it stops other jobs param stop on failure whether it s enabled return true if there were failed jobs and stop on failure is enabled private boolean stop Jobs On Failure boolean stop on failure throws Exception Interrupted Exception if jc get Failed Jobs is Empty return false if stop on failure List Controlled Job ready Jobs List jc get Ready Jobs List List Controlled Job running Job List jc get Running Job List if ready Jobs List size running Job List size log info Some job s failed Failing other ready and running jobs as stop on failure is on for Controlled Job job ready Jobs List job fail Job Failing ready job for stop on failure job get Mapred Job Id for Controlled Job job running Job List job fail Job Failing running job for stop on failure job get Mapred Job Id return true return false Log the progress and notify listeners if there is sufficient progress param prog current progress param last Prog progress last time private boolean notify Progress double prog double last Prog if prog last Prog int per Com int prog if per Com log info per Com complete Script State get emit Progress Updated Notification per Com return true return false Override public void explain Physical Plan php Pig Context pc Print Stream ps String format boolean verbose throws Plan Exception Visitor Exception Exception log trace Entering Map Reduce Launcher explain Oper Plan mrp compile php pc if format equals text Printer printer new Printer ps mrp printer set Verbose verbose printer visit else if format equals xml try Printer printer new Printer ps mrp printer visit printer close Plan catch Parser Configuration Exception e e print Stack Trace catch Transformer Exception e e print Stack Trace else ps println ps println Map Reduce Plan ps println Dot Printer printer new Dot Printer mrp ps printer set Verbose verbose printer dump ps println public Oper Plan compile Physical Plan php Pig Context pc throws Plan Exception Exception Visitor Exception Compiler comp new Compiler php pc comp compile comp aggregate Scalars Files comp connect Soft Link Oper Plan plan comp get Plan display the warning message s from the Compiler comp get Message Collector log Messages Message Type Warning aggregate Warning log String last Input Chunk Size pc get Properties get Property last input chunksize Join Packager String prop pc get Properties get Property Pig Configuration if pc in Illustrator true equals prop boolean do Map Agg Boolean value Of pc get Properties get Property Pig Configuration false Combiner Optimizer co new Combiner Optimizer plan do Map Agg co visit display the warning message s from the Combiner Optimizer co get Message Collector log Messages Message Type Warning aggregate Warning log Optimize the jobs that have a load store only first job followed by a sample job Sample Optimizer so new Sample Optimizer plan pc so visit We must ensure that there is only reducer for a limit Add a single reducer job if pc in Illustrator Limit Adjuster la new Limit Adjuster plan pc la visit la adjust Optimize to use secondary sort key if possible prop pc get Properties get Property Pig Configuration if pc in Illustrator true equals prop Secondary Key Optimizer sk Optimizer new Secondary Key Optimizer plan sk Optimizer visit optimize key value handling in package Package Annotator pkg Annotator new Package Annotator plan pkg Annotator visit optimize joins Last Input Streaming Optimizer liso new Compiler Last Input Streaming Optimizer plan last Input Chunk Size liso visit figure out the type of the key for the map plan this is needed when the key is null to create an appropriate Nullable Writable object Key Type Discovery Visitor kdv new Key Type Discovery Visitor plan kdv visit removes the filter constant true operators introduced by splits Noop Filter Remover f Rem new Noop Filter Remover plan f Rem visit boolean is Multi Query Boolean value Of pc get Properties get Property Pig Configuration true if is Multi Query reduces the number of Opers in the plan generated by multi query multi store script Multi Query Optimizer mq Optimizer new Multi Query Optimizer plan pc in Illustrator mq Optimizer visit removes unnecessary stores as can happen with splits in some cases This has to run after the Multi Query and Noop Filter Remover Noop Store Remover s Rem new Noop Store Remover plan s Rem visit check whether stream operator is present after Multi Query Optimizer because it can shift streams from map to reduce etc End Of All Input Setter checker new End Of All Input Setter plan checker visit boolean is Accum Boolean value Of pc get Properties get Property opt accumulator true if is Accum Accumulator Optimizer accum new Accumulator Optimizer plan accum visit return plan private boolean should Mark Output Dir Job job return job get Job Conf get Boolean Configuration false private void create Success File Job job Store store throws Exception if should Mark Output Dir job Path output Path new Path store get File get File Name String scheme output Path to Uri get Scheme if Hadoop Shims has File System Impl output Path job get Job Conf File System fs output Path get File System job get Job Conf if fs exists output Path create a file in the folder to mark it Path file Path new Path output Path if fs exists file Path fs create file Path close else log warn No File System for scheme scheme Not creating success file Suppress Warnings deprecation void compute Warning Aggregate Job job Map Enum Long agg Map try Counters counters Job Stats get Counters job if counters null long null Counter Count agg Map get Pig Warning null agg Map get Pig Warning null Counter Count agg Map put Pig Warning null Counter Count for Enum e Pig Warning values if e Pig Warning Long current Count agg Map get e current Count current Count null current Count This code checks if the counters is null if it is we need to report to the user that the number of warning aggregations may not be correct In fact Counters should not be null it is a hadoop bug once this bug is fixed in hadoop the null handling code should never be hit See Pig if counters null current Count counters get Counter e agg Map put e current Count catch Exception e String msg Unable to retrieve job to compute warning aggregation log warn msg private void get Stats Job job boolean err Not Dbg Pig Context pig Context throws Exec Exception Job Job job get Assigned Job String job Message job get Message Exception backend Exception null if Job null try Log Utils write Log Backend error message during job submission job Message pig Context get Properties get Property pig logfile log backend Exception get Exception From String job Message catch Exception e int err Code String msg Unable to recreate exception from backend error job Message throw new Exec Exception msg err Code Pig Exception throw new Exec Exception backend Exception try Iterator Task Report map Rep Job Stats get Task Reports job Task Type if map Rep null get Error Messages map Rep map err Not Dbg pig Context total Hadoop Time Spent compute Time Spent map Rep map Rep null Iterator Task Report red Rep Job Stats get Task Reports job Task Type if red Rep null get Error Messages red Rep reduce err Not Dbg pig Context total Hadoop Time Spent compute Time Spent red Rep red Rep null catch Exception e if job get State Job if the job succeeded let the user know that we were unable to get statistics log warn Unable to get job related diagnostics else throw new Exec Exception e catch Exception e throw new Exec Exception e 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Byte Array Output Stream import java util Array List import java util Hash Set import java util Iterator import java util List import java util Set import org apache pig backend hadoop executionengine map Reduce Layer plans Op Plan Visitor import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Counter import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map An operator model for a Map Reduce job Acts as a host to the plans that will execute in map reduce and optionally combine phases These will be embedded in the Oper Plan in order to capture the dependencies amongst jobs public class Map Reduce Oper extends Operator Op Plan Visitor private static final long serial Version The physical plan that should be executed in the map phase public Physical Plan map Plan The physical plan that should be executed in the reduce phase public Physical Plan reduce Plan The physical plan that should be executed in the combine phase if one exists Will be used by the optimizer public Physical Plan combine Plan key for the map plan this is needed when the key is null to create an appropriate Nullable Writable object public byte map Key Type record the map key types of all splittees public byte map Key Type Of Splittees Indicates that the map plan creation is complete boolean map Done false Indicates that the reduce plan creation is complete boolean reduce Done false Indicates that there is an operator which uses end Of All Input flag in the map plan boolean end Of All Input In Map false Indicates that there is an operator which uses end Of All Input flag in the reduce plan boolean end Of All Input In Reduce false Indicates if this job is an order by job boolean global Sort false Indicates if this is a limit after a sort boolean limit After Sort false Indicate if the entire purpose for this map reduce job is doing limit does not change anything else This is to help Package Annotator to find the right Package to annotate boolean limit Only false feature If true putting an identity combine in this mapreduce job will speed things up boolean needs Distinct Combiner false If true we will use secondary key in the map reduce job boolean use Secondary Key false The quantiles file name if global Sort is true String quant File The sort order of the columns asc is true and desc is false boolean sort Order Sort order for secondary keys boolean secondary Sort Order public Set String Fs public Set Physical Operator scalars Indicates if a comparator is used boolean is Comparator Used false transient Node Id Generator nig private String scope int requested Parallelism estimated at runtime int estimated Parallelism calculated at runtime int runtime Parallelism Name of the Custom Partitioner used String custom Partitioner null Last Limit value in this map reduce operator needed by Limit Adjuster to add additional map reduce operator with reducer after this long limit Limit can also have an expression See Physical Plan limit Plan null Indicates that this Oper is a splitter Oper That is this Oper ends due to a Plit operator private boolean splitter false Set to true if it is skewed join private boolean skewed Join false Name of the partition file generated by sampling process Used by Skewed Join private String skewed Join Partition File Flag to communicate from Compiler to Job Control Compiler what kind of comparator is used by Hadoop for sorting for this Oper By default set to false which will make Pig provide raw comparators Set to true in indexing job generated in map side cogroup merge join private boolean using Typed Comparator false Flag to indicate if the small input splits need to be combined to form a larger one in order to reduce the number of mappers For merge join both tables are combinable for correctness private boolean combine Small Splits true Map of the physical operator in physical plan to the one in plan only needed if the physical operator is changed replaced in compilation due to e g optimization public Multi Map Physical Operator Physical Operator phy To Map private static enum Indicate if this job is a sampling job Indicate if this job is a merge indexer Indicate if this job is a group by job Indicate if this job is a cogroup job Indicate if this job is a regular join job public Map Reduce Oper Operator Key k super k map Plan new Physical Plan combine Plan new Physical Plan reduce Plan new Physical Plan Fs new Hash Set String scalars new Hash Set Physical Operator nig Node Id Generator get Generator scope k get Scope phy To Map new Multi Map Physical Operator Physical Operator Override public String name return Map Reduce m Key to String private String shift String By Tabs String Str String tab String Builder sb new String Builder String spl Str split n for int i i spl length i sb append tab sb append spl i sb append n sb delete sb length n length sb length return sb to String Uses the string representation of the component plans to identify itself Override public String name String udf Str get Fs As Str String Builder sb new String Builder Map Reduce requested Parallelism udf Str equals udf Str m Key to String n int index sb length Byte Array Output Stream baos new Byte Array Output Stream if map Plan is Empty map Plan explain baos String mp new String baos to Byte Array sb append shift String By Tabs mp else sb append Map Plan Empty if reduce Plan is Empty baos reset reduce Plan explain baos String rp new String baos to Byte Array sb insert index shift String By Tabs rp n else sb insert index Reduce Plan Empty n return sb to String private String get Fs As Str String Builder sb new String Builder if Fs null Fs size for String str Fs sb append str substring str last Index Of sb append sb delete Char At sb length return sb to String Override public boolean supports Multiple Inputs return true Override public boolean supports Multiple Outputs return true Override public void visit Op Plan Visitor v throws Visitor Exception v visit Op this public boolean is Map Done return map Done public void set Map Done boolean map Done this map Done map Done public void set Map Done Single boolean map Done throws Plan Exception this map Done map Done if map Done map Plan get Leaves size map Plan add As Leaf get Union public void set Map Done Multiple boolean map Done throws Plan Exception this map Done map Done if map Done map Plan get Leaves size map Plan add As Leaf get Union private Union get Union return new Union new Operator Key scope nig get Next Node Id scope public boolean is Reduce Done return reduce Done public void set Reduce Done boolean reduce Done this reduce Done reduce Done public boolean is Global Sort return global Sort public boolean is Skewed Join return skewed Join Partition File null public void set Skewed Join Partition File String file skewed Join Partition File file public String get Skewed Join Partition File return skewed Join Partition File public void set Skewed Join boolean sk Join this skewed Join sk Join public boolean get Skewed Join return skewed Join public void set Global Sort boolean global Sort this global Sort global Sort public boolean is Limit After Sort return limit After Sort public void set Limit After Sort boolean las limit After Sort las public boolean is Limit Only return limit Only public void set Limit Only boolean limit Only this limit Only limit Only public boolean is Indexer return feature public void mark Indexer feature public boolean is Sampler return feature public void mark Sampler feature public boolean is Group By return feature public void mark Group By feature public boolean is Cogroup return feature public void mark Cogroup feature public boolean is Regular Join return feature public void mark Regular Join feature public boolean needs Distinct Combiner return needs Distinct Combiner public void set Needs Distinct Combiner boolean nic needs Distinct Combiner nic public String get Quant File return quant File public void set Quant File String quant File this quant File quant File public void set Sort Order boolean sort Order if null sort Order return this sort Order new boolean sort Order length for int i i sort Order length i this sort Order i sort Order i public void set Secondary Sort Order boolean secondary Sort Order if null secondary Sort Order return this secondary Sort Order new boolean secondary Sort Order length for int i i secondary Sort Order length i this secondary Sort Order i secondary Sort Order i public boolean get Sort Order return sort Order public boolean get Secondary Sort Order return secondary Sort Order return whether end of all input is set in the map plan public boolean is End Of All Input Set In Map return end Of All Input In Map param end Of All Input In Map the stream In Map to set public void set End Of All Input In Map boolean end Of All Input In Map this end Of All Input In Map end Of All Input In Map return whether end of all input is set in the reduce plan public boolean is End Of All Input Set In Reduce return end Of All Input In Reduce param end Of All Input In Reduce the stream In Reduce to set public void set End Of All Input In Reduce boolean end Of All Input In Reduce this end Of All Input In Reduce end Of All Input In Reduce public int get Requested Parallelism return requested Parallelism public String get Custom Partitioner return custom Partitioner public void set Splitter boolean spl splitter spl public boolean is Splitter return splitter public boolean get Use Secondary Key return use Secondary Key public void set Use Secondary Key boolean use Secondary Key this use Secondary Key use Secondary Key protected boolean using Typed Comparator return using Typed Comparator protected void use Typed Comparator boolean use Typed Comparator this using Typed Comparator use Typed Comparator protected void no Combine Small Splits combine Small Splits false public boolean combine Small Splits return combine Small Splits public boolean is Rank Operation return get Rank Operation Id size public Array List String get Rank Operation Id Array List String operation Ds new Array List String Iterator Physical Operator map Roots this map Plan get Roots iterator while map Roots has Next Physical Operator operation map Roots next if operation instanceof Rank operation Ds add Rank operation get Operation return operation Ds public boolean is Counter Operation return get Counter Operation null public boolean is Row Number Counter counter get Counter Operation return counter null counter is Row Number false public String get Operation Counter counter get Counter Operation return counter null counter get Operation null private Counter get Counter Operation Counter counter get Counter Operation this map Plan if counter null counter get Counter Operation this reduce Plan return counter private Counter get Counter Operation Physical Plan plan Physical Operator operator Iterator Physical Operator it plan get Leaves iterator while it has Next operator it next if operator instanceof Counter return Counter operator else if operator instanceof Store List Physical Operator preds plan get Predecessors operator if preds null for Physical Operator pred preds if pred instanceof Counter return Counter pred return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import org apache hadoop conf Configuration import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce Task Attempt Context import org apache hadoop mapreduce Task Input Output Context import org apache pig Store Func Interface import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Store Impl import org apache pig backend hadoop executionengine shims Hadoop Shims import org apache pig tools pigstats Pig Stats Util import org apache pig tools pigstats Pig Status Reporter This class is used to have a Store write to via a output collector record writer It sets up a modified job configuration to force a write to a specific subdirectory of the main output directory This is done so that multiple output directories can be used in the same job public class Map Reduce Store Impl extends Store Impl private Task Attempt Context context private Pig Status Reporter reporter private Record Writer writer public Map Reduce Store Impl Task Input Output Context context get a copy of the Configuration so that changes to the configuration below like setting the output location do not affect the caller s copy Configuration output Conf new Configuration context get Configuration reporter Pig Status Reporter get Instance reporter set Context new Task Context context make a copy of the Context to use here since in the same task map or reduce we could have multiple stores we should make this copy so that the same context does not get over written by the different stores this context Hadoop Shims create Task Attempt Context output Conf context get Task Attempt Override public Store Func Interface create Store Func Store store throws Exception Store Func Interface store Func store get Store Func call the set Store Location on the store Func giving it the Job Typically this will result in the Output Format of the store Func storing the output location in the Configuration in the Job The Pig Out Format set Location method will merge this modified Configuration into the configuration of the Context we have Pig Output Format set Location context store Output Format output Format store Func get Output Format create a new record writer try writer output Format get Record Writer context catch Interrupted Exception e throw new Exception e store Func prepare To Write writer return store Func Override public void tear Down throws Exception if writer null try writer close context catch Interrupted Exception e throw new Exception e writer null Override public void clean Up throws Exception if writer null try writer close context catch Interrupted Exception e throw new Exception e writer null public void incr Record Counter String name long incr reporter incr Counter Pig Stats Util name incr 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical relational import java util List import org apache pig Sort Col Info import org apache pig Sort Col Info Order import org apache pig backend hadoop executionengine physical Layer Logical To Physical Translator Exception import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical visitor Finder public class Map Side Merge Validator public boolean validate Map Side Merge List Operator preds Operator Plan lp throws Logical To Physical Translator Exception int err Code String err Msg Merge join Cogroup only supports Filter Foreach Ascending Sort or Load as its predecessors Found if preds null for Operator lo preds if lo instanceof Filter lo instanceof Generate lo instanceof Inner Load lo instanceof Load lo instanceof Split Output lo instanceof Split lo instanceof Join Join lo get Join Type Join is Acceptable Sort Op lo is Acceptable For Each Op lo throw new Logical To Physical Translator Exception err Msg err Code Repeat until Sort or top of the tree is reached if lo instanceof Sort validate Map Side Merge lp get Predecessors lo lp We visited everything and all is good return true private boolean is Acceptable For Each Op Operator lo throws Logical To Physical Translator Exception if lo instanceof For Each Operator Plan inner Plan For Each lo get Inner Plan return validate Map Side Merge inner Plan get Sinks inner Plan else return false private boolean is Acceptable Sort Op Operator op throws Logical To Physical Translator Exception if op instanceof Sort return false Sort sort Sort op try for Sort Col Info col Info sort get Sort Info get Sort Col Info List really we should check that the sort is on the join keys in the same order if col Info get Sort Order Order return false catch Frontend Exception e throw new Logical To Physical Translator Exception e return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java util Array List import java util List import org apache pig Func Spec import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the maximum of a set of values This class implements link org apache pig Algebraic so if possible the execution will performed in a distributed fashion p can operate on any numeric type and on chararrays It can also operate on bytearrays which it will cast to doubles It expects a bag of tuples of one record each If Pig knows from the schema that this function will be passed a bag of integers or longs it will use a specially adapted version of that uses integer arithmetic for comparing the data The return type of will match the input type p implements the link org apache pig Accumulator interface as well While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation public class extends Algebraic Byte Array Math Base public set Op public static class Intermediate extends Algebraic Byte Array Math Base Intermediate Override public get Op return public static class Final extends Algebraic Byte Array Math Base Final Override public get Op return non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec func List add new Func Spec this get Class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Double Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Float Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Int Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Long Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec String Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Date Time Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Big Decimal Max class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Big Integer Max class get Name Schema generate Nested Schema Data Type Data Type return func List 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java util Array List import java util Collection import java util Hash Set import java util Iterator import java util List import java util Set import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Operator Sub Plan import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan optimizer Rule import org apache pig newplan optimizer Transformer import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig impl util Utils public class Merge For Each extends Rule private Operator Sub Plan sub Plan public Merge For Each String name super name false Override protected Operator Plan build Pattern match each foreach Logical Plan plan new Logical Plan Logical Relational Operator foreach new For Each plan plan add foreach return plan Override public Transformer get New Transformer return new Merge For Each Transformer public class Merge For Each Transformer extends Transformer Override public boolean check Operator Plan matched throws Frontend Exception For Each foreach For Each matched get Sources get List Operator succs current Plan get Successors foreach if succs null succs size succs get instanceof For Each return false For Each foreach For Each succs get Check if the second foreach has only Generate and Inner Load Iterator Operator it foreach get Inner Plan get Operators while it has Next Operator op it next if op instanceof Generate op instanceof Inner Load return false Check if the first foreach has flatten in its generate statement Generate gen Generate foreach get Inner Plan get Sinks get for boolean flatten gen get Flatten Flags if flatten return false if gen get User Defined Schema null for Logical Schema s gen get User Defined Schema if s null return false Check if non of the st foreach output is referred more than once in second foreach Otherwise we may do expression calculation more than once defeat the benefit of this optimization Set Integer inputs new Hash Set Integer boolean duplicate Inputs false for Operator op foreach get Inner Plan get Sources If the source is not Inner Load then it must be Generate This happens when the st For Each does not rely on any input of nd For Each if op instanceof Inner Load Inner Load inner Load Inner Load op int input inner Load get Projection get Col Num if inputs contains input duplicate Inputs true break else inputs add input if inner Load get Projection is Range Or Star Project return false Duplicate inputs in the case first foreach only containing Inner Load and Generate is allowed and output plan is simple projection if duplicate Inputs Iterator Operator it foreach get Inner Plan get Operators while it has Next Operator op it next if op instanceof Generate op instanceof Inner Load return false if op instanceof Generate List Logical Expression Plan output Plans Generate op get Output Plans for Logical Expression Plan output Plan output Plans Iterator Operator iter output Plan get Operators while iter has Next if iter next instanceof Project Expression return false return true Override public Operator Plan report Changes return sub Plan If op is Inner Load get a copy of it otherwise return op itself private Operator get Operator To Merge Operator op Operator Plan new Plan For Each new For Each Operator op To Merge op if op instanceof Inner Load op To Merge new Inner Load new Plan new For Each Inner Load op get Col Num else op To Merge set Plan new Plan return op To Merge private Operator add Branch To Plan Generate gen int branch Operator Plan new Plan For Each new For Each Operator op Next To Gen Operator op gen get Plan get Predecessors gen get branch Operator op To Merge get Operator To Merge op new Plan new For Each new Plan add op To Merge op Next To Gen op To Merge Operator pred if gen get Plan get Predecessors op null pred gen get Plan get Predecessors op get else pred null while pred null Operator pred To Merge get Operator To Merge pred new Plan new For Each new Plan add pred To Merge new Plan connect pred To Merge op op pred if gen get Plan get Predecessors pred null pred gen get Plan get Predecessors pred get else pred null return op Next To Gen Override public void transform Operator Plan matched throws Frontend Exception sub Plan new Operator Sub Plan current Plan For Each foreach For Each matched get Sources get Generate gen Generate foreach get Inner Plan get Sinks get For Each foreach For Each current Plan get Successors foreach get Generate gen Generate foreach get Inner Plan get Sinks get For Each new For Each new For Each current Plan Logical Plan new For Each Inner Plan new Logical Plan new For Each set Inner Plan new For Each Inner Plan new For Each set Alias foreach get Alias new For Each set Requested Parallelism foreach get Requested Parallelism List Logical Expression Plan new Exp List new Array List Logical Expression Plan Generate new Gen new Generate new For Each Inner Plan new Exp List gen get Flatten Flags new Gen set User Defined Schema gen get User Defined Schema new For Each Inner Plan add new Gen for Logical Expression Plan exp gen get Output Plans Logical Expression Plan new Exp Plan new Logical Expression Plan Logical Expression Plan exp Copy exp deep Copy new Exp Plan merge exp Copy Add expression plan in nd For Each List Operator exp Sinks new Array List Operator exp Sinks add All new Exp Plan get Sinks for Operator exp Sink exp Sinks if exp Sink instanceof Project Expression Find referred expression plan in st For Each Project Expression proj Project Expression exp Sink Inner Load inner Load Inner Load foreach get Inner Plan get Predecessors gen get proj get Input Num int exp Pos inner Load get Projection get Col Num Logical Expression Plan exp gen get Output Plans get exp Pos Logical Expression Plan exp Copy exp deep Copy List Operator exp Sources new Exp Plan merge exp Copy Copy expression plan to the new For Each connect to the expression plan of nd For Each Operator exp Source exp Sources get if new Exp Plan get Predecessors exp Sink null Operator exp Next To Sink new Exp Plan get Predecessors exp Sink get Pair Integer Integer pos new Exp Plan disconnect exp Next To Sink exp Sink new Exp Plan remove exp Sink new Exp Plan connect exp Next To Sink pos first exp Source else new Exp Plan remove exp Sink Copy referred For Each inner plan to new For Each List Operator exp Sinks new Exp Plan get Sinks for Operator exp Sink exp Sinks if exp Sink instanceof Project Expression Operator op Next To Gen add Branch To Plan gen Project Expression exp Sink get Input Num new For Each Inner Plan new For Each new For Each Inner Plan connect op Next To Gen new Gen int input new For Each Inner Plan get Predecessors new Gen index Of op Next To Gen Project Expression exp Sink set Input Num input new Exp List add new Exp Plan Adjust attached Op for Logical Expression Plan p new Gen get Output Plans Iterator Operator iter p get Operators while iter has Next Operator op iter next if op instanceof Project Expression Project Expression op set Attached Relational Op new Gen Iterator Operator iter new For Each get Inner Plan get Operators while iter has Next Operator op iter next if op instanceof Inner Load Inner Load op get Projection set Attached Relational Op new For Each remove foreach foreach add new foreach rebuild soft link Collection Operator new Soft Link Preds Utils merge Collection current Plan get Soft Link Predecessors foreach current Plan get Soft Link Predecessors foreach Collection Operator foreach Soft Link Pred null if current Plan get Soft Link Predecessors foreach null foreach Soft Link Pred new Array List Operator foreach Soft Link Pred add All current Plan get Soft Link Predecessors foreach if foreach Soft Link Pred null for Operator soft Pred foreach Soft Link Pred current Plan remove Soft Link soft Pred foreach Collection Operator foreach Soft Link Pred null if current Plan get Soft Link Predecessors foreach null foreach Soft Link Pred new Array List Operator foreach Soft Link Pred add All current Plan get Soft Link Predecessors foreach if foreach Soft Link Pred null for Operator soft Pred foreach Soft Link Pred current Plan remove Soft Link soft Pred foreach current Plan remove And Reconnect foreach current Plan replace foreach new For Each if new Soft Link Preds null for Operator soft Pred new Soft Link Preds current Plan create Soft Link soft Pred new For Each sub Plan add new For Each 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java util List import org apache hadoop io Writable Comparable import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Record Reader import org apache pig Load Caster import org apache pig Load Func import org apache pig Ordered Load Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl util Object Serializer Merge Join indexer is used to generate on the fly index for doing Merge Join efficiently It samples first record from every block of right side input and returns tuple in the following format key key position split Index These tuples are then sorted before being written out to index file on public class Merge Join Indexer extends Load Func private boolean first Rec true private transient Tuple Factory m Tuple Factory private Local Rearrange lr private Physical Plan preceding Phy Plan private int keys Cnt private Physical Operator right Pipeline Leaf private Physical Operator right Pipeline Root private Load Func loader private Pig Split pig Split null private boolean ignore Null Keys param func Spec Loader specification param inner Plan This is serialized version of plan We want to keep only keys in our index file and not the whole tuple So we need and thus its plan to get keys out of the sampled tuple param serialized Phy Plan Serialized physical plan on right side throws Exec Exception Suppress Warnings unchecked public Merge Join Indexer String func Spec String inner Plan String serialized Phy Plan String udf Cntxt Signature String scope String ignore Nulls throws Exec Exception loader Load Func Pig Context instantiate Func From Spec func Spec loader set Context Signature udf Cntxt Signature this ignore Null Keys Boolean parse Boolean ignore Nulls try List Physical Plan inner Plans List Physical Plan Object Serializer deserialize inner Plan lr new Local Rearrange new Operator Key scope Node Id Generator get Generator get Next Node Id scope lr set Plans inner Plans keys Cnt inner Plans size preceding Phy Plan Physical Plan Object Serializer deserialize serialized Phy Plan if preceding Phy Plan null if preceding Phy Plan get Leaves size preceding Phy Plan get Roots size int err Code String err Msg Expected physical plan with exactly one root and one leaf throw new Exec Exception err Msg err Code Pig Exception this right Pipeline Leaf preceding Phy Plan get Leaves get this right Pipeline Root preceding Phy Plan get Roots get this right Pipeline Root set Inputs null catch Exception e int err Code String msg Unable to deserialize plans in Indexer throw new Exec Exception msg err Code e m Tuple Factory Tuple Factory get Instance Override public Tuple get Next throws Exception if first Rec We sample only one record per block return null Writable Comparable position Ordered Load Func loader get Split Comparable pig Split get Wrapped Split Object key null Tuple wrapper Tuple m Tuple Factory new Tuple keys Cnt while true Tuple read Tuple loader get Next if null read Tuple We hit the end for int i i keys Cnt i wrapper Tuple set i null wrapper Tuple set keys Cnt position first Rec false return wrapper Tuple if null preceding Phy Plan lr attach Input read Tuple key Tuple lr get Next Tuple result get lr detach Input if null key ignore Null Keys Tuple with null key Drop it continue break There is a physical plan right Pipeline Root attach Input read Tuple boolean fetch New Tup while true Result res right Pipeline Leaf get Next Tuple switch res return Status case Status lr attach Input Tuple res result key Tuple lr get Next Tuple result get lr detach Input if null key ignore Null Keys Tuple with null key Drop it continue fetch New Tup false break case Status fetch New Tup true break default int err Code String err Msg Expected as return status Found res return Status throw new Exec Exception err Msg err Code break if fetch New Tup break if key instanceof Tuple Tuple tup Key Tuple key for int i i tup Key size i wrapper Tuple set i tup Key get i else wrapper Tuple set key wrapper Tuple set keys Cnt position wrapper Tuple set keys Cnt pig Split get Split Index first Rec false return wrapper Tuple non Javadoc see org apache pig Load Func get Input Format Override public Input Format get Input Format throws Exception return loader get Input Format non Javadoc see org apache pig Load Func get Load Caster Override public Load Caster get Load Caster throws Exception return loader get Load Caster non Javadoc see org apache pig Load Func prepare To Read org apache hadoop mapreduce Record Reader org apache hadoop mapreduce Input Split Override public void prepare To Read Record Reader reader Pig Split split throws Exception loader prepare To Read reader split pig Split split non Javadoc see org apache pig Load Func set Location java lang String org apache hadoop mapreduce Job Override public void set Location String location Job job throws Exception loader set Location location job 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java util Array List import java util List import org apache pig Func Spec import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the minimum of a set of values This class implements link org apache pig Algebraic so if possible the execution will performed in a distributed fashion p can operate on any numeric type and on chararrays It can also operate on bytearrays which it will cast to doubles It expects a bag of tuples of one record each If Pig knows from the schema that this function will be passed a bag of integers or longs it will use a specially adapted version of that uses integer arithmetic for comparing the data The return type of will match the input type p implements the link org apache pig Accumulator interface as well While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation public class extends Algebraic Byte Array Math Base public set Op public static class Intermediate extends Algebraic Byte Array Math Base Intermediate Override public get Op return public static class Final extends Algebraic Byte Array Math Base Final Override public get Op return non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec func List add new Func Spec this get Class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Double Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Float Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Int Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Long Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec String Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Date Time Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Big Decimal Min class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Big Integer Min class get Name Schema generate Nested Schema Data Type Data Type return func List 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java math Big Integer import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Mod extends Binary Expression Operator private static final long serial Version public Mod Operator Key k super k public Mod Operator Key k int rp super k rp Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Mod this Override public String name return Mod Data Type find Type Name result Type m Key to String protected Number mod Number a Number b byte data Type throws Exec Exception switch data Type case Data Type return Integer value Of Integer a Integer b case Data Type return Long value Of Long a Long b case Data Type return Big Integer a mod Big Integer b default throw new Exec Exception called on unsupported Number class Data Type find Type Name data Type protected Result generic Get Next byte data Type throws Exec Exception Result r accum Child null data Type if r null return r byte status Result res res lhs get Next data Type status res return Status if status Status res result null return res Number left Number res result res rhs get Next data Type status res return Status if status Status res result null return res Number right Number res result res result mod left right data Type return res Override public Result get Next Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Long throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Integer throws Exec Exception return generic Get Next Data Type Override public Mod clone throws Clone Not Supported Exception Mod clone new Mod new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Mod Operator public class Mod Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Mod Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Mod plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Mod Expression Mod Expression ao Mod Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null get Lhs get Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Mod Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java io Serializable import java util Array List import java util Collections import java util Hash Map import java util Hash Set import java util Iterator import java util List import java util Map import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache pig Collectable Load Func import org apache pig Exec Type import org apache pig Func Spec import org apache pig Indexable Load Func import org apache pig Load Func import org apache pig Ordered Load Func import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig Pig Warning import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine map Reduce Layer plans Op Plan Visitor import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine map Reduce Layer plans Scalar Phy Finder import org apache pig backend hadoop executionengine map Reduce Layer plans Finder import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Join Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Lite Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Counter import org apache pig backend hadoop executionengine physical Layer relational Operators Cross import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Native import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Partition Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig backend hadoop executionengine physical Layer relational Operators Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Packager Package Type import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig backend hadoop executionengine shims Hadoop Shims import org apache pig backend hadoop executionengine util Map Red Util import org apache pig data Data Type import org apache pig impl Pig Context import org apache pig impl builtin Default Indexable Loader import org apache pig impl builtin Find Quantiles import org apache pig impl builtin Get Mem Num Rows import org apache pig impl builtin Is First Reduce Of Key import org apache pig impl builtin Partition Skewed Keys import org apache pig impl builtin Poisson Sample Loader import org apache pig impl builtin Random Sample Loader import org apache pig impl io File Localizer import org apache pig impl io File Spec import org apache pig impl plan Compilation Message Collector import org apache pig impl plan Compilation Message Collector Message Type import org apache pig impl plan Depth First Walker import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator import org apache pig impl plan Operator Key import org apache pig impl plan Operator Plan import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Compiler Utils import org apache pig impl util Multi Map import org apache pig impl util Object Serializer import org apache pig impl util Pair import org apache pig impl util Uri Util import org apache pig impl util Utils import org apache pig newplan logical relational Join The compiler that compiles a given physical plan into a of Map Reduce operators which can then be converted into the Job Control structure Is implemented as a visitor of the Physical Plan it is compiling Currently supports all operators except the Sort operator Uses a predecessor based depth first traversal To compile an operator first compiles the predecessors into Map Reduce Operators and tries to merge the current operator into one of them The goal being to keep the number of Opers to a minimum It also merges multiple Map jobs created by compiling the inputs individually into a single job Here a new map job is created and then the contents of the previous map plans are added However any other state that was in the previous map plans should be manually moved over So if you are adding something new take care about this Ex of this is in requested Parallelism Only in case of blocking operators and splits a new Map Reduce operator is started using a store load combination to connect the two operators Whenever this happens care is taken to add the Oper into the Plan and connect it appropriately public class Compiler extends Phy Plan Visitor Pig Context pig Context The plan that is being compiled Physical Plan plan The plan of Map Reduce Operators Oper Plan Plan The current Map Reduce Operator that is being compiled Map Reduce Oper cur Op The output of compiling the inputs Map Reduce Oper compiled Inputs null The split operators seen till now If not maintained they will haunt you During the traversal a split is the only operator that can be revisited from a different path So this map stores the split job So whenever we hit the split we create a new Oper and connect the split job using load store and also in the Plan Map Operator Key Map Reduce Oper splits Seen Node Id Generator nig private String scope private Finder udf Finder private Compilation Message Collector message Collector null private Map Physical Operator Map Reduce Oper phy To Op Map public static final String user comparator func private static final Log Log Factory get Log Compiler class public static final String pig files concatenation threshold public static final String pig optimistic files concatenation private int file Concatenation Threshold private boolean optimistic File Concatenation false public Compiler Physical Plan plan throws Compiler Exception this plan null public Compiler Physical Plan plan Pig Context pig Context throws Compiler Exception super plan new Depth First Walker Physical Operator Physical Plan plan this plan plan this pig Context pig Context splits Seen new Hash Map Operator Key Map Reduce Oper Plan new Oper Plan nig Node Id Generator get Generator udf Finder new Finder List Physical Operator roots plan get Roots if roots null roots size int err Code String msg Internal error Did not find roots in the physical plan throw new Compiler Exception msg err Code Pig Exception scope roots get get Operator Key get Scope message Collector new Compilation Message Collector phy To Op Map new Hash Map Physical Operator Map Reduce Oper file Concatenation Threshold Integer parse Int pig Context get Properties get Property optimistic File Concatenation pig Context get Properties get Property false equals true info File concatenation threshold file Concatenation Threshold optimistic optimistic File Concatenation public void aggregate Scalars Files throws Plan Exception Exception List Map Reduce Oper mr Op List new Array List Map Reduce Oper for Map Reduce Oper mr Op Plan mr Op List add mr Op Configuration conf Configuration Util to Configuration pig Context get Properties boolean combinable conf get Boolean pig no Split Combination false Set File Spec seen new Hash Set File Spec for Map Reduce Oper mro scalar consumer mr Op List for Physical Operator scalar mro scalar consumer scalars Map Reduce Oper mro scalar producer phy To Op Map get scalar if scalar instanceof Store File Spec old Spec Store scalar get File if seen contains old Spec continue seen add old Spec if combinable mro scalar producer reduce Plan is Empty has Too Many Input Files mro scalar producer conf mro scalar producer requested Parallelism file Concatenation Threshold Physical Plan pl mro scalar producer reduce Plan is Empty mro scalar producer map Plan mro scalar producer reduce Plan File Spec new Spec get Temp File Spec replace old Spec in mro with new Spec new Find Store Name Visitor pl new Spec old Spec visit seen add new Spec Store new Sto get Store new Sto set File old Spec Map Reduce Oper cat Op get Concatenate Job new Spec mro scalar producer new Sto Plan connect mro scalar producer cat Op Need to add it to the Physical Plan and phy To Op Map so that softlink can be created phy To Op Map put new Sto cat Op plan add new Sto for Physical Operator succ plan get Soft Link Successors scalar to Array new Physical Operator plan create Soft Link new Sto succ plan remove Soft Link scalar succ Used to get the compiled plan return map reduce plan built by the compiler public Oper Plan get Plan return Plan Used to get the plan that was compiled return physical plan Override public Physical Plan get Plan return plan public Compilation Message Collector get Message Collector return message Collector The front end method that the user calls to compile the plan Assumes that all submitted plans have a Store operators as the leaf return map reduce plan throws Exception throws Plan Exception throws Visitor Exception public Oper Plan compile throws Exception Plan Exception Visitor Exception List Physical Operator leaves plan get Leaves if pig Context in Illustrator for Physical Operator op leaves if op instanceof Store int err Code String msg Expected leaf of reduce plan to always be Store Found op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception get all stores and native operators sort them in order operator id and compile their plans List Store stores Plan Helper get Physical Operators plan Store class List Native native Rs Plan Helper get Physical Operators plan Native class List Physical Operator ops if pig Context in Illustrator ops new Array List Physical Operator stores size native Rs size ops add All stores else ops new Array List Physical Operator leaves size native Rs size ops add All leaves ops add All native Rs Collections sort ops for Physical Operator op ops compile op return Plan public void connect Soft Link throws Plan Exception Exception for Physical Operator op plan if plan get Soft Link Predecessors op null for Physical Operator pred plan get Soft Link Predecessors op Map Reduce Oper from phy To Op Map get pred Map Reduce Oper to phy To Op Map get op if from to continue if Plan get Predecessors to null Plan get Predecessors to contains from Plan connect from to Compiles the plan below op into a Map Reduce Operator and stores it in cur Op param op throws Exception throws Plan Exception throws Visitor Exception private void compile Physical Operator op throws Exception Plan Exception Visitor Exception An artifact of the Visitor Need to save this so that it is not overwritten Map Reduce Oper prev Comp Inp compiled Inputs Compile each predecessor into the Oper and store them away so that we can use them for compiling op List Physical Operator predecessors plan get Predecessors op if op instanceof Native the predecessor store has already been processed do n t process it again else if predecessors null predecessors size When processing an entire script multiquery we can get into a situation where a load has predecessors This means that it depends on some store earlier in the plan We need to take that dependency and connect the respective operators while at the same time removing the connection between the Physical operators That way the jobs will run in the right order if op instanceof Load if predecessors size int err Code String msg Expected at most one predecessor of load Got predecessors size throw new Plan Exception msg err Code Pig Exception Physical Operator p predecessors get Map Reduce Oper oper null if p instanceof Store p instanceof Native oper phy To Op Map get p else int err Code String msg Predecessor of load should be a store or mapreduce operator Got p get Class throw new Plan Exception msg err Code Pig Exception Need new operator cur Op get Op cur Op map Plan add op Plan add cur Op plan disconnect op p Plan connect oper cur Op phy To Op Map put op cur Op return Collections sort predecessors compiled Inputs new Map Reduce Oper predecessors size int i for Physical Operator pred predecessors if pred instanceof Split splits Seen contains Key pred get Operator Key compiled Inputs i start New Split pred get Split Store splits Seen get pred get Operator Key continue compile pred compiled Inputs i cur Op else No predecessors Mostly a load But this is where we start We create a new Op and add its first operator op Also this should be added to the Plan cur Op get Op cur Op map Plan add op if op null op instanceof Load if Load op get File null Load op get File get Func Spec null cur Op Fs add Load op get File get Func Spec to String Plan add cur Op phy To Op Map put op cur Op return Now we have the inputs compiled Do something with the input oper op op visit this if op get Requested Parallelism cur Op requested Parallelism we do n t want to change prallelism for skewed join due to sampling and pre allocated reducers for skewed keys if cur Op is Skewed Join cur Op requested Parallelism op get Requested Parallelism compiled Inputs prev Comp Inp private Map Reduce Oper get Op return new Map Reduce Oper new Operator Key scope nig get Next Node Id scope private Native Map Reduce Oper get Native Op String mr Jar String parameters return new Native Map Reduce Oper new Operator Key scope nig get Next Node Id scope mr Jar parameters private Load get Load Load ld new Load new Operator Key scope nig get Next Node Id scope ld set Pc pig Context ld set Is Tmp Load true return ld private Store get Store Store st new Store new Operator Key scope nig get Next Node Id scope mark store as tmp store These could be removed by the optimizer because it was n t the user requesting it st set Is Tmp Store true return st map Oper is an Oper whose map plan is still open for taking more non blocking operators reduce Oper is an Oper whose map plan is done but the reduce plan is open for taking more non blocking opers Used for compiling non blocking operators The logic here is simple If there is a single input just push the operator into whichever phase is open Otherwise we merge the compiled inputs into a list of Opers where the first oper is the merged oper consisting of all map Opers and the rest are reduce Opers as reduce plans ca n t be merged Then we add the input oper op into the merged map Oper s map plan as a leaf and connect the reduce Opers using store load combinations to the input operator which is the leaf Also care is taken to connect the Opers according to the dependencies param op throws Plan Exception throws Exception private void non Blocking Physical Operator op throws Plan Exception Exception if compiled Inputs length For speed Map Reduce Oper mro compiled Inputs if mro is Map Done mro map Plan add As Leaf op else if mro is Map Done mro is Reduce Done mro reduce Plan add As Leaf op else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception cur Op mro else List Map Reduce Oper merged Plans merge compiled Inputs The first Oper is always the merged map Oper Map Reduce Oper mro merged Plans remove Push the input operator into the merged map Oper mro map Plan add As Leaf op Connect all the reduce Opers if merged Plans size conn Red Oper merged Plans mro return the compiled Oper cur Op mro private void add To Map Physical Operator op throws Plan Exception Exception if compiled Inputs length For speed Map Reduce Oper mro compiled Inputs if mro is Map Done mro map Plan add As Leaf op else if mro is Map Done mro is Reduce Done File Spec f Spec get Temp File Spec Store st get Store st set File f Spec mro reduce Plan add As Leaf st mro set Reduce Done true mro start New f Spec mro mro map Plan add As Leaf op compiled Inputs mro else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception cur Op mro else List Map Reduce Oper merged Plans merge compiled Inputs The first Oper is always the merged map Oper Map Reduce Oper mro merged Plans remove Push the input operator into the merged map Oper mro map Plan add As Leaf op Connect all the reduce Opers if merged Plans size conn Red Oper merged Plans mro return the compiled Oper cur Op mro Used for compiling blocking operators If there is a single input and its map phase is still open then close it so that further operators can be compiled into the reduce phase If its reduce phase is open add a store and close it Start a new map Oper into which further operators can be compiled into If there are multiple inputs the logic is to merge all map Opers into one map Oper and retain the reduce Opers Since the operator is blocking it has to be a Global Rerrange at least now This operator need not be inserted into our plan as it is implemented by hadoop But this creates the map reduce boundary So the merged map Oper is closed and its reduce phase is started Depending on the number of reduce Opers and the number of pipelines in the map Roper a Union operator is inserted whenever necessary This also leads to the possibility of empty map plans So have to be careful while handling it in the Pig Map Reduce class If there are no map plans then a new one is created as a side effect of the merge process If there are no reduce Opers and only a single pipeline in the map then no union oper is added Otherwise a Union oper is added to the merged map Oper to which all the reduce Opers are connected by store load combinations Care is taken to connect the Opers in the Plan param op throws Exception throws Plan Exception private void blocking Physical Operator op throws Exception Plan Exception if compiled Inputs length Map Reduce Oper mro compiled Inputs if mro is Map Done mro set Map Done Single true cur Op mro else if mro is Map Done mro is Reduce Done File Spec f Spec get Temp File Spec Store st get Store st set File f Spec mro reduce Plan add As Leaf st mro set Reduce Done true cur Op start New f Spec mro cur Op set Map Done true else List Map Reduce Oper merged Plans merge compiled Inputs Map Reduce Oper mro merged Plans remove if merged Plans size mro set Map Done Multiple true else mro set Map Done Single true Connect all the reduce Opers if merged Plans size conn Red Oper merged Plans mro cur Op mro Connect the reduce Opers to the leaf node in the map Oper mro by adding appropriate loads param merged Plans The list of reduce Opers param mro The map Oper throws Plan Exception throws Exception private void conn Red Oper List Map Reduce Oper merged Plans Map Reduce Oper mro throws Plan Exception Exception Physical Operator leaf null List Physical Operator leaves mro map Plan get Leaves if leaves null leaves size leaf leaves get for Map Reduce Oper mmro merged Plans mmro set Reduce Done true File Spec file Spec get Temp File Spec Load ld get Load ld set File file Spec Store str get Store str set File file Spec mmro reduce Plan add As Leaf str mro map Plan add ld if leaf null mro map Plan connect ld leaf Plan connect mmro mro Force an end to the current map reduce job with a store into a temporary file param f Spec Temp file to force a store into return operator that now is finished with a store throws Plan Exception private Map Reduce Oper end Single Input Plan With Str File Spec f Spec throws Plan Exception if compiled Inputs length int err Code String msg Received a multi input plan when expecting only a single input one throw new Plan Exception msg err Code Pig Exception Map Reduce Oper mro compiled Inputs Store str get Store str set File f Spec if mro is Map Done mro map Plan add As Leaf str mro set Map Done Single true else if mro is Map Done mro is Reduce Done mro reduce Plan add As Leaf str mro set Reduce Done true else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception return mro Starts a new Roper and connects it to the old one by load store The assumption is that the store is already inserted into the old Oper param f Spec param old return throws Exception throws Plan Exception private Map Reduce Oper start New File Spec f Spec Map Reduce Oper old throws Plan Exception Load ld get Load ld set File f Spec Map Reduce Oper ret get Op ret map Plan add ld Plan add ret Plan connect old ret return ret Returns a temporary Path return throws Exception private File Spec get Temp File Spec throws Exception return new File Spec File Localizer get Temporary Path pig Context to String new Func Spec Utils get Tmp File Compressor Name pig Context Merges the map Opers in the compiled Inputs into a single merged map Roper and returns a List with the merged map Oper as the first oper and the rest being reduce Opers Care is taken to remove the map Opers that are merged from the Plan and their connections moved over to the merged map Oper Merge is implemented as a sequence of binary merges merge Phy Plan fin Plan List Phy Plan lst fin Plan merge p foreach p in lst param compiled Inputs return throws Plan Exception throws Exception private List Map Reduce Oper merge Map Reduce Oper compiled Inputs throws Plan Exception List Map Reduce Oper ret new Array List Map Reduce Oper Map Reduce Oper merged Map get Op ret add merged Map Plan add merged Map Set Map Reduce Oper to Be Connected new Hash Set Map Reduce Oper List Map Reduce Oper rem Lst new Array List Map Reduce Oper List Physical Plan mp Lst new Array List Physical Plan for Map Reduce Oper mro compiled Inputs if mro is Map Done rem Lst add mro mp Lst add mro map Plan List Map Reduce Oper pmros Plan get Predecessors mro if pmros null for Map Reduce Oper pmro pmros to Be Connected add pmro else if mro is Map Done mro is Reduce Done ret add mro else int err Code String msg Both map and reduce phases have been done This is unexpected for a merge throw new Plan Exception msg err Code Pig Exception merge ret get map Plan mp Lst Iterator Map Reduce Oper it to Be Connected iterator while it has Next Plan connect it next merged Map for Map Reduce Oper rmro rem Lst if rmro requested Parallelism merged Map requested Parallelism merged Map requested Parallelism rmro requested Parallelism for String udf rmro Fs if merged Map Fs contains udf merged Map Fs add udf We also need to change scalar marking for Physical Operator phys Op rmro scalars if merged Map scalars contains phys Op merged Map scalars add phys Op Set Physical Operator ops To Change new Hash Set Physical Operator for Map Entry Physical Operator Map Reduce Oper entry phy To Op Map entry Set if entry get Value rmro ops To Change add entry get Key for Physical Operator op ops To Change phy To Op Map put op merged Map Plan remove rmro return ret The merge of a list of map plans param param param fin Plan Final Plan into which the list of plans is merged param plans list of map plans to be merged throws Plan Exception private extends Operator extends Operator Plan void merge fin Plan List plans throws Plan Exception for e plans fin Plan merge e private void process Fs Physical Plan plan throws Visitor Exception if plan null Process Scalars with referenced Operators Scalar Phy Finder scalar Phy Finder new Scalar Phy Finder plan scalar Phy Finder visit cur Op scalars add All scalar Phy Finder get Scalars Process Fs udf Finder set Plan plan udf Finder visit cur Op Fs add All udf Finder get Fs The visit Op methods that decide what to do with the current operator Compiles a split operator The logic is to close the split job by replacing the split oper by a store and creating a new Map Roper and return that as the current Oper to which other operators would be compiled into The new Oper would be connected to the split job by load store Also add the split oper to the splits Seen map param op The split operator throws Visitor Exception Override public void visit Split Split op throws Visitor Exception try File Spec f Spec op get Split Store Map Reduce Oper mro end Single Input Plan With Str f Spec mro set Splitter true splits Seen put op get Operator Key mro cur Op start New f Spec mro phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Load Load op throws Visitor Exception try non Blocking op phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Native Native op throws Visitor Exception We will explode the native operator here to add a new Oper for native Mapreduce job try add a map reduce boundary Map Reduce Oper native Oper get Native Op op get Native Rjar op get Params Plan add native Oper Plan connect cur Op native Oper phy To Op Map put op native Oper cur Op native Oper catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Store Store op throws Visitor Exception try non Blocking op phy To Op Map put op cur Op if op get File null op get File get Func Spec null cur Op Fs add op get File get Func Spec to String catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Filter Filter op throws Visitor Exception try non Blocking op process Fs op get Plan phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Cross Cross op throws Visitor Exception try non Blocking op phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Stream Stream op throws Visitor Exception try non Blocking op phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Limit Limit op throws Visitor Exception try Map Reduce Oper mro compiled Inputs mro limit op get Limit if op get Limit Plan null process Fs op get Limit Plan mro limit Plan op get Limit Plan if mro is Map Done if map plan is open add a limit for optimization eventually we will add another limit to reduce plan if pig Context in Illustrator mro map Plan add As Leaf op mro set Map Done true if mro reduce Plan is Empty Util simple Connect Map To Reduce mro scope nig mro requested Parallelism if pig Context in Illustrator Limit p Limit new Limit new Operator Key scope nig get Next Node Id scope p Limit set Limit op get Limit p Limit set Limit Plan op get Limit Plan mro reduce Plan add As Leaf p Limit else mro reduce Plan add As Leaf op else message Collector collect Something in the reduce plan while map plan is not done Something wrong Message Type Warning Pig Warning else if mro is Map Done mro is Reduce Done limit should add into reduce plan mro reduce Plan add As Leaf op else message Collector collect Both map and reduce phases have been done This is unexpected while compiling Message Type Warning Pig Warning phy To Op Map put op mro catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Local Rearrange Local Rearrange op throws Visitor Exception try add To Map op List Physical Plan plans op get Plans if plans null for Physical Plan ep plans process Fs ep phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Collected Group Collected Group op throws Visitor Exception if cur Op map Done List Physical Operator roots cur Op map Plan get Roots if roots size int err Code String err Msg Expected one but found more then one root physical operator in physical plan throw new Compiler Exception err Msg err Code Pig Exception Physical Operator phy Op roots get if phy Op instanceof Load int err Code String err Msg Expected physical operator at root to be Load Found phy Op get Class get Canonical Name throw new Compiler Exception err Msg err Code Pig Exception Load Func load Func Load phy Op get Load Func try if Collectable Load Func class is Assignable From load Func get Class int err Code throw new Compiler Exception While using collected on group data must be loaded via loader implementing Collectable Load Func err Code Collectable Load Func load Func ensure All Key Instances In Same Split catch Compiler Exception e throw e catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e try non Blocking op List Physical Plan plans op get Plans if plans null for Physical Plan ep plans process Fs ep phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e else if cur Op reduce Done int err Code String msg Blocking operators are not allowed before Collected Group Consider dropping using collected throw new Compiler Exception msg err Code Pig Exception else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Compiler Exception msg err Code Pig Exception Override public void visit For Each For Each op throws Visitor Exception try if op is Map Side Only cur Op is Map Done File Spec f Spec get Temp File Spec Map Reduce Oper prev Oper end Single Input Plan With Str f Spec cur Op start New f Spec prev Oper cur Op map Plan add As Leaf op else non Blocking op List Physical Plan plans op get Input Plans if plans null for Physical Plan plan plans process Fs plan phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Global Rearrange Global Rearrange op throws Visitor Exception try blocking op cur Op custom Partitioner op get Custom Partitioner phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Package Package op throws Visitor Exception try non Blocking op phy To Op Map put op cur Op if op get Pkgr get Package Type Package Type op get Pkgr get Package Type Package Type Bloom join is not implemented in mapreduce mode and falls back to regular join cur Op mark Regular Join else if op get Pkgr get Package Type Package Type if op get Num Inps cur Op mark Group By else if op get Num Inps cur Op mark Cogroup catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Union Union op throws Visitor Exception try non Blocking op phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e This is an operator which will have multiple inputs to number of join inputs But it prunes off all inputs but the fragment input and creates separate jobs for each of the replicated inputs and uses these as the replicated files that are configured in the Join operator It also sets that this is Join job and some parametes associated with it Override public void visit Join Join op throws Visitor Exception try File Spec repl Files new File Spec op get Inputs size for int i i repl Files length i if i op get Fragment continue repl Files i get Temp File Spec op set Repl Files repl Files cur Op phy To Op Map get op get Inputs get op get Fragment for int i i compiled Inputs length i Map Reduce Oper mro compiled Inputs i if cur Op equals mro continue Store str get Store str set File repl Files i Configuration conf Configuration Util to Configuration pig Context get Properties boolean combinable conf get Boolean pig no Split Combination false if mro is Map Done if combinable has Too Many Input Files mro conf Store tmp Sto get Store File Spec f Spec get Temp File Spec tmp Sto set File f Spec mro map Plan add As Leaf tmp Sto mro set Map Done Single true Map Reduce Oper cat Op get Concatenate Job f Spec mro str Plan connect cat Op cur Op else mro map Plan add As Leaf str mro set Map Done Single true Plan connect mro cur Op else if mro is Map Done mro is Reduce Done if combinable mro requested Parallelism file Concatenation Threshold Store tmp Sto get Store File Spec f Spec get Temp File Spec tmp Sto set File f Spec mro reduce Plan add As Leaf tmp Sto mro set Reduce Done true Map Reduce Oper cat Op get Concatenate Job f Spec mro str Plan connect cat Op cur Op else mro reduce Plan add As Leaf str mro set Reduce Done true Plan connect mro cur Op else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception if cur Op is Map Done cur Op map Plan add As Leaf op else if cur Op is Map Done cur Op is Reduce Done cur Op reduce Plan add As Leaf op else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception List List Physical Plan join Plans op get Join Plans if join Plans null for List Physical Plan join Plan join Plans if join Plan null for Physical Plan plan join Plan process Fs plan phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Suppress Warnings unchecked private boolean has Too Many Input Files Map Reduce Oper mro Configuration conf if pig Context null pig Context get Exec Type Exec Type return false if mro instanceof Native Map Reduce Oper return optimistic File Concatenation false true Physical Plan map Plan mro map Plan List Physical Operator roots map Plan get Roots if roots null roots size return false int num Files boolean ret false try for Physical Operator root roots Load ld Load root String file Name ld get File get File Name if Uri Util is File file Name Only if the input is an hdfs file this optimization is useful to reduce load on namenode separate out locations separated by comma String locations Load Func get Path Strings file Name for String location locations if Uri Util is File location continue Path path new Path location File System fs path get File System conf if fs exists path Load Func loader Load Func Pig Context instantiate Func From Spec ld get File get Func Spec Job job new Job conf loader set Context Signature ld get Signature loader set Location location job Input Format inf loader get Input Format List Input Split splits inf get Splits Hadoop Shims clone Job Context job List List Input Split results Map Red Util get Combine Pig Splits splits fs get Default Block Size path conf num Files results size else List Map Reduce Oper preds Plan get Predecessors mro if preds null preds size Map Reduce Oper pred preds get if pred reduce Plan is Empty num Files pred requested Parallelism else map only job ret has Too Many Input Files pred conf break else if optimistic File Concatenation ca n t determine the number of input files Treat it as having too manyfiles num Files file Concatenation Threshold break catch Exception e warn failed to get number of input files e catch Interrupted Exception e warn failed to get number of input files e info number of input files num Files return ret true num Files file Concatenation Threshold Use Mult File Combiner to concatenate small input files private Map Reduce Oper get Concatenate Job File Spec f Spec Map Reduce Oper old Store str throws Plan Exception Exec Exception Map Reduce Oper mro start New f Spec old mro map Plan add As Leaf str mro set Map Done true info Insert a file concatenation job return mro Leftmost relation is referred as base relation this is the one fed into mappers First close all Opers except for first one referred as base Per Then create a Oper which will do indexing job idx Oper Connect idx Oper before the mapped Oper in the Plan Override public void visit Merge Co Group Merge Cogroup po Co Grp throws Visitor Exception if compiled Inputs length int err Code String err Msg Merge Cogroup work on two or more relations To use map side group by on single relation use collected qualifier throw new Compiler Exception err Msg err Code List Func Spec func Specs new Array List Func Spec compiled Inputs length List String file Specs new Array List String compiled Inputs length List String loader Signs new Array List String compiled Inputs length try Iterate through all the Opers disconnect side Pers from Per Plan and collect all the information needed in different lists for int i i compiled Inputs length i Map Reduce Oper mr Oper compiled Inputs i Physical Plan map Plan mr Oper map Plan if map Plan get Roots size int err Code String err Msg Expected one but found more then one root physical operator in physical plan throw new Compiler Exception err Msg err Code Pig Exception Physical Operator root Op map Plan get Roots get if root Op instanceof Load int err Code String err Msg Expected physical operator at root to be Load Found root Op get Class get Canonical Name throw new Compiler Exception err Msg err Code Load side Loader Load root Op File Spec load File Spec side Loader get File Func Spec func Spec load File Spec get Func Spec Load Func loadfunc side Loader get Load Func if i if Collectable Load Func class is Assignable From loadfunc get Class int err Code throw new Compiler Exception Base loader in Cogroup must implement Collectable Load Func err Code Collectable Load Func loadfunc ensure All Key Instances In Same Split continue if Indexable Load Func class is Assignable From loadfunc get Class int err Code throw new Compiler Exception Side loaders in cogroup must implement Indexable Load Func err Code func Specs add func Spec file Specs add load File Spec get File Name loader Signs add side Loader get Signature Plan remove mr Oper po Co Grp set Side Load Funcs func Specs po Co Grp set Side File Specs file Specs po Co Grp set Loader Signatures loader Signs Use map reduce operator of base relation for the cogroup operation Map Reduce Oper base Op phy To Op Map get po Co Grp get Inputs get if base Op map Done base Op reduce Plan is Empty int err Code throw new Compiler Exception Currently merged cogroup is not supported after blocking operators err Code Create new map reduce operator for indexing job and then configure it Map Reduce Oper indexer Op get Op File Spec idx File Spec get Indexing Job indexer Op base Op po Co Grp get Inner Plans Of po Co Grp set Idx Func Spec idx File Spec get Func Spec po Co Grp set Index File Name idx File Spec get File Name base Op map Plan add As Leaf po Co Grp for Func Spec func Spec func Specs base Op Fs add func Spec to String Plan add indexer Op Plan connect indexer Op base Op phy To Op Map put po Co Grp base Op Going forward new operators should be added in base Rop To make sure reset cur Op cur Op base Op catch Exec Exception e throw new Compiler Exception e get Detailed Message e get Error Code e get Error Source e catch Compiler Exception mrce throw mrce catch Clone Not Supported Exception e throw new Compiler Exception e catch Plan Exception e int err Code String msg Error compiling operator po Co Grp get Class get Canonical Name throw new Compiler Exception msg err Code Pig Exception e catch Exception e int err Code String err Msg Exception caught while compiling Merge Co Group throw new Compiler Exception err Msg err Code e Sets up the indexing job for map side cogroups private File Spec get Indexing Job Map Reduce Oper indexer Op final Map Reduce Oper base Op final List Physical Plan mapper Inner Plans throws Compiler Exception Plan Exception Exec Exception Exception Clone Not Supported Exception First replace loader with Merge Join Indexer Physical Plan base Map Plan base Op map Plan Load base Loader Load base Map Plan get Roots get File Spec orig Loader File Spec base Loader get File Func Spec func Spec orig Loader File Spec get Func Spec Load Func load Func base Loader get Load Func if Ordered Load Func class is Assignable From load Func get Class int err Code String err Msg Base relation of merge co Group must implement Ordered Load Func interface The specified loader func Spec does n t implement it throw new Compiler Exception err Msg err Code String indexer Args new String indexer Args func Spec to String indexer Args Object Serializer serialize Serializable mapper Inner Plans indexer Args base Loader get Signature indexer Args base Loader get Operator Key scope indexer Args Boolean to String false we care for nulls Physical Plan phy Plan if base Map Plan get Successors base Loader null base Map Plan get Successors base Loader is Empty Load Load Cogroup case phy Plan null else We got something Yank it and set it as inner plan phy Plan base Map Plan clone Physical Operator root phy Plan get Roots get phy Plan disconnect root phy Plan get Successors root get phy Plan remove root indexer Args Object Serializer serialize phy Plan Load idx Job Loader get Load idx Job Loader set File new File Spec orig Loader File Spec get File Name new Func Spec Merge Join Indexer class get Name indexer Args indexer Op map Plan add idx Job Loader indexer Op Fs add base Loader get File get Func Spec to String Loader of mro will return a tuple of form key key Writable Comparable split Index See Merge Join Indexer for details After getting an index entry in each mapper send all of them to one reducer where they will be sorted on the way by Hadoop Util simple Connect Map To Reduce indexer Op scope nig indexer Op requested Parallelism we need exactly one reducer for indexing job We want to use typed tuple comparator for this job instead of default raw binary comparator used by Pig to make sure index entries are sorted correctly by Hadoop indexer Op use Typed Comparator true Store st get Store File Spec str File get Temp File Spec st set File str File indexer Op reduce Plan add As Leaf st indexer Op set Reduce Done true return str File Since merge join works on two inputs there are exactly two Oper predecessors identified as left and right Instead of merging two operators both are used to generate a job each First oper is run to generate on the fly index on right side Second is used to actually do the join First oper is identified as right Oper and second as cur Oper Right Oper If it is in map phase It can be preceded only by Load If there is anything else in physical plan that is yanked and set as inner plans of join Op If it is reduce phase Close this operator and start new Oper Left Oper If it is in map phase add the Join operator in it If it is in reduce phase Close it and start new Oper Override public void visit Merge Join Merge Join join Op throws Visitor Exception try if compiled Inputs length join Op get Inputs size int err Code throw new Compiler Exception Merge Join must have exactly two inputs Found compiled Inputs length err Code cur Op phy To Op Map get join Op get Inputs get Map Reduce Oper right Opr null if cur Op equals compiled Inputs right Opr compiled Inputs else right Opr compiled Inputs We will first operate on right side which is indexer job First yank plan of the compiled right input and set that as an inner plan of right operator Physical Plan right Pipeline Plan if right Opr map Done Physical Plan right Map Plan right Opr map Plan if right Map Plan get Roots size int err Code String err Msg Expected one but found more then one root physical operator in physical plan throw new Compiler Exception err Msg err Code Pig Exception Physical Operator right Loader right Map Plan get Roots get if right Loader instanceof Load int err Code String err Msg Expected physical operator at root to be Load Found right Loader get Class get Canonical Name throw new Compiler Exception err Msg err Code if right Map Plan get Successors right Loader null right Map Plan get Successors right Loader is Empty Load Join case right Pipeline Plan null else We got something on right side Yank it and set it as inner plan of right input right Pipeline Plan right Map Plan clone Physical Operator root right Pipeline Plan get Roots get right Pipeline Plan disconnect root right Pipeline Plan get Successors root get right Pipeline Plan remove root right Map Plan trim Below right Loader else if right Opr reduce Done Indexer must run in map If we are in reduce close it and start new Oper No need of yanking in this case Since we are starting brand new Operator and it will contain nothing Store right Store get Store File Spec right Str File get Temp File Spec right Store set File right Str File right Opr reduce Plan add As Leaf right Store right Opr set Reduce Done true right Opr start New right Str File right Opr right Pipeline Plan null else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception join Op setup Right Pipeline right Pipeline Plan right Opr requested Parallelism we need exactly one reducer for indexing job At this point we must be operating on map plan of right input and it would contain nothing else other then a Load Load right Loader Load right Opr map Plan get Roots get join Op set Signature right Loader get Signature Load Func right Load Func right Loader get Load Func List String udfs new Array List String if Indexable Load Func class is Assignable From right Load Func get Class join Op set Right Loader Func Spec right Loader get File get Func Spec join Op set Right Input File Name right Loader get File get File Name udfs add right Loader get File get Func Spec to String we do n t need the right Oper since the right loader is an Indexable Load Func which can handle the index itself Plan remove right Opr if right Opr compiled Inputs compiled Inputs null else if right Opr compiled Inputs compiled Inputs null right Opr null validate that the join keys in merge join are only simple column projections or and not expression expressions can not be handled when the index is built by the storage layer on the sorted data when the sorted data and corresponding index is written So merge join will be restricted not have expressions as join keys int num Inputs m Plan get Predecessors join Op size should be for int i i num Inputs i List Physical Plan key Plans join Op get Inner Plans Of i for Physical Plan key Plan key Plans for Physical Operator op key Plan if op instanceof Project int err Code String err Msg Merge join is possible only for simple column or join keys when using right Loader get File get Func Spec as the loader throw new Compiler Exception err Msg err Code Pig Exception else Load Func load Func right Loader get Load Func Replacing Load with indexer is disabled for merge sparse joins While this feature would be useful the current implementation of Default Indexable Loader is not designed to handle multiple calls to seek Near Specifically it rereads the entire index for each call Some refactoring of this class is required and then the check below could be removed if join Op get Join Type Join int err Code String err Msg Right input of merge join must implement Indexable Load Func The specified loader load Func does n t implement it throw new Compiler Exception err Msg err Code Replace Load with indexer if Ordered Load Func class is Assignable From load Func get Class int err Code String err Msg Right input of merge join must implement Ordered Load Func interface The specified loader load Func does n t implement it throw new Compiler Exception err Msg err Code String indexer Args new String List Physical Plan right Inp Plans join Op get Inner Plans Of File Spec orig Right Loader File Spec right Loader get File indexer Args orig Right Loader File Spec get Func Spec to String indexer Args Object Serializer serialize Serializable right Inp Plans indexer Args Object Serializer serialize right Pipeline Plan indexer Args right Loader get Signature indexer Args right Loader get Operator Key scope indexer Args Boolean to String true File Spec l File new File Spec right Loader get File get File Name new Func Spec Merge Join Indexer class get Name indexer Args right Loader set File l File Loader of mro will return a tuple of form key First key First position split Index See Merge Join Indexer Util simple Connect Map To Reduce right Opr scope nig right Opr use Typed Comparator true Store st get Store File Spec str File get Temp File Spec st set File str File right Opr reduce Plan add As Leaf st right Opr set Reduce Done true set up the Default Indexable Loader for the join operator String default Indexable Loader Args new String default Indexable Loader Args orig Right Loader File Spec get Func Spec to String default Indexable Loader Args str File get File Name default Indexable Loader Args str File get Func Spec to String default Indexable Loader Args join Op get Operator Key scope default Indexable Loader Args orig Right Loader File Spec get File Name join Op set Right Loader Func Spec new Func Spec Default Indexable Loader class get Name default Indexable Loader Args join Op set Right Input File Name orig Right Loader File Spec get File Name join Op set Index File str File get File Name udfs add orig Right Loader File Spec get Func Spec to String We are done with right side Lets work on left now Join will be materialized in left Oper if cur Op map Done Life is easy cur Op map Plan add As Leaf join Op else if cur Op reduce Done This is a map side join Close this Oper and start afresh Store left Store get Store File Spec left Str File get Temp File Spec left Store set File left Str File cur Op reduce Plan add As Leaf left Store cur Op set Reduce Done true cur Op start New left Str File cur Op cur Op map Plan add As Leaf join Op else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception if right Opr null right Opr mark Indexer We want to ensure indexing job runs prior to actual join job So connect them in order Plan connect right Opr cur Op phy To Op Map put join Op cur Op no combination of small splits as there is currently no way to guarantee the sortness of the combined splits cur Op no Combine Small Splits cur Op Fs add All udfs catch Plan Exception e int err Code String msg Error compiling operator join Op get Class get Canonical Name throw new Compiler Exception msg err Code Pig Exception e catch Exception e int err Code String err Msg Exception caught while compiling Merge Join throw new Compiler Exception err Msg err Code e catch Clone Not Supported Exception e int err Code String err Msg Cloning exception caught while compiling Merge Join throw new Compiler Exception err Msg err Code Pig Exception e Override public void visit Distinct Distinct op throws Visitor Exception try Physical Plan ep new Physical Plan Project prj Star new Project new Operator Key scope nig get Next Node Id scope prj Star set Result Type Data Type prj Star set Star true ep add prj Star List Physical Plan eps new Array List Physical Plan eps add ep Local Rearrange lr new Local Rearrange new Operator Key scope nig get Next Node Id scope lr set Index lr set Key Type Data Type lr set Plans eps lr set Result Type Data Type lr set Distinct true add To Map lr blocking op cur Op custom Partitioner op get Custom Partitioner Package pkg new Package new Operator Key scope nig get Next Node Id scope Packager pkgr pkg get Pkgr pkgr set Key Type Data Type pkgr set Distinct true pkg set Num Inps boolean inner false pkgr set Inner inner cur Op reduce Plan add pkg List Physical Plan eps new Array List Physical Plan List Boolean flat new Array List Boolean Physical Plan ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Result Type Data Type prj set Star false prj set Column prj set Overloaded false ep add prj eps add ep flat add true For Each nfe new For Each new Operator Key scope nig get Next Node Id scope op get Requested Parallelism eps flat nfe set Result Type Data Type cur Op reduce Plan add As Leaf nfe cur Op set Needs Distinct Combiner true phy To Op Map put op cur Op cur Op phy To Map put op nfe catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Skewed Join Skewed Join op throws Visitor Exception try if compiled Inputs length int err Code throw new Visitor Exception Skewed Join operator has compiled Inputs length inputs It should have err Code change plan to store the first join input into a temp file File Spec f Spec get Temp File Spec Map Reduce Oper mro compiled Inputs Store str get Store str set File f Spec if mro is Map Done mro map Plan add As Leaf str mro set Map Done Single true else if mro is Map Done mro is Reduce Done mro reduce Plan add As Leaf str mro set Reduce Done true else int err Code String msg Both map and reduce phases have been done This is unexpected while compiling throw new Plan Exception msg err Code Pig Exception File Spec partition File get Temp File Spec int rp op get Requested Parallelism Pair Map Reduce Oper Integer sample Job Pair get Skewed Join Sample Job op mro f Spec partition File rp rp sample Job Pair second set parallelism of Skewed Join as the value calculated by sampling job if parallel is specified in join statement rp is equal to that number if not specified use the value that sampling process calculated based on default op set Requested Parallelism rp load the temp file for first table as input of join Map Reduce Oper join Inputs new Map Reduce Oper start New f Spec sample Job Pair first compiled Inputs Map Reduce Oper rearrange Outputs new Map Reduce Oper compiled Inputs new Map Reduce Oper join Inputs run Local Rearrange for first join table Local Rearrange lr new Local Rearrange new Operator Key scope nig get Next Node Id scope rp try lr set Index catch Exec Exception e int err Code String msg Unable to set index on newly created Local Rearrange throw new Plan Exception msg err Code Pig Exception e List Physical Operator l plan get Predecessors op Multi Map Physical Operator Physical Plan join Plans op get Join Plans List Physical Plan groups join Plans get l get check the type of group keys if there are more than one field the key is byte type Data Type if groups size type groups get get Leaves get get Result Type lr set Key Type type lr set Plans groups lr set Result Type Data Type lr visit this if lr get Requested Parallelism cur Op requested Parallelism cur Op requested Parallelism lr get Requested Parallelism rearrange Outputs cur Op compiled Inputs new Map Reduce Oper join Inputs if the map for current input is already closed then start a new job if compiled Inputs is Map Done compiled Inputs is Reduce Done File Spec f get Temp File Spec Store s get Store s set File f compiled Inputs reduce Plan add As Leaf s compiled Inputs set Reduce Done true compiled Inputs start New f compiled Inputs run Partition Rearrange for second join table Partition Rearrange pr new Partition Rearrange new Operator Key scope nig get Next Node Id scope rp pr set Pig Context pig Context lr pr try lr set Index catch Exec Exception e int err Code String msg Unable to set index on newly created Local Rearrange throw new Plan Exception msg err Code Pig Exception e groups join Plans get l get lr set Plans groups lr set Key Type type lr set Result Type Data Type lr visit this if lr get Requested Parallelism cur Op requested Parallelism cur Op requested Parallelism lr get Requested Parallelism rearrange Outputs cur Op compiled Inputs rearrange Outputs create Global Rearrange Global Rearrange gr new Global Rearrange new Operator Key scope nig get Next Node Id scope rp Skewed join has its own special partitioner gr set Result Type Data Type gr visit this if gr get Requested Parallelism cur Op requested Parallelism cur Op requested Parallelism gr get Requested Parallelism compiled Inputs new Map Reduce Oper cur Op create Pakcage Package pkg new Package new Operator Key scope nig get Next Node Id scope rp Packager pkgr pkg get Pkgr pkgr set Key Type type pkg set Result Type Data Type pkg set Num Inps boolean inner op get Inner Flags pkgr set Inner inner pkg visit this compiled Inputs new Map Reduce Oper cur Op create For Each List Physical Plan eps new Array List Physical Plan List Boolean flat new Array List Boolean Physical Plan ep Add corresponding Projects for int i i i ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Column i prj set Overloaded false prj set Result Type Data Type ep add prj eps add ep if inner i Add an empty bag for outer join if i For right outer add Is First Reduce Of Key as well Compiler Utils add Empty Bag Outer Join ep op get Schema i true Is First Reduce Of Key class get Name else Compiler Utils add Empty Bag Outer Join ep op get Schema i false Is First Reduce Of Key class get Name flat add true For Each fe new For Each new Operator Key scope nig get Next Node Id scope eps flat fe set Result Type Data Type fe visit this cur Op set Skewed Join Partition File partition File get File Name phy To Op Map put op cur Op catch Plan Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e Override public void visit Sort Sort op throws Visitor Exception try File Spec f Spec get Temp File Spec Map Reduce Oper mro end Single Input Plan With Str f Spec File Spec quant File get Temp File Spec int rp op get Requested Parallelism Pair Project Byte fields get Sort Cols op get Sort Plans Pair Map Reduce Oper Integer quant Job Parallelism Pair get Quantile Job op mro f Spec quant File rp cur Op get Sort Job op quant Job Parallelism Pair first f Spec quant File quant Job Parallelism Pair second fields if op is Comparator Used cur Op Fs add op get Sort Func get Func Spec to String cur Op is Comparator Used true phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e For the counter job it depends if it is row number or not In case of being a row number any previous jobs are saved and Counter is added as a leaf on a map task If it is not then Counter is added as a leaf on a reduce task last sorting phase Override public void visit Counter Counter op throws Visitor Exception try if op is Row Number List Physical Operator mp Leaves cur Op map Plan get Leaves Physical Operator leaf mp Leaves get if cur Op is Map Done cur Op is Rank Operation cur Op map Plan add As Leaf op else File Spec f Spec get Temp File Spec Map Reduce Oper prev Oper end Single Input Plan With Str f Spec Map Reduce Oper mr Counter start New f Spec prev Oper mr Counter map Plan add As Leaf op cur Op mr Counter else cur Op reduce Plan add As Leaf op phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e In case of Rank it is closed any other previous job containing Counter as a leaf and Rank is added on map phase Override public void visit Rank Rank op throws Visitor Exception try File Spec f Spec get Temp File Spec Map Reduce Oper prev Oper end Single Input Plan With Str f Spec cur Op start New f Spec prev Oper cur Op map Plan add As Leaf op phy To Op Map put op cur Op catch Exception e int err Code String msg Error compiling operator op get Class get Simple Name throw new Compiler Exception msg err Code Pig Exception e private Pair Project Byte get Sort Cols List Physical Plan plans throws Plan Exception Exec Exception if plans null Suppress Warnings unchecked Pair Project Byte ret new Pair plans size int i for Physical Plan plan plans Physical Operator op plan get Leaves get Project proj if op instanceof Project if Project op is Star return null proj Project op else proj null byte type op get Result Type ret i new Pair Project Byte proj type return ret int err Code String msg No expression plan found in Sort throw new Plan Exception msg err Code Pig Exception private Map Reduce Oper get Sort Job Sort sort Map Reduce Oper quant Job File Spec l File File Spec quant File int rp Pair Project Byte fields throws Plan Exception Map Reduce Oper mro start New l File quant Job mro set Quant File quant File get File Name mro set Global Sort true mro requested Parallelism rp long limit sort get Limit mro limit limit List Physical Plan eps new Array List Physical Plan byte key Type Data Type boolean sort Order List Boolean sort Order List sort get Asc Cols if sort Order List null sort Order new boolean sort Order List size for int i i sort Order List size i sort Order i sort Order List get i mro set Sort Order sort Order if fields null This is project Physical Plan ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Star true prj set Overloaded false prj set Result Type Data Type ep add prj eps add ep else for int i fields Physical Plan ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Column i prj set Overloaded false prj set Result Type Data Type ep add prj eps add ep Attach the sort plans to the local rearrange to get the projection eps add All sort get Sort Plans Visit the first sort plan to figure out our key type We only have to visit the first because if we have more than one plan then the key type will be tuple try Find Key Type Visitor fktv new Find Key Type Visitor sort get Sort Plans get fktv visit key Type fktv key Type catch Visitor Exception ve int err Code String msg Internal error Could not compute key type of sort operator throw new Plan Exception msg err Code Pig Exception ve Local Rearrange lr new Local Rearrange new Operator Key scope nig get Next Node Id scope try lr set Index catch Exec Exception e int err Code String msg Unable to set index on newly created Local Rearrange throw new Plan Exception msg err Code Pig Exception e lr set Key Type fields null fields length Data Type key Type lr set Plans eps lr set Result Type Data Type lr add Original Location sort get Alias sort get Original Locations mro map Plan add As Leaf lr mro set Map Done true if limit Package pkg c new Package new Operator Key scope nig get Next Node Id scope Lite Packager pkgr new Lite Packager pkgr set Key Type fields length Data Type key Type pkg c set Pkgr pkgr pkg c set Num Inps mro combine Plan add pkg c List Physical Plan eps c new Array List Physical Plan List Boolean flat c new Array List Boolean Physical Plan ep c new Physical Plan Project prj c new Project new Operator Key scope nig get Next Node Id scope prj c set Column prj c set Overloaded false prj c set Result Type Data Type ep c add prj c eps c add ep c flat c add true For Each fe c new For Each new Operator Key scope nig get Next Node Id scope eps c flat c fe c set Result Type Data Type mro combine Plan add As Leaf fe c Limit p Limit new Limit new Operator Key scope nig get Next Node Id scope p Limit set Limit limit mro combine Plan add As Leaf p Limit List Physical Plan eps c new Array List Physical Plan eps c add All sort get Sort Plans Local Rearrange lr c new Local Rearrange new Operator Key scope nig get Next Node Id scope try lr c set Index catch Exec Exception e int err Code String msg Unable to set index on newly created Local Rearrange throw new Plan Exception msg err Code Pig Exception e lr c set Key Type fields length Data Type key Type lr c set Plans eps c lr c set Result Type Data Type mro combine Plan add As Leaf lr c Package pkg new Package new Operator Key scope nig get Next Node Id scope Lite Packager pkgr new Lite Packager pkgr set Key Type fields null fields length Data Type key Type pkg set Pkgr pkgr pkg set Num Inps mro reduce Plan add pkg Physical Plan ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Column prj set Overloaded false prj set Result Type Data Type ep add prj List Physical Plan eps new Array List Physical Plan eps add ep List Boolean flattened new Array List Boolean flattened add true For Each nfe new For Each new Operator Key scope nig get Next Node Id scope eps flattened mro reduce Plan add nfe mro reduce Plan connect pkg nfe mro phy To Map put sort nfe if limit Limit p Limit new Limit new Operator Key scope nig get Next Node Id scope p Limit set Limit limit mro reduce Plan add As Leaf p Limit mro phy To Map put sort p Limit return mro private Pair Map Reduce Oper Integer get Quantile Job Sort inp Sort Map Reduce Oper prev Job File Spec l File File Spec quant File int rp throws Plan Exception Visitor Exception Sort sort new Sort inp Sort get Operator Key inp Sort get Requested Parallelism null inp Sort get Sort Plans inp Sort get Asc Cols inp Sort get Sort Func sort add Original Location inp Sort get Alias inp Sort get Original Locations Turn the asc desc array into an array of strings so that we can pass it to the Find Quantiles function List Boolean asc Cols inp Sort get Asc Cols String ascs new String asc Cols size for int i i asc Cols size i ascs i asc Cols get i to String check if user defined comparator is used in the sort if so prepend the name of the comparator as the first fields in the constructor args array to the Find Quantiles udf String ctor Args ascs if sort is Comparator Used String user Comparator Func Spec sort get Sort Func get Func Spec to String ctor Args new String ascs length ctor Args user Comparator Func Spec for int j j ascs length j ctor Args j ascs j return get Sampling Job sort prev Job null l File quant File rp null Find Quantiles class get Name ctor Args Random Sample Loader class get Name Create Sampling job for skewed join private Pair Map Reduce Oper Integer get Skewed Join Sample Job Skewed Join op Map Reduce Oper prev Job File Spec l File File Spec sample File int rp throws Plan Exception Visitor Exception Multi Map Physical Operator Physical Plan join Plans op get Join Plans List Physical Operator l plan get Predecessors op List Physical Plan groups join Plans get l get List Boolean asc Col new Array List Boolean for int i i groups size i asc Col add false Sort sort new Sort op get Operator Key op get Requested Parallelism null groups asc Col null set up transform plan to get keys and memory size of input tuples it first adds all the plans to get key columns List Physical Plan transform Plans new Array List Physical Plan transform Plans add All groups then it adds a column for memory size Project prj Star new Project new Operator Key scope nig get Next Node Id scope prj Star set Result Type Data Type prj Star set Star true List Physical Operator uf Inps new Array List Physical Operator uf Inps add prj Star Physical Plan ep new Physical Plan User Func uf new User Func new Operator Key scope nig get Next Node Id scope uf Inps new Func Spec Get Mem Num Rows class get Name String null uf set Result Type Data Type ep add uf ep add prj Star ep connect prj Star uf transform Plans add ep try pass configurations to the User Function String per pig Context get Properties get Property pig skewedjoin reduce memusage String value Of Partition Skewed Keys String mc pig Context get Properties get Property pig skewedjoin reduce maxtuple String input File l File get File Name return get Sampling Job sort prev Job transform Plans l File sample File rp null Partition Skewed Keys class get Name new String per mc input File Poisson Sample Loader class get Name catch Exception e throw new Plan Exception e Create a sampling job to collect statistics by sampling an input file The sequence of operations is as following li Transform input sample tuples into another tuple li li Add an extra field quot all quot into the tuple li li Package all tuples into one bag li li Add constant field for number of reducers li li Sorting the bag li li Invoke with the number of reducers and the sorted bag li li Data generated by is stored into a file li param sort the Sort operator used to sort the bag param prev Job previous job of current sampling job param transform Plans Physical Plans to transform input samples param l File path of input file param sample File path of output file param rp configured parallemism param sort Key Plans Physical Plans to be set into Sort operator to get sorting keys param udf Class Name the class name of param udf Args the arguments of param sample Ldr Class Name class name for the sample loader return pair mapreduceoper integer throws Plan Exception throws Visitor Exception Suppress Warnings deprecation private Pair Map Reduce Oper Integer get Sampling Job Sort sort Map Reduce Oper prev Job List Physical Plan transform Plans File Spec l File File Spec sample File int rp List Physical Plan sort Key Plans String udf Class Name String udf Args String sample Ldr Class Name throws Plan Exception Visitor Exception String rslargs new String Sample Loader expects string version of Func Spec as its first constructor argument rslargs new Func Spec Utils get Tmp File Compressor Name pig Context to String This value is only used by order by For skewed join it s calculated based on the file size rslargs pig Context get Properties get Property Pig Configuration File Spec quant Ld Fil Name new File Spec l File get File Name new Func Spec sample Ldr Class Name rslargs Map Reduce Oper mro start New quant Ld Fil Name prev Job if sort is Comparator Used mro Fs add sort get Sort Func get Func Spec to String cur Op is Comparator Used true List Boolean flat new Array List Boolean List Physical Plan eps new Array List Physical Plan if transform plans are not specified project the columns of sorting keys if transform Plans null Pair Project Byte sort Projs null try sort Projs get Sort Cols sort get Sort Plans catch Exception e throw new Runtime Exception e Set up the projections of the key columns if sort Projs null Physical Plan ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Star true prj set Overloaded false prj set Result Type Data Type ep add prj eps add ep flat add false else for Pair Project Byte sort Proj sort Projs Check for proj being null null is used by get Sort Cols for a non Project operator Since Order by does not allow expression operators it should never be set to null if sort Proj null int err Code String msg Internal exception Could not create a sampler job throw new Compiler Exception msg err Code Pig Exception Physical Plan ep new Physical Plan Project prj try prj sort Proj first clone catch Clone Not Supported Exception e should not get here throw new Assertion Error Error cloning project caught exception e ep add prj eps add ep flat add false else for int i i transform Plans size i eps add transform Plans get i flat add i transform Plans size true false This foreach will pick the sort key columns from the Random Sample Loader output For Each nfe new For Each new Operator Key scope nig get Next Node Id scope eps flat mro map Plan add As Leaf nfe Now set up a Local Rearrange which has all as the key and the output of the foreach will be the value out of Local Rearrange Physical Plan ep new Physical Plan Constant Expression ce new Constant Expression new Operator Key scope nig get Next Node Id scope ce set Value all ce set Result Type Data Type ep add ce List Physical Plan eps new Array List Physical Plan eps add ep Local Rearrange lr new Local Rearrange new Operator Key scope nig get Next Node Id scope try lr set Index catch Exec Exception e int err Code String msg Unable to set index on newly created Local Rearrange throw new Plan Exception msg err Code Pig Exception e lr set Key Type Data Type lr set Plans eps lr set Result Type Data Type lr add Original Location sort get Alias sort get Original Locations mro map Plan add lr mro map Plan connect nfe lr mro set Map Done true Package pkg new Package new Operator Key scope nig get Next Node Id scope Packager pkgr new Packager pkg set Pkgr pkgr pkgr set Key Type Data Type pkg set Num Inps boolean inner false pkgr set Inner inner mro reduce Plan add pkg Lets start building the plan which will have the sort for the foreach Physical Plan fe Plan new Physical Plan Top level project which just projects the tuple which is coming from the foreach after the package Project top Prj new Project new Operator Key scope nig get Next Node Id scope top Prj set Column top Prj set Result Type Data Type top Prj set Overloaded true fe Plan add top Prj the projections which will form sort plans List Physical Plan nes Sort Plan Lst new Array List Physical Plan if sort Key Plans null for int i i sort Key Plans size i nes Sort Plan Lst add sort Key Plans get i else Pair Project Byte sort Projs null try sort Projs get Sort Cols sort get Sort Plans catch Exception e throw new Runtime Exception e Set up the projections of the key columns if sort Projs null Physical Plan ep new Physical Plan Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Star true prj set Overloaded false prj set Result Type Data Type ep add prj nes Sort Plan Lst add ep else for int i i sort Projs length i Project prj new Project new Operator Key scope nig get Next Node Id scope prj set Result Type sort Projs i second if sort Projs i first null sort Projs i first is Project To End if i sort Projs length project to end has to be the last sort column throw new Assertion Error Project range to end x is supported in order by only as last sort column prj set Project To End i break else prj set Column i prj set Overloaded false Physical Plan ep new Physical Plan ep add prj nes Sort Plan Lst add ep sort set Sort Plans nes Sort Plan Lst sort set Result Type Data Type fe Plan add sort fe Plan connect top Prj sort The plan which will have a constant representing the degree of parallelism for the final order by map reduce job this will either come from a order by parallel x in the script or will be the default number of reducers for the cluster if parallel x is not used in the script Physical Plan rpep new Physical Plan Constant Expression rpce new Constant Expression new Operator Key scope nig get Next Node Id scope rpce set Requested Parallelism rp We temporarily set it to rp and will adjust it at runtime because the final degree of parallelism is unknown until we are ready to submit it See rpce set Value rp rpce set Result Type Data Type rpep add rpce List Physical Plan gen Eps new Array List Physical Plan gen Eps add rpep gen Eps add fe Plan List Boolean flattened new Array List Boolean flattened add false flattened add false For Each nfe new For Each new Operator Key scope nig get Next Node Id scope gen Eps flattened mro reduce Plan add nfe mro reduce Plan connect pkg nfe Let s connect the output from the foreach containing number of quantiles and the sorted bag of samples to another foreach with the Find Quantiles udf The input to the Find Quantiles udf is a project which takes the foreach input and gives it to the udf Physical Plan ep new Physical Plan Project prj Star new Project new Operator Key scope nig get Next Node Id scope prj Star set Result Type Data Type prj Star set Star true ep add prj Star List Physical Operator uf Inps new Array List Physical Operator uf Inps add prj Star User Func uf new User Func new Operator Key scope nig get Next Node Id scope uf Inps new Func Spec udf Class Name udf Args ep add uf ep connect prj Star uf List Physical Plan ep s new Array List Physical Plan ep s add ep List Boolean flattened new Array List Boolean flattened add false For Each nfe new For Each new Operator Key scope nig get Next Node Id scope ep s flattened mro reduce Plan add nfe mro reduce Plan connect nfe nfe Store str get Store str set File sample File mro reduce Plan add str mro reduce Plan connect nfe str mro set Reduce Done true mro requested Parallelism mro mark Sampler return new Pair Map Reduce Oper Integer mro rp static class Last Input Streaming Optimizer extends Op Plan Visitor String chunk Size Last Input Streaming Optimizer Oper Plan plan String chunk Size super plan new Depth First Walker Map Reduce Oper Oper Plan plan this chunk Size chunk Size ind Tup Iter Look for pattern Package For Each if both are flatten change it to Join Package We can avoid materialize the input and construct the result of join on the fly param mr map reduce plan to optimize Override public void visit Op Map Reduce Oper mr throws Visitor Exception Only optimize Package For Each is the root of reduce plan Union is the leaf of map plan so that we exclude distinct sort No combiner plan For Each nested plan only contains Project in any depth Inside For Each all occurrences of the last input are flattened if mr map Plan is Empty return if mr reduce Plan is Empty return Check combiner plan if mr combine Plan is Empty return Check map plan List Physical Operator mp Leaves mr map Plan get Leaves if mp Leaves size return Physical Operator op mp Leaves get if op instanceof Union return Check reduce plan List Physical Operator mr Roots mr reduce Plan get Roots if mr Roots size return op mr Roots get if op instanceof Package return Package pack Package op List Physical Operator sucs mr reduce Plan get Successors pack if sucs null sucs size return op sucs get boolean last Input Flattened true boolean all Simple true if op instanceof For Each For Each for Each For Each op List Physical Plan plan List for Each get Input Plans List Boolean flatten for Each get To Be Flattened Project proj Of Last Input null int i check all nested foreach plans If it is simple projection If last input is all flattened for Physical Plan p plan List Physical Operator op Proj p get Roots get if op Proj instanceof Project all Simple false break Project proj Project op Proj the project should just be for one column from the input if proj is Project To End proj get Columns size all Simple false break try if input to project is the last input if proj get Column pack get Num Inps if we had already seen another project which was also for the last input then we might be trying to flatten twice on the last input in which case we ca n t optimize by just streaming the tuple to those projects if proj Of Last Input null all Simple false break proj Of Last Input proj make sure the project is on a bag which needs to be flattened if flatten get i proj get Result Type Data Type last Input Flattened false break catch Exec Exception e int err Code String msg Error during map reduce compilation Problem in accessing column from project operator throw new Compiler Exception msg err Code Pig Exception e if all deeper operators are all project Physical Operator succ p get Successors proj null p get Successors proj get null while succ null if succ instanceof Project all Simple false break make sure successors of the last project also project bags we will be changing it to project tuples if proj proj Of Last Input Project succ get Result Type Data Type all Simple false break succ p get Successors succ null p get Successors succ get null i if all Simple false break if last Input Flattened all Simple proj Of Last Input null Now we can optimize the map reduce plan Replace Package Foreach to Join Package replace With Join Package mr reduce Plan mr pack for Each chunk Size public static void replace With Join Package Physical Plan plan Map Reduce Oper mr Package pack For Each for Each String chunk Size throws Visitor Exception Join Packager pkgr new Join Packager pack get Pkgr for Each pkgr set Chunk Size Long parse Long chunk Size pack set Pkgr pkgr List Physical Operator succs plan get Successors for Each if succs null if succs size int err Code String msg For Each can only have one successor Found succs size successors throw new Compiler Exception msg err Code Pig Exception plan remove pack try plan replace for Each pack catch Plan Exception e int err Code String msg Error rewriting join package throw new Compiler Exception msg err Code Pig Exception e mr phy To Map put for Each pack Log Factory get Log Last Input Streaming Optimizer class info Rewrite Package For Each to Package Join Packager private static class Find Key Type Visitor extends Phy Plan Visitor byte key Type Data Type Find Key Type Visitor Physical Plan plan super plan new Depth First Walker Physical Operator Physical Plan plan Override public void visit Project Project p throws Visitor Exception key Type p get Result Type private static class Find Store Name Visitor extends Phy Plan Visitor File Spec new Spec File Spec old Spec Find Store Name Visitor Physical Plan plan File Spec new Spec File Spec old Spec super plan new Depth First Walker Physical Operator Physical Plan plan this new Spec new Spec this old Spec old Spec Override public void visit Store Store sto throws Visitor Exception File Spec spec sto get File if old Spec equals spec sto set File new Spec 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util Properties import org apache pig Exec Type import org apache pig backend executionengine Execution Engine import org apache pig impl Pig Context Exec Type is the Exec Type for distributed mode in Hadoop Mapreduce public class Exec Type implements Exec Type private static final long serial Version private static final String modes Override public boolean accepts Properties properties String exec Type Specified properties get Property exectype to Upper Case for String mode modes if exec Type Specified equals mode return true return false Override public Execution Engine get Execution Engine Pig Context pig Context return new Execution Engine pig Context Override public Class extends Execution Engine get Execution Engine Class return Execution Engine class Override public boolean is Local return false Override public String name return public String to String return name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util import org apache pig backend hadoop executionengine Execution Engine import org apache pig impl Pig Context import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Script State import org apache pig tools pigstats mapreduce Script State import org apache pig tools pigstats mapreduce Simple Pig Stats public class Execution Engine extends Execution Engine public Execution Engine Pig Context pig Context super pig Context this launcher new Map Reduce Launcher Override public Script State instantiate Script State Script State ss new Script State random to String ss set Pig Context pig Context return ss Override public Pig Stats instantiate Pig Stats return new Simple Pig Stats 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats mapreduce import java io Exception import java util Array List import java util Collections import java util Hash Map import java util Iterator import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop mapred Counters import org apache hadoop mapred Counters Counter import org apache hadoop mapred Job import org apache hadoop mapreduce Cluster import org apache hadoop mapreduce Task Report import org apache hadoop mapred jobcontrol Job import org apache hadoop mapreduce Task Type import org apache pig Pig Configuration import org apache pig Pig Counters import org apache pig backend hadoop executionengine map Reduce Layer Job Control Compiler import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Object Serializer import org apache pig newplan Plan Visitor import org apache pig tools pigstats Input Stats import org apache pig tools pigstats Job Stats import org apache pig tools pigstats Output Stats import org apache pig tools pigstats Pig Stats Job Graph import org apache pig tools pigstats Pig Stats Job Graph Printer import org python google common collect Lists This class encapsulates the runtime statistics of a Map Reduce job Job statistics is collected when job is completed Interface Audience Public Interface Stability Evolving public final class Job Stats extends Job Stats Job Stats String name Job Graph plan super name plan public static final String Job Id t Maps t Reduces t Max Map Time t Min Map Time t Avg Map Time t Median Map Time t Max Reduce Time t Min Reduce Time t Avg Reduce Time t Median Reducetime t Alias t Feature t Outputs public static final String Job Id t Alias t Feature t Message t Outputs private static final Log Log Factory get Log Job Stats class private List Store map Stores null private List Store reduce Stores null private List File Spec loads null private Boolean disable Counter false private Job job Id private long max Map Time private long min Map Time private long avg Map Time private long median Map Time private long max Reduce Time private long min Reduce Time private long avg Reduce Time private long median Reduce Time private int number Maps private int number Reduces private long map Input Records private long map Output Records private long reduce Input Records private long reduce Output Records private long spill Count private long active Spill Count Obj private long active Spill Count Recs private Hash Map String Long multi Store Counters new Hash Map String Long private Hash Map String Long multi Input Counters new Hash Map String Long private Counters counters null Override public String get Job Id return job Id null null job Id to String Override public int get Number Maps return number Maps Override public int get Number Reduces return number Reduces Override public long get Max Map Time return max Map Time Override public long get Min Map Time return min Map Time Override public long get Avg Map Time return avg Map Time Override public long get Max Reduce Time return max Reduce Time Override public long get Min Reduce Time return min Reduce Time Override public long get Avg Educe Time return avg Reduce Time Override public long get Map Input Records return map Input Records Override public long get Map Output Records return map Output Records Override public long get Reduce Input Records return reduce Input Records Override public long get Reduce Output Records return reduce Output Records Override public long get Spill Count return spill Count Override public long get Proactive Spill Count Objects return active Spill Count Obj Override public long get Proactive Spill Count Recs return active Spill Count Recs Override public Counters get Hadoop Counters return counters Override public Map String Long get Multi Store Counters return Collections unmodifiable Map multi Store Counters Override public Map String Long get Multi Input Counters return Collections unmodifiable Map multi Input Counters Override public String get Alias return String get Annotation Override public String get Alias Location return String get Annotation Override public String get Feature return String get Annotation Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Job Graph Printer Job Graph Printer jpp Job Graph Printer v jpp visit this void set Id Job job Id this job Id job Id Override Suppress Warnings unchecked public void set Conf Configuration conf super set Conf conf try this map Stores List Store Object Serializer deserialize conf get Job Control Compiler this reduce Stores List Store Object Serializer deserialize conf get Job Control Compiler this loads Array List File Spec Object Serializer deserialize conf get pig inputs this disable Counter conf get Boolean pig disable counter false catch Exception e warn Failed to deserialize the store list e void set Map Stat int size long max long min long avg long median number Maps size max Map Time max min Map Time min avg Map Time avg median Map Time median void set Reduce Stat int size long max long min long avg long median number Reduces size max Reduce Time max min Reduce Time min avg Reduce Time avg median Reduce Time median private static void append Stat long stat String Builder sb if stat sb append stat else sb append n a sb append t Override public String get Display String String Builder sb new String Builder String id job Id null job Id to String if state Job State sb append id append t append get Alias append t append get Feature append t if state Job State sb append Message append get Error Message append t else if state Job State sb append id append t append number Maps append t append number Reduces append t append Stat max Map Time sb append Stat min Map Time sb append Stat avg Map Time sb append Stat median Map Time sb append Stat max Reduce Time sb append Stat min Reduce Time sb append Stat avg Reduce Time sb append Stat median Reduce Time sb sb append get Alias append t append get Feature append t for Output Stats os outputs sb append os get Location append sb append n return sb to String void add Counters Job job try counters get Counters job catch Exception e warn Unable to get job counters e if counters null Counters Group taskgroup counters get Group Pig Stats Util Counters Group hdfsgroup counters get Group Pig Stats Util Counters Group multistoregroup counters get Group Pig Stats Util Counters Group multiloadgroup counters get Group Pig Stats Util map Input Records taskgroup get Counter For Name Pig Stats Util get Counter map Output Records taskgroup get Counter For Name Pig Stats Util get Counter reduce Input Records taskgroup get Counter For Name Pig Stats Util get Counter reduce Output Records taskgroup get Counter For Name Pig Stats Util get Counter hdfs Bytes Read hdfsgroup get Counter For Name Pig Stats Util get Counter hdfs Bytes Written hdfsgroup get Counter For Name Pig Stats Util get Counter spill Count counters find Counter Pig Counters get Counter active Spill Count Obj counters find Counter Pig Counters get Counter active Spill Count Recs counters find Counter Pig Counters get Counter Iterator Counter iter multistoregroup iterator while iter has Next Counter cter iter next multi Store Counters put cter get Name cter get Value Iterator Counter iter multiloadgroup iterator while iter has Next Counter cter iter next multi Input Counters put cter get Name cter get Value private class Task Stat int size long max long min long avg long median public Task Stat int size long max long min long avg long median this size size this max max this min min this avg avg this median median void add Map Reduce Statistics Job job Iterator Task Report maps null try maps get Task Reports job Task Type catch Exception e warn Failed to get map task report e Iterator Task Report reduces null try reduces get Task Reports job Task Type catch Exception e warn Failed to get reduce task report e add Map Reduce Statistics maps reduces private Task Stat get Task Stat Iterator Task Report tasks int size long max long min Long long median long total List Long durations new Array List Long while tasks has Next Task Report rpt tasks next long duration rpt get Finish Time rpt get Start Time durations add duration max duration max duration max min duration min duration min total duration size long avg total size median calculate Median Value durations return new Task Stat size max min avg median private void add Map Reduce Statistics Iterator Task Report maps Iterator Task Report reduces if maps null maps has Next Task Stat st get Task Stat maps set Map Stat st size st max st min st avg st median else int m conf get Int Configuration if m set Map Stat m if reduces null reduces has Next Task Stat st get Task Stat reduces set Reduce Stat st size st max st min st avg st median else int m conf get Int Configuration if m set Reduce Stat m void set Alias Map Reduce Oper mro Script State ss Script State get annotate ss get Alias mro annotate ss get Alias Location mro annotate ss get Pig Feature mro void add Output Statistics if map Stores null reduce Stores null warn unable to get stores of the job return if map Stores size reduce Stores size Store sto map Stores size map Stores get reduce Stores get if sto is Tmp Store long records map Stores size map Output Records reduce Output Records Output Stats ds new Output Stats sto get File get File Name hdfs Bytes Written records state Job State ds set Store sto ds set Conf conf outputs add ds if state Job State Script State get emit Output Completed Notification ds else for Store sto map Stores if sto is Tmp Store continue add One Output Stats sto for Store sto reduce Stores if sto is Tmp Store continue add One Output Stats sto private void add One Output Stats Store sto long records if sto is Multi Store Long n multi Store Counters get Pig Stats Util get Multi Store Counter Name sto if n null records n else records map Output Records long bytes get Output Size sto conf String location sto get File get File Name Output Stats ds new Output Stats location bytes records state Job State ds set Store sto ds set Conf conf outputs add ds if state Job State Script State get emit Output Completed Notification ds void add Input Statistics if loads null warn unable to get inputs of the job return if loads size File Spec fsp loads get if Pig Stats Util is Temp File fsp get File Name long records map Input Records Input Stats is new Input Stats fsp get File Name hdfs Bytes Read records state Job State is set Conf conf if is Sampler is mark Sample Input if is Indexer is mark Indexer Input inputs add is else for int i i loads size i File Spec fsp loads get i if Pig Stats Util is Temp File fsp get File Name continue add One Input Stats fsp get File Name i private void add One Input Stats String file Name int index long records Long n multi Input Counters get Pig Stats Util get Multi Inputs Counter Name file Name index if n null records n else the file could be empty if disable Counter records else warn unable to get input counter for file Name Input Stats is new Input Stats file Name records state Job State is set Conf conf inputs add is public static Iterator Task Report get Task Reports Job job Task Type type throws Exception if job get Job Conf get Boolean Pig Configuration false info Task Reports are disabled for job job get Assigned Job return null Cluster cluster new Cluster job get Job Conf try org apache hadoop mapreduce Job mr Job cluster get Job job get Assigned Job if mr Job null In local mode mr Job will be null mr Job job get Job org apache hadoop mapreduce Task Report reports mr Job get Task Reports type return Lists new Array List reports iterator catch Interrupted Exception ir throw new Exception ir public static Counters get Counters Job job throws Exception try Cluster cluster new Cluster job get Job Conf org apache hadoop mapreduce Job mr Job cluster get Job job get Assigned Job if mr Job null In local mode mr Job will be null mr Job job get Job return new Counters mr Job get Counters catch Exception ir throw new Exception ir 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats mapreduce import java io Exception import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop mapred Counters import org apache hadoop mapred Job Client import org apache hadoop mapred Running Job import org apache hadoop mapred jobcontrol Job import org apache hadoop mapred jobcontrol Job Control import org apache hadoop mapreduce Job import org apache pig backend hadoop executionengine map Reduce Layer Job Control Compiler import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer Native Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig classification Interface Audience Private import org apache pig impl Pig Context import org apache pig tools pigstats Job Stats import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Pig Stats Job Graph import org apache pig tools pigstats Pig Stats Util utility class for Pig Statistics public class Pig Stats Util extends Pig Stats Util public static final String org apache hadoop mapred Task Counter public static final String org apache hadoop mapreduce File System Counter private static final Log Log Factory get Log Pig Stats Util class Returns the count for the given counter name in the counter group Multi Store Counters param job the job param job Client the Hadoop job client param counter Name the counter name return the count of the given counter name public static long get Multi Store Count Job job Job Client job Client String counter Name long value try Running Job rj job Client get Job job get Assigned Job if rj null Counters Counter counter rj get Counters get Group get Counter For Name counter Name value counter get Value catch Exception e warn Failed to get the counter for counter Name e return value Starts collecting statistics for the given plan param pc the Pig context param client the Hadoop job client param jcc the job compiler param plan the plan public static void start Collection Pig Context pc Job Client client Job Control Compiler jcc Oper Plan plan Simple Pig Stats ps Simple Pig Stats Pig Stats get ps initialize pc client jcc plan Script State get emit Initial Plan Notification plan Script State get emit Launch Started Notification plan size Stops collecting statistics for a plan param display if true log collected statistics in the Pig log file at level public static void stop Collection boolean display Simple Pig Stats ps Simple Pig Stats Pig Stats get ps finish if ps is Successful error ps get Number Failed Jobs map reduce job s failed String err Msg ps get Error Message if err Msg null error Error message err Msg Script State get emit Launch Completed Notification ps get Number Successful Jobs if display ps display Add stats for a new Job which does n t yet need to be completed param job the job being run return Job Stats for the job public static Job Stats add Job Stats Job job Simple Pig Stats ps Simple Pig Stats Pig Stats get return ps add Job Stats job Logs the statistics in the Pig log file at level public static void display Statistics Simple Pig Stats Pig Stats get display Updates the link Job Graph of the link Pig Stats The initial link Job Graph is created without job ids using link Oper Plan before any job is submitted for execution The link Job Graph then is updated with job ids after jobs are executed param job Mro Map the map that maps link Job s to link Map Reduce Oper s public static void update Job Mro Map Map Job Map Reduce Oper job Mro Map Simple Pig Stats ps Simple Pig Stats Pig Stats get for Map Entry Job Map Reduce Oper entry job Mro Map entry Set Map Reduce Oper mro entry get Value ps map Oper To Job mro entry get Key Updates the statistics after a patch of jobs is done param jc the job control public static void accumulate Stats Job Control jc Simple Pig Stats ps Simple Pig Stats Pig Stats get Script State ss Script State get for Job job jc get Successful Jobs Job Stats js add Success Job Stats ps job if js null ss emitjob Finished Notification js for Job job jc get Failed Jobs Job Stats js add Failed Job Stats ps job if js null js set Error Msg job get Message ss emit Job Failed Notification js Private public static void set Backend Exception Job job Exception e Job job Id job get Assigned Job if job Id null return Pig Stats get set Backend Exception job Id to String e private static Job Stats add Failed Job Stats Simple Pig Stats ps Job job if ps is Job Seen job return null Job Stats js ps add Job Stats job if js null warn unable to add failed job stats else js set Successful false js add Output Statistics js add Input Statistics return js public static Job Stats add Native Job Stats Pig Stats ps Native Map Reduce Oper mr boolean success return add Native Job Stats ps mr success null public static Job Stats add Native Job Stats Pig Stats ps Native Map Reduce Oper mr boolean success Exception e if ps is Embedded throw new Illegal Argument Exception Job Stats js Simple Pig Stats ps add Job Stats For Native mr if js null warn unable to add native job stats else js set Successful success if e null js set Backend Exception e return js private static Job Stats add Success Job Stats Simple Pig Stats ps Job job if ps is Job Seen job return null Job Stats js ps add Job Stats job if js null warn unable to add job stats else js set Successful true js add Map Reduce Statistics job js add Counters job js add Output Statistics js add Input Statistics return js 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer plans import java util Array List import java util Iterator import java util List import java io Print Stream import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer Native Map Reduce Oper import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer plans Plan Printer import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Visitor Exception visitor mechanism printing out the logical plan public class Printer extends Op Plan Visitor private Print Stream m Stream null private boolean is Verbose true param ps Print Stream to output plan information to param plan plan to print public Printer Print Stream ps Oper Plan plan super plan new Dependency Order Walker Map Reduce Oper Oper Plan plan true m Stream ps m Stream println m Stream println Map Reduce Plan m Stream println public void set Verbose boolean verbose is Verbose verbose Override public void visit Op Map Reduce Oper mr throws Visitor Exception m Stream println Map Reduce node mr get Operator Key to String if mr instanceof Native Map Reduce Oper m Stream println Native Map Reduce Oper mr get Command String m Stream println m Stream println return if mr map Plan null mr map Plan size m Stream println Map Plan Plan Printer Physical Operator Physical Plan printer new Plan Printer Physical Operator Physical Plan mr map Plan m Stream printer set Verbose is Verbose printer visit m Stream println if mr combine Plan null mr combine Plan size m Stream println Combine Plan Plan Printer Physical Operator Physical Plan printer new Plan Printer Physical Operator Physical Plan mr combine Plan m Stream printer set Verbose is Verbose printer visit m Stream println if mr reduce Plan null mr reduce Plan size m Stream println Reduce Plan Plan Printer Physical Operator Physical Plan printer new Plan Printer Physical Operator Physical Plan mr reduce Plan m Stream printer set Verbose is Verbose printer visit m Stream println m Stream println Global sort mr is Global Sort if mr get Quant File null m Stream println Quantile file mr get Quant File if mr get Use Secondary Key m Stream println Secondary sort mr get Use Secondary Key m Stream println m Stream println 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats mapreduce import java util Array List import java util Bit Set import java util Collections import java util Hash Map import java util Iterator import java util Linked List import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache pig Load Func import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer Native Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig impl plan Visitor Exception import org apache pig newplan Operator import org apache pig tools pigstats Pig Stats Job Graph import org apache pig tools pigstats Script State import org apache pig tools pigstats Job Stats import org apache pig tools pigstats Pig Stats import com google common collect Maps Script States encapsulates settings for a Pig script that runs on a hadoop cluster These settings are added to all jobs spawned by the script and in turn are persisted in the hadoop job xml With the properties already in the job xml users who want to know the relations between the script and jobs can derive them from the job xmls public class Script State extends Script State private static final Log Log Factory get Log Script State class private Map Map Reduce Oper String feature Map null private Map Map Reduce Oper String alias Map Maps new Hash Map private Map Map Reduce Oper String alias Location Map Maps new Hash Map public Script State String id super id public static Script State get return Script State Script State get public void add Settings To Conf Map Reduce Oper mro Configuration conf info Pig script settings are added to the job conf set to String get Hadoop Version conf set to String get Pig Version conf set to String id conf set to String get Serialized Script conf set to String get Command Line try Linked List Store stores Plan Helper get Physical Operators mro map Plan Store class Array List String output Dirs new Array List String for Store st stores output Dirs add st get File get File Name conf set to String Load Func join output Dirs catch Visitor Exception e warn unable to get the map stores e if mro reduce Plan is Empty try Linked List Store stores Plan Helper get Physical Operators mro reduce Plan Store class Array List String output Dirs new Array List String for Store st stores output Dirs add st get File get File Name conf set to String Load Func join output Dirs catch Visitor Exception e warn unable to get the reduce stores e try List Load lds Plan Helper get Physical Operators mro map Plan Load class Array List String input Dirs new Array List String if lds null lds size for Load ld lds input Dirs add ld get File get File Name conf set to String Load Func join input Dirs catch Visitor Exception e warn unable to get the map loads e set Pig Feature mro conf set Job Parents mro conf conf set mapreduce workflow id pig id conf set mapreduce workflow name get File Name is Empty default get File Name conf set mapreduce workflow node name mro get Operator Key to String public void add Workflow Adjacencies To Conf Oper Plan mrop Configuration conf for Map Reduce Oper source mrop List String targets new Array List String if mrop get Successors source null for Map Reduce Oper target mrop get Successors source targets add target get Operator Key to String String s new String targets size conf set Strings mapreduce workflow adjacency source get Operator Key to String targets to Array s private void set Pig Feature Map Reduce Oper mro Configuration conf conf set to String get Pig Feature mro if script Features conf set to String String value Of script Features conf set to String get Alias mro conf set to String get Alias Location mro private void set Job Parents Map Reduce Oper mro Configuration conf Pig Stats maintains a job with the job id being updated upon available Therefore before a job is submitted the ids of its parent jobs are already available Job Graph jg Pig Stats get get Job Graph Job Stats js null Iterator Job Stats iter jg iterator while iter has Next Job Stats job iter next if job get Name equals mro get Operator Key to String js job break if js null List Operator preds jg get Predecessors js if preds null String Builder sb new String Builder for Operator op preds Job Stats job Job Stats op if sb length sb append sb append job get Job Id conf set to String sb to String public String get Alias Map Reduce Oper mro if alias Map contains Key mro set Alias mro return alias Map get mro private void set Alias Map Reduce Oper mro Array List String alias new Array List String String alias Location Str try Array List String alias Location new Array List String new Alias Visitor mro map Plan alias alias Location visit alias Location Str Load Func join alias Location if mro combine Plan null alias Location new Array List String new Alias Visitor mro combine Plan alias alias Location visit alias Location Str Load Func join alias Location alias Location new Array List String new Alias Visitor mro reduce Plan alias alias Location visit alias Location Str Load Func join alias Location if alias is Empty Collections sort alias catch Visitor Exception e warn unable to get alias e alias Map put mro Load Func join alias alias Location Map put mro alias Location Str public String get Alias Location Map Reduce Oper mro if alias Location Map contains Key mro set Alias mro return alias Location Map get mro public String get Pig Feature Map Reduce Oper mro if feature Map null feature Map new Hash Map Map Reduce Oper String String ret Str feature Map get mro if ret Str null Bit Set feature new Bit Set feature clear if mro is Skewed Join feature set ordinal if mro is Global Sort feature set ordinal if mro is Sampler feature set ordinal if mro is Indexer feature set ordinal if mro is Cogroup feature set ordinal if mro is Group By feature set ordinal if mro is Regular Join feature set ordinal if mro needs Distinct Combiner feature set ordinal if mro combine Plan is Empty feature set ordinal if mro instanceof Native Map Reduce Oper feature set ordinal else if it is do n t explore its plans try new Feature Visitor mro map Plan feature visit if mro reduce Plan is Empty feature set ordinal else new Feature Visitor mro reduce Plan feature visit catch Visitor Exception e warn Feature visitor failed e String Builder sb new String Builder for int i feature next Set Bit i i feature next Set Bit i if sb length sb append sb append values i name ret Str sb to String feature Map put mro ret Str return ret Str 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java math Big Decimal import java math Big Integer import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Multiply extends Binary Expression Operator private static final long serial Version public Multiply Operator Key k super k public Multiply Operator Key k int rp super k rp Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Multiply this Override public String name return Multiply Data Type find Type Name result Type m Key to String protected Number multiply Number a Number b byte data Type throws Exec Exception switch data Type case Data Type return Double value Of Double a Double b case Data Type return Integer value Of Integer a Integer b case Data Type return Long value Of Long a Long b case Data Type return Float value Of Float a Float b case Data Type return Big Integer a multiply Big Integer b case Data Type return Big Decimal a multiply Big Decimal b default throw new Exec Exception called on unsupported Number class Data Type find Type Name data Type protected Result generic Get Next byte data Type throws Exec Exception Result r accum Child null data Type if r null return r byte status Result res res lhs get Next data Type status res return Status if status Status res result null return res Number left Number res result res rhs get Next data Type status res return Status if status Status res result null return res Number right Number res result res result multiply left right data Type return res Override public Result get Next Double throws Exec Exception return generic Get Next Data Type Override public Result get Next Float throws Exec Exception return generic Get Next Data Type Override public Result get Next Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Long throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Decimal throws Exec Exception return generic Get Next Data Type Override public Multiply clone throws Clone Not Supported Exception Multiply clone new Multiply new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Multiply Operator public class Multiply Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Multiply Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Multiply plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Multiply Expression Multiply Expression ao Multiply Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null get Lhs get Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Multiply Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Negative Expression extends Unary Expression public Negative Expression Operator Plan plan Logical Expression exp super Negative plan exp Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Negative Expression Negative Expression of Negative Expression other return get Expression is Equal of get Expression else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema Logical Expression exp Logical Expression plan get Successors this get field Schema new Logical Schema Logical Field Schema null null exp get Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Negative Expression lg Exp Plan this get Expression deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Not Equality test expression public class Not Equal Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Not Equal Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Not Equal plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Not Equal Expression Not Equal Expression eo Not Equal Expression other return eo get Lhs is Equal get Lhs eo get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Not Equal Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Hash Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Not Equal To Expr extends Binary Comparison Operator private static final long serial Version transient private final Log log Log Factory get Log get Class public Not Equal To Expr Operator Key k this k public Not Equal To Expr Operator Key k int rp super k rp result Type Data Type Override public String name return Not Equal To Data Type find Type Name result Type m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Not Equal To this Override public Result get Next Boolean throws Exec Exception Result left right switch operand Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type Result r accum Child null operand Type if r null return r left lhs get Next operand Type right rhs get Next operand Type return do Comparison left right default int err Code String msg this get Class get Simple Name does not know how to handle type Data Type find Type Name operand Type throw new Exec Exception msg err Code Pig Exception Suppress Warnings unchecked private Result do Comparison Result left Result right throws Exec Exception if left return Status Status return left if right return Status Status return right if either operand is null the result should be null if left result null right result null left result null left return Status Status return left if left result instanceof Comparable right result instanceof Comparable if Comparable left result compare To right result left result Boolean else left result Boolean else if left result instanceof Hash Map right result instanceof Hash Map Hash Map left Map Hash Map left result Hash Map right Map Hash Map right result if left Map equals right Map left result Boolean else left result Boolean else throw new Exec Exception The left side and right side has the different types illustrator Markup null left result Boolean left result return left Override public Not Equal To Expr clone throws Clone Not Supported Exception Not Equal To Expr clone new Not Equal To Expr new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location public class Not Expression extends Unary Expression public Not Expression Operator Plan plan Logical Expression exp super Not plan exp Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Not Expression Not Expression of Not Expression other return get Expression is Equal of get Expression else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Not Expression lg Exp Plan this get Expression deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan import java util Iterator import java util List import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan logical relational Load import org apache pig newplan logical relational Store An interface that defines graph operations on plans Plans are modeled as graphs with restrictions on the types of connections and operations allowed Interface Audience Private Interface Stability Unstable public interface Operator Plan Get number of nodes in the plan return number of nodes in the plan public int size Get all operators in the plan that have no predecessors return all operators in the plan that have no predecessors or an empty list if the plan is empty public List Operator get Sources Get all operators in the plan that have no successors return all operators in the plan that have no successors or an empty list if the plan is empty public List Operator get Sinks For a given operator get all operators immediately before it in the plan param op operator to fetch predecessors of return list of all operators immediately before op or an empty list if op is a root public List Operator get Predecessors Operator op For a given operator get all operators immediately after it param op operator to fetch successors of return list of all operators immediately after op or an empty list if op is a leaf public List Operator get Successors Operator op For a given operator get all operators softly immediately before it in the plan param op operator to fetch predecessors of return list of all operators immediately before op or an empty list if op is a root public List Operator get Soft Link Predecessors Operator op For a given operator get all operators softly immediately after it param op operator to fetch successors of return list of all operators immediately after op or an empty list if op is a leaf public List Operator get Soft Link Successors Operator op Add a new operator to the plan It will not be connected to any existing operators param op operator to add public void add Operator op Remove an operator from the plan param op Operator to be removed throws Frontend Exception if the remove operation attempts to remove an operator that is still connected to other operators public void remove Operator op throws Frontend Exception Connect two operators in the plan controlling which position in the edge lists that the from and to edges are placed param from Operator edge will come from param from Pos Position in the array for the from edge param to Operator edge will go to param to Pos Position in the array for the to edge public void connect Operator from int from Pos Operator to int to Pos Connect two operators in the plan param from Operator edge will come from param to Operator edge will go to public void connect Operator from Operator to Create an soft edge between two nodes param from Operator dependent upon param to Operator having the dependency public void create Soft Link Operator from Operator to Remove an soft edge param from Operator dependent upon param to Operator having the dependency public void remove Soft Link Operator from Operator to Disconnect two operators in the plan param from Operator edge is coming from param to Operator edge is going to return pair of positions indicating the position in the from and to arrays throws Frontend Exception if the two operators are n t connected public Pair Integer Integer disconnect Operator from Operator to throws Frontend Exception Get an iterator of all operators in this plan return an iterator of all operators in this plan public Iterator Operator get Operators This is like a shallow comparison Two plans are equal if they have equivalent operators and equivalent structure param other object to compare return boolean if both the plans are equivalent throws Frontend Exception public boolean is Equal Operator Plan other throws Frontend Exception This method replace the old Operator with the new Operator make all connection to the new operator in the place of old operator param old Operator operator to be replaced param new Operator operator to replace throws Frontend Exception public void replace Operator old Operator Operator new Operator throws Frontend Exception This method remove a node operator To Remove It also Connect all its successors to predecessor connect all it s predecessors to successor param operator To Remove operator to remove throws Frontend Exception public void remove And Reconnect Operator operator To Remove throws Frontend Exception This method insert node operator To Insert between pred and succ Both pred and succ can not be null param pred predecessor of inserted node after this method param operator To Insert operato to insert param succ successor of inserted node after this method throws Frontend Exception public void insert Between Operator pred Operator operator To Insert Operator succ throws Frontend Exception check if there is a path in the plan graph between the load operator to the store operator param load load operator param store store operator return true if yes no otherwise public boolean path Exists Operator load Operator store 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java math Big Decimal import java math Big Integer import java sql Timestamp import java util Array List import java util Arrays import java util List import java util Properties import org apache commons cli Command Line import org apache commons cli Command Line Parser import org apache commons cli Gnu Parser import org apache commons cli Help Formatter import org apache commons cli Options import org apache commons cli Parse Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop fs File Status import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop fs Path Filter import org apache hadoop hive conf Hive Conf import org apache hadoop hive ql io orc Compression Kind import org apache hadoop hive ql io orc Orc File import org apache hadoop hive ql io orc Orc New Input Format import org apache hadoop hive ql io orc Orc New Output Format import org apache hadoop hive ql io orc Orc Serde import org apache hadoop hive ql io orc Orc Struct import org apache hadoop hive ql io orc Reader import org apache hadoop hive ql io orc Orc File Version import org apache hadoop hive ql io sarg Search Argument import org apache hadoop hive ql io sarg Search Argument Builder import org apache hadoop hive ql io sarg Search Argument Factory import org apache hadoop hive serde Abstract Ser De import org apache hadoop hive serde Column Projection Utils import org apache hadoop hive serde objectinspector Object Inspector import org apache hadoop hive serde typeinfo Struct Type Info import org apache hadoop hive serde typeinfo Type Info import org apache hadoop hive serde typeinfo Type Info Utils import org apache hadoop hive shims Hadoop Shims import org apache hadoop hive shims Hadoop Shims Secure import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib output File Output Format import org apache pig Expression import org apache pig Expression Between Expression import org apache pig Expression Column import org apache pig Expression Const import org apache pig Expression In Expression import org apache pig Expression Op Type import org apache pig Expression Unary Expression import org apache pig Load Func import org apache pig Load Metadata import org apache pig Load Predicate Pushdown import org apache pig Load Push Down import org apache pig Pig Exception import org apache pig Pig Warning import org apache pig Resource Schema import org apache pig Store Func import org apache pig Expression Binary Expression import org apache pig Resource Schema Resource Field Schema import org apache pig Resource Statistics import org apache pig Store Func Interface import org apache pig Store Resources import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Object Serializer import org apache pig impl util Context import org apache pig impl util Utils import org apache pig impl util hive Hive Utils import org joda time Date Time import com esotericsoftware kryo io Input import com google common annotations Visible For Testing load function and store function for file An optional constructor argument is provided that allows one to customize advanced behaviors list of available options is below ul li code s stripe Size code Set the stripe size for the file li code r row Index Stride code Set the distance between entries in the row index li code b buffer Size code The size of the memory buffers used for compressing and storing the stripe in memory li code p block Padding code Sets whether the blocks are padded to prevent stripes from straddling blocks li code c compress code Sets the generic compression that is used to compress the data Valid codecs are li code v version code Sets the version of the file that will be written ul public class Orc Storage extends Load Func implements Store Func Interface Load Metadata Load Push Down Load Predicate Pushdown Store Resources Make Orc Input Format visible private static final String sarg pushdown protected Record Reader in null protected Record Writer writer null private Type Info type Info null private Object Inspector oi null private Orc Serde serde new Orc Serde private String signature private Long stripe Size private Integer row Index Stride private Integer buffer Size private Boolean block Padding private Compression Kind compress private Version version private static final Options valid Options private final Command Line Parser parser new Gnu Parser protected final static Log log Log Factory get Log Orc Storage class protected boolean m Required Columns null private static final String Schema Signature Suffix schema private static final String Required Columns Suffix columns private static final String Search Args Suffix sarg static valid Options new Options valid Options add Option s stripe Size true Set the stripe size for the file valid Options add Option r row Index Stride true Set the distance between entries in the row index valid Options add Option b buffer Size true The size of the memory buffers used for compressing and storing the stripe in memory valid Options add Option p block Padding false Sets whether the blocks are padded to prevent stripes from straddling blocks valid Options add Option c compress true Sets the generic compression that is used to compress the data valid Options add Option v version true Sets the version of the file that will be written public Orc Storage public Orc Storage String options String opts Arr options split try Command Line configured Options parser parse valid Options opts Arr if configured Options has Option s stripe Size Long parse Long configured Options get Option Value s if configured Options has Option r row Index Stride Integer parse Int configured Options get Option Value r if configured Options has Option b buffer Size Integer parse Int configured Options get Option Value b block Padding configured Options has Option p if configured Options has Option c compress Compression Kind value Of configured Options get Option Value c if configured Options has Option v version Version by Name configured Options get Option Value v catch Parse Exception e log error Exception in Orc Storage e log error Orc Storage called with arguments options warn Parse Exception in Orc Storage Pig Warning Help Formatter formatter new Help Formatter formatter print Help Orc Storage options valid Options throw new Runtime Exception e Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir Override public Output Format get Output Format throws Exception return new Orc New Output Format Override public void set Store Location String location Job job throws Exception if Context get Context is Frontend if stripe Size null job get Configuration set Long Hive Conf Conf Vars varname stripe Size if row Index Stride null job get Configuration set Int Hive Conf Conf Vars varname row Index Stride if buffer Size null job get Configuration set Int Hive Conf Conf Vars varname buffer Size if block Padding null job get Configuration set Boolean Hive Conf Conf Vars varname block Padding if compress null job get Configuration set Hive Conf Conf Vars varname compress to String if version null job get Configuration set Hive Conf Conf Vars varname version get Name File Output Format set Output Path job new Path location if type Info null Properties p Context get Context get Properties this get Class type Info Type Info Object Serializer deserialize p get Property signature Schema Signature Suffix if oi null oi Hive Utils create Object Inspector type Info Override public void check Schema Resource Schema rs throws Exception Resource Field Schema fs new Resource Field Schema fs set Type Data Type fs set Schema rs type Info Hive Utils get Type Info fs Properties p Context get Context get Properties this get Class p set Property signature Schema Signature Suffix Object Serializer serialize type Info Override public void prepare To Write Record Writer writer throws Exception this writer writer Override public void put Next Tuple t throws Exception try writer write null serde serialize t oi catch Interrupted Exception e throw new Exception e Override public void set Store Func Context Signature String signature this signature signature Override public void set Context Signature String signature this signature signature Override public void cleanup On Failure String location Job job throws Exception Store Func cleanup On Failure Impl location job Override public void cleanup On Success String location Job job throws Exception Override public void set Location String location Job job throws Exception Properties p Context get Context get Properties this get Class if Context get Context is Frontend type Info Type Info Object Serializer deserialize p get Property signature Schema Signature Suffix else if type Info null type Info get Type Info location job if type Info null oi null oi Orc Struct create Object Inspector type Info if Context get Context is Frontend if p get Property signature Required Columns Suffix null m Required Columns boolean Object Serializer deserialize p get Property signature Required Columns Suffix job get Configuration set Boolean Column Projection Utils false job get Configuration set Column Projection Utils get Reqired Column Id String m Required Columns if p get Property signature Search Args Suffix null Bug in set Search Argument which always expects to be set job get Configuration set Column Projection Utils get Reqired Column Names String get Schema location job m Required Columns else if p get Property signature Search Args Suffix null Bug in set Search Argument which always expects to be set job get Configuration set Column Projection Utils get Reqired Column Names String get Schema location job if p get Property signature Search Args Suffix null job get Configuration set p get Property signature Search Args Suffix File Input Format set Input Paths job location private String get Reqired Column Id String boolean required Columns String Builder sb new String Builder for int i i required Columns length i if required Columns i sb append i append if sb char At sb length sb delete Char At sb length return sb to String private String get Reqired Column Names String Resource Schema schema String Builder sb new String Builder for Resource Field Schema field schema get Fields sb append field get Name append if sb char At sb length sb delete Char At sb length return sb to String private String get Reqired Column Names String Resource Schema schema boolean required Columns String Builder sb new String Builder Resource Field Schema fields schema get Fields for int i i required Columns length i if required Columns i sb append fields i append if sb char At sb length sb delete Char At sb length return sb to String Override public Input Format get Input Format throws Exception return new Orc New Input Format Override public void prepare To Read Record Reader reader Pig Split split throws Exception in reader Override public Tuple get Next throws Exception try boolean not Done in next Key Value if not Done return null Object value in get Current Value Tuple t Tuple Hive Utils convert Hive To Pig value oi m Required Columns return t catch Interrupted Exception e int err Code String err Msg Error while reading input throw new Exec Exception err Msg err Code Pig Exception e Override public List String get Ship Files Class class List new Class Orc File class Hive Conf class Abstract Ser De class org apache hadoop hive shims Hadoop Shims class Hadoop Shims Secure class Hadoop Shims class Input class return Func Utils get Ship Files class List private static Path get First File String location File System fs Path Filter filter throws Exception String locations get Path Strings location Path paths new Path locations length for int i i paths length i paths i new Path locations i List File Status status List new Array List File Status for int i i paths length i File Status files fs glob Status paths i if files null for File Status tempf files status List add tempf File Status status Array File Status status List to Array new File Status status List size Path p Utils depth First Search For File status Array fs filter return p Override public Resource Schema get Schema String location Job job throws Exception if type Info null type Info get Type Info location job still null means case of multiple load store if type Info null return null Resource Field Schema fs Hive Utils get Resource Field Schema type Info return fs get Schema private Type Info get Type Info String location Job job throws Exception Properties p Context get Context get Properties this get Class Type Info type Info Type Info Object Serializer deserialize p get Property signature Schema Signature Suffix if type Info null type Info get Type Info From Location location job if type Info null p set Property signature Schema Signature Suffix Object Serializer serialize type Info return type Info private Type Info get Type Info From Location String location Job job throws Exception File System fs File System get new Path location to Uri job get Configuration Path path get First File location fs new Non Empty Orc File Filter fs if path null log info Can not find any files from location Probably multiple load store in script return null Reader reader Orc File create Reader fs path Object Inspector oip Object Inspector reader get Object Inspector return Type Info Utils get Type Info From Object Inspector oip public static class Non Empty Orc File Filter implements Path Filter private File System fs public Non Empty Orc File Filter File System fs this fs fs Override public boolean accept Path path Reader reader try reader Orc File create Reader fs path Object Inspector oip Object Inspector reader get Object Inspector Resource Field Schema rs Hive Utils get Resource Field Schema Type Info Utils get Type Info From Object Inspector oip if rs get Schema get Fields length return true catch Exception e throw new Runtime Exception e return false Override public Resource Statistics get Statistics String location Job job throws Exception return null Override public String get Partition Keys String location Job job throws Exception return null Override public void set Partition Filter Expression partition Filter throws Exception Override public List Operator Set get Features return Arrays as List Load Push Down Operator Set Override public Required Field Response push Projection Required Field List required Field List throws Frontend Exception if required Field List null return null if required Field List get Fields null int schema Size Struct Type Info type Info get All Struct Field Type Infos size m Required Columns new boolean schema Size for Required Field rf required Field List get Fields if rf get Index m Required Columns rf get Index true Properties p Context get Context get Properties this get Class try p set Property signature Required Columns Suffix Object Serializer serialize m Required Columns catch Exception e throw new Runtime Exception Can not serialize m Required Columns return new Required Field Response true Override public List String get Predicate Fields String location Job job throws Exception Resource Schema schema get Schema location job List String predicate Fields new Array List String for Resource Field Schema field schema get Fields switch field get Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type predicate Fields add field get Name break default Skip Data Type Data Type Data Type and Data Type break return predicate Fields Override public List Op Type get Supported Expression Types List Op Type types new Array List Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type types add Op Type return types Override public void set Pushdown Predicate Expression expr throws Exception Search Argument s Arg get Search Argument expr if s Arg null log info Pushdown predicate expression is expr log info Pushdown predicate Search Argument is n s Arg Properties p Context get Context get Properties this get Class try p set Property signature Search Args Suffix s Arg to Kryo catch Exception e throw new Exception Can not serialize Search Argument s Arg Visible For Testing Search Argument get Search Argument Expression expr if expr null return null Builder builder Search Argument Factory new Builder boolean begin With And expr get Op Type equals Op Type expr get Op Type equals Op Type expr get Op Type equals Op Type if begin With And builder start And build Search Argument expr builder if begin With And builder end Search Argument s Arg builder build return s Arg private void build Search Argument Expression expr Builder builder if expr instanceof Binary Expression Expression lhs Binary Expression expr get Lhs Expression rhs Binary Expression expr get Rhs switch expr get Op Type case builder start And build Search Argument lhs builder build Search Argument rhs builder builder end break case builder start Or build Search Argument lhs builder build Search Argument rhs builder builder end break case builder equals get Column Name lhs get Expression Value rhs break case builder start Not builder equals get Column Name lhs get Expression Value rhs builder end break case builder less Than get Column Name lhs get Expression Value rhs break case builder less Than Equals get Column Name lhs get Expression Value rhs break case builder start Not builder less Than Equals get Column Name lhs get Expression Value rhs builder end break case builder start Not builder less Than get Column Name lhs get Expression Value rhs builder end break case Between Expression between Between Expression rhs builder between get Column Name lhs get Search Arg Obj Value between get Lower get Search Arg Obj Value between get Upper case In Expression in In Expression rhs builder in get Column Name lhs get Search Arg Obj Values in get Values to Array default throw new Runtime Exception Unsupported binary expression type expr get Op Type in expr else if expr instanceof Unary Expression Expression unary Expr Unary Expression expr get Expression switch expr get Op Type case builder is Null get Column Name unary Expr break case builder start Not build Search Argument unary Expr builder builder end break default throw new Runtime Exception Unsupported unary expression type expr get Op Type in expr else throw new Runtime Exception Unsupported expression type expr get Op Type in expr private String get Column Name Expression expr try return Column expr get Name catch Class Cast Exception e throw new Runtime Exception Expected a Column but found expr get Class get Name in expression expr e private Object get Expression Value Expression expr switch expr get Op Type case return Column expr get Name case return get Search Arg Obj Value Const expr get Value default throw new Runtime Exception Unsupported expression type expr get Op Type in expr private List Object get Search Arg Obj Values List Object values if values get instanceof Big Integer values get instanceof Big Decimal values get instanceof Date Time return values List Object new Values new Array List Object values size for Object value values new Values add get Search Arg Obj Value value return values private Object get Search Arg Obj Value Object value if value instanceof Big Integer return new Big Decimal Big Integer value else if value instanceof Big Decimal return value else if value instanceof Date Time return new Timestamp Date Time value get Millis else return value 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Boolean Expression public class Or Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Or Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Or plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Or Expression Or Expression ao Or Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Or Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import java io Exception import java util Iterator import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache pig Load Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io Read To End Loader This class encapsulates the runtime statistics of an user specified output Interface Audience Public Interface Stability Evolving public final class Output Stats private String name private String location private long bytes private long records private boolean success private Store store null private Configuration conf private static final Log Log Factory get Log Output Stats class public Output Stats String location long bytes long records boolean success this location location this bytes bytes this records records this success success try this name new Path location get Name catch Exception e location is a mal formatted this name location public String get Name return name public String get Location return location public long get Bytes return bytes public void set Bytes long bytes this bytes bytes public long get Records return records public void set Records long records this records records public long get Number Records return records public String get Function Name return store null null store get File get Func Spec get Class Name public boolean is Successful return success public void set Successful boolean success this success success public String get Alias return store null null store get Alias public Store get Store return store public Configuration get Conf return conf public String get Display String String Builder sb new String Builder if success sb append Successfully stored if records sb append records append records else sb append records if bytes sb append append bytes append bytes sb append in append location append n else sb append Failed to produce result in append location append n return sb to String public void set Store Store store this store store public void set Conf Configuration conf this conf conf public Iterator Tuple iterator throws Exception final Load Func p Pig Context pig Context Script State get get Pig Context if pig Context null store null throw new Illegal Argument Exception try Load Func original Load Func Load Func Pig Context instantiate Func From Spec store get File get Func Spec p Load Func new Read To End Loader original Load Func Configuration Util to Configuration pig Context get Properties store get File get File Name catch Exception e int err Code String msg Unable to get results for store get File throw new Exec Exception msg err Code Pig Exception e return new Iterator Tuple Tuple t boolean at End Override public boolean has Next if at End return false try if t null t p get Next if t null at End true catch Exception e error e t null at End true throw new Error e return at End Override public Tuple next Tuple next t if next null t null return next try next p get Next catch Exception e error e if next null at End true return next Override public void remove throw new Runtime Exception Removal not supported 
Generated By Java Do not edit this line Parse Exception java Version Java Options null package org apache pig tools parameters This exception is thrown when parse errors are encountered You can explicitly create objects of this exception type by calling the method generate Parse Exception in the generated parser You can modify this class to customize your error reporting mechanisms so long as you retain the public fields public class Parse Exception extends Exception The version identifier for this Serializable class Increment only if the i serialized i form of the class changes private static final long serial Version This constructor is used by the method generate Parse Exception in the generated parser Calling this constructor generates a new object of this type with the fields current Token expected Token Sequences and token Image set public Parse Exception Token current Token Val int expected Token Sequences Val String token Image Val super initialise current Token Val expected Token Sequences Val token Image Val current Token current Token Val expected Token Sequences expected Token Sequences Val token Image token Image Val The following constructors are for use by you for whatever purpose you can think of Constructing the exception in this manner makes the exception behave in the normal way i e as documented in the class Throwable The fields error Token expected Token Sequences and token Image do not contain relevant information The Java generated code does not use these constructors public Parse Exception super Constructor with message public Parse Exception String message super message This is the last token that has been consumed successfully If this object has been created due to a parse error the token followng this token will therefore be the first error token public Token current Token Each entry in this array is an array of integers Each array of integers represents a sequence of tokens by their ordinal values that is expected at this point of the parse public int expected Token Sequences This is a reference to the token Image array of the generated parser within which the parse error occurred This array is defined in the generated Constants interface public String token Image It uses current Token and expected Token Sequences to generate a parse error message and returns it If this object has been created due to a parse error and you do not catch it it gets thrown from the parser the correct error message gets displayed private static String initialise Token current Token int expected Token Sequences String token Image String eol System get Property line separator n String Buffer expected new String Buffer int max Size for int i i expected Token Sequences length i if max Size expected Token Sequences i length max Size expected Token Sequences i length for int j j expected Token Sequences i length j expected append token Image expected Token Sequences i j append if expected Token Sequences i expected Token Sequences i length expected append expected append eol append String retval Encountered Token tok current Token next for int i i max Size i if i retval if tok kind retval token Image break retval token Image tok kind retval retval add escapes tok image retval tok tok next retval at line current Token next begin Line column current Token next begin Column retval eol if expected Token Sequences length retval Was expecting eol else retval Was expecting one of eol retval expected to String return retval The end of line string for this machine protected String eol System get Property line separator n Used to convert raw characters to their escaped version when these raw version can not be used as part of an string literal static String add escapes String str String Buffer retval new String Buffer char ch for int i i str length i switch str char At i case continue case b retval append b continue case t retval append t continue case n retval append n continue case f retval append f continue case r retval append r continue case retval append continue case retval append continue case retval append continue default if ch str char At i x ch x e String s Integer to String ch retval append u s substring s length s length else retval append ch continue return retval to String Java Original Checksum e c cb ba a d fa do not edit this line 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig parser import org antlr runtime Recognition Exception import org apache pig impl logical Layer Frontend Exception public class Parser Exception extends Frontend Exception private static final long serial Version private static final int error Code public Parser Exception Recognition Exception reco Exception super Pig script failed to parse reco Exception error Code reco Exception public Parser Exception String msg super msg error Code public Parser Exception String msg Source Location location super msg error Code location public Parser Exception String msg Throwable cause super msg error Code cause Override public String to String return Failed to parse get Message 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java io Exception import java util Array List import java util Arrays import java util Hash Map import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop mapreduce Job import org apache pig Expression import org apache pig Expression Binary Expression import org apache pig Expression Column import org apache pig Load Func import org apache pig Load Metadata import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Filter Extractor import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Operator Sub Plan import org apache pig newplan Partition Filter Extractor import org apache pig newplan logical relational Filter import org apache pig newplan logical relational Load import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan optimizer Rule import org apache pig newplan optimizer Transformer public class Partition Filter Optimizer extends Rule private static final Log Log Factory get Log Partition Filter Optimizer class private String partition Keys a reference to the Load Metada implementation private Load Metadata load Metadata a reference to the Load Func implementation private Load Func load Func private Load lo Load private Filter lo Filter a map between column names as reported in link Load Metadata get Schema String org apache hadoop conf Configuration and as present in link Load get Schema The two will be different when the user has provided a schema in the load statement private Map String String col Name Map new Hash Map String String a map between column nameas as present in link Load get Schema and as reported in link Load Metadata get Schema String org apache hadoop conf Configuration The two will be different when the user has provided a schema in the load statement private Map String String reverse Col Name Map new Hash Map String String public Partition Filter Optimizer String name super name false Override protected Operator Plan build Pattern match each foreach Logical Plan plan new Logical Plan Logical Relational Operator load new Load null plan plan add load Logical Relational Operator filter new Filter plan plan add filter plan connect load filter return plan Override public Transformer get New Transformer return new Partition Filter Push Down Transformer public class Partition Filter Push Down Transformer extends Transformer protected Operator Sub Plan sub Plan private boolean plan Changed Override public boolean check Operator Plan matched throws Frontend Exception lo Load Load matched get Sources get Match filter List Operator succeds current Plan get Successors lo Load if succeds null succeds size succeds get instanceof Filter return false lo Filter Filter succeds get Filter has dependency other than load skip optimization if current Plan get Soft Link Predecessors lo Filter null return false we have to check more only if Load Func implements Load Metada load Func lo Load get Load Func if load Func instanceof Load Metadata return false load Metadata Load Metadata load Func try partition Keys load Metadata get Partition Keys lo Load get File Spec get File Name new Job lo Load get Configuration catch Exception e throw new Frontend Exception e if partition Keys null partition Keys length return false return true Override public Operator Plan report Changes Return null in case there is no partition filter extracted which means the plan has n t changed If not return the modified plan which has filters removed return plan Changed sub Plan null Override public void transform Operator Plan matched throws Frontend Exception sub Plan new Operator Sub Plan current Plan setup Col Name Maps Filter Extractor filter Finder new Partition Filter Extractor lo Filter get Filter Plan get Mapped Keys partition Keys filter Finder visit info Partition keys are Arrays as List partition Keys Expression partition Filter filter Finder get Push Down Expression if partition Filter null the column names in the filter may be the ones provided by the user in the schema in the load statement we may need to replace them with partition column names as given by Load Func get Schema update Mapped Col Names partition Filter try info Setting partition filter partition Filter on loader load Metadata load Metadata set Partition Filter partition Filter plan Changed true catch Exception e throw new Frontend Exception e if filter Finder is Filter Removable current Plan remove And Reconnect lo Filter else lo Filter set Filter Plan filter Finder get Filtered Plan protected void update Mapped Col Names Expression expr if expr instanceof Binary Expression update Mapped Col Names Binary Expression expr get Lhs update Mapped Col Names Binary Expression expr get Rhs else if expr instanceof Column Column col Column expr col set Name reverse Col Name Map get col get Name The partition keys in the argument are as reported by link Load Metadata get Partition Keys String org apache hadoop conf Configuration The user may have renamed these by providing a schema with different names in the load statement this method will replace the former names with the latter names param partition Keys return protected List String get Mapped Keys String partition Keys List String mapped Keys new Array List String partition Keys length for int i i partition Keys length i mapped Keys add col Name Map get partition Keys i return mapped Keys protected void setup Col Name Maps throws Frontend Exception Logical Schema lo Load Schema lo Load get Schema Logical Schema load Func Schema lo Load get Determined Schema for int i i load Func Schema size i col Name Map put load Func Schema get Field i alias i lo Load Schema size lo Load Schema get Field i alias load Func Schema get Field i alias reverse Col Name Map put i lo Load Schema size lo Load Schema get Field i alias load Func Schema get Field i alias load Func Schema get Field i alias 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java util List import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Add import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Divide import org apache pig backend hadoop executionengine physical Layer expression Operators Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Greater Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Less Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Mod import org apache pig backend hadoop executionengine physical Layer expression Operators Multiply import org apache pig backend hadoop executionengine physical Layer expression Operators Not Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators And import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Cast import org apache pig backend hadoop executionengine physical Layer expression Operators Is Null import org apache pig backend hadoop executionengine physical Layer expression Operators Map Look Up import org apache pig backend hadoop executionengine physical Layer expression Operators Negative import org apache pig backend hadoop executionengine physical Layer expression Operators Not import org apache pig backend hadoop executionengine physical Layer expression Operators Or import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators Regexp import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer expression Operators Subtract import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Demux import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Native import org apache pig backend hadoop executionengine physical Layer relational Operators Optimized For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Poisson Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Pre Combiner Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Reservoir Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Visitor Exception Sets the parent plan for all Physical Operators Note parent Plan is a bit of a misnomer We actually want all the operators to point to the same plan not necessarily the one they re a member of public class Phy Plan Setter extends Phy Plan Visitor Physical Plan parent public Phy Plan Setter Physical Plan plan super plan new Dependency Order Walker Physical Operator Physical Plan plan parent plan Override public void visit Physical Operator op op set Parent Plan parent Override public void visit Load Load ld throws Visitor Exception ld set Parent Plan parent Override public void visit Native Native nt throws Visitor Exception nt set Parent Plan parent Override public void visit Store Store st throws Visitor Exception st set Parent Plan parent Override public void visit Filter Filter fl throws Visitor Exception super visit Filter fl fl set Parent Plan parent Override public void visit Local Rearrange Local Rearrange lr throws Visitor Exception super visit Local Rearrange lr lr set Parent Plan parent Override public void visit Collected Group Collected Group mg throws Visitor Exception super visit Collected Group mg mg set Parent Plan parent Override public void visit Global Rearrange Global Rearrange gr throws Visitor Exception gr set Parent Plan parent Override public void visit Package Package pkg throws Visitor Exception pkg set Parent Plan parent Override public void visit For Each For Each nfe throws Visitor Exception super visit For Each nfe nfe set Parent Plan parent Override public void visit Union Union un throws Visitor Exception un set Parent Plan parent Override public void visit Split Split spl throws Visitor Exception Physical Plan old Plan parent List Physical Plan plans spl get Plans for Physical Plan plan plans parent plan push Walker m Current Walker spawn Child Walker plan visit pop Walker parent old Plan spl set Parent Plan parent Override public void visit Demux Demux demux throws Visitor Exception super visit Demux demux demux set Parent Plan parent Override public void visit Distinct Distinct distinct throws Visitor Exception distinct set Parent Plan parent Override public void visit Sort Sort sort throws Visitor Exception super visit Sort sort sort set Parent Plan parent Override public void visit Rank Rank rank throws Visitor Exception rank set Parent Plan parent Override public void visit Constant Constant Expression cnst throws Visitor Exception cnst set Parent Plan parent Override public void visit Project Project proj throws Visitor Exception proj set Parent Plan parent Override public void visit Greater Than Greater Than Expr grt throws Visitor Exception grt set Parent Plan parent Override public void visit Less Than Less Than Expr lt throws Visitor Exception lt set Parent Plan parent Override public void visit Or Equal Or Equal To Expr gte throws Visitor Exception gte set Parent Plan parent Override public void visit Or Equal Or Equal To Expr lte throws Visitor Exception lte set Parent Plan parent Override public void visit Equal To Equal To Expr eq throws Visitor Exception eq set Parent Plan parent Override public void visit Not Equal To Not Equal To Expr eq throws Visitor Exception eq set Parent Plan parent Override public void visit Regexp Regexp re throws Visitor Exception re set Parent Plan parent Override public void visit Is Null Is Null is Null throws Visitor Exception is Null set Parent Plan parent Override public void visit Add Add add throws Visitor Exception add set Parent Plan parent Override public void visit Subtract Subtract sub throws Visitor Exception sub set Parent Plan parent Override public void visit Multiply Multiply mul throws Visitor Exception mul set Parent Plan parent Override public void visit Divide Divide dv throws Visitor Exception dv set Parent Plan parent Override public void visit Mod Mod mod throws Visitor Exception mod set Parent Plan parent Override public void visit And And and throws Visitor Exception and set Parent Plan parent Override public void visit Or Or or throws Visitor Exception or set Parent Plan parent Override public void visit Not Not not throws Visitor Exception not set Parent Plan parent Override public void visit Bin Cond Bin Cond bin Cond bin Cond set Parent Plan parent Override public void visit Negative Negative negative negative set Parent Plan parent Override public void visit User Func User Func user Func throws Visitor Exception user Func set Parent Plan parent Override public void visit Comparison Func User Comparison Func comp Func throws Visitor Exception comp Func set Parent Plan parent Override public void visit Map Look Up Map Look Up map Look Up map Look Up set Parent Plan parent Override public void visit Cast Cast cast cast set Parent Plan parent Override public void visit Limit Limit lim throws Visitor Exception lim set Parent Plan parent Override public void visit Join Join join throws Visitor Exception join set Parent Plan parent Override public void visit Merge Join Merge Join join throws Visitor Exception join set Parent Plan parent Override public void visit Skewed Join Skewed Join join throws Visitor Exception join set Parent Plan parent Override public void visit Stream Stream stream throws Visitor Exception stream set Parent Plan parent Override public void visit Partition Rearrange Partition Rearrange lrfi throws Visitor Exception super visit Partition Rearrange lrfi lrfi set Parent Plan parent Override public void visit Partial Agg Partial Agg po Partial Agg throws Visitor Exception po Partial Agg set Parent Plan parent Override public void visit Optimized For Each Optimized For Each optimized For Each throws Visitor Exception optimized For Each set Parent Plan parent Override public void visit Pre Combiner Local Rearrange Pre Combiner Local Rearrange pre Combiner Local Rearrange throws Visitor Exception super visit Pre Combiner Local Rearrange pre Combiner Local Rearrange pre Combiner Local Rearrange set Parent Plan parent Override public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception merge Co Grp set Parent Plan parent Override public void visit Reservoir Sample Reservoir Sample reservoir Sample throws Visitor Exception reservoir Sample set Parent Plan parent Override public void visit Poisson Sample Poisson Sample poisson Sample throws Visitor Exception poisson Sample set Parent Plan parent 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer plans import java util List import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Add import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Divide import org apache pig backend hadoop executionengine physical Layer expression Operators Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Greater Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Less Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Mod import org apache pig backend hadoop executionengine physical Layer expression Operators Multiply import org apache pig backend hadoop executionengine physical Layer expression Operators Not Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators And import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Cast import org apache pig backend hadoop executionengine physical Layer expression Operators Is Null import org apache pig backend hadoop executionengine physical Layer expression Operators Map Look Up import org apache pig backend hadoop executionengine physical Layer expression Operators Negative import org apache pig backend hadoop executionengine physical Layer expression Operators Not import org apache pig backend hadoop executionengine physical Layer expression Operators Or import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators Regexp import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer expression Operators Subtract import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Counter import org apache pig backend hadoop executionengine physical Layer relational Operators Cross import org apache pig backend hadoop executionengine physical Layer relational Operators Demux import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Native import org apache pig backend hadoop executionengine physical Layer relational Operators Optimized For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Partition Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Poisson Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Pre Combiner Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Reservoir Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig backend hadoop executionengine physical Layer relational Operators Broadcast Spark import org apache pig impl plan Plan Visitor import org apache pig impl plan Plan Walker import org apache pig impl plan Visitor Exception The visitor class for the Physical Plan To use this create the visitor with the plan to be visited Call the visit method to traverse the plan in a depth first fashion This class also visits the nested plans inside the operators One has to extend this class to modify the nature of each visit and to maintain any relevant state information between the visits to two different operators public class Phy Plan Visitor extends Plan Visitor Physical Operator Physical Plan public Phy Plan Visitor Physical Plan plan Plan Walker Physical Operator Physical Plan walker super plan walker public void visit Physical Operator op do nothing public void visit Load Load ld throws Visitor Exception do nothing public void visit Store Store st throws Visitor Exception do nothing public void visit Native Native nat throws Visitor Exception do nothing public void visit Filter Filter fl throws Visitor Exception push Walker m Current Walker spawn Child Walker fl get Plan visit pop Walker public void visit Collected Group Collected Group mg throws Visitor Exception List Physical Plan inp Plans mg get Plans for Physical Plan plan inp Plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Local Rearrange Local Rearrange lr throws Visitor Exception List Physical Plan inp Plans lr get Plans for Physical Plan plan inp Plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Global Rearrange Global Rearrange gr throws Visitor Exception do nothing public void visit Package Package pkg throws Visitor Exception do nothing public void visit For Each For Each nfe throws Visitor Exception List Physical Plan inp Plans nfe get Input Plans for Physical Plan plan inp Plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Union Union un throws Visitor Exception do nothing public void visit Split Split spl throws Visitor Exception List Physical Plan plans spl get Plans for Physical Plan plan plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Demux Demux demux throws Visitor Exception List Physical Plan plans demux get Plans for Physical Plan plan plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Counter Counter po Counter throws Visitor Exception do nothing public void visit Rank Rank rank throws Visitor Exception do nothing public void visit Distinct Distinct distinct throws Visitor Exception do nothing public void visit Sort Sort sort throws Visitor Exception List Physical Plan inp Plans sort get Sort Plans for Physical Plan plan inp Plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Constant Constant Expression cnst throws Visitor Exception do nothing public void visit Project Project proj throws Visitor Exception do nothing public void visit Greater Than Greater Than Expr grt throws Visitor Exception do nothing public void visit Less Than Less Than Expr lt throws Visitor Exception do nothing public void visit Or Equal Or Equal To Expr gte throws Visitor Exception do nothing public void visit Or Equal Or Equal To Expr lte throws Visitor Exception do nothing public void visit Equal To Equal To Expr eq throws Visitor Exception do nothing public void visit Not Equal To Not Equal To Expr eq throws Visitor Exception do nothing public void visit Regexp Regexp re throws Visitor Exception do nothing public void visit Is Null Is Null is Null throws Visitor Exception public void visit Add Add add throws Visitor Exception do nothing public void visit Subtract Subtract sub throws Visitor Exception do nothing public void visit Multiply Multiply mul throws Visitor Exception do nothing public void visit Divide Divide dv throws Visitor Exception do nothing public void visit Mod Mod mod throws Visitor Exception do nothing public void visit And And and throws Visitor Exception do nothing public void visit Or Or or throws Visitor Exception do nothing public void visit Not Not not throws Visitor Exception do nothing public void visit Bin Cond Bin Cond bin Cond do nothing public void visit Negative Negative negative do nothing public void visit User Func User Func user Func throws Visitor Exception do nothing public void visit Comparison Func User Comparison Func comp Func throws Visitor Exception do nothing public void visit Map Look Up Map Look Up map Look Up Auto generated method stub public void visit Cast Cast cast Auto generated method stub public void visit Limit Limit lim throws Visitor Exception Physical Plan inp Plan lim get Limit Plan if inp Plan null push Walker m Current Walker spawn Child Walker inp Plan visit pop Walker public void visit Cross Cross cross throws Visitor Exception do nothing public void visit Join Join join throws Visitor Exception do nothing public void visit Merge Join Merge Join join throws Visitor Exception do nothing public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception param stream throws Visitor Exception public void visit Stream Stream stream throws Visitor Exception Auto generated method stub public void visit Skewed Join Skewed Join sk throws Visitor Exception public void visit Partition Rearrange Partition Rearrange pr throws Visitor Exception List Physical Plan inp Plans pr get Plans for Physical Plan plan inp Plans push Walker m Current Walker spawn Child Walker plan visit pop Walker param optimized For Each public void visit Optimized For Each Optimized For Each optimized For Each throws Visitor Exception Auto generated method stub param pre Combiner Local Rearrange public void visit Pre Combiner Local Rearrange Pre Combiner Local Rearrange pre Combiner Local Rearrange throws Visitor Exception List Physical Plan inp Plans pre Combiner Local Rearrange get Plans for Physical Plan plan inp Plans push Walker m Current Walker spawn Child Walker plan visit pop Walker public void visit Partial Agg Partial Agg po Partial Agg throws Visitor Exception public void visit Reservoir Sample Reservoir Sample reservoir Sample throws Visitor Exception public void visit Poisson Sample Poisson Sample poisson Sample throws Visitor Exception public void visit Broadcast Spark Broadcast Spark po Broadcast Spark 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer plans import java io Byte Array Output Stream import java io Exception import java io Output Stream import java io Print Stream import java util Array List import java util Collection import java util Collections import java util Comparator import java util Hash Map import java util List import java util Map import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Binary Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Comparison Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Unary Expression Operator import org apache pig data Tuple import org apache pig impl plan Operator Plan import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map import com google common collect Hash Bi Map The base class for all types of physical plans This extends the Operator Plan public class Physical Plan extends Operator Plan Physical Operator implements Cloneable private static final long serial Version marker to indicate whether all input for this plan has been sent this is currently used in Stream to know if all map calls and reduce calls are finished and that there is no more input expected and in Partial Agg public boolean end Of All Input false private Multi Map Physical Operator Physical Operator opmap null public Physical Plan super public void attach Input Tuple t List Physical Operator roots get Roots for Physical Operator operator roots operator attach Input t public void detach Input for Physical Operator op get Roots op detach Input Write a visual representation of the Physical Plan into the given output stream param out Output Stream to which the visual representation is written public void explain Output Stream out explain out true Write a visual representation of the Physical Plan into the given output stream param out Output Stream to which the visual representation is written param verbose Amount of information to print public void explain Output Stream out boolean verbose Plan Printer Physical Operator Physical Plan mpp new Plan Printer Physical Operator Physical Plan this mpp set Verbose verbose try mpp print out catch Visitor Exception e Auto generated catch block e print Stack Trace catch Exception e Auto generated catch block e print Stack Trace Write a visual representation of the Physical Plan into the given printstream param ps Print Stream to which the visual representation is written param format Format to print in param verbose Amount of information to print public void explain Print Stream ps String format boolean verbose if format equals xml ps println physical Plan Not Supported physical Plan return ps println ps println Physical Plan ps println if format equals text explain Output Stream ps verbose ps println else if format equals dot Dot Printer pp new Dot Printer this ps pp set Verbose verbose pp dump ps println Override public void connect Physical Operator from Physical Operator to throws Plan Exception super connect from to to set Inputs get Predecessors to Override public void remove Physical Operator op op set Inputs null List Physical Operator sucs get Successors op if sucs null sucs size for Physical Operator suc sucs successor could have multiple inputs for example Union remove op from its list of inputs if after removal there are no other inputs set successor s inputs to null List Physical Operator succ Inputs suc get Inputs succ Inputs remove op if succ Inputs size suc set Inputs null else suc set Inputs succ Inputs super remove op non Javadoc see org apache pig impl plan Operator Plan replace org apache pig impl plan Operator org apache pig impl plan Operator Override public void replace Physical Operator old Node Physical Operator new Node throws Plan Exception List Physical Operator old Node Successors get Successors old Node super replace old Node new Node if old Node Successors null for Physical Operator preds old Node Successors List Physical Operator inputs preds get Inputs now replace old Node with new Node in the input list of old Node s successors for int i i inputs size i if inputs get i old Node inputs set i new Node non Javadoc see org apache pig impl plan Operator Plan add org apache pig impl plan Operator Override public void add Physical Operator op attach this plan as the plan the operator is part of op set Parent Plan this super add op public boolean is Empty return m Ops size Override public String to String if is Empty return Empty Plan else Byte Array Output Stream baos new Byte Array Output Stream explain baos true return baos to String Override public Physical Plan clone throws Clone Not Supported Exception Physical Plan clone new Physical Plan Get all the nodes in this plan and clone them As we make clones create a map between clone and original Then walk the connections in this plan and create equivalent connections in the clone Map Physical Operator Physical Operator matches new Hash Map Physical Operator Physical Operator m Ops size Sorting just so that explain output scope ids is same in jdk and jdk List Physical Operator ops To Clone new Array List Physical Operator m Ops key Set Collections sort ops To Clone for Physical Operator op ops To Clone Physical Operator c op clone clone add c if opmap null opmap put op c matches put op c Build the edges for Physical Operator op m From Edges key Set Physical Operator clone From matches get op if clone From null String msg Unable to find clone for op op name throw new Clone Not Supported Exception msg Collection Physical Operator to Ops m From Edges get op for Physical Operator to Op to Ops Physical Operator clone To matches get to Op if clone To null String msg Unable to find clone for op to Op name throw new Clone Not Supported Exception msg try clone connect clone From clone To catch Plan Exception pe Clone Not Supported Exception cnse new Clone Not Supported Exception cnse init Cause pe throw cnse Fix up all the inputs in the operators themselves for Physical Operator op m Ops key Set List Physical Operator inputs op get Inputs if inputs null inputs size continue List Physical Operator new Inputs new Array List Physical Operator inputs size Physical Operator clone Op matches get op if clone Op null String msg Unable to find clone for op op name throw new Clone Not Supported Exception msg for Physical Operator i Op inputs Physical Operator clone Op matches get i Op if clone Op null String msg Unable to find clone for op i Op name throw new Clone Not Supported Exception msg new Inputs add clone Op clone Op set Inputs new Inputs for Physical Operator op m Ops key Set if op instanceof Comparison Operator Comparison Operator orig Comparison Operator op Comparison Operator clone Op Comparison Operator matches get op clone Op set Operand Type orig get Operand Type if op instanceof Unary Expression Operator Unary Expression Operator orig Unary Expression Operator op Unary Expression Operator clone Op Unary Expression Operator matches get op clone Op set Expr Expression Operator matches get orig get Expr else if op instanceof Binary Expression Operator Binary Expression Operator orig Binary Expression Operator op Binary Expression Operator clone Op Binary Expression Operator matches get op clone Op set Rhs Expression Operator matches get orig get Rhs clone Op set Lhs Expression Operator matches get orig get Lhs else if op instanceof Bin Cond Bin Cond orig Bin Cond op Bin Cond clone Op Bin Cond matches get op clone Op set Rhs Expression Operator matches get orig get Rhs clone Op set Lhs Expression Operator matches get orig get Lhs clone Op set Cond Expression Operator matches get orig get Cond Fix order of edges in m To Edges lists Map Physical Operator Physical Operator inverted Matches Hash Bi Map create matches inverse for Physical Operator new Op clone m To Edges key Set List Physical Operator new List clone m To Edges get new Op if new List size List Physical Operator original List this m To Edges get inverted Matches get new Op Collections sort new List new Edge Order Helper original List inverted Matches return clone public void set Op Map Multi Map Physical Operator Physical Operator opmap this opmap opmap public void reset Op Map opmap null private static class Edge Order Helper implements Comparator Physical Operator private final Map Physical Operator Physical Operator inverted Matches private final List Physical Operator original List public Edge Order Helper List Physical Operator original List Map Physical Operator Physical Operator inverted Matches this original List original List this inverted Matches inverted Matches Override public int compare Physical Operator o Physical Operator o return original List index Of inverted Matches get o original List index Of inverted Matches get o 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Byte Array Output Stream import java io Exception import java util Array List import java util Properties import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io Writable import org apache hadoop mapreduce Reducer import org apache log j Property Configurator import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop Data Type import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Join Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine tez plan operator Bloom Packager import org apache pig backend hadoop executionengine util Map Red Util import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl util Object Serializer import org apache pig impl util Context import org apache pig tools pigstats Pig Status Reporter public class Pig Combiner public static class Combine extends Reducer Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable private static final Log log Log Factory get Log Combine class Combiners can be called multiple times in both map and reduce private static boolean first Time true private byte key Type The reduce plan private Physical Plan cp The Package operator which is the root of every Map Reduce plan is obtained through the job conf The portion remaining after its removal is the reduce plan private Package pack Progressable Reporter pig Reporter Physical Operator roots Physical Operator leaf private volatile boolean initialized false Static Data Cleanup public static void static Data Cleanup first Time true Configures the Reduce plan the Package operator and the reporter thread Suppress Warnings unchecked Override protected void setup Context context throws Exception Interrupted Exception super setup context Configuration j Conf context get Configuration try Pig Context set Package Import List Array List String Object Serializer deserialize j Conf get udf import list Properties log j Properties Properties Object Serializer deserialize j Conf get Pig Impl Constants if log j Properties null Property Configurator configure log j Properties Context get Context reset Map Red Util setup Context context get Configuration cp Physical Plan Object Serializer deserialize j Conf get pig combine Plan pack Package Object Serializer deserialize j Conf get pig combine package To be removed if cp is Empty log debug Combine Plan empty else Byte Array Output Stream baos new Byte Array Output Stream cp explain baos log debug baos to String key Type byte Object Serializer deserialize j Conf get pig map keytype till here pig Reporter new Progressable Reporter if cp is Empty roots cp get Roots to Array new Physical Operator leaf cp get Leaves get catch Exception ioe String msg Problem while configuring combiner s reduce plan throw new Runtime Exception msg ioe Avoid log spamming if first Time log info Aliases being processed per job phase Alias Name line offset j Conf get pig alias location first Time false The reduce function which packages the key and List lt Tuple gt into key Bag lt Tuple gt after converting Hadoop type key into Pig type The package result is either collected as is if the reduce plan is empty or after passing through the reduce plan Override protected void reduce Pig Nullable Writable key Iterable Nullable Tuple tup Iter Context context throws Exception Interrupted Exception if initialized initialized true pig Reporter set Rep context Physical Operator set Reporter pig Reporter boolean aggregate Warning true equals Ignore Case context get Configuration get aggregate warning Pig Status Reporter pig Status Reporter Pig Status Reporter get Instance pig Status Reporter set Context new Task Context context Pig Hadoop Logger pig Hadoop Logger Pig Hadoop Logger get Instance pig Hadoop Logger set Reporter pig Status Reporter pig Hadoop Logger set Aggregate aggregate Warning Physical Operator set Pig Logger pig Hadoop Logger In the case we optimize we combine Package and Foreach so we could get many tuples out of the getnext call of Join Package In this case we process till we see from Join Pacakage get Next if pack get Pkgr instanceof Join Packager pack get Pkgr instanceof Bloom Packager pack attach Input key tup Iter iterator while true if process One Package Output context break else not optimized so package will give only one tuple out for the key pack attach Input key tup Iter iterator process One Package Output context return false more output true end of processing public boolean process One Package Output Context oc throws Exception Interrupted Exception try Result res pack get Next Tuple if res return Status Status Tuple pack Res Tuple res result if cp is Empty oc write null pack Res return false for int i i roots length i roots i attach Input pack Res while true Result red Res leaf get Next Tuple if red Res return Status Status Tuple tuple Tuple red Res result Byte index Byte tuple get Pig Nullable Writable out Key Data Type get Writable Comparable Types tuple get this key Type Nullable Tuple val new Nullable Tuple Tuple tuple get Both the key and the value need the index The key needs it so that it can be sorted on the index in addition to the key value The value needs it so that Package can properly assign the tuple to its slot in the projection out Key set Index index val set Index index oc write out Key val continue if red Res return Status Status break if red Res return Status Status continue if red Res return Status Status int err Code String msg Received Error while processing the combine plan if red Res result null msg red Res result throw new Exec Exception msg err Code Pig Exception if res return Status Status return false if res return Status Status int err Code String msg Packaging error while processing group throw new Exec Exception msg err Code Pig Exception if res return Status Status return true return false catch Exec Exception e throw e Will be called once all the intermediate keys and values are processed cleanup references to the Physical Plan Override protected void cleanup Context context throws Exception Interrupted Exception super cleanup context leaf null pack null pig Reporter null Avoid in Tez Physical Operator set Reporter null roots null cp null return the key Type public byte get Key Type return key Type param key Type the key Type to set public void set Key Type byte key Type this key Type key Type 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig validator import java io Exception import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl logical Layer Frontend Exception Interface defining Pig commands and a link Pig Command Filter validate Command method to operate on it Interface Audience Private Interface Stability Evolving public interface Pig Command Filter public enum Command Validates a Pig command as defined by link Command param command throws Exception public void validate Command command throws Frontend Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl import com google common base Splitter import com google common collect Lists import java io Buffered Reader import java io Buffered Writer import java io File import java io File Not Found Exception import java io File Reader import java io File Writer import java io Exception import java io Input Stream import java io Input Stream Reader import java io Object Input Stream import java io Object Output Stream import java io Serializable import java io String Writer import java lang reflect Constructor import java net Malformed Exception import java net import java net import java net Class Loader import java util Array List import java util Arrays import java util Hash Map import java util Linked Hash Map import java util Linked List import java util List import java util Map import java util Properties import java util String Tokenizer import java util Vector import org antlr runtime tree Tree import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache log j Level import org apache pig Exec Type import org apache pig Exec Type Provider import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend datastorage Data Storage import org apache pig backend datastorage Data Storage Exception import org apache pig backend datastorage Element Descriptor import org apache pig backend executionengine Exec Exception import org apache pig backend executionengine Execution Engine import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop datastorage Data Storage import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig impl streaming Executable Manager import org apache pig impl streaming Streaming Command import org apache pig tools parameters Parameter Substitution Preprocessor import org apache pig tools parameters Parse Exception import org apache pig tools parameters Preprocessor Context public class Pig Context implements Serializable private static final long serial Version private static final Log log Log Factory get Log Pig Context class private static Object instantiation Lock new Object public static final String job Name public static final String Pig Latin public static final String job Priority public static final String pig cmd args remainders we only serialize some of the stuff to make it smaller given that it s not all needed on the Hadoop side and also because some is not serializable e g the Configuration one of local mapreduce or a custom exec type for a different execution engine private Exec Type exec Type main file system that jobs and shell commands access transient private Data Storage dfs local file system where jar files etc reside transient private Data Storage lfs handle to the back end transient private Execution Engine execution Engine private Properties properties Resources for the job jars scripting udf files cached macro abstract syntax trees Jar files that are global to the whole Pig script includes registered jars Jars defined in Dpig additional jars transient public List extra Jars new Linked List original paths each extra jar came from used to avoid redundant imports transient private Map String extra Jar Original Paths new Hash Map String jars needed for scripting udfs jython jar etc transient public List String script Jars new Array List String jars that are predeployed to the cluster and thus should not be merged in at all even subsets transient public Vector String predeployed Jars new Vector String script files that are needed to run a job Deprecated public List String script Files new Array List String private Map String File aliased Script Files new Linked Hash Map String File record of scripting udf file path which namespace it was registered to used to avoid redundant imports transient public Map String String scripting Fs cache of macro file path abstract syntax tree used to avoid re parsing the same macros over and over transient public Map String Tree macros a table mapping function names to function specs private Map String Func Spec defined Functions new Hash Map String Func Spec a table mapping names to streaming commands private Map String Streaming Command defined Commands new Hash Map String Streaming Command private static Thread Local Array List String package Import List new Thread Local Array List String private static Thread Local Map String Class class Cache new Thread Local Map String Class private Properties log j Properties new Properties private Level default Log Level Level public int default Parallel Says whether we re processing an explain right now Explain might skip some check in the logical plan validation file existence checks etc public boolean in Explain false Where we are processing a dump schema right now public boolean in Dump Schema false whether we re processing an right now public boolean in Illustrator false private String last alias null List of paths skipped for automatic shipping List String skipped Ship Paths new Array List String Static Data Cleanup public static void static Data Cleanup package Import List set null extends Class Loader to allow adding to classpath as new jars are registered private static class Context Class Loader extends Class Loader public Context Class Loader Class Loader class Loader this new class Loader public Context Class Loader urls Class Loader class Loader super urls class Loader Override public void add url super add url static private Context Class Loader classloader new Context Class Loader Pig Context class get Class Loader Parameter related fields params list of strings key value from the command line param Files list of paths to parameter files preprocessor Context manages parsing params and param Files into an actual map private List String params private List String param Files transient private Preprocessor Context preprocessor Context new Preprocessor Context public List String get Params return params public void set Params List String params this params params public List String get Param Files return param Files public void set Param Files List String param Files this param Files param Files public Preprocessor Context get Preprocessor Context return preprocessor Context public Map String String get Param Val throws Exception Map String String param Val preprocessor Context get Param Val if param Val null try preprocessor Context load Param Val params param Files catch Parse Exception e throw new Exception e get Message return preprocessor Context get Param Val else return param Val public Pig Context this Exec Type new Properties public Pig Context Configuration conf throws Pig Exception this Configuration Util to Properties conf public Pig Context Properties properties throws Pig Exception this Exec Type Provider select Exec Type properties properties public Pig Context Exec Type exec Type Configuration conf this exec Type Configuration Util to Properties conf public Pig Context Exec Type exec Type Properties properties this exec Type exec Type this properties properties this properties set Property exectype this exec Type name this execution Engine exec Type get Execution Engine this Add the default paths to be skipped for auto shipping of commands skipped Ship Paths add bin skipped Ship Paths add usr bin skipped Ship Paths add usr local bin skipped Ship Paths add sbin skipped Ship Paths add usr sbin skipped Ship Paths add usr local sbin macros new Hash Map String Tree scripting Fs new Hash Map String String init This method is created with the aim of unifying the Grunt and Pig Server approaches so all common initializations can go in here private void init if properties get udf import list null Pig Context initialize Import List String properties get udf import list public static void initialize Import List String import List Command Line Properties String Tokenizer tokenizer new String Tokenizer import List Command Line Properties int pos Leave as the first import Array List String import List get Package Import List while tokenizer has More Tokens String import Item tokenizer next Token if import Item ends With import Item import List add pos import Item pos public void connect throws Exec Exception execution Engine init dfs execution Engine get Data Storage lfs new Data Storage create file properties public void set Jobtracker Location String new Location execution Engine set Property Configuration new Location calls add Script File path new File path ensuring that a given path is added to the jar at most once param path public void add Script File String path add Script File path path this method adds script files that must be added to the shipped jar named differently from their local fs path param name name in the jar param path path on the local fs public void add Script File String name String path if path null aliased Script Files put name replace First replace All new File path public void add Script Jar String path if path null script Jars contains path script Jars add path public void add Jar String path throws Malformed Exception if path null resource new File path to to add Jar resource path public void add Jar resource String original Path throws Malformed Exception if resource null extra Jars contains resource extra Jars add resource extra Jar Original Paths put resource original Path classloader add resource Thread current Thread set Context Class Loader Pig Context classloader public boolean has Jar String path for url extra Jars if extra Jar Original Paths get url equals path return true return false Adds the specified path to the predeployed jars list These jars will never be included in generated job jar p This can be called for jars that are pre installed on the Hadoop cluster to reduce the size of the job jar public void mark Jar As Predeployed String path if path null predeployed Jars contains path predeployed Jars add path public String do Param Substitution Input Stream in List String params List String param Files throws Exception return do Param Substitution new Buffered Reader new Input Stream Reader in params param Files public String do Param Substitution Buffered Reader reader List String params List String param Files throws Exception this params params this param Files param Files return do Param Substitution reader public String do Param Substitution Buffered Reader reader throws Exception try preprocessor Context set Pig Context this preprocessor Context load Param Val params param Files Parameter Substitution Preprocessor psp new Parameter Substitution Preprocessor preprocessor Context String Writer writer new String Writer psp gen Substituted File reader writer return writer to String catch Parse Exception e log error e get Localized Message throw new Exception e public Buffered Reader do Param Substitution Output To File Buffered Reader reader String output File Path List String params List String param Files throws Exception this params params this param Files param Files return do Param Substitution Output To File reader output File Path public Buffered Reader do Param Substitution Output To File Buffered Reader reader String output File Path throws Exception try preprocessor Context load Param Val params param Files Parameter Substitution Preprocessor psp new Parameter Substitution Preprocessor preprocessor Context Buffered Writer writer new Buffered Writer new File Writer output File Path psp gen Substituted File reader writer return new Buffered Reader new File Reader output File Path catch Parse Exception e log error e get Localized Message throw new Exception e catch File Not Found Exception e throw new Exception Could not find file to substitute parameters for output File Path script files as name file pairs to be added to the job jar return name file pairs public Map String File get Script Files return aliased Script Files public void rename String old Name String new Name throws Exception if old Name equals new Name return System out println Renaming old Name to new Name Element Descriptor dst null Element Descriptor src null try dst dfs as Element new Name src dfs as Element old Name catch Data Storage Exception e byte err Src get Error Source int err Code switch err Src case Pig Exception err Code break case Pig Exception err Code break default err Code break String msg Unable to rename old Name to new Name throw new Exec Exception msg err Code err Src e if dst exists dst delete src rename dst public void copy String src String dst boolean local Dst throws Exception Data Storage dst Storage dfs if local Dst dst Storage lfs Element Descriptor src Element null Element Descriptor dst Element null try src Element dfs as Element src dst Element dst Storage as Element dst catch Data Storage Exception e byte err Src get Error Source int err Code switch err Src case Pig Exception err Code break case Pig Exception err Code break default err Code break String msg Unable to copy src to dst throw new Exec Exception msg err Code err Src e src Element copy dst Element this properties false public Execution Engine get Execution Engine return execution Engine public Data Storage get Dfs return dfs public Data Storage get Lfs return lfs public Data Storage get Fs return dfs Provides configuration information return information about the configuration used to connect to execution engine public Properties get Properties return this properties deprecated use link get Properties instead Deprecated public Properties get Conf return get Properties public String get Last Alias return this last alias public void set Last Alias String value this last alias value Defines an alias for the given function spec This is useful for functions that require arguments to the constructor param function the new function alias to define param function Spec the Func Spec object representing the name of the function class and any arguments to constructor public void register Function String function Func Spec function Spec if function Spec null defined Functions remove function else defined Functions put function function Spec Defines an alias for the given streaming command This is useful for complicated streaming command specs param alias the new command alias to define param command the command public void register Stream Cmd String alias Streaming Command command if command null defined Commands remove alias else defined Commands put alias command Returns the type of execution currently in effect return current execution type public Exec Type get Exec Type return exec Type Creates a Classloader based on the passed jar File and any extra jar files param jar File the jar file to be part of the newly created Classloader This jar file plus any jars in the extra Jars list will constitute the classpath return the new Classloader throws Malformed Exception public Class Loader create Cl String jar File throws Malformed Exception int len extra Jars size int passed Jar jar File null urls new len passed Jar if jar File null urls new file jar File for int i i extra Jars size i urls i passed Jar extra Jars get i return new Class Loader urls Pig Map Reduce class get Class Loader return new Context Class Loader urls Pig Context class get Class Loader private static Map String Class get Class Cache Map String Class c class Cache get if c null c new Hash Map String Class class Cache set c return c Suppress Warnings rawtypes public static Class resolve Class Name String name throws Exception Map String Class cache get Class Cache Class c cache get name if c null return c for String prefix get Package Import List try c Class for Name prefix name true Pig Context classloader cache put name c return c catch Class Not Found Exception e do nothing catch Unsupported Class Version Error e int err Code String msg Problem resolving class version numbers for class name throw new Exec Exception msg err Code Pig Exception e create Class Not Found Exception exception and attach to Exception so that we do n t need to buble interface changes throughout the code int err Code String msg Could not resolve name using imports package Import List get throw new Exec Exception msg err Code Pig Exception common Pig pattern for initializing objects via system properties is to support passing something like this on the command line code Dpig notification listener My Class code code Dpig notification listener arg my Constructor String Arg code This method will properly initialize the class with the args if they exist param conf param class Param Key the property used to identify the class param arg Param Key the property used to identify the class args param clazz The class that is expected return public static instantiate Object From Params Configuration conf String class Param Key String arg Param Key Class clazz throws Exec Exception String class Name conf get class Param Key if class Name null Func Spec fs if conf get arg Param Key null fs new Func Spec class Name conf get arg Param Key else fs new Func Spec class Name try return clazz cast Pig Context instantiate Func From Spec fs catch Class Cast Exception e throw new Exec Exception The class defined by class Param Key in conf is not of type clazz get Name e else return null Suppress Warnings unchecked rawtypes public static Object instantiate Func From Spec Func Spec func Spec Object ret String class Name func Spec get Class Name String args func Spec get Ctor Args Class obj Class null try obj Class resolve Class Name class Name catch Exception ioe throw new Runtime Exception Can not instantiate class Name ioe Option Builder is not thread safe and Base Storage elephantbird Sequence File Config etc use them in constructor This leads to No Such Method Exception Unrecognized Option Exception etc when processor inputs and outputs are initialized in parallel in Tez synchronized instantiation Lock try Do normal instantiation if args null args length Class param Types new Class args length for int i i param Types length i param Types i String class Constructor c obj Class get Constructor param Types ret c new Instance Object args else ret obj Class new Instance catch No Such Method Exception nme Second chance Try with var arg constructor try Constructor c obj Class get Constructor String class Object wrapped Args new Object wrapped Args args ret c new Instance wrapped Args catch Throwable e bad luck String Builder sb new String Builder sb append could not instantiate sb append class Name sb append with arguments sb append Arrays to String args sb append throw new Runtime Exception sb to String e catch Throwable e bad luck String Builder sb new String Builder sb append could not instantiate sb append class Name sb append with arguments sb append Arrays to String args sb append throw new Runtime Exception sb to String e return ret public static Object instantiate Func From Spec String func Spec return instantiate Func From Spec new Func Spec func Spec Suppress Warnings rawtypes public Class get Class For Alias String alias throws Exception String class Name null Func Spec func Spec null if defined Functions null func Spec defined Functions get alias if func Spec null class Name func Spec get Class Name else class Name Func Spec get Class Name From Spec alias return resolve Class Name class Name public Object instantiate Func From Alias String alias throws Exception Func Spec func Spec if defined Functions null func Spec defined Functions get alias null return instantiate Func From Spec func Spec else return instantiate Func From Spec alias Get the link Streaming Command for the given alias param alias the alias for the code Streaming Command code return code Streaming Command code for the alias public Streaming Command get Command For Alias String alias return defined Commands get alias public void set Exec Type Exec Type exec Type this exec Type exec Type Create a new link Executable Manager depending on the Exec Type return a new link Executable Manager depending on the Exec Type throws Exec Exception public Executable Manager create Executable Manager throws Exec Exception if execution Engine null return execution Engine get Executable Manager return null public Func Spec get Func Spec From Alias String alias Func Spec func Spec if defined Functions null func Spec defined Functions get alias null return func Spec else return null Add a path to be skipped while automatically shipping binaries for streaming param path path to be skipped public void add Path To Skip String path skipped Ship Paths add path Get paths which are to skipped while automatically shipping binaries for streaming return paths which are to skipped while automatically shipping binaries for streaming public List String get Paths To Skip return skipped Ship Paths Check the execution mode and return the appropriate error source return error source public byte get Error Source return Pig Exception public static Array List String get Package Import List if package Import List get null Array List String importlist new Array List String importlist add importlist add java lang importlist add org apache pig builtin importlist add org apache pig impl builtin package Import List set importlist return package Import List get public static void set Package Import List Array List String list package Import List set list public void set Log j Properties Properties p log j Properties p public Properties get Log j Properties return log j Properties public Level get Default Log Level return default Log Level public void set Default Log Level Level l default Log Level l public int get Default Parallel return default Parallel public static Class Loader get Class Loader return classloader public static void set Class Loader Class Loader cl if cl instanceof Context Class Loader classloader Context Class Loader cl else classloader new Context Class Loader cl 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Byte Array Output Stream import java io Exception import java util Array List import java util List import java util Properties import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io Text import org apache hadoop io Writable import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Mapper import org apache log j Property Configurator import org apache pig Pig Constants import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig backend hadoop executionengine util Map Red Util import org apache pig data Data Bag import org apache pig data Schema Tuple Backend import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl Pig Impl Constants import org apache pig impl io Pig Nullable Writable import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig impl util Object Serializer import org apache pig impl util Pair import org apache pig impl util Spillable Memory Manager import org apache pig impl util Utils import org apache pig tools pigstats Pig Status Reporter This class is the base class for Pig Map Base which has slightly difference among different versions of hadoop Pig Map Base implementation is located in shims public abstract class Pig Generic Map Base extends Mapper Text Tuple Pig Nullable Writable Writable private final Log log Log Factory get Log get Class protected byte key Type Map Plan protected Physical Plan mp null Store operators protected List Store stores protected Tuple Factory tf Tuple Factory get Instance boolean in Illustrator false Context output Collector Reporter that will be used by operators to transmit heartbeat Progressable Reporter pig Reporter protected boolean error In Map false Physical Operator roots private Physical Operator leaf private volatile boolean initialized false for local map reduce simulation param plan the map plan public void set Map Plan Physical Plan plan mp plan Will be called when all the tuples in the input are done So reporter thread should be closed Override public void cleanup Context context throws Exception Interrupted Exception super cleanup context if error In Map error in map returning return if Pig Map Reduce s Job Conf Internal get get Job Control Compiler false equals true mp is Empty If there is a stream in the pipeline or if this map job belongs to merge join we could potentially have more to process so lets set the flag stating that all map input has been sent already and then lets run the pipeline one more time This will result in nothing happening in the case where there is no stream or it is not a merge join in the pipeline mp end Of All Input true run Pipeline leaf if in Illustrator for Store store stores if initialized Map Reduce Store Impl impl new Map Reduce Store Impl context store set Store Impl impl store set Up store tear Down Calling Eval Func finish Finish Visitor finisher new Finish Visitor mp new Dependency Order Walker Physical Operator Physical Plan mp try finisher visit catch Visitor Exception e int err Code String msg Error while calling finish method on Fs throw new Visitor Exception msg err Code Pig Exception e mp null Physical Operator set Reporter null initialized false Configures the mapper with the map plan and the reproter thread Suppress Warnings unchecked Override public void setup Context context throws Exception Interrupted Exception super setup context Configuration job context get Configuration Spillable Memory Manager get Instance configure job context get Configuration set Pig Constants Integer to String context get Task Attempt get Task get Id Pig Map Reduce s Job Context context Pig Map Reduce s Job Conf Internal set context get Configuration Pig Map Reduce s Job Conf context get Configuration in Illustrator in Illustrator context Pig Context set Package Import List Array List String Object Serializer deserialize job get udf import list This attempts to fetch all of the generated code from the distributed cache and resolve it Schema Tuple Backend initialize job Properties log j Properties Properties Object Serializer deserialize job get Pig Impl Constants if log j Properties null Property Configurator configure log j Properties if mp null mp Physical Plan Object Serializer deserialize job get pig map Plan stores Plan Helper get Physical Operators mp Store class To be removed if mp is Empty log debug Map Plan empty else Byte Array Output Stream baos new Byte Array Output Stream mp explain baos log debug baos to String key Type byte Object Serializer deserialize job get pig map keytype till here pig Reporter new Progressable Reporter Get the specific context Map Red Util setup Context job if mp is Empty Pig Split split Pig Split context get Input Split List Operator Key target Op Keys split get Target Ops Array List Physical Operator target Ops As List new Array List Physical Operator for Operator Key target Key target Op Keys target Ops As List add mp get Operator target Key roots target Ops As List to Array new Physical Operator leaf mp get Leaves get Pig Status Reporter pig Status Reporter Pig Status Reporter get Instance pig Status Reporter set Context new Task Context context log info Aliases being processed per job phase Alias Name line offset job get pig alias location Utils set Default Time Zone Pig Map Reduce s Job Conf Internal get The map function that attaches the inp Tuple appropriately and executes the map plan if its not empty Collects the result of execution into oc or the input directly to oc if map plan empty The collection is left abstract for the map only or map reduce job to implement Map only collects the tuple as is whereas map reduce collects it after extracting the key and indexed tuple Override protected void map Text key Tuple inp Tuple Context context throws Exception Interrupted Exception if initialized initialized true cache the collector for use in run Pipeline which can be called from close this output Collector context pig Reporter set Rep context Physical Operator set Reporter pig Reporter boolean aggregate Warning true equals Ignore Case context get Configuration get aggregate warning Pig Status Reporter pig Status Reporter Pig Status Reporter get Instance pig Status Reporter set Context new Task Context context Pig Hadoop Logger pig Hadoop Logger Pig Hadoop Logger get Instance pig Hadoop Logger set Reporter pig Status Reporter pig Hadoop Logger set Aggregate aggregate Warning Physical Operator set Pig Logger pig Hadoop Logger if in Illustrator for Store store stores Map Reduce Store Impl impl new Map Reduce Store Impl context store set Store Impl impl store set Up if mp is Empty collect context inp Tuple return for Physical Operator root roots if in Illustrator if root null root attach Input inp Tuple else root attach Input tf new Tuple No Copy inp Tuple get All run Pipeline leaf protected void run Pipeline Physical Operator leaf throws Exception Interrupted Exception while true Result res leaf get Next Tuple if res return Status Status collect output Collector Tuple res result continue if res return Status Status return if res return Status Status continue if res return Status Status remember that we had an issue so that in close we can do the right thing error In Map true if there is an errmessage use it String err Msg if res result null err Msg Received Error while processing the map plan res result else err Msg Received Error while processing the map plan int err Code Exec Exception ee new Exec Exception err Msg err Code Pig Exception throw ee abstract public void collect Context oc Tuple tuple throws Interrupted Exception Exception abstract public boolean in Illustrator Context context return the key Type public byte get Key Type return key Type param key Type the key Type to set public void set Key Type byte key Type this key Type key Type abstract public Context get Illustrator Context Configuration conf Data Bag input List Pair Pig Nullable Writable Writable output Input Split split throws Exception Interrupted Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Byte Array Output Stream import java io Exception import java util Array List import java util Iterator import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io Writable import org apache hadoop mapred jobcontrol Job import org apache hadoop mapreduce Job Context import org apache hadoop mapreduce Reducer import org apache pig Pig Constants import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop Data Type import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Join Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer util Plan Helper import org apache pig backend hadoop executionengine util Map Red Util import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Schema Tuple Backend import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io Nullable Partition Writable import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Visitor Exception import org apache pig impl util Object Serializer import org apache pig impl util Pair import org apache pig impl util Spillable Memory Manager import org apache pig impl util Context import org apache pig impl util Utils import org apache pig tools pigstats Pig Status Reporter This class is the static Mapper amp Reducer classes that are used by Pig to execute Pig Map Reduce jobs Since there is a reduce phase the leaf is bound to be a Local Rearrange So the map phase has to separate the key and tuple and collect it into the output collector The shuffle and sort phase sorts these keys amp tuples and creates key List lt Tuple gt and passes the key and iterator to the list The deserialized Package operator is used to package the key List lt Tuple gt into pig Key Bag lt Tuple gt where pig Key is of the appropriate pig type and then the result of the package is attached to the reduce plan which is executed if its not empty Either the result of the reduce plan or the package res is collected into the output collector The index of the tuple that is which bag it should be placed in by the package is packed into the key This is done so that hadoop sorts the keys in order of index for join This class is the base class for Pig Map Reduce which has slightly difference among different versions of hadoop Pig Map Reduce implementation is located in shims public class Pig Generic Map Reduce public static Job Context s Job Context null deprecated Use link Context instead in the following way to get the job s link Configuration pre Udf Context get Udf Context get Job Conf pre Deprecated public static Configuration s Job Conf null public static Thread Local Configuration s Job Conf Internal new Thread Local Configuration Static Data Cleanup public static void static Data Cleanup s Job Context null s Job Conf null s Job Conf Internal new Thread Local Configuration public static class Map extends Pig Map Base Override public void collect Context oc Tuple tuple throws Interrupted Exception Exception Byte index Byte tuple get Pig Nullable Writable key Data Type get Writable Comparable Types tuple get key Type Nullable Tuple val new Nullable Tuple Tuple tuple get Both the key and the value need the index The key needs it so that it can be sorted on the index in addition to the key value The value needs it so that Package can properly assign the tuple to its slot in the projection key set Index index val set Index index oc write key val This specialized map class is to be used in pig queries with order by a udf used for comparison in the order by expects to be handed tuples Hence this map class ensures that the key used in the order by is wrapped into a tuple if it is n t already a tuple public static class Map With Comparator extends Pig Map Base Override public void collect Context oc Tuple tuple throws Interrupted Exception Exception Object key Tuple null if key Type Data Type Object k tuple get key Tuple tf new Tuple k else key Tuple tuple get Byte index Byte tuple get Pig Nullable Writable key Data Type get Writable Comparable Types key Tuple Data Type Nullable Tuple val new Nullable Tuple Tuple tuple get Both the key and the value need the index The key needs it so that it can be sorted on the index in addition to the key value The value needs it so that Package can properly assign the tuple to its slot in the projection key set Index index val set Index index oc write key val Used by Skewed Join public static class Map With Partition Index extends Map Override public void collect Context oc Tuple tuple throws Interrupted Exception Exception Byte tuple Key Idx Byte tuple Val Idx Byte index Byte tuple get Integer partition Index for partitioning table the partition index is n t present if tuple size super collect oc tuple return tuple Key Idx tuple Val Idx else partition Index Integer tuple get Pig Nullable Writable key Data Type get Writable Comparable Types tuple get tuple Key Idx key Type Nullable Partition Writable wrapped Key new Nullable Partition Writable key Nullable Tuple val new Nullable Tuple Tuple tuple get tuple Val Idx Both the key and the value need the index The key needs it so that it can be sorted on the index in addition to the key value The value needs it so that Package can properly assign the tuple to its slot in the projection wrapped Key set Index index set the partition wrapped Key set Partition partition Index val set Index index oc write wrapped Key val Override protected void run Pipeline Physical Operator leaf throws Exception Interrupted Exception while true Result res leaf get Next Tuple if res return Status Status For Partition Rearrange the result is a bag This operator is used for skewed join if res result instanceof Data Bag Iterator Tuple its Data Bag res result iterator while its has Next collect output Collector its next else collect output Collector Tuple res result continue if res return Status Status return if res return Status Status continue if res return Status Status remember that we had an issue so that in close we can do the right thing error In Map true if there is an errmessage use it String err Msg if res result null err Msg Received Error while processing the map plan res result else err Msg Received Error while processing the map plan int err Code throw new Exec Exception err Msg err Code Pig Exception abstract public static class Reduce extends Reducer Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable protected final Log log Log Factory get Log get Class The reduce plan protected Physical Plan rp null Store operators protected List Store stores The Package operator which is the root of every Map Reduce plan is obtained through the job conf The portion remaining after its removal is the reduce plan protected Package pack Progressable Reporter pig Reporter protected Context output Collector protected boolean error In Reduce false Physical Operator roots private Physical Operator leaf protected volatile boolean initialized false private boolean in Illustrator false Set the reduce plan to be used by local runner for illustrator param plan Reduce plan public void set Reduce Plan Physical Plan plan rp plan Configures the Reduce plan the Package operator and the reporter thread Suppress Warnings unchecked Override protected void setup Context context throws Exception Interrupted Exception super setup context in Illustrator in Illustrator context if in Illustrator pack get Pack context Configuration j Conf context get Configuration Spillable Memory Manager get Instance configure j Conf context get Configuration set Pig Constants Integer to String context get Task Attempt get Task get Id s Job Context context s Job Conf Internal set context get Configuration s Job Conf context get Configuration try Pig Context set Package Import List Array List String Object Serializer deserialize j Conf get udf import list This attempts to fetch all of the generated code from the distributed cache and resolve it Schema Tuple Backend initialize j Conf if rp null rp Physical Plan Object Serializer deserialize j Conf get pig reduce Plan stores Plan Helper get Physical Operators rp Store class if in Illustrator pack Package Object Serializer deserialize j Conf get pig reduce package To be removed if rp is Empty log debug Reduce Plan empty else Byte Array Output Stream baos new Byte Array Output Stream rp explain baos log debug baos to String pig Reporter new Progressable Reporter if rp is Empty roots rp get Roots to Array new Physical Operator leaf rp get Leaves get Get the specific context Map Red Util setup Context j Conf catch Exception ioe String msg Problem while configuring reduce plan throw new Runtime Exception msg ioe log info Aliases being processed per job phase Alias Name line offset j Conf get pig alias location Utils set Default Time Zone Pig Map Reduce s Job Conf Internal get The reduce function which packages the key and List lt Tuple gt into key Bag lt Tuple gt after converting Hadoop type key into Pig type The package result is either collected as is if the reduce plan is empty or after passing through the reduce plan Override protected void reduce Pig Nullable Writable key Iterable Nullable Tuple tup Iter Context context throws Exception Interrupted Exception if initialized initialized true cache the collector for use in run Pipeline which could additionally be called from close this output Collector context pig Reporter set Rep context Physical Operator set Reporter pig Reporter boolean aggregate Warning true equals Ignore Case context get Configuration get aggregate warning Pig Status Reporter pig Status Reporter Pig Status Reporter get Instance pig Status Reporter set Context new Task Context context Pig Hadoop Logger pig Hadoop Logger Pig Hadoop Logger get Instance pig Hadoop Logger set Reporter pig Status Reporter pig Hadoop Logger set Aggregate aggregate Warning Physical Operator set Pig Logger pig Hadoop Logger if in Illustrator for Store store stores Map Reduce Store Impl impl new Map Reduce Store Impl context store set Store Impl impl store set Up In the case we optimize the join we combine Package and Foreach so we could get many tuples out of the getnext call of Join Package In this case we process till we see from Join Pacakage get Next if pack get Pkgr instanceof Join Packager pack attach Input key tup Iter iterator while true if process One Package Output context break else join is not optimized so package will give only one tuple out for the key pack attach Input key tup Iter iterator process One Package Output context return false more output true end of processing public boolean process One Package Output Context oc throws Exception Interrupted Exception Result res pack get Next Tuple if res return Status Status Tuple pack Res Tuple res result if rp is Empty oc write null pack Res return false for int i i roots length i roots i attach Input pack Res run Pipeline leaf if res return Status Status return false if res return Status Status int err Code String msg Encountered error in package operator while processing group throw new Exec Exception msg err Code Pig Exception if res return Status Status return true return false param leaf throws Interrupted Exception throws Exception protected void run Pipeline Physical Operator leaf throws Interrupted Exception Exception while true Result red Res leaf get Next Tuple if red Res return Status Status try output Collector write null Tuple red Res result catch Exception e throw new Exception e continue if red Res return Status Status return if red Res return Status Status continue if red Res return Status Status remember that we had an issue so that in close we can do the right thing error In Reduce true if there is an errmessage use it String msg if red Res result null msg Received Error while processing the reduce plan red Res result else msg Received Error while processing the reduce plan int err Code throw new Exec Exception msg err Code Pig Exception Will be called once all the intermediate keys and values are processed So right place to stop the reporter thread Override protected void cleanup Context context throws Exception Interrupted Exception super cleanup context if error In Reduce there was an error in reduce just return return if Pig Map Reduce s Job Conf Internal get get pig stream in reduce false equals true rp is Empty If there is a stream in the pipeline we could potentially have more to process so lets set the flag stating that all map input has been sent already and then lets run the pipeline one more time This will result in nothing happening in the case where there is no stream in the pipeline rp end Of All Input true run Pipeline leaf if in Illustrator for Store store stores if initialized Map Reduce Store Impl impl new Map Reduce Store Impl context store set Store Impl impl store set Up store tear Down Calling Eval Func finish Finish Visitor finisher new Finish Visitor rp new Dependency Order Walker Physical Operator Physical Plan rp try finisher visit catch Visitor Exception e throw new Exception Error trying to finish Fs e Physical Operator set Reporter null initialized false Get reducer s illustrator context param input Input buffer as output by maps param pkg package return reducer s illustrator context throws Exception throws Interrupted Exception abstract public Context get Illustrator Context Job job List Pair Pig Nullable Writable Writable input Package pkg throws Exception Interrupted Exception abstract public boolean in Illustrator Context context abstract public Package get Pack Context context This specialized reduce class is to be used in pig queries with order by a udf used for comparison in the order by expects to be handed tuples Hence a specialized map class Pig Map Reduce Map With Comparator ensures that the key used in the order by is wrapped into a tuple if it is n t already a tuple This reduce class unwraps this tuple in the case where the map had wrapped into a tuple and handes the unwrapped key to the Package for processing public static class Reduce With Comparator extends Pig Map Reduce Reduce private byte key Type Configures the Reduce plan the Package operator and the reporter thread Override protected void setup Context context throws Exception Interrupted Exception super setup context key Type pack get Pkgr get Key Type The reduce function which packages the key and List lt Tuple gt into key Bag lt Tuple gt after converting Hadoop type key into Pig type The package result is either collected as is if the reduce plan is empty or after passing through the reduce plan Override protected void reduce Pig Nullable Writable key Iterable Nullable Tuple tup Iter Context context throws Exception Interrupted Exception if initialized initialized true cache the collector for use in run Pipeline which could additionally be called from close this output Collector context pig Reporter set Rep context Physical Operator set Reporter pig Reporter boolean aggregate Warning true equals Ignore Case context get Configuration get aggregate warning Pig Status Reporter pig Status Reporter Pig Status Reporter get Instance pig Status Reporter set Context new Task Context context Pig Hadoop Logger pig Hadoop Logger Pig Hadoop Logger get Instance pig Hadoop Logger set Reporter pig Status Reporter pig Hadoop Logger set Aggregate aggregate Warning Physical Operator set Pig Logger pig Hadoop Logger for Store store stores Map Reduce Store Impl impl new Map Reduce Store Impl context store set Store Impl impl store set Up If the key Type is not a tuple the Map With Comparator collect would have wrapped the key into a tuple so that the comparison used in the order by can process it We need to unwrap the key out of the tuple and hand it to the Package for processing if key Type Data Type Tuple t Tuple key get Value As Pig Type try key Data Type get Writable Comparable Types t get key Type catch Exec Exception e throw e pack attach Input key tup Iter iterator Result res pack get Next Tuple if res return Status Status Tuple pack Res Tuple res result if rp is Empty context write null pack Res return rp attach Input pack Res List Physical Operator leaves rp get Leaves Physical Operator leaf leaves get run Pipeline leaf if res return Status Status return if res return Status Status int err Code String msg Encountered error in package operator while processing group throw new Exec Exception msg err Code Pig Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl import org apache pig classification Interface Audience Private internal constants for use by Pig itself Please see link org apache pig Pig Constants if looking for public constants Interface Audience Private public class Pig Impl Constants private Pig Impl Constants throw new Illegal State Exception value is a pig internal properties key for serializing the set of disabled optimizer rules public static final String pig optimizer rules Used by pig to indicate that current job is running in local mode local tez local ie Exec Type is Local is true public static final String pig exectype mode local Used by pig to indicate that current job has been converted to run in local mode public static final String pig job converted local Used by pig to indicate that current job has been converted to run in fetch mode public static final String pig job converted fetch Indicate the split index of the task Used by merge cogroup public static final String pig split index Parallelism for the reducer public static final String pig info reducers default parallel public static final String pig info reducers requested parallel public static final String pig info reducers estimated parallel Parallelism to be used for operation by Cross public static final String pig cross parallelism Pig context public static final String pig pig Context Pig log j properties public static final String pig log j properties unique id for a Pig session used as caller Id for underlining component public static final String pig script id Used to carry zone list from frontend to backend generated by frontend during Job creation public static final String pig datetime zones list Kill the jobs before cleaning up tmp files public static int public static int public static int 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java util Array List import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop io Text import org apache hadoop mapred Job Conf import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Job Context import org apache hadoop mapreduce Task Attempt Context import org apache pig Collectable Load Func import org apache pig Func Spec import org apache pig Indexable Load Func import org apache pig Load Func import org apache pig Ordered Load Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine shims Hadoop Shims import org apache pig backend hadoop executionengine util Map Red Util import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl plan Operator Key import org apache pig impl util Object Serializer import org apache pig impl util Context import org apache pig impl util Utils public class Pig Input Format extends Input Format Text Tuple public static final Log log Log Factory get Log Pig Input Format class public static final String pig inputs public static final String pig inp Targets public static final String pig inp Signatures public static final String pig inp Limits non Javadoc see org apache hadoop mapreduce Input Format create Record Reader org apache hadoop mapreduce Input Split org apache hadoop mapreduce Task Attempt Context Suppress Warnings rawtypes unchecked Override public org apache hadoop mapreduce Record Reader Text Tuple create Record Reader org apache hadoop mapreduce Input Split split Task Attempt Context context throws Exception Interrupted Exception Record Reader Factory factory new Record Reader Factory split context return factory create Record Reader Helper class to create record reader protected static class Record Reader Factory protected Input Format input Format protected Pig Split pig Split protected Load Func load Func protected Task Attempt Context context protected long limit public Record Reader Factory org apache hadoop mapreduce Input Split split Task Attempt Context context throws Exception We need to create a Task Attempt Context based on the Configuration which was used in the get Splits to produce the split supplied here For this let s find out the input of the script which produced the split supplied here and then get the corresponding Configuration and setup Task Attempt Context based on it and then call the real Input Format s create Record Reader method Pig Split pig Split Pig Split split hadoop new integration get around a hadoop bug by passing total of splits to each split so it can be retrieved here and set it to the configuration object This number is needed by Poisson Sample Loader to compute the number of samples int n pig Split get Total Splits context get Configuration set Int pig mapsplits count n Configuration conf context get Configuration Pig Context set Package Import List Array List String Object Serializer deserialize conf get udf import list Map Red Util setup Context conf Load Func load Func get Load Func pig Split get Input Index conf Pass loader signature to Load Func and to Input Format through the conf pass Load Signature load Func pig Split get Input Index conf merge entries from split specific conf into the conf we got Pig Input Format merge Split Specific Conf load Func pig Split conf Input Format input Format load Func get Input Format List Long inp Limit Lists Array List Long Object Serializer deserialize conf get this input Format input Format this pig Split pig Split this load Func load Func this context context this limit inp Limit Lists get pig Split get Input Index public org apache hadoop mapreduce Record Reader Text Tuple create Record Reader throws Exception Interrupted Exception return new Pig Record Reader input Format pig Split load Func context limit get the corresponding configuration for the input on which the split is based and merge it with the Conf supplied package level access so that this is not publicly used elsewhere throws Exception static void merge Split Specific Conf Load Func load Func Pig Split pig Split Configuration original Conf throws Exception set up conf with entries from input specific conf Job job new Job original Conf load Func set Location get Load Location pig Split get Input Index original Conf job The above set Location call could write to the conf within the job merge that updated conf with original conf Configuration Util merge Conf original Conf job get Configuration param input Index param conf return throws Exception Suppress Warnings unchecked private static Load Func get Load Func int input Index Configuration conf throws Exception Array List File Spec inputs Array List File Spec Object Serializer deserialize conf get Func Spec load Func Spec inputs get input Index get Func Spec return Load Func Pig Context instantiate Func From Spec load Func Spec Suppress Warnings unchecked private static String get Load Location int input Index Configuration conf throws Exception Array List File Spec inputs Array List File Spec Object Serializer deserialize conf get return inputs get input Index get File Name Pass loader signature to Load Func and to Input Format through the conf param load Func the Loadfunc to set the signature on param input Index the index of the input corresponding to the loadfunc param conf the Configuration object into which the signature should be set throws Exception on failure Suppress Warnings unchecked static void pass Load Signature Load Func load Func int input Index Configuration conf throws Exception List String inp Signature Lists Array List String Object Serializer deserialize conf get signature can be null for intermediate jobs where it will not be required to be passed down if inp Signature Lists get input Index null load Func set Context Signature inp Signature Lists get input Index conf set pig loader signature inp Signature Lists get input Index Map Red Util setup Context conf non Javadoc see org apache hadoop mapreduce Input Format get Splits org apache hadoop mapreduce Job Context Suppress Warnings unchecked rawtypes Override public List Input Split get Splits Job Context jobcontext throws Exception Interrupted Exception Configuration conf jobcontext get Configuration Array List File Spec inputs Array List Array List Operator Key inp Targets try inputs Array List File Spec Object Serializer deserialize conf get inp Targets Array List Array List Operator Key Object Serializer deserialize conf get Pig Context set Package Import List Array List String Object Serializer deserialize conf get udf import list Map Red Util setup Context conf catch Exception e int err Code String msg Unable to deserialize object throw new Exec Exception msg err Code Pig Exception e Array List Input Split splits new Array List Input Split for int i i inputs size i try Path path new Path inputs get i get File Name File System fs boolean is Fs Path true try fs path get File System conf catch Exception e If an application specific scheme was used e g hbase table we will fail getting the file system That s ok we just use the dfs in that case fs new Path get File System conf is Fs Path false if the execution is against Mapred set working dir to user userid if Utils is Local conf fs set Working Directory jobcontext get Working Directory first pass input location to the loader for this send a clone of the configuration we have this is so that if the loader or the inputformat of the loader decide to store the input location into the configuration for example File Input Format stores this in mapred input dir in the conf then for different inputs the loader s do n t end up over writing the same conf Func Spec load Func Spec inputs get i get Func Spec Load Func load Func Load Func Pig Context instantiate Func From Spec load Func Spec boolean combinable load Func instanceof Merge Join Indexer load Func instanceof Indexable Load Func load Func instanceof Collectable Load Func load Func instanceof Ordered Load Func if combinable combinable conf get Boolean pig no Split Combination false Job Conf conf Clone new Job Conf conf Job input Specific Job new Job conf Clone Pass loader signature to Load Func and to Input Format through the conf pass Load Signature load Func i input Specific Job get Configuration load Func set Location inputs get i get File Name input Specific Job The above set Location call could write to the conf within the input Specific Job use this updated conf get the Input Format from it and ask for splits Input Format inp Format load Func get Input Format List Input Split one Input Splits inp Format get Splits Hadoop Shims create Job Context input Specific Job get Configuration jobcontext get Job List Input Split one Input Pig Splits get Pig Splits one Input Splits i inp Targets get i fs get Default Block Size is Fs Path path fs get Working Directory combinable conf Clone splits add All one Input Pig Splits catch Exec Exception ee throw ee catch Exception e int err Code String msg Unable to create input splits for inputs get i get File Name if e get Message null e get Message is Empty throw new Exec Exception e get Message err Code Pig Exception e else throw new Exec Exception msg err Code Pig Exception e hadoop new integration get around a hadoop bug by passing total of splits to each split so that it can be retrieved in the Record Reader method when called by mapreduce framework later int n splits size also passing the multi input flag to the back end so that the multi input record counters can be created int m inputs size boolean disable Counter conf get Boolean pig disable counter false if m disable Counter log info Disable Pig custom input counters for Input Split split splits Pig Split split set Total Splits n if m Pig Split split set Multi Inputs true Pig Split split set Disable Counter disable Counter return splits protected List Input Split get Pig Splits List Input Split one Input Splits int input Index Array List Operator Key target Ops long block Size boolean combinable Configuration conf throws Exception Interrupted Exception Array List Input Split pig Splits new Array List Input Split if combinable int split Index for Input Split input Split one Input Splits Pig Split pig Split new Pig Split new Input Split input Split input Index target Ops split Index pig Split set Conf conf pig Splits add pig Split return pig Splits else long max Combined Split Size conf get Long pig max Combined Split Size if max Combined Split Size default is the block size max Combined Split Size block Size List List Input Split combined Splits Map Red Util get Combine Pig Splits one Input Splits max Combined Split Size conf for int i i combined Splits size i pig Splits add create Pig Split combined Splits get i input Index target Ops i conf return pig Splits private Input Split create Pig Split List Input Split combined Splits int input Index Array List Operator Key target Ops int split Index Configuration conf Pig Split pig Split new Pig Split combined Splits to Array new Input Split input Index target Ops split Index pig Split set Conf conf return pig Split 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig parser import java io Buffered Reader import java io Exception import java io Stream Tokenizer import java io String Reader import java io String Writer import java util Arrays import java util Array List import java util Hash Set import java util List import java util Map import java util Set import org antlr runtime Char Stream import org antlr runtime Common Token Stream import org antlr runtime Recognition Exception import org antlr runtime tree Common Tree import org antlr runtime tree Common Tree Node Stream import org antlr runtime tree Tree import org apache commons logging Log import org apache commons logging Log Factory import org apache pig impl Pig Context import org apache pig parser Pig Parser Node Invocation Point import org apache pig tools parameters Parameter Substitution Preprocessor import org apache pig tools parameters Preprocessor Context class Pig Macro private static final Log Log Factory get Log Pig Macro class private String file Name private String name private String body private List String params private List String rets private Map String Pig Macro seen private Set String macro Stack private Pig Context pig Context private long idx The start line number of this macro in the script private int start Line Pig Macro String name String file List String params List String returns String body Map String Pig Macro seen this name name this params params null new Array List String params this rets returns null new Array List String returns this file Name file this body body this seen seen this macro Stack new Hash Set String debug Macro name is defined String get Name return name void set Stack Set String stack macro Stack stack Set String get Stack return macro Stack void set Start Line int start this start Line start int get Start Line return start Line void set Pig Context Pig Context pig Context this pig Context pig Context private Common Tree inline String inputs String outputs Common Tree t String file throws Parser Exception String in substitute Params inputs outputs t get Line file Set String masks new Hash Set String if inputs null for String s inputs masks add s for String s outputs masks add s return mask Alias in masks t file private String substitute Params String inputs String outputs int line String file throws Parser Exception if inputs null params is Empty inputs null inputs length params size String msg get Error Message file line Failed to expand macro name Expected number of parameters params size actual number of inputs inputs null inputs length throw new Parser Exception msg boolean is Void Return false if rets is Empty if outputs null outputs length String msg get Error Message file line Can not expand macro name Expected number of return aliases actual number of return values outputs length throw new Parser Exception msg is Void Return true if is Void Return outputs null rets is Empty outputs null outputs length rets size String msg get Error Message file line Failed to expand macro name Expected number of return aliases rets size actual number of return values outputs null outputs length throw new Parser Exception msg String args new String params size for int i i params size i if inputs i starts With inputs i inputs i args i params get i inputs i if is Void Return String args new String params size rets size System arraycopy args args params size args args for int i i rets size i args params size i rets get i outputs i String Writer writer new String Writer Buffered Reader in new Buffered Reader new String Reader body try Preprocessor Context pc new Preprocessor Context pc load Param Val Arrays as List args null Map String String param Val pc get Param Val for Map Entry String String e pig Context get Param Val entry Set overwrite false since macro parameters should have precedence over commandline parameters if keys overlap pc process Ord Line e get Key e get Value false Parameter Substitution Preprocessor psp new Parameter Substitution Preprocessor pc psp gen Substituted File in writer catch Exception e catch both Parser Exception and Runtime Exception String msg get Error Message file line Macro inline failed for macro name e get Message n Macro content body throw new Parser Exception msg debug after substition n writer to String return writer to String private Common Tree mask Alias String in Set String masks Common Tree tree String file throws Parser Exception this is the node the real line number is in the macro name node int line tree get Child get Line Char Stream input null try parse macro body into ast input new Query Parser String Stream in file catch Exception e String msg get Error Message file line Failed to inline macro name e get Message nmacro content in throw new Parser Exception msg Query Lexer lex new Query Lexer input Common Token Stream tokens new Common Token Stream lex Query Parser query return result null Query Parser parser Query Parser Utils create Parser tokens start Line try result parser query catch Recognition Exception e e line start Line String msg file Name null parser get Error Header e Query Parser Utils generate Error Header e file Name msg parser get Error Message e parser get Token Names String msg get Error Message file line Failed to parse macro name msg nmacro content in throw new Parser Exception msg Common Tree ast Common Tree result get Tree debug for macro name n ast to String Tree List Common Tree macro Def Nodes new Array List Common Tree traverse Macro ast macro Def Nodes if macro Def Nodes is Empty String fname Pig Parser Node ast get File Name String msg get Error Message fname ast get Line Invalid macro definition macro name contains macro definition nmacro content body throw new Parser Exception msg add macro invocation points to the expanded macro tree Pig Parser Node pnode Pig Parser Node tree List Invocation Point inv Stack pnode get Invocation Stack List Invocation Point new Inv Stack inv Stack null new Array List Invocation Point new Array List Invocation Point inv Stack Invocation Point pt new Invocation Point line file name new Inv Stack add pt set Invocation Stack ast new Inv Stack recursively expand the inline macros List Common Tree inline Nodes new Array List Common Tree traverse Macro ast inline Nodes for Common Tree t inline Nodes Common Tree new Tree macro Inline t new Array List Pig Macro seen values macro Stack pig Context Query Parser Utils replace Node With Node List t new Tree null mask the aliases in the inlined macro Common Tree Node Stream nodes new Common Tree Node Stream ast Alias Masker walker new Alias Masker nodes walker set Params masks name idx Alias Masker query return result null Common Tree common Tree null try result walker query catch Recognition Exception e e line start Line String msg walker get Error Header e walker get Error Message e walker get Token Names String msg get Error Message file line Failed to mask macro name msg nmacro content in throw new Parser Exception msg common Tree result tree debug for masked macro name n common Tree to String Tree return common Tree private static void set Invocation Stack Tree ast List Invocation Point stack Pig Parser Node node Pig Parser Node ast node set Invocation Stack stack int n node get Child Count for int i i n i set Invocation Stack node get Child i stack Validates that return alias exists in the macro body void validate throws Exception if rets is Empty return Hash Set String test Set new Hash Set String Stream Tokenizer st new Stream Tokenizer new String Reader body st word Chars st word Chars st word Chars st word Chars st lower Case Mode false st ordinary Char st slash Star Comments true while st next Token Stream Tokenizer if match Word st define false match Dollar Alias st true test Set add st sval substring else if match Dollar Alias st false String prev Word st sval if match Word st if true match Word st otherwise true test Set add prev Word substring else if match Char st true match Char st true test Set add prev Word substring else if match Char st true possible mult alias inlining of a macro Array List String mlist new Array List String mlist add prev Word if is Multi Value Return st mlist true for String s mlist test Set add s substring else if match Char st false match Char st true skip Single Line Comment st for String s rets if test Set contains s throw new Exception Macro name missing return alias s check for multi value return pattern alias alias alias private static boolean is Multi Value Return Stream Tokenizer st List String mlist boolean comma throws Exception int lookahead st next Token if comma lookahead Stream Tokenizer comma match Char st false if match Dollar Alias st false mlist add st sval return is Multi Value Return st mlist comma if comma lookahead match Char st true return true return false private static boolean match Dollar Alias Stream Tokenizer st boolean next throws Exception int type next st next Token st ttype if type Stream Tokenizer st sval char At st sval length return true if next st push Back return false private static boolean match Word Stream Tokenizer st String word boolean next throws Exception int type next st next Token st ttype if type Stream Tokenizer st sval equals Ignore Case word return true if next st push Back return false private static boolean match Char Stream Tokenizer st int c boolean next throws Exception int type next st next Token st ttype if type c return true if next st push Back return false private static void skip Single Line Comment Stream Tokenizer st throws Exception int line No st lineno int lookahead st next Token while lookahead Stream Tokenizer lookahead n if st lineno line No break lookahead st next Token st push Back private static void traverse Macro Tree t List Common Tree nodes String node Type if t get Text equals node Type nodes add Common Tree t int n t get Child Count for int i i n i Tree t t get Child i traverse Macro t nodes node Type Macro inline nodes have the following form name values values Child nodes macro name list of return values list of parameters static Common Tree macro Inline Common Tree t List Pig Macro macro Defs Set String macro Stack Pig Context pig Context throws Parser Exception get name String mn t get Child get Text get macro Def Pig Macro macro null for Pig Macro pm macro Defs if pm get Name equals mn macro pm break String file Pig Parser Node t get File Name if macro null String msg get Error Message file t get Line Can not expand macro mn Macro must be defined before expansion throw new Parser Exception msg if macro Stack contains macro name String msg get Error Message file t get Line Can not expand macro mn Macro ca n t be defined circularly throw new Parser Exception msg set nested macro call stack Set String new Stack new Hash Set String macro Stack new Stack add macro name macro set Stack new Stack inform the macro of the Pig Context so it can substitute parameters from the main pigscript macro set Pig Context pig Context get return values int n t get Child get Child Count String rets new String n for int i i n i rets i t get Child get Child i get Text get parameters int m t get Child get Child Count String params new String m for int i i m i params i t get Child get Child i get Text return macro inline params rets t file private static String get Error Message String file int line String header String reason String Builder sb new String Builder sb append if file null sb append file append file append sb append line append line append append header if reason null sb append Reason append reason return sb to String 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java net import java util Iterator import java util List import org apache hadoop conf Configuration import org apache hadoop conf Configuration Integer Ranges import org apache hadoop fs Path import org apache hadoop io Raw Comparator import org apache hadoop io Text import org apache hadoop io Writable import org apache hadoop mapreduce Counter import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Mapper import org apache hadoop mapreduce Output Committer import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Partitioner import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce Reducer import org apache hadoop mapreduce Status Reporter import org apache hadoop mapreduce Task Attempt import org apache hadoop mapreduce Reducer Context import org apache hadoop mapreduce lib map Wrapped Mapper import org apache hadoop mapreduce task Map Context Impl import org apache hadoop security Credentials import org apache pig backend hadoop executionengine map Reduce Layer Pig Generic Map Base import org apache pig data Data Bag import org apache pig data Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl util Pair import org apache hadoop mapreduce lib map Wrapped Mapper abstract public class Pig Map Base extends Pig Generic Map Base Get mapper s illustrator context param conf Configuration param input Input bag to serve as data source param output Map output buffer param split the split return Illustrator s context throws Exception throws Interrupted Exception Override public Context get Illustrator Context Configuration conf Data Bag input List Pair Pig Nullable Writable Writable output Input Split split throws Exception Interrupted Exception org apache hadoop mapreduce Mapper Context mapper Context new Wrapped Mapper Text Tuple Pig Nullable Writable Writable get Map Context new Illustrator Context conf input output split return mapper Context public class Illustrator Context extends Map Context Impl Text Tuple Pig Nullable Writable Writable private Data Bag input List Pair Pig Nullable Writable Writable output private Iterator Tuple it null private Tuple value null private boolean init false public Illustrator Context Configuration conf Data Bag input List Pair Pig Nullable Writable Writable output Input Split split throws Exception Interrupted Exception super conf new Task Attempt null null null new Illustrate Dummy Reporter split conf set in Illustrator true if output null throw new Exception Null output can not be used this input input this output output Override public boolean next Key Value throws Exception Interrupted Exception if input null if init init true return true return false if it null it input iterator if it has Next return false value it next return true Override public Text get Current Key return null Override public Tuple get Current Value return value Override public void write Pig Nullable Writable key Writable value throws Exception Interrupted Exception output add new Pair Pig Nullable Writable Writable key value Override public void progress Override public boolean in Illustrator Context context return Wrapped Mapper Context context get Configuration get in Illustrator null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Byte Array Output Stream import java io Data Output Stream import java io Exception import java net import java util Array List import java util Collections import java util Comparator import java util Iterator import java util List import org apache hadoop classification Interface Stability import org apache hadoop conf Configuration import org apache hadoop conf Configuration Integer Ranges import org apache hadoop fs Path import org apache hadoop io Raw Comparator import org apache hadoop io Text import org apache hadoop io Writable import org apache hadoop mapred jobcontrol Job import org apache hadoop mapreduce Counter import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Mapper import org apache hadoop mapreduce Output Committer import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Partitioner import org apache hadoop mapreduce Reduce Context import org apache hadoop mapreduce Reducer import org apache hadoop mapreduce Task Attempt import org apache hadoop mapreduce Reducer Context import org apache hadoop mapreduce lib reduce Wrapped Reducer import org apache hadoop mapreduce task Reduce Context Impl import org apache hadoop security Credentials import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Base Illustrator Context import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig data Tuple import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl util Pair import org apache pig pen Fake Raw Key Value Iterator public class Pig Map Reduce extends Pig Generic Map Reduce static class Illustrate Reducer Context extends Wrapped Reducer Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable public Illustrator Context get Reducer Context Reduce Context Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable reduce Context return new Illustrator Context reduce Context public class Illustrator Context extends Wrapped Reducer Context public Illustrator Context Reduce Context Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable reduce Context super reduce Context public Package get Pack return Reduce Illustrator Context Impl reduce Context pack public static class Reduce extends Pig Generic Map Reduce Reduce Get reducer s illustrator context param input Input buffer as output by maps param pkg package return reducer s illustrator context throws Exception throws Interrupted Exception Override public Context get Illustrator Context Job job List Pair Pig Nullable Writable Writable input Package pkg throws Exception Interrupted Exception org apache hadoop mapreduce Reducer Context reducer Context new Illustrate Reducer Context get Reducer Context new Illustrator Context Impl job input pkg return reducer Context Suppress Warnings unchecked public class Illustrator Context Impl extends Reduce Context Impl Pig Nullable Writable Nullable Tuple Pig Nullable Writable Writable private Pig Nullable Writable current Key null next Key null private Nullable Tuple next Value null private List Nullable Tuple current Values null private Iterator Pair Pig Nullable Writable Writable it private final Byte Array Output Stream bos private final Data Output Stream dos private final Raw Comparator sort Comparator grouping Comparator public Package pack null private Illustrator Value Iterable iterable new Illustrator Value Iterable public Illustrator Context Impl Job job List Pair Pig Nullable Writable Writable input Package pkg throws Exception Interrupted Exception super job get Job Conf new Task Attempt new Fake Raw Key Value Iterator input iterator has Next null null null null new Illustrate Dummy Reporter null Pig Nullable Writable class Nullable Tuple class bos new Byte Array Output Stream dos new Data Output Stream bos org apache hadoop mapreduce Job nw Job new org apache hadoop mapreduce Job job get Job Conf sort Comparator nw Job get Sort Comparator grouping Comparator nw Job get Grouping Comparator Collections sort input new Comparator Pair Pig Nullable Writable Writable Override public int compare Pair Pig Nullable Writable Writable o Pair Pig Nullable Writable Writable o try o first write dos int l bos size o first write dos int l bos size byte bytes bos to Byte Array bos reset return sort Comparator compare bytes l bytes l l l catch Exception e throw new Runtime Exception Serialization exception in sort e get Message current Values new Array List Nullable Tuple it input iterator if it has Next Pair Pig Nullable Writable Writable entry it next next Key entry first next Value Nullable Tuple entry second pack pkg public class Illustrator Value Iterator implements Reduce Context Value Iterator Nullable Tuple private int pos private int mark Override public void mark throws Exception mark pos if mark mark Override public void reset throws Exception pos mark Override public void clear Mark throws Exception mark Override public boolean has Next return pos current Values size Override public Nullable Tuple next pos return current Values get pos Override public void remove throw new Unsupported Operation Exception remove not implemented Override public void reset Backup Store throws Exception pos mark protected class Illustrator Value Iterable implements Iterable Nullable Tuple private Illustrator Value Iterator iterator new Illustrator Value Iterator Override public Iterator Nullable Tuple iterator return iterator Override public Pig Nullable Writable get Current Key return current Key Override public boolean next Key if next Key null return false current Key next Key current Values clear current Values add next Value next Key null for it has Next Pair Pig Nullable Writable Writable entry it next Why ca n t raw comparison be used byte bytes int l l try current Key write dos l bos size entry first write dos l bos size bytes bos to Byte Array catch Exception e throw new Runtime Exception next Key exception e get Message bos reset if grouping Comparator compare bytes l bytes l l l if grouping Comparator compare current Key entry first current Values add Nullable Tuple entry second else next Key entry first next Value Nullable Tuple entry second break return true Override public Iterable Nullable Tuple get Values return iterable Override public void write Pig Nullable Writable k Writable t Override public void progress Override public boolean in Illustrator org apache hadoop mapreduce Reducer Context context return context instanceof Pig Map Reduce Illustrate Reducer Context Illustrator Context Override public Package get Pack org apache hadoop mapreduce Reducer Context context return Pig Map Reduce Illustrate Reducer Context Illustrator Context context get Pack 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java util Array List import java util List import org apache hadoop conf Configuration import org apache hadoop mapreduce Job import org apache hadoop mapreduce Job Context import org apache hadoop mapreduce Job Status State import org apache hadoop mapreduce Output Committer import org apache hadoop mapreduce Task Attempt Context import org apache pig Resource Schema import org apache pig Store Func Interface import org apache pig Store Metadata import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine shims Hadoop Shims import org apache pig backend hadoop executionengine util Map Red Util import org apache pig impl logical Layer schema Schema import org apache pig impl util Pair import org apache pig Pig Configuration specialization of the default File Output Committer to allow pig to inturn delegate calls to the Output Commiter s of the Store Func s Output Format s public class Pig Output Committer extends Output Committer Output Committer s of Store s in the map List Pair Output Committer Store map Output Committers Output Committer s of Store s in the reduce List Pair Output Committer Store reduce Output Committers boolean recovery Supported param context param map Stores param reduce Stores throws Exception public Pig Output Committer Task Attempt Context context List Store map Stores List Store reduce Stores throws Exception create and store the map and reduce output committers map Output Committers get Committers context map Stores reduce Output Committers get Committers context reduce Stores recovery Supported context get Configuration get Boolean Pig Configuration false param conf param map Stores return throws Exception Suppress Warnings unchecked private List Pair Output Committer Store get Committers Task Attempt Context context List Store stores throws Exception List Pair Output Committer Store committers new Array List Pair Output Committer Store for Store store stores Store Func Interface s Func store get Store Func Task Attempt Context updated Context set Up Context context store try committers add new Pair Output Committer Store s Func get Output Format get Output Committer updated Context store catch Interrupted Exception e throw new Exception e return committers private Task Attempt Context set Up Context Task Attempt Context context Store store throws Exception Setup Context so Store Func can make use of it Map Red Util setup Context context get Configuration make a copy of the context so that the actions after this call do not end up updating the same context Task Attempt Context context Copy Hadoop Shims create Task Attempt Context context get Configuration context get Task Attempt call set Location on the store Func so that if there are any side effects like setting map output dir on the Configuration in the Context are needed by the Output Committer those actions will be done before the committer is created Pig Output Format set Location context Copy store return context Copy static public Job Context set Up Context Job Context context Store store throws Exception make a copy of the context so that the actions after this call do not end up updating the same context Job Context context Copy Hadoop Shims create Job Context context get Configuration context get Job Map Red Util setup Context context get Configuration call set Location on the store Func so that if there are any side effects like setting map output dir on the Configuration in the Context are needed by the Output Committer those actions will be done before the committer is created Also the String version of Store Func for the specific store need to be set up in the context in case the committer needs them Pig Output Format set Location context Copy store return context Copy static public void store Cleanup Store store Configuration conf throws Exception Store Func Interface store Func store get Store Func if store Func instanceof Store Metadata Schema schema store get Schema if schema null Store Metadata store Func store Schema new Resource Schema schema store get Sort Info store get File get File Name new Job conf public boolean is Recovery Supported if recovery Supported return false boolean all Output Committer Support Recovery true call recover Task on all map and reduce committers for Pair Output Committer Store map Committer map Output Committers if map Committer first null try all Output Committer Support Recovery all Output Committer Support Recovery map Committer first is Recovery Supported catch Exception e throw new Runtime Exception e if all Output Committer Support Recovery return false for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null try all Output Committer Support Recovery all Output Committer Support Recovery reduce Committer first is Recovery Supported catch Exception e throw new Runtime Exception e if all Output Committer Support Recovery return false return true public void recover Task Task Attempt Context context throws Exception call recover Task on all map and reduce committers for Pair Output Committer Store map Committer map Output Committers if map Committer first null Task Attempt Context updated Context set Up Context context map Committer second try Use reflection Hadoop x line does not have such method map Committer first recover Task updated Context catch Exception e throw new Exception e for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Task Attempt Context updated Context set Up Context context reduce Committer second try reduce Committer first recover Task updated Context catch Exception e throw new Exception e Override public void cleanup Job Job Context context throws Exception call clean up on all map and reduce committers for Pair Output Committer Store map Committer map Output Committers if map Committer first null Job Context updated Context set Up Context context map Committer second store Cleanup map Committer second updated Context get Configuration map Committer first cleanup Job updated Context for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Job Context updated Context set Up Context context reduce Committer second store Cleanup reduce Committer second updated Context get Configuration reduce Committer first cleanup Job updated Context This method only be called in public void commit Job Job Context context throws Exception call commit Job on all map and reduce committers for Pair Output Committer Store map Committer map Output Committers if map Committer first null Job Context updated Context set Up Context context map Committer second promote files before calling store Cleanup store Schema try map Committer first commit Job updated Context catch Exception e throw new Exception e store Cleanup map Committer second updated Context get Configuration for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Job Context updated Context set Up Context context reduce Committer second promote files before calling store Cleanup store Schema try reduce Committer first commit Job updated Context catch Exception e throw new Exception e store Cleanup reduce Committer second updated Context get Configuration This method only be called in public void abort Job Job Context context State state throws Exception call abort Job on all map and reduce committers for Pair Output Committer Store map Committer map Output Committers if map Committer first null Job Context updated Context set Up Context context map Committer second try map Committer first abort Job updated Context state catch Exception e throw new Exception e store Cleanup map Committer second updated Context get Configuration for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Job Context updated Context set Up Context context reduce Committer second try reduce Committer first abort Job updated Context state catch Exception e throw new Exception e store Cleanup reduce Committer second updated Context get Configuration Override public void abort Task Task Attempt Context context throws Exception if Hadoop Shims is Map context get Task Attempt for Pair Output Committer Store map Committer map Output Committers if map Committer first null Task Attempt Context updated Context set Up Context context map Committer second map Committer first abort Task updated Context else for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Task Attempt Context updated Context set Up Context context reduce Committer second reduce Committer first abort Task updated Context Override public void commit Task Task Attempt Context context throws Exception if Hadoop Shims is Map context get Task Attempt for Pair Output Committer Store map Committer map Output Committers if map Committer first null Task Attempt Context updated Context set Up Context context map Committer second map Committer first commit Task updated Context else for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Task Attempt Context updated Context set Up Context context reduce Committer second reduce Committer first commit Task updated Context Override public boolean needs Task Commit Task Attempt Context context throws Exception boolean need Commit false if Hadoop Shims is Map context get Task Attempt for Pair Output Committer Store map Committer map Output Committers if map Committer first null Task Attempt Context updated Context set Up Context context map Committer second need Commit need Commit map Committer first needs Task Commit updated Context return need Commit else for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Task Attempt Context updated Context set Up Context context reduce Committer second need Commit need Commit reduce Committer first needs Task Commit updated Context return need Commit Override public void setup Job Job Context context throws Exception call set up on all map and reduce committers for Pair Output Committer Store map Committer map Output Committers if map Committer first null Job Context updated Context set Up Context context map Committer second map Committer first setup Job updated Context for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Job Context updated Context set Up Context context reduce Committer second reduce Committer first setup Job updated Context Override public void setup Task Task Attempt Context context throws Exception if Hadoop Shims is Map context get Task Attempt for Pair Output Committer Store map Committer map Output Committers if map Committer first null Task Attempt Context updated Context set Up Context context map Committer second map Committer first setup Task updated Context else for Pair Output Committer Store reduce Committer reduce Output Committers if reduce Committer first null Task Attempt Context updated Context set Up Context context reduce Committer second reduce Committer first setup Task updated Context 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import static org apache pig Pig Configuration import static org apache pig Pig Configuration import static org apache pig Pig Constants import java io Exception import java util Array List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io Text import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Task Attempt Context import org apache pig Load Func import org apache pig backend hadoop datastorage Configuration Util import org apache pig data Tuple import org apache pig impl io File Spec import org apache pig impl util Object Serializer import org apache pig tools pigstats Pig Stats Util import org apache pig tools pigstats Pig Status Reporter wrapper around the actual Record Reader and loadfunc this is needed for two reasons To intercept the initialize call from hadoop and initialize the underlying actual Record Reader with the right Context object this is achieved by looking up the Context corresponding to the input split this Reader is supposed to process We need to give hadoop consistent key value types text and tuple respectively so Pig Record Reader will call underlying Loader s get Next to get the Tuple value the key is null text since key is not used in input to map in Pig public class Pig Record Reader extends Record Reader Text Tuple private static final Log Log Factory get Log Pig Record Reader class transient private String counter Group private long timing Frequency private boolean do Timing false the current Tuple value as returned by underlying link Load Func get Next Tuple cur Value null the current wrapped Record Reader used by the loader private Record Reader cur Reader the loader object private Load Func loadfunc the Hadoop counter name transient private String counter Name null the wrapped inputformat private Input Format inputformat the wrapped splits private Pig Split pig Split the wrapped split index in use private int idx private long progress private Task Attempt Context context private Pig Status Reporter reporter private final long limit private long record Count the Configuration object with data specific to the input the underlying Record Reader will process this is obtained after a link Load Func set Location String org apache hadoop mapreduce Job call and hence can contain specific properties the underlying link Input Format might have put in private Configuration input Specific Conf param context public Pig Record Reader Input Format inputformat Pig Split pig Split Load Func load Func Task Attempt Context context long limit throws Exception Interrupted Exception this inputformat inputformat this pig Split pig Split this loadfunc load Func this context context this reporter Pig Status Reporter get Instance this input Specific Conf context get Configuration cur Reader null progress idx this limit limit init Next Record Reader do Timing input Specific Conf get Boolean false if do Timing counter Group load Func to String timing Frequency input Specific Conf get Long Override public void close throws Exception if cur Reader null cur Reader close cur Reader null Override public Text get Current Key throws Exception Interrupted Exception In pig we do n t really use the key in the input to the map so send null return null Override public Tuple get Current Value throws Exception Interrupted Exception Increment the multi input record counter if counter Name null cur Value null reporter incr Counter Pig Stats Util counter Name return cur Value Override public float get Progress throws Exception Interrupted Exception long subprogress bytes processed in current split if null cur Reader idx is always one past the current subsplit s true index subprogress long cur Reader get Progress pig Split get Length idx return Math min f progress subprogress float pig Split get Length Override public void initialize Input Split split Task Attempt Context context throws Exception Interrupted Exception initialize the underlying actual Record Reader with the right Context object this is achieved by merging the Context corresponding to the input split this Reader is supposed to process with the context passed in this pig Split Pig Split split this context context Configuration Util merge Conf context get Configuration input Specific Conf Pass loader signature to Load Func and to Input Format through the conf Pig Input Format pass Load Signature loadfunc pig Split get Input Index context get Configuration now invoke initialize on underlying Record Reader with the adjusted conf if null cur Reader cur Reader initialize pig Split get Wrapped Split context loadfunc prepare To Read cur Reader pig Split if pig Split is Multi Inputs pig Split disable Counter counter Name get Multi Inputs Couner Name pig Split input Specific Conf if counter Name null Create the counter This is needed because incr Counter may never be called in case of empty file reporter incr Counter Pig Stats Util counter Name Override public boolean next Key Value throws Exception Interrupted Exception if limit record Count limit return false boolean time This do Timing record Count timing Frequency long start Nanos if time This start Nanos System nano Time while cur Reader null cur Value loadfunc get Next null if init Next Record Reader return false if time This reporter incr Counter counter Group Math round System nano Time start Nanos timing Frequency record Count return true Suppress Warnings unchecked private static String get Multi Inputs Couner Name Pig Split pig Split Configuration conf throws Exception Array List File Spec inputs Array List File Spec Object Serializer deserialize conf get Pig Input Format String fname inputs get pig Split get Input Index get File Name return Pig Stats Util get Multi Inputs Counter Name fname pig Split get Input Index Get the record reader for the next chunk in this Combine File Split protected boolean init Next Record Reader throws Exception Interrupted Exception if cur Reader null cur Reader close cur Reader null if idx progress pig Split get Length idx done processing so far context progress if all chunks have been processed nothing more to do if idx pig Split get Num Paths return false get a record reader for the idx th chunk try pig Split set Current Idx idx cur Reader inputformat create Record Reader pig Split get Wrapped Split context info Current split being processed pig Split get Wrapped Split if idx initialize for the first Record Reader will be called by Map Task we re responsible for initializing subsequent Record Readers cur Reader initialize pig Split get Wrapped Split context loadfunc prepare To Read cur Reader pig Split catch Exception e throw new Runtime Exception e idx return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Buffered Reader import java io File import java io File Input Stream import java io File Not Found Exception import java io Exception import java io Input Stream import java io Print Stream import java io String Reader import java lang reflect Constructor import java lang reflect Method import java net import java util Array List import java util Collection import java util Deque import java util Enumeration import java util Hash Map import java util Hash Set import java util Iterator import java util Linked List import java util List import java util Map import java util Properties import java util Queue import java util Set import java util concurrent atomic Atomic Integer import org apache commons lang String Utils import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop io compress Zip Codec import org apache log j Level import org apache log j Logger import org apache pig backend datastorage Container Descriptor import org apache pig backend datastorage Data Storage import org apache pig backend datastorage Element Descriptor import org apache pig backend executionengine Exec Exception import org apache pig backend executionengine Exec Job import org apache pig backend executionengine Exec Job import org apache pig backend hadoop Pig Client import org apache pig backend hadoop executionengine Job import org apache pig builtin Pig Storage import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig data Data Bag import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Localizer import org apache pig impl io File Localizer Fetch File Ret import org apache pig impl io compress Zip Codec With Extension import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl streaming Streaming Command import org apache pig impl util Log Utils import org apache pig impl util Properties Util import org apache pig impl util Context import org apache pig impl util Uri Util import org apache pig impl util Utils import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan logical Util import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical expression Scalar Expression import org apache pig newplan logical optimizer All Expression Visitor import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Load import org apache pig newplan logical relational Store import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Plan Data import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig parser Query Parser Driver import org apache pig parser Query Parser Utils import org apache pig pen Example Generator import org apache pig scripting Script Engine import org apache pig tools grunt Grunt Parser import org apache pig tools pigstats Empty Pig Stats import org apache pig tools pigstats Job Stats import org apache pig tools pigstats Output Stats import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Pig Stats Job Graph import org apache pig tools pigstats Script State import org apache pig validator Black And Whitelist Filter import org apache pig validator Pig Command Filter import com google common annotations Visible For Testing class for Java programs to connect to Pig Typically a program will create a Pig Server instance The programmer then registers queries using register Query and retrieves results using open Iterator or store After doing so the shutdown method should be called to free any resources used by the current Pig Server instance Not doing so could result in a memory leak Interface Audience Public Interface Stability Stable public class Pig Server protected final Log log Log Factory get Log get Class public static final String pig pretty print schema private static final String pig location check strict The data structure to support grunt shell operations The grunt shell can only work on one graph at a time If a script is contained inside another script the grunt shell first saves the current graph on the stack and works on a new graph After the nested script is done the grunt shell pops up the saved graph and continues working on it protected final Deque Graph graphs new Linked List Graph The current Graph the grunt shell is working on private Graph curr protected final Pig Context pig Context private String job Name private String job Priority private final static Atomic Integer scope Counter new Atomic Integer protected final String scope construct Scope private boolean validate Each Statement false private boolean skip Parse In Register For Batch false private final Black And Whitelist Filter filter private String construct Scope scope servers for now as a session id String user System get Property user name String date new Date to String scope is not really used in the system right now It will however make your explain statements look lengthy if set to username date For now let s simplify the scope if a real scope is needed again we might need to update all the operators to not include scope in their name return scope Counter increment And Get Visible For Testing public static void reset Scope scope Counter set param exec Type String can be mapreduce or local Local mode will use Hadoop s local job runner to execute the job on the local machine Mapreduce mode will connect to a cluster to execute the job If exec Type String is not one of these two Pig will deduce the Execution Engine if it is on the classpath and use it for the backend execution throws Exec Exception throws Exception public Pig Server String exec Type String throws Exec Exception Exception this add Exec Type Property Properties Util load Default Properties exec Type String public Pig Server String exec Type String Properties properties throws Exec Exception Exception this add Exec Type Property properties exec Type String public Pig Server Properties properties throws Exec Exception Exception this new Pig Context properties private static Properties add Exec Type Property Properties properties String exec Type properties set Property exectype exec Type return properties param exec Type execution type to start the engine Local mode will use Hadoop s local job runner to execute the job on the local machine Mapreduce mode will connect to a cluster to execute the job throws Exec Exception public Pig Server Exec Type exec Type throws Exec Exception this exec Type Properties Util load Default Properties public Pig Server Exec Type exec Type Properties properties throws Exec Exception this new Pig Context exec Type properties public Pig Server Exec Type exec Type Configuration conf throws Exec Exception this new Pig Context exec Type conf public Pig Server Pig Context context throws Exec Exception this context true public Pig Server Pig Context context boolean connect throws Exec Exception this pig Context context curr new Graph false job Name pig Context get Properties get Property Pig Context Pig Context Default Job Name if connect pig Context connect this filter new Black And Whitelist Filter this add Hadoop Properties add Jars From Properties mark Predeployed Jars From Properties if Script State get null If Pig was started via command line Script State should have been already initialized in Main If so we should not overwrite it Script State start pig Context get Execution Engine instantiate Script State Pig Stats start pig Context get Execution Engine instantiate Pig Stats log event includes the caller context String audit Id Pig Client get Pig Audit Id pig Context String caller Id String pig Context get Properties get Pig Configuration log info Pig Script for the session audit Id if caller Id null log info Caller for session caller Id if Boolean parse Boolean pig Context get Properties get Property Pig Configuration if Boolean parse Boolean pig Context get Properties get Property yarn timeline service enabled false Pig Client Event event new Pig Client Event audit Id caller Id try Pig Client get Instance log Event event catch Exception e log warn Error posting to e else log warn is disabled since yarn timeline service enabled set to false set hdfs caller context Class caller Context Class null try caller Context Class Class for Name org apache hadoop ipc Caller Context catch Class Not Found Exception e If pre Hadoop skip setting Caller Context if caller Context Class null try Reflection for the following code since it is only available since hadoop Caller Context hdfs Context new Caller Context Builder audit Id build Caller Context set Current hdfs Context Class caller Context Builder Class Class for Name org apache hadoop ipc Caller Context Builder Constructor caller Context Builder Construct caller Context Builder Class get Constructor String class Object builder caller Context Builder Construct new Instance audit Id Method builder Build Method builder get Class get Method build Object hdfs Context builder Build Method invoke builder Method caller Context Set Current Method caller Context Class get Method set Current hdfs Context get Class caller Context Set Current Method invoke caller Context Class hdfs Context catch Exception e Shall not happen unless change in future Hadoop commons throw new Exec Exception e private void add Hadoop Properties throws Exec Exception For Zip input on hadoop with turned on Pig Text Input Format depends on hadoop s Text Input Format for handling bzip input One problem is it only recognize bz as extension and not bz Adding custom Zip codec that returns bz as extension for backward compatibility String codecs pig Context get Properties get Property io compression codecs if codecs null codecs contains Zip Codec class get Canonical Name pig Context get Properties set Property io compression codecs codecs Zip Codec With Extension class get Canonical Name private void add Jars From Properties throws Exec Exception add jars from properties to extra Jars String jar str pig Context get Properties get Property pig additional jars if jar str null jar str jar str jar str replace All File path Separator if jar str is Empty jar str String jar str comma pig Context get Properties get Property pig additional jars uris if jar str comma null jar str comma is Empty jar str jar str jar str comma if jar str null jar str is Empty Use File path Separator on Linux on Windows to correctly handle path aggregates as they are represented on the Operating System for String jar jar str split try register Jar jar catch Exception e int err Code String msg Failed to register jar jar Caught exception throw new Exec Exception msg err Code Pig Exception e private void mark Predeployed Jars From Properties throws Exec Exception mark jars as predeployed from properties String jar str pig Context get Properties get Property pig predeployed jars if jar str null Use File path Separator on Linux on Windows to correctly handle path aggregates as they are represented on the Operating System for String jar jar str split File path Separator if jar length pig Context mark Jar As Predeployed jar public Pig Context get Pig Context return pig Context Current return public Graph get Current return this curr Set the logging level to public void debug On Logger get Logger org apache pig set Level Level pig Context get Log j Properties set Property log j logger org apache pig Level to String Set the logging level to the default public void debug Off Logger get Logger org apache pig set Level pig Context get Default Log Level pig Context get Log j Properties set Property log j logger org apache pig pig Context get Default Log Level to String Set the default parallelism for this job param p default number of reducers to use for this job public void set Default Parallel int p pig Context default Parallel p Starts batch execution mode public void set Batch On log debug Create a new graph if curr null graphs push curr curr new Graph true Retrieve the current execution mode return true if the execution mode is batch false otherwise public boolean is Batch On Batch is on when there are multiple graphs on the stack That gives the right response even if multiquery was turned off return graphs size Returns whether there is anything to process in the current batch throws Frontend Exception return true if there are no stores to process in the current batch false otherwise public boolean is Batch Empty throws Frontend Exception if curr null int err Code String msg set Batch On must be called first throw new Frontend Exception msg err Code Pig Exception return curr is Batch Empty This method parses the scripts and builds the Logical Plan This method should be followed by link Pig Server execute Batch boolean with argument as false Do Not use link Pig Server execute Batch after calling this method as that will re parse and build the script throws Exception public void parse And Build throws Exception if curr null is Batch On int err Code String msg set Batch On must be called first throw new Frontend Exception msg err Code Pig Exception curr parse Query curr build Plan null Submits a batch of Pig commands for execution return list of jobs being executed throws Exception public List Exec Job execute Batch throws Exception return execute Batch true Submits a batch of Pig commands for execution Parse and build of script should be skipped if user called link Pig Server parse And Build before Pass false as an argument in which case param parse And Build return throws Exception public List Exec Job execute Batch boolean parse And Build throws Exception if parse And Build parse And Build Pig Stats stats execute return get Jobs stats Retrieves a list of Job objects from the Pig Stats object param stats return list of Exec Job objects protected List Exec Job get Jobs Pig Stats stats Linked List Exec Job jobs new Linked List Exec Job if stats instanceof Empty Pig Stats Job job new Job Job pig Context stats result null get Store null jobs add job return jobs Job Graph j Graph stats get Job Graph Iterator Job Stats iter j Graph iterator while iter has Next Job Stats js iter next for Output Stats output js get Outputs if js is Successful jobs add new Job Job pig Context output get Store output get Alias stats else Job hjob new Job Job pig Context output get Store output get Alias stats hjob set Exception js get Exception jobs add hjob return jobs Discards a batch of Pig commands throws Frontend Exception public void discard Batch throws Frontend Exception if curr null is Batch On int err Code String msg set Batch On must be called first throw new Frontend Exception msg err Code Pig Exception curr graphs pop Add a path to be skipped while automatically shipping binaries for streaming param path path to be skipped public void add Path To Skip String path pig Context add Path To Skip path Defines an alias for the given function spec This is useful for functions that require arguments to the constructor param function the new function alias to define param func Spec the Func Spec object representing the name of the function class and any arguments to constructor public void register Function String function Func Spec func Spec pig Context register Function function func Spec Defines an alias for the given streaming command param command Alias the new command alias to define param command streaming command to be executed public void register Streaming Command String command Alias Streaming Command command pig Context register Stream Cmd command Alias command private locate Jar From Resources String jar Name throws Exception Enumeration urls Class Loader get System Resources jar Name resource Location null if urls has More Elements resource Location urls next Element if urls has More Elements String Buffer sb new String Buffer Found multiple resources that match sb append jar Name sb append sb append resource Location while urls has More Elements sb append urls next Element sb append log debug sb to String return resource Location Registers a jar file Name of the jar file can be an absolute or relative path If multiple resources are found with the specified name the first one is registered as returned by get System Resources warning is issued to inform the user param name of the jar file to register throws Exception public void register Jar String name throws Exception Check if this operation is permitted filter validate Pig Command Filter Command if pig Context has Jar name log debug Ignoring duplicate registration for jar name return first try to locate jar via system resources if this fails try by using name as File this preserves compatibility with case when user passes absolute path or path relative to current working directory if name null if name is Empty log warn Empty string specified for jar path return resource locate Jar From Resources name if resource null Fetch File Ret files File Localizer fetch Files pig Context get Properties name for Fetch File Ret file files File f file file if f can Read int err Code String msg Ca n t read jar file name throw new Frontend Exception msg err Code Pig Exception pig Context add Jar f to to name else pig Context add Jar resource name Universal Scripting Language Support see param path path of the script file param scripting Lang language keyword or scripting Engine used to interpret the script param namespace namespace defined for functions of this script throws Exception public void register Code String path String scripting Lang String namespace throws Exception if pig Context scripting Fs contains Key path pig Context scripting Fs get path equals namespace log debug Ignoring duplicate registration for scripting udf file path in namespace namespace return else pig Context scripting Fs put path namespace Fetch File Ret ret File Localizer fetch File pig Context get Properties path File f ret file if f can Read int err Code String msg Ca n t read file path throw new Frontend Exception msg err Code Pig Exception String cwd new File get Canonical Path String file Path f get Canonical Path String name In Jar file Path Use the relative path in the jar if the path specified is relative if ret did Fetch if new File path is Absolute path index Of File separator In case of Oozie the localized files are in a different directory symlinked to the current directory Canonical path will not point to cwd name In Jar path else if file Path equals cwd File separator path If user specified absolute path and it refers to cwd name In Jar file Path substring cwd length pig Context add Script File name In Jar file Path if scripting Lang null Script Engine se Script Engine get Instance scripting Lang se register Functions name In Jar namespace pig Context Register a query with the Pig runtime The query is parsed and registered but it is not executed until it is needed param query a Pig Latin expression to be evaluated param start Line line number of the query within the whole script throws Exception public void register Query String query int start Line throws Exception curr register Query query start Line validate Each Statement skip Parse In Register For Batch Register a query with the Pig runtime The query is parsed and registered but it is not executed until it is needed Equivalent to calling link register Query String int with start Line set to param query a Pig Latin expression to be evaluated throws Exception public void register Query String query throws Exception register Query query Register a pig script from Input Stream source which is more general and extensible the pig script can be from local file then you can use File Input Stream or pig script can be in memory which you build it dynamically the you can use Byte Array Input Stream even pig script can be in remote machine which you get wrap it as Socket Input Stream param in throws Exception public void register Script Input Stream in throws Exception register Script in null null Register a pig script from Input Stream source which is more general and extensible the pig script can be from local file then you can use File Input Stream or pig script can be in memory which you build it dynamically the you can use Byte Array Input Stream even pig script can be in remote machine which you get wrap it as Socket Input Stream The parameters in the pig script will be substituted with the values in params param in param params the key is the parameter name and the value is the parameter value throws Exception public void register Script Input Stream in Map String String params throws Exception register Script in params null Register a pig script from Input Stream source which is more general and extensible the pig script can be from local file then you can use File Input Stream or pig script can be in memory which you build it dynamically the you can use Byte Array Input Stream even pig script can be in remote machine which you get wrap it as Socket Input Stream The parameters in the pig script will be substituted with the values in the parameter files param in param params Files files which have the parameter setting throws Exception public void register Script Input Stream in List String params Files throws Exception register Script in null params Files Register a pig script from Input Stream br The pig script can be from local file then you can use File Input Stream Or pig script can be in memory which you build it dynamically the you can use Byte Array Input Stream Pig script can even be in remote machine which you get wrap it as Socket Input Stream br The parameters in the pig script will be substituted with the values in the map and the parameter files The values in params Map will override the value in parameter file if they have the same parameter param in param params the key is the parameter name and the value is the parameter value param params Files files which have the parameter setting throws Exception public void register Script Input Stream in Map String String params List String params Files throws Exception try String substituted pig Context do Param Substitution in param Map To List params params Files Grunt Parser grunt new Grunt Parser new String Reader substituted this grunt set Interactive false grunt parse Stop On Error true catch org apache pig tools pigscript parser Parse Exception e log error e get Localized Message throw new Exception e protected List String param Map To List Map String String params List String param List new Array List String if params null for Map Entry String String entry params entry Set param List add entry get Key entry get Value return param List Creates a clone of the current return Graph object which is a clone of the current throws Exception protected Graph get Cloned Graph throws Exception Graph graph curr duplicate if graph null int err Code String msg Cloning of plan failed throw new Frontend Exception msg err Code Pig Exception return graph Register a query with the Pig runtime The query will be read from the indicated file param file Name file to read query from throws Exception public void register Script String file Name throws Exception register Script file Name null null Register a pig script file The parameters in the file will be substituted with the values in params param file Name pig script file param params the key is the parameter name and the value is the parameter value throws Exception public void register Script String file Name Map String String params throws Exception register Script file Name params null Register a pig script file The parameters in the file will be substituted with the values in the parameter files param file Name pig script file param params Files files which have the parameter setting throws Exception public void register Script String file Name List String params Files throws Exception register Script file Name null params Files Register a pig script file The parameters in the file will be substituted with the values in the map and the parameter files The values in params Map will override the value in parameter file if they have the same parameter param file Name pig script param params the key is the parameter name and the value is the parameter value param params Files files which have the parameter setting throws Exception public void register Script String file Name Map String String params List String params Files throws Exception File Input Stream fis null try fis new File Input Stream file Name register Script fis params params Files catch File Not Found Exception e log error e get Localized Message throw new Exception e finally if fis null fis close Intended to be used by unit tests only Print a list of all aliases in in the current Pig Latin script Output is written to System out throws Frontend Exception public void print Aliases throws Frontend Exception System out println aliases curr get Alias Op key Set Write the schema for an alias to System out param alias Alias whose schema will be written out return Schema of alias dumped throws Exception public Schema dump Schema String alias throws Exception try pig Context in Dump Schema true if equals alias alias get Last Rel Logical Relational Operator op get Operator For Alias alias Logical Schema schema op get Schema boolean pretty true equals pig Context get Properties get Property if schema null Schema s org apache pig newplan logical Util translate Schema schema System out println alias pretty s pretty Print s to String return s else System out println Schema for alias unknown return null catch Frontend Exception fee int err Code String msg Unable to describe schema for alias alias throw new Frontend Exception msg err Code Pig Exception false null fee finally pig Context in Dump Schema false Write the schema for a nested Alias to System out Denoted by alias nested Alias param alias Alias whose schema has nested Alias param nested Alias Alias whose schema will be written out return Schema of alias dumped throws Exception public Schema dump Schema Nested String alias String nested Alias throws Exception try pig Context in Dump Schema true if equals alias alias get Last Rel Operator op get Operator For Alias alias if op instanceof For Each Logical Schema nested Sc For Each op dump Nested Schema alias nested Alias if nested Sc null Schema s org apache pig newplan logical Util translate Schema nested Sc System out println alias nested Alias s to String return s else System out println Schema for alias nested Alias unknown return null else int err Code String msg Unable to describe schema for alias nested Alias throw new Frontend Exception msg err Code Pig Exception false null finally pig Context in Dump Schema false Set the name of the job This name will get translated to mapred job name param name of job public void set Job Name String name job Name Pig Context name Set Hadoop job priority This value will get translated to mapred job priority param priority valid values are found in link org apache hadoop mapred Job Priority public void set Job Priority String priority job Priority priority Executes a Pig Latin script up to and including indicated alias That is if a user does pre Pig Server server new Pig Server server register Query load foo server register Query filter by gt server register Query order by pre Then pre server open Iterator pre filtered but unsorted data will be returned If instead a user does pre server open Iterator pre filtered and sorted data will be returned param id Alias to open iterator for return iterator of tuples returned from the script throws Exception public Iterator Tuple open Iterator String id throws Exception try pig Context get Properties set Property Pig Context job Name if job Priority null pig Context get Properties set Property Pig Context job Priority Exec Job job store id File Localizer get Temporary Path pig Context to String Utils get Tmp File Compressor Name pig Context invocation of execute is synchronous if job get Status return job get Results else if job get Status job get Exception null throw the backend exception in the failed case Exception e job get Exception int err Code String msg Unable to open iterator for alias id Backend error e get Message throw new Frontend Exception msg err Code Pig Exception e else throw new Exception Job terminated with anomalous status job get Status to String catch Frontend Exception e throw e catch Exception e int err Code String msg Unable to open iterator for alias id throw new Frontend Exception msg err Code Pig Exception e Executes a Pig Latin script up to and including indicated alias and stores the resulting records into a file That is if a user does pre Pig Server server new Pig Server server register Query load foo server register Query filter by gt server register Query order by pre Then pre server store bar pre filtered but unsorted data will be stored to the file tt bar tt If instead a user does pre server store bar pre filtered and sorted data will be stored to the file tt bar tt Equivalent to calling link store String String String with tt org apache pig Pig Storage tt as the store function param id The alias to store param filename The file to which to store to return link Exec Job containing information about this job throws Exception public Exec Job store String id String filename throws Exception return store id filename Pig Storage class get Name Pig is the default store function Executes a Pig Latin script up to and including indicated alias and stores the resulting records into a file That is if a user does pre Pig Server server new Pig Server server register Query load foo server register Query filter by gt server register Query order by pre Then pre server store bar mystorefunc pre filtered but unsorted data will be stored to the file tt bar tt using tt mystorefunc tt If instead a user does pre server store bar mystorefunc pre filtered and sorted data will be stored to the file tt bar tt using tt mystorefunc tt p param id The alias to store param filename The file to which to store to param func store function to use return link Exec Job containing information about this job throws Exception public Exec Job store String id String filename String func throws Exception Pig Stats stats store Ex id filename func if stats get Output Stats size throw new Exception Could n t retrieve job Output Stats output stats get Output Stats get if stats is Successful return new Job pig Context output get Store output get Alias stats else Job job new Job pig Context output get Store output get Alias stats check for exception Exception ex null for Job Stats js stats get Job Graph if js get Exception null ex js get Exception job set Exception ex return job private Pig Stats store Ex String alias String filename String func throws Exception if equals alias alias get Last Rel curr parse Query curr skip Stores skip the stores that have already been processed curr build Plan alias try Query Parser Utils attach Store Plan scope curr lp filename func curr get Operator alias alias pig Context curr compile return execute Compiled Logical Plan catch Pig Exception e int err Code String msg Unable to store alias alias throw new Pig Exception msg err Code Pig Exception e Provide information on how a pig query will be executed For now this information is very developer focussed and probably not very useful to the average user param alias Name of alias to explain param stream Print Stream to write explanation to throws Exception if the requested alias can not be found public void explain String alias Print Stream stream throws Exception explain alias text true false stream stream null null Provide information on how a pig query will be executed param alias Name of alias to explain param format Format in which the explain should be printed If text then the plan will be printed in plain text Otherwise the execution plan will be printed in a href http en wikipedia org wiki language a format param verbose Controls the amount of information printed param mark As Execute When set will treat the explain like a call to execute in the respoect that all the pending stores are marked as complete param lps Stream to print the logical tree param eps Stream to print the Execution Engine trees If null then will print to files param dir Directory to print Execution Engine trees If null will use eps param suffix Suffix of file names throws Exception if the requested alias can not be found public void explain String alias String format boolean verbose boolean mark As Execute Print Stream lps Print Stream eps File dir String suffix throws Exception try pig Context in Explain true build Store Plan alias curr lp optimize pig Context Only add root xml node if all plans are being written to same stream if format xml lps eps lps println plan curr lp explain lps format verbose if curr lp size if format xml lps eps lps println plan return pig Context get Execution Engine explain curr lp pig Context eps format verbose dir suffix if format equals xml lps eps lps println plan if mark As Execute curr mark As Executed catch Exception e int err Code String msg Unable to explain alias alias throw new Frontend Exception msg err Code Pig Exception e finally pig Context in Explain false Returns the unused byte capacity of an filesystem This value does not take into account a replication factor as that can vary from file to file Thus if you are using this to determine if you data set will fit in the you need to divide the result of this call by your specific replication setting return unused byte capacity of the file system throws Exception public long capacity throws Exception if pig Context get Exec Type is Local throw new Exception capacity only supported for non local execution else Data Storage dds pig Context get Dfs Map String Object stats dds get Statistics String raw Capacity Str String stats get Data Storage String raw Used Str String stats get Data Storage if raw Capacity Str null raw Used Str null throw new Exception Failed to retrieve capacity stats long raw Capacity Bytes new Long raw Capacity Str long Value long raw Used Bytes new Long raw Used Str long Value return raw Capacity Bytes raw Used Bytes Returns the length of a file in bytes which exists in the accounts for replication param filename return length of the file in bytes throws Exception public long file Size String filename throws Exception Data Storage dfs pig Context get Dfs Element Descriptor elem dfs as Element filename Map String Object stats elem get Statistics long length Long stats get Element Descriptor int replication Short stats get Element Descriptor return length replication Test whether a file exists param filename to test return true if file exists false otherwise throws Exception public boolean exists File String filename throws Exception Element Descriptor elem pig Context get Dfs as Element filename return elem exists Delete a file param filename to delete return true throws Exception public boolean delete File String filename throws Exception Check if this operation is permitted filter validate Pig Command Filter Command filter validate Pig Command Filter Command Element Descriptor elem pig Context get Dfs as Element filename elem delete return true Rename a file param source file to rename param target new file name return true throws Exception public boolean rename File String source String target throws Exception Check if this operation is permitted filter validate Pig Command Filter Command pig Context rename source target return true Make a directory param dirs directory to make return true throws Exception public boolean mkdirs String dirs throws Exception Check if this operation is permitted filter validate Pig Command Filter Command Container Descriptor container pig Context get Dfs as Container dirs container create return true List the contents of a directory param dir name of directory to list return array of strings one for each file name throws Exception public String list Paths String dir throws Exception Check if this operation is permitted filter validate Pig Command Filter Command Collection String all Paths new Array List String Container Descriptor container pig Context get Dfs as Container dir Iterator Element Descriptor iter container iterator while iter has Next Element Descriptor elem iter next all Paths add elem to String String type new String return all Paths to Array type Return a map containing the logical plan associated with each alias return map public Map String Logical Plan get Aliases Map String Logical Plan alias Plans new Hash Map String Logical Plan for Logical Relational Operator op curr get Aliases key Set String alias op get Alias if null alias alias Plans put alias curr get Aliases get op return alias Plans Reclaims resources used by this instance of Pig Server This method deletes all temporary files generated by the current thread while executing Pig commands public void shutdown clean up activities reclaim scope to free up resources Currently this is not implemented and throws an exception hence for now we wo n t call it pig Context get Execution Engine reclaim Scope this scope File Localizer delete Temp Files Get the set of all current aliases return set public Set String get Alias Key Set return curr get Alias Op key Set public Map Operator Data Bag get Examples String alias throws Exception try if curr is Batch On alias null curr parse Query curr build Plan null execute curr parse Query curr skip Stores curr build Plan alias curr compile catch Exception e Since the original script is parsed anyway there should not be an error in this parsing The only reason there can be an error is when the files being loaded in load do n t exist anymore e print Stack Trace Example Generator exgen new Example Generator curr lp pig Context try return exgen get Examples catch Exec Exception e e print Stack Trace System out throw new Exception Exec Exception e catch Exception e e print Stack Trace System out throw new Exception Exception e public void print History boolean with Numbers List String sc curr get Script Cache if sc is Empty for int i i sc size i if with Numbers System out print i System out println sc get i private void build Store Plan String alias throws Exception curr parse Query curr build Plan alias if is Batch On alias null Compiler needs a store to be the leaf hence add a store to the plan to explain Query Parser Utils attach Store Plan scope curr lp fakefile null curr get Operator alias fake pig Context curr compile Compile and execute the current plan return throws Exception private Pig Stats execute throws Exception pig Context get Properties set Property Pig Context job Name if job Priority null pig Context get Properties set Property Pig Context job Priority In this plan all stores in the plan will be executed They should be ignored if the plan is reused curr count Executed Stores curr compile if curr lp size return Pig Stats get pig Context get Properties set Property pig logical plan signature curr lp get Signature Pig Stats stats execute Compiled Logical Plan return stats private Pig Stats execute Compiled Logical Plan throws Exec Exception Frontend Exception discover pig features used in this script Script State get set Script Features curr lp curr lp optimize pig Context return launch Plan curr lp job pigexec common method for launching the jobs according to the logical plan param lp The logical plan param job Name String containing the job name to be used return The Pig Stats object throws Exec Exception throws Frontend Exception protected Pig Stats launch Plan Logical Plan lp String job Name throws Exec Exception Frontend Exception Pig Stats stats null try stats pig Context get Execution Engine launch Pig lp job Name pig Context catch Exec Exception e throw e catch Frontend Exception e throw e catch Exception e There are a lot of exceptions thrown by the launcher If this is an Exec Exception just let it through Else wrap it int err Code String msg Unexpected error during execution throw new Exec Exception msg err Code Pig Exception e return stats For testing only Do n t use throws Exception Suppress Warnings unused private Logical Plan build Lp throws Exception curr build Plan null curr compile return curr lp private Logical Relational Operator get Operator For Alias String alias throws Exception build Store Plan alias Logical Relational Operator op Logical Relational Operator curr get Operator alias if op null int err Code String msg No plan for alias to describe throw new Frontend Exception msg err Code Pig Exception false null return op Returns data associated with Logical Plan It makes sense to call this method only after a query script has been registered with one of the link register Query String or link register Script Input Stream methods return Logical Plan Data public Logical Plan Data get Logical Plan Data return new Logical Plan Data curr lp This class holds the internal states of a grunt shell session protected class Graph private final Map Logical Relational Operator Logical Plan aliases new Hash Map Logical Relational Operator Logical Plan private Map String Operator operators new Hash Map String Operator private String last Rel private final List String script Cache new Array List String the file Name Map contains filename to canonical filename mappings This is done so we can reparse the cached script and remember the translation current directory might only be correct during the first parse private Map String String file Name Map new Hash Map String String private final boolean batch Mode private int processed Stores private Logical Plan lp private int current Line Num public Graph boolean batch Mode this batch Mode batch Mode this lp new Logical Plan Call back method for counting executed stores private void count Executed Stores throws Frontend Exception List Store sinks Util get Logical Relational Operators lp Store class processed Stores sinks size Map Logical Relational Operator Logical Plan get Aliases return aliases Map String Operator get Alias Op return operators boolean is Batch On return batch Mode boolean is Batch Empty for Operator op lp get Sinks if op instanceof Store return false return true void mark As Executed public Logical Plan get Logical Plan return this lp Get the operator with the given alias in the raw plan Null if not found Operator get Operator String alias throws Frontend Exception return operators get alias public Logical Plan get Plan String alias throws Exception Logical Plan plan lp if alias null Logical Relational Operator op Logical Relational Operator operators get alias if op null int err Code String msg Unable to find an operator for alias alias throw new Frontend Exception msg err Code Pig Exception plan aliases get op return plan Build a plan for the given alias Extra branches and child branch under alias will be ignored Dependent branch i e scalar will be kept throws Exception void build Plan String alias throws Exception if alias null skip Stores final Queue Operator queue new Linked List Operator if alias null Operator op get Operator alias if op null String msg Unable to find an operator for alias alias throw new Frontend Exception msg Pig Exception queue add op else List Store stores Util get Logical Relational Operators lp Store class for Store op stores boolean add Sink true Only add if all the successors are loads List Operator succs lp get Successors op if succs null succs size for Operator succ succs if succ instanceof Load add Sink false break if add Sink queue add op Logical Plan plan new Logical Plan while queue is Empty Operator curr Op queue poll plan add curr Op List Operator preds lp get Predecessors curr Op if preds null List Operator ops new Array List Operator preds for Operator pred ops if queue contains pred queue add pred plan connect pred curr Op visit expression associated with curr Op If it refers to any other operator that operator is also going to be enqueued curr Op accept new All Expression Visitor plan new Dependency Order Walker plan Override protected Logical Expression Visitor get Visitor Logical Expression Plan expr Plan throws Frontend Exception return new Logical Expression Visitor expr Plan new Dependency Order Walker expr Plan Override public void visit Scalar Expression expr throws Frontend Exception Operator ref Op expr get Implicit Referenced Operator if queue contains ref Op queue add ref Op curr Op set Plan plan lp plan Remove stores that have been executed previously from the overall plan private void skip Stores throws Exception Get stores specifically List Store sinks Util get Logical Relational Operators lp Store class List Operator sinks To Remove new Array List Operator int skip Count processed Stores if skip Count for Store sink sinks sinks To Remove add sink skip Count if skip Count break for Operator op sinks To Remove It s fully possible in the multiquery case that a store that is not a leaf sink and therefor has successors that need to be removed remove To Load op Operator pred lp get Predecessors op get lp disconnect pred op lp remove op private void remove To Load Operator to Remove throws Exception List Operator successors lp get Successors to Remove List Operator succ To Remove new Array List Operator if successors null successors size succ To Remove add All successors for Operator succ succ To Remove lp disconnect to Remove succ if succ instanceof Load remove To Load succ lp remove succ Accumulate the given statement to previous query statements and generate an overall raw plan void register Query String query int start Line boolean validate Each Statement boolean skip Parse For Batch throws Exception if batch Mode if start Line current Line Num String line script Cache remove script Cache size script Cache add line query else while start Line current Line Num script Cache add current Line Num Buffered Reader br new Buffered Reader new String Reader query String line br read Line while line null script Cache add line current Line Num line br read Line if skip Parse For Batch return else script Cache add query if validate Each Statement validate Query parse Query if batch Mode build Plan null for Operator sink lp get Sinks if sink instanceof Store try execute catch Exception e int err Code String msg Unable to store alias Store sink get Alias throw new Frontend Exception msg err Code Pig Exception e break We should have at most one store so break here private void validate Query throws Frontend Exception String query build Query Query Parser Driver parser Driver new Query Parser Driver pig Context scope file Name Map try Logical Plan plan parser Driver parse query plan validate pig Context scope true catch Frontend Exception ex script Cache remove script Cache size throw ex public List String get Script Cache return script Cache Parse the accumulated pig statements and generate an overall plan private void parse Query throws Frontend Exception Context get Context reset Context get Context set Client System Props pig Context get Properties String query build Query if query is Empty lp new Logical Plan return try Query Parser Driver parser Driver new Query Parser Driver pig Context scope file Name Map lp parser Driver parse query operators parser Driver get Operators last Rel parser Driver get Last Rel catch Exception ex script Cache remove script Cache size remove the bad script from the cache Pig Exception pe Log Utils get Pig Exception ex int err Code String msg Error during parsing pe null ex get Message pe get Message log error exception during parsing msg ex if null pe throw new Frontend Exception msg err Code Pig Exception ex else throw new Frontend Exception msg err Code Pig Exception ex pe get Source Location public String get Last Rel return last Rel private String build Query String Builder accu Query new String Builder for String line script Cache accu Query append line n return accu Query to String private void compile throws Exception lp validate pig Context scope false curr post Process private void post Process throws Exception The following code deals with store load combination of intermediate files In this case we will replace the load operator with a implicit split operator iff the load store func is reversible because that s when we can safely skip the load and keep going with the split output If the load store func is not reversible or they are different functions we connect the store and the load to remember the dependency Set Load load Ops new Hash Set Load List Operator sources lp get Sources for Operator source sources if source instanceof Load load Ops add Load source Set Store store Ops new Hash Set Store List Operator sinks lp get Sinks for Operator sink sinks if sink instanceof Store store Ops add Store sink if true equals pig Context get Properties get Property log info Output location strick check enabled check Duplicate Store Loc store Ops for Load load load Ops for Store store store Ops String ifile load get File Spec get File Name String ofile store get File Spec get File Name if ofile equals ifile if there is no path from the load to the store then connect the store to the load to create the dependency of the store on the load If there is a path from the load to the store then we should not connect the store to the load and create a cycle if store get Plan path Exists load store store get Plan connect store load This method checks whether the multiple sinks use the same file based location If yes throws a Runtime Exception param store Ops private void check Duplicate Store Loc Set Store store Ops Set String unique Store Loc new Hash Set String for Store store store Ops String file Name store get File Spec get File Name if unique Store Loc add file Name Uri Util is File Or Local Or file Name new Configuration true throw new Runtime Exception Script contains or more statements writing to same location file Name protected Graph duplicate There are two choices on how we duplicate the logical plan we really clone each operator and connect up the cloned operators we cache away the script till the point we need to clone and then simply re parse the script The latter approach is used here There is one open issue with this now Consider the following script load file somefile filter by store into bla rm file somefile load file someotherfile when we try to clone we try to reparse from the beginning and currently the parser checks for file existence of files in the load in the case where the file is a local one i e with file prefix This will be a known issue now and we will need to revisit later parse each line of the cached script int line Number create data structures needed for parsing Graph graph new Graph is Batch On graph processed Stores processed Stores graph file Name Map new Hash Map String String file Name Map try for Iterator String it script Cache iterator it has Next line Number always doing register Query irrespective of the batch mode Need to figure out if anything different needs to happen if batch mode is not on Do n t have to do the validation again so set validate Each Statement param to false graph register Query it next line Number false false graph post Process catch Exception ioe ioe print Stack Trace graph null return graph This can be called to indicate if the query is being parsed compiled in a mode that expects each statement to be validated as it is entered instead of just doing it once for whole script param validate Each Statement public void set Validate Each Statement boolean validate Each Statement this validate Each Statement validate Each Statement Set whether to skip parsing while registering the query in batch mode param skip Parse In Register For Batch public void set Skip Parse In Register For Batch boolean skip Parse In Register For Batch this skip Parse In Register For Batch skip Parse In Register For Batch public String get Last Rel return curr get Last Rel public boolean is Debug On if Logger get Logger org apache pig get Level Level return true else return false public String get Job Name return job Name public String get Job Priority return job Priority 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import java util Array List import java util Collections import java util Comparator import java util Iterator import java util List import java util Map import java util Properties import org apache commons collections Iterator Utils import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop fs Path import org apache hadoop mapred Job Client import org apache pig Pig Exception import org apache pig Pig Runner Return Code import org apache pig classification Interface Audience import org apache pig classification Interface Audience Private import org apache pig classification Interface Stability import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Spillable Memory Manager import org apache pig newplan Base Operator Plan import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig tools pigstats Job Stats Job State import com google common collect Maps Pig Stats encapsulates the statistics collected from a running script It includes status of the execution the of its Hadoop jobs as well as information about outputs and inputs of the script Interface Audience Public Interface Stability Evolving public abstract class Pig Stats private static final Log Log Factory get Log Pig Stats class private static Thread Local Pig Stats tps new Thread Local Pig Stats protected static final String yyyy dd mm ss protected long start Time protected long end Time protected String user Id protected Job Graph job Plan protected Pig Context pig Context protected Map String Output Stats alias Ouput Map protected int error Code protected String error Message null protected Throwable error Throwable null protected int return Code Return Code public static Pig Stats get return tps get public static Pig Stats start Pig Stats stats tps set stats return tps get Returns code are defined in link Return Code public int get Return Code return return Code Returns error message string public String get Error Message return error Message Returns the error code of link Pig Exception public int get Error Code return error Code Returns the error code of link Pig Exception public Throwable get Error Throwable return error Throwable Deprecated public abstract Job Client get Job Client public abstract boolean is Embedded public boolean is Successful return get Number Jobs return Code Return Code return Code Return Code public abstract Map String List Pig Stats get All Stats public abstract List String get All Error Messages Returns the properties associated with the script public Properties get Pig Properties if pig Context null return null return pig Context get Properties Returns the display message in pig grunt public abstract String get Display String Returns the of jobs spawned by the script public Job Graph get Job Graph return job Plan Returns the list of output locations in the script public List String get Output Locations Array List String locations new Array List String for Output Stats output get Output Stats locations add output get Location return Collections unmodifiable List locations Returns the list of output names in the script public List String get Output Names Array List String names new Array List String for Output Stats output get Output Stats names add output get Name return Collections unmodifiable List names Returns the number of bytes for the given output location for invalid location or name public long get Number Bytes String location if location null return String name new Path location get Name long count for Output Stats output get Output Stats if name equals output get Name count output get Bytes break return count Returns the number of records for the given output location for invalid location or name public long get Number Records String location if location null return String name new Path location get Name long count for Output Stats output get Output Stats if name equals output get Name count output get Number Records break return count Returns the alias associated with this output location public String get Output Alias String location if location null return null String name new Path location get Name String alias null for Output Stats output get Output Stats if name equals output get Name alias output get Alias break return alias Returns the total spill counts from link Spillable Memory Manager public abstract long get Spill Count Returns the total number of bags that spilled proactively public abstract long get Proactive Spill Count Objects Returns the total number of records that spilled proactively public abstract long get Proactive Spill Count Records Returns the total bytes written to user specified locations of this script public long get Bytes Written Iterator Job Stats it job Plan iterator long ret while it has Next long n it next get Bytes Written if n ret n return ret Returns the total number of records in user specified output locations of this script public long get Record Written Iterator Job Stats it job Plan iterator long ret while it has Next long n it next get Record Writtern if n ret n return ret public String get Hadoop Version return Script State get get Hadoop Version public String get Pig Version return Script State get get Pig Version Returns the contents of the script that was run public String get Script return Script State get get Script public String get Script Id return Script State get get Id public String get File Name return Script State get get File Name public String get Features return Script State get get Script Features public long get Duration return start Time end Time end Time start Time Returns the number of jobs for this script public int get Number Jobs return job Plan size public List Output Stats get Output Stats List Output Stats outputs new Array List Output Stats Iterator Job Stats iter job Plan iterator while iter has Next for Output Stats os iter next get Outputs outputs add os return Collections unmodifiable List outputs public Output Stats result String alias if alias Ouput Map null alias Ouput Map Maps new Hash Map Iterator Job Stats iter job Plan iterator while iter has Next for Output Stats os iter next get Outputs String a os get Alias if a null a length warn Output alias is n t avalable for os get Location continue alias Ouput Map put a os return alias Ouput Map get alias public List Input Stats get Input Stats List Input Stats inputs new Array List Input Stats Iterator Job Stats iter job Plan iterator while iter has Next for Input Stats is iter next get Inputs inputs add is return Collections unmodifiable List inputs public void set Error Message String error Message this error Message error Message public void set Error Code int error Code this error Code error Code public void set Error Throwable Throwable t this error Throwable t public void set Return Code int return Code this return Code return Code This class prints a Job Graph public static class Job Graph Printer extends Plan Visitor String Buffer buf protected Job Graph Printer Operator Plan plan super plan new org apache pig newplan Dependency Order Walker plan buf new String Buffer public void visit Job Stats op throws Frontend Exception buf append op get Job Id List Operator succs plan get Successors op if succs null buf append t t for Operator p succs buf append Job Stats p get Job Id append buf append n Override public String to String buf append n return buf to String Job Graph is an link Operator Plan whose members are link Job Stats public static class Job Graph extends Base Operator Plan implements Iterable Job Stats Override public String to String Job Graph Printer jp new Job Graph Printer this try jp visit catch Frontend Exception e warn unable to print job plan e return jp to String Returns a List representation of the Job graph Returned list is an Array List return List Job Stats Suppress Warnings unchecked public List Job Stats get Job List return Iterator Utils to List iterator public Iterator Job Stats iterator return new Iterator Job Stats private Iterator Operator iter get Operators Override public boolean has Next return iter has Next Override public Job Stats next return Job Stats iter next Override public void remove public boolean is Connected Operator from Operator to List Operator succs null succs get Successors from if succs null for Operator succ succs if succ get Name equals to get Name is Connected succ to return true return false public List Job Stats get Successful Jobs Array List Job Stats lst new Array List Job Stats Iterator Job Stats iter iterator while iter has Next Job Stats js iter next if js get State Job State lst add js Collections sort lst new Job Comparator return lst public List Job Stats get Failed Jobs Array List Job Stats lst new Array List Job Stats Iterator Job Stats iter iterator while iter has Next Job Stats js iter next if js get State Job State lst add js return lst private static class Job Comparator implements Comparator Job Stats Override public int compare Job Stats o Job Stats o return o get Job Id compare To o get Job Id Private public void set Backend Exception String job Id Exception e if e instanceof Pig Exception error Pig Exception e get Error Code e get Localized Message else if e null error e get Localized Message if job Id null e null debug unable to set backend exception return Iterator Job Stats iter job Plan iterator while iter has Next Job Stats js iter next if job Id equals js get Job Id js set Backend Exception e break Private public Pig Context get Pig Context return pig Context public void start start Time System current Time Millis user Id System get Property user name public void stop end Time System current Time Millis int failed get Number Failed Jobs int succeeded get Number Successful Jobs if failed succeeded succeeded job Plan size return Code Return Code else if succeeded succeeded job Plan size return Code Return Code else return Code Return Code public int get Number Successful Jobs Iterator Job Stats iter job Plan iterator int count while iter has Next if iter next get State Job State count return count public int get Number Failed Jobs Iterator Job Stats iter job Plan iterator int count while iter has Next if iter next get State Job State count return count 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import java util List import java util Map import java util regex Matcher import java util regex Pattern import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig tools pigstats mapreduce Pig Stats Util import org apache pig tools pigstats mapreduce Simple Pig Stats utility class for Pig Statistics public class Pig Stats Util public static final String public static final String public static final String public static final String public static final String public static final String public static final String public static final String public static final String Input records from public static final String Multi Input Counters public static final String Output records in public static final String Multi Store Counters deprecated use link org apache pig tools pigstats mapreduce Pig Stats Util instead Deprecated public static final String org apache hadoop mapred Task Counter deprecated use link org apache pig tools pigstats mapreduce Pig Stats Util instead Deprecated public static final String Pig Stats Util Returns an empty Pig Stats object Use of this method is not advised as it will return the execution engine version of Pig Stats by default and is not necessarily empty depending on the timing return an empty Pig Stats object Deprecated public static Pig Stats get Empty Pig Stats return Pig Stats start new Simple Pig Stats Returns the Pig Stats with the given return code param code the return code return the Pig Stats with the given return code public static Pig Stats get Pig Stats int code Pig Stats ps Pig Stats get if ps null ps Pig Stats start new Empty Pig Stats ps set Return Code code return ps public static void set Error Message String msg Pig Stats ps Pig Stats get if ps null ps Pig Stats start new Empty Pig Stats ps set Error Message msg public static void set Error Code int code Pig Stats ps Pig Stats get if ps null ps Pig Stats start new Empty Pig Stats ps set Error Code code public static void set Error Throwable Throwable t Pig Stats ps Pig Stats get if ps null ps Pig Stats start new Empty Pig Stats ps set Error Throwable t private static Pattern pattern Pattern compile tmp d public static boolean is Temp File String file Name Matcher result pattern matcher file Name return result find public static void set Stats Map Map String List Pig Stats stats Map Pig Stats start new Embedded Pig Stats stats Map Returns the counter name for the given input file name param fname the input file name return the counter name public static String get Multi Inputs Counter Name String fname int index String short Name get Short Name fname return short Name null null index short Name Returns the counter name for the given link Store param store the Store return the counter name public static String get Multi Store Counter Name Store store String short Name get Short Name store get File get File Name return short Name null null store get Index short Name Restrict total string size of a counter name to characters Leave characters for prefix string private static final int private static final String private static final String public static String get Short Name String uri int scolon uri index Of int slash if scolon slash uri last Index Of scolon else slash uri last Index Of String short Name null if scolon short Name uri substring slash if slash scolon short Name uri substring slash scolon if short Name null short Name length short Name short Name substring short Name length return short Name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import org apache hadoop mapreduce Counter import org apache hadoop mapreduce Status Reporter import org apache hadoop util Progressable import org apache pig backend hadoop executionengine Task Context import org apache pig classification Interface Audience import org apache pig classification Interface Stability Interface Audience Public Interface Stability Evolving public class Pig Status Reporter extends Status Reporter implements Progressable Pig Warn Counter private static Pig Status Reporter reporter null private Task Context context null Static Data Cleanup public static void static Data Cleanup reporter null private Pig Status Reporter Get singleton instance of the context public static Pig Status Reporter get Instance if reporter null reporter new Pig Status Reporter return reporter public void set Context Task Context context this context context deprecated use link org apache pig tools pigstats Pig Status Reporter incr Counter instead This method returns counter which is not compatible with Tez mode Use incr Counter that is compatible with both and Tez mode Override Deprecated public Counter get Counter Enum name return context null null context get Counter name deprecated use link org apache pig tools pigstats Pig Status Reporter incr Counter instead This method returns counter which is not compatible with Tez mode Use incr Counter that is compatible with both and Tez mode Override Deprecated public Counter get Counter String group String name return context null null context get Counter group name public boolean incr Counter Enum name long incr return context null false context incr Counter name incr public boolean incr Counter String group String name long incr return context null false context incr Counter group name incr Override public boolean incr Warn Counter Enum name Object incr return incr Counter name Long incr Override public boolean incr Warn Counter String group String name Object incr return incr Counter group name Long incr Override public void progress if context null context progress Override public void set Status String status if context null context set Status status public float get Progress return context null f context get Progress 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util Arrays import java util List import java util Properties import org apache commons cli Command Line import org apache commons cli Command Line Parser import org apache commons cli Gnu Parser import org apache commons cli Help Formatter import org apache commons cli Option import org apache commons cli Options import org apache commons cli Parse Exception import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop io Text import org apache hadoop io compress Zip Codec import org apache hadoop io compress Compression Codec import org apache hadoop io compress Gzip Codec import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib input File Split import org apache hadoop mapreduce lib output File Output Format import org apache pig Expression import org apache pig File Input Load Func import org apache pig Load Caster import org apache pig Load Func import org apache pig Load Metadata import org apache pig Load Push Down import org apache pig Overwritable Store Func import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Resource Statistics import org apache pig Store Func import org apache pig Store Func Interface import org apache pig Store Metadata import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig backend hadoop executionengine map Reduce Layer Pig Text Input Format import org apache pig backend hadoop executionengine map Reduce Layer Pig Text Output Format import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig bzip r Bzip Text Input Format import org apache pig data Data Byte Array import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Cast Utils import org apache pig impl util Object Serializer import org apache pig impl util Storage Util import org apache pig impl util Context import org apache pig impl util Utils import org apache pig parser Parser Exception load function that parses a line of input into fields using a character delimiter The default delimiter is a tab You can specify any character as a literal a a known escape character t or a dec or hex value u x p An optional second constructor argument is provided that allows one to customize advanced behaviors list of available options is below ul li code schema code Reads Stores the schema of the relation using a hidden file li code noschema code Ignores a stored schema during loading li code tag File code Appends input source file name to beginning of each tuple li code tag Path code Appends input source file path to beginning of each tuple ul p h Schemas h If code schema code is specified a hidden pig schema file is created in the output directory when storing data It is used by Pig Storage with or without schema during loading to determine the field names and types of the data without the need for a user to explicitly provide the schema in an code as code clause unless code noschema code is specified No attempt to merge conflicting schemas is made during loading The first schema encountered during a file system scan is used If the schema file is not present while schema option is used during loading it results in an error p In addition using code schema code drops a pig headers file in the output directory This file simply lists the delimited aliases This is intended to make export to tools that can read files with header lines easier just cat the header to your data p h Source tagging h If code tag File code is specified Pig Storage will prepend input split name to each Tuple row Usage input using Pig Storage tag File foreach generate The first field th index in each Tuple will contain input file name If code tag Path code is specified Pig Storage will prepend input split path to each Tuple row Usage input using Pig Storage tag Path foreach generate The first field th index in each Tuple will contain input file path p Note that regardless of whether or not you store the schema you b always b need to specify the correct delimiter to read your data If you store reading delimiter and then load using the default delimiter your data will not be parsed correctly h Compression h Storing to a directory whose name ends in bz or gz or lzo if you have installed support for compression in Hadoop will automatically use the corresponding compression codec br code output compression enabled code and code output compression codec code job properties also work p Loading from directories ending in bz or bz works automatically other compression formats are not auto detected on loading Suppress Warnings unchecked public class Pig Storage extends File Input Load Func implements Store Func Interface Load Push Down Load Metadata Store Metadata Overwritable Store Func protected Record Reader in null protected Record Writer writer null protected final Log m Log Log Factory get Log get Class protected String signature private byte field Del t private Array List Object m Proto Tuple null private Tuple Factory m Tuple Factory Tuple Factory get Instance private String load Location boolean is Schema On false boolean dont Load Schema false boolean overwrite Output false protected Resource Schema schema protected Load Caster caster protected boolean m Required Columns null private boolean m Required Columns Initialized false Indicates whether the input file name path should be read private boolean tag File false private static final String tag File private boolean tag Path false private static final String tag Path private Path source Path null it determines whether to depend on pig s own Bzip Text Input Format or to simply depend on hadoop for handling bzip inputs private boolean bzipinput usehadoops private Options populate Valid Options Options valid Options new Options valid Options add Option schema false Loads Stores the schema of the relation using a hidden file valid Options add Option noschema false Disable attempting to load data schema from the filesystem valid Options add Option false Appends input source file name to beginning of each tuple valid Options add Option false Appends input source file path to beginning of each tuple valid Options add Option tagsource false Appends input source file name to beginning of each tuple Option overwrite new Option overwrite Overwrites the destination overwrite set Long Opt overwrite overwrite set Optional Arg true overwrite set Args overwrite set Arg Name overwrite valid Options add Option overwrite return valid Options public Pig Storage this t Constructs a Pig loader that uses specified character as a field delimiter param delimiter the single byte character that is used to separate fields t is the default throws Parse Exception public Pig Storage String delimiter this delimiter Constructs a Pig loader that uses specified character as a field delimiter p Understands the following options which can be specified in the second paramter ul li code schema code Loads Stores the schema of the relation using a hidden file li code noschema code Ignores a stored schema during loading li code tag File code Appends input source file name to beginning of each tuple li code tag Path code Appends input source file path to beginning of each tuple ul param delimiter the single byte character that is used to separate fields param options a list of options that can be used to modify Pig Storage behavior throws Parse Exception public Pig Storage String delimiter String options field Del Storage Util parse Field Del delimiter Options valid Options populate Valid Options String opts Arr options split try Command Line Parser parser new Gnu Parser Command Line configured Options parser parse valid Options opts Arr is Schema On configured Options has Option schema if configured Options has Option overwrite String value configured Options get Option Value overwrite if true equals Ignore Case value overwrite Output true dont Load Schema configured Options has Option noschema tag File configured Options has Option tag Path configured Options has Option Remove tagsource in For backward compatibility we need tagsource to be supported until at least if configured Options has Option tagsource m Log warn tagsource is deprecated Use tag File instead tag File true catch Parse Exception e Help Formatter formatter new Help Formatter formatter print Help Pig Storage options valid Options We wrap this exception in a Runtime exception so that existing loaders that extend Pig Storage do n t break throw new Runtime Exception e Override public Tuple get Next throws Exception m Proto Tuple new Array List Object if m Required Columns Initialized if signature null Properties p Context get Context get Properties this get Class m Required Columns boolean Object Serializer deserialize p get Property signature m Required Columns Initialized true Prepend input source path if source tagging is enabled if tag File m Proto Tuple add new Data Byte Array source Path get Name else if tag Path m Proto Tuple add new Data Byte Array source Path to String try boolean not Done in next Key Value if not Done return null Text value Text in get Current Value byte buf value get Bytes int len value get Length int start int field for int i i len i if buf i field Del if m Required Columns null m Required Columns length field m Required Columns field add Tuple Value m Proto Tuple buf start i start i field pick up the last field if start len m Required Columns null m Required Columns length field m Required Columns field add Tuple Value m Proto Tuple buf start len Tuple t m Tuple Factory new Tuple No Copy m Proto Tuple return dont Load Schema t apply Schema t catch Interrupted Exception e int err Code String err Msg Error while reading input throw new Exec Exception err Msg err Code Pig Exception e private Tuple apply Schema Tuple tup throws Exception if caster null caster get Load Caster if signature null schema null Properties p Context get Context get Properties this get Class new String signature String serialized Schema p get Property signature schema if serialized Schema null return tup try schema new Resource Schema Utils get Schema From String serialized Schema catch Parser Exception e m Log error Unable to parse serialized schema serialized Schema e all bets are off there s no guarantee that we ll return either the fields in the data or the fields in the schema the user specified or required if schema null Resource Field Schema field Schemas schema get Fields int tuple Idx If some fields have been projected out the tuple only contains required fields We walk the required Columns array to find required fields and cast those for int i i field Schemas length i if m Required Columns null m Required Columns length i m Required Columns i if tuple Idx tup size tup append null Object val null if tup get tuple Idx null byte bytes Data Byte Array tup get tuple Idx get val Cast Utils convert To Type caster bytes field Schemas i field Schemas i get Type tup set tuple Idx val tuple Idx If input record somehow has more fields than the provided schema drop the extra fields if tup size field Schemas length int lastindex tup size List Object list tup get All for int i lastindex i field Schemas length i list remove i Tuple get All may not return reference to the interal List so creating a new Tuple tup m Tuple Factory new Tuple No Copy list return tup Override public void put Next Tuple f throws Exception try writer write null f catch Interrupted Exception e throw new Exception e private void add Tuple Value Array List Object tuple byte buf int start int end tuple add read Field buf start end Read the bytes between start and end into a Data Byte Array for inclusion in the return tuple param bytes byte array to copy data from param start starting point to copy from param end ending point to copy to exclusive return protected Data Byte Array read Field byte bytes int start int end if start end return null else return new Data Byte Array bytes start end Override public Required Field Response push Projection Required Field List required Field List throws Frontend Exception if required Field List null return null if required Field List get Fields null int last Column for Required Field rf required Field List get Fields if rf get Index last Column last Column rf get Index m Required Columns new boolean last Column for Required Field rf required Field List get Fields if rf get Index m Required Columns rf get Index true Properties p Context get Context get Properties this get Class try p set Property signature Object Serializer serialize m Required Columns catch Exception e throw new Runtime Exception Can not serialize m Required Columns return new Required Field Response true Override public boolean equals Object obj if obj instanceof Pig Storage return equals Pig Storage obj else return false public boolean equals Pig Storage other return this field Del other field Del Override public Input Format get Input Format if load Location ends With bz load Location ends With bz bzipinput usehadoops m Log info Using Bzip Text Input Format return new Bzip Text Input Format else m Log info Using Pig Text Input Format return new Pig Text Input Format Override public void prepare To Read Record Reader reader Pig Split split in reader if tag File tag Path source Path File Split split get Wrapped Split get Path Override public void set Location String location Job job throws Exception load Location location File Input Format set Input Paths job location bzipinput usehadoops job get Configuration get Boolean Pig Configuration true Override public Output Format get Output Format return new Pig Text Output Format field Del Override public void prepare To Write Record Writer writer this writer writer Override public void set Store Location String location Job job throws Exception job get Configuration set Configuration File Output Format set Output Path job new Path location if true equals job get Configuration get output compression enabled File Output Format set Compress Output job true String codec job get Configuration get output compression codec try File Output Format set Output Compressor Class job Class extends Compression Codec Class for Name codec catch Class Not Found Exception e throw new Runtime Exception Class not found codec else This makes it so that storing to a directory ending with gz or bz works set Compression new Path location job private void set Compression Path path Job job String location path get Name if location ends With bz location ends With bz File Output Format set Compress Output job true File Output Format set Output Compressor Class job Zip Codec class else if location ends With gz File Output Format set Compress Output job true File Output Format set Output Compressor Class job Gzip Codec class else File Output Format set Compress Output job false Override public void check Schema Resource Schema s throws Exception Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir Override public int hash Code return field Del Override public void set Context Signature String signature this signature signature Override public List Operator Set get Features return Arrays as List Load Push Down Operator Set Override public void set Store Func Context Signature String signature Override public void cleanup On Failure String location Job job throws Exception Store Func cleanup On Failure Impl location job Override public void cleanup On Success String location Job job throws Exception do nothing Implementation of Load Meta Data interface Override public Resource Schema get Schema String location Job job throws Exception if dont Load Schema schema new Json Metadata get Schema location job is Schema On if signature null schema null if tag File schema Utils get Schema With Input Source Tag schema else if tag Path schema Utils get Schema With Input Source Tag schema Properties p Context get Context get Properties this get Class new String signature p set Property signature schema schema to String return schema Override public Resource Statistics get Statistics String location Job job throws Exception return null Override public void set Partition Filter Expression partition Filter throws Exception Override public String get Partition Keys String location Job job throws Exception return null Implementation of Store Metadata Override public void store Schema Resource Schema schema String location Job job throws Exception if is Schema On Json Metadata metadata Writer new Json Metadata byte record Del n metadata Writer set Field Del field Del metadata Writer set Record Del record Del metadata Writer store Schema schema location job Override public void store Statistics Resource Statistics stats String location Job job throws Exception Override public boolean should Overwrite return this overwrite Output Override public void cleanup Output Store store Job job throws Exception Configuration conf job get Configuration String output conf get Configuration Path output Path null if output null output Path new Path output File System fs output Path get File System conf try fs delete output Path true catch Exception e m Log warn Could not delete output output 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig An enum to enumerate the warning types in Pig public enum Pig Warning placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings placeholder for warnings bulk collection of warnings under Spark exec engine same as above but for custom warnings only see 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer util import java net import java util Linked List import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop fs Path import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Add import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators Divide import org apache pig backend hadoop executionengine physical Layer expression Operators Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Greater Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Or Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Less Than Expr import org apache pig backend hadoop executionengine physical Layer expression Operators Mod import org apache pig backend hadoop executionengine physical Layer expression Operators Multiply import org apache pig backend hadoop executionengine physical Layer expression Operators Not Equal To Expr import org apache pig backend hadoop executionengine physical Layer expression Operators And import org apache pig backend hadoop executionengine physical Layer expression Operators Bin Cond import org apache pig backend hadoop executionengine physical Layer expression Operators Cast import org apache pig backend hadoop executionengine physical Layer expression Operators Is Null import org apache pig backend hadoop executionengine physical Layer expression Operators Map Look Up import org apache pig backend hadoop executionengine physical Layer expression Operators Negative import org apache pig backend hadoop executionengine physical Layer expression Operators Not import org apache pig backend hadoop executionengine physical Layer expression Operators Or import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators Regexp import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer expression Operators Subtract import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Cross import org apache pig backend hadoop executionengine physical Layer relational Operators Demux import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Native import org apache pig backend hadoop executionengine physical Layer relational Operators Optimized For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Partition Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Poisson Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Pre Combiner Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Reservoir Sample import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import com google common collect Lists Utility class with a few helper functions to deal with physical plans public class Plan Helper private final static Log log Log Factory get Log new Plan Helper get Class private Plan Helper Creates a relative path that can be used to build a temporary place to store the output from a number of map reduce tasks public static String make Store Tmp Path String orig Path path new Path orig uri path to Uri uri normalize String path Str uri get Path if path is Absolute return new Path abs path Str to String else return new Path rel path Str to String public static extends Physical Operator boolean contains Physical Operator Physical Plan plan Class op Class throws Visitor Exception Op Finder finder new Op Finder plan op Class finder visit return finder plan Contains Op Returns a Linked List of operators contained within the physical plan which implement the supplied class in dependency order Returns an empty Linked List of no such operators exist param plan param op Class return a Linked List of operators contained within the plan which implement the supplied class empty if no such ops exist throws Visitor Exception public static Linked List get Physical Operators Physical Plan plan Class op Class throws Visitor Exception Op Finder finder new Op Finder plan op Class finder visit return finder get Found Ops Finds Local Rearrange from Split sub plan param plan physical plan param rearrange Key operator key of the Local Rearrange return Local Rearrange with the specified operator key which is in a sub plan of Split throws Visitor Exception public static Physical Plan get Local Rearrange Plan From Split Physical Plan plan Operator Key rearrange Key throws Visitor Exception List Split splits Plan Helper get Physical Operators plan Split class for Split split splits for Physical Plan sub Plan split get Plans if sub Plan get Operator rearrange Key null return sub Plan return plan private static class Op Finder extends Phy Plan Visitor final Class op Class private Linked List found Ops Lists new Linked List public Op Finder Physical Plan plan Class op Class super plan new Dependency Order Walker Physical Operator Physical Plan plan this op Class op Class public Linked List get Found Ops return found Ops public boolean plan Contains Op return found Ops is Empty Override Suppress Warnings unchecked public void visit Physical Operator op if op Class is Assignable From op get Class found Ops add op Override public void visit Load Load ld throws Visitor Exception super visit Load ld visit ld Override public void visit Store Store st throws Visitor Exception super visit Store st visit st Override public void visit Native Native nat throws Visitor Exception super visit Native nat visit nat Override public void visit Filter Filter fl throws Visitor Exception super visit Filter fl visit fl Override public void visit Collected Group Collected Group mg throws Visitor Exception super visit Collected Group mg visit mg Override public void visit Local Rearrange Local Rearrange lr throws Visitor Exception super visit Local Rearrange lr visit lr Override public void visit Global Rearrange Global Rearrange gr throws Visitor Exception super visit Global Rearrange gr visit gr Override public void visit Package Package pkg throws Visitor Exception super visit Package pkg visit pkg Override public void visit For Each For Each nfe throws Visitor Exception super visit For Each nfe visit nfe Override public void visit Union Union un throws Visitor Exception super visit Union un visit un Override public void visit Split Split spl throws Visitor Exception super visit Split spl visit spl Override public void visit Demux Demux demux throws Visitor Exception super visit Demux demux visit demux Override public void visit Distinct Distinct distinct throws Visitor Exception super visit Distinct distinct visit distinct Override public void visit Sort Sort sort throws Visitor Exception super visit Sort sort visit sort Override public void visit Constant Constant Expression cnst throws Visitor Exception super visit Constant cnst visit cnst Override public void visit Project Project proj throws Visitor Exception super visit Project proj visit proj Override public void visit Greater Than Greater Than Expr grt throws Visitor Exception super visit Greater Than grt visit grt Override public void visit Less Than Less Than Expr lt throws Visitor Exception super visit Less Than lt visit lt Override public void visit Or Equal Or Equal To Expr gte throws Visitor Exception super visit Or Equal gte visit gte Override public void visit Or Equal Or Equal To Expr lte throws Visitor Exception super visit Or Equal lte visit lte Override public void visit Equal To Equal To Expr eq throws Visitor Exception super visit Equal To eq visit eq Override public void visit Not Equal To Not Equal To Expr eq throws Visitor Exception super visit Not Equal To eq visit eq Override public void visit Regexp Regexp re throws Visitor Exception super visit Regexp re visit re Override public void visit Is Null Is Null is Null throws Visitor Exception super visit Is Null is Null visit is Null Override public void visit Add Add add throws Visitor Exception super visit Add add visit add Override public void visit Subtract Subtract sub throws Visitor Exception super visit Subtract sub visit sub Override public void visit Multiply Multiply mul throws Visitor Exception super visit Multiply mul visit mul Override public void visit Divide Divide dv throws Visitor Exception super visit Divide dv visit dv Override public void visit Mod Mod mod throws Visitor Exception super visit Mod mod visit mod Override public void visit And And and throws Visitor Exception super visit And and visit and Override public void visit Or Or or throws Visitor Exception super visit Or or visit or Override public void visit Not Not not throws Visitor Exception super visit Not not visit not Override public void visit Bin Cond Bin Cond bin Cond super visit Bin Cond bin Cond visit bin Cond Override public void visit Negative Negative negative super visit Negative negative visit negative Override public void visit User Func User Func user Func throws Visitor Exception super visit User Func user Func visit user Func Override public void visit Comparison Func User Comparison Func comp Func throws Visitor Exception super visit Comparison Func comp Func visit comp Func Override public void visit Map Look Up Map Look Up map Look Up super visit Map Look Up map Look Up visit map Look Up Override public void visit Cast Cast cast super visit Cast cast visit cast Override public void visit Limit Limit lim throws Visitor Exception super visit Limit lim visit lim Override public void visit Cross Cross cross throws Visitor Exception super visit Cross cross visit cross Override public void visit Join Join join throws Visitor Exception super visit Join join visit join Override public void visit Merge Join Merge Join join throws Visitor Exception super visit Merge Join join visit join Override public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception super visit Merge Co Group merge Co Grp visit merge Co Grp Override public void visit Stream Stream stream throws Visitor Exception super visit Stream stream visit stream Override public void visit Skewed Join Skewed Join sk throws Visitor Exception super visit Skewed Join sk visit sk Override public void visit Partition Rearrange Partition Rearrange pr throws Visitor Exception super visit Partition Rearrange pr visit pr Override public void visit Optimized For Each Optimized For Each optimized For Each throws Visitor Exception super visit Optimized For Each optimized For Each visit optimized For Each Override public void visit Pre Combiner Local Rearrange Pre Combiner Local Rearrange pre Combiner Local Rearrange throws Visitor Exception super visit Pre Combiner Local Rearrange pre Combiner Local Rearrange visit pre Combiner Local Rearrange Override public void visit Partial Agg Partial Agg po Partial Agg throws Visitor Exception super visit Partial Agg po Partial Agg visit po Partial Agg Override public void visit Reservoir Sample Reservoir Sample reservoir Sample throws Visitor Exception super visit Reservoir Sample reservoir Sample visit reservoir Sample Override public void visit Poisson Sample Poisson Sample poisson Sample throws Visitor Exception super visit Poisson Sample poisson Sample visit poisson Sample 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer plans import java io Byte Array Output Stream import java io Exception import java io Output Stream import java io Print Stream import java util Array List import java util Collection import java util Collections import java util Hash Set import java util List import java util Set import org apache pig Pig Exception import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer relational Operators Multi Query Packager import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Counter import org apache pig backend hadoop executionengine physical Layer relational Operators Demux import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Global Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Package import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Rank import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig backend hadoop executionengine physical Layer relational Operators Packager import org apache pig impl plan Depth First Walker import org apache pig impl plan Operator import org apache pig impl plan Operator Plan import org apache pig impl plan Plan Visitor import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map public class Plan Printer extends Operator extends Operator Plan extends Plan Visitor String String More String Sep n String Sep n int level Cntr Print Stream stream System out boolean is Verbose true public Plan Printer plan super plan new Depth First Walker plan public Plan Printer plan Print Stream stream super plan new Depth First Walker plan this stream stream public void set Verbose boolean verbose is Verbose verbose Override public void visit throws Visitor Exception try stream write depth First get Bytes catch Exception ioe int err Code String msg Unexpected error while printing physical plan throw new Visitor Exception msg err Code Pig Exception ioe public void print Output Stream printer throws Visitor Exception Exception printer write depth First get Bytes protected void breadth First throws Visitor Exception List leaves m Plan get Leaves Set seen new Hash Set breadth First leaves seen Suppress Warnings unchecked private void breadth First Collection predecessors Set seen throws Visitor Exception level Cntr disp Tabs List new Predecessors new Array List for pred predecessors if seen add pred List pred Lst m Plan get Predecessors pred if pred Lst null new Predecessors add All pred Lst pred visit this if new Predecessors size stream println breadth First new Predecessors seen Suppress Warnings unchecked protected String depth First throws Visitor Exception String Builder sb new String Builder List leaves m Plan get Leaves Collections sort leaves for leaf leaves sb append depth First leaf sb append n sb delete sb length n length sb length sb delete sb length n length sb length return sb to String private String plan String Physical Plan pp String Builder sb new String Builder Byte Array Output Stream baos new Byte Array Output Stream if pp null pp explain baos is Verbose else return sb append Sep sb append shift String By Tabs baos to String return sb to String private String plan String List Physical Plan lep String Builder sb new String Builder if lep null for Physical Plan ep lep sb append plan String ep return sb to String Suppress Warnings unchecked private String depth First node throws Visitor Exception String Builder sb new String Builder node name n if is Verbose if node instanceof Filter sb append plan String Filter node get Plan else if node instanceof Local Rearrange sb append plan String Local Rearrange node get Plans else if node instanceof Partial Agg sb append plan String Partial Agg node get Key Plan sb append plan String Partial Agg node get Value Plans else if node instanceof Collected Group sb append plan String Collected Group node get Plans else if node instanceof Rank sb append plan String Rank node get Rank Plans else if node instanceof Counter sb append plan String Counter node get Counter Plans else if node instanceof Sort sb append plan String Sort node get Sort Plans else if node instanceof For Each sb append plan String For Each node get Input Plans else if node instanceof Package Packager pkgr Package node get Pkgr if pkgr instanceof Multi Query Packager List Packager pkgrs Multi Query Packager pkgr get Packagers for Packager child pkgrs sb append Sep child name n else if node instanceof Join Join frj Join node List List Physical Plan join Plans frj get Join Plans if join Plans null for List Physical Plan list join Plans sb append plan String list else if node instanceof Skewed Join Skewed Join skewed Skewed Join node Multi Map Physical Operator Physical Plan join Plans skewed get Join Plans if join Plans null List Physical Plan inner plans new Array List Physical Plan inner plans add All join Plans values sb append plan String inner plans else if node instanceof Limit Physical Plan limit Plan Limit node get Limit Plan if limit Plan null sb append plan String limit Plan if node instanceof Split sb append plan String Split node get Plans else if node instanceof Demux List Physical Plan plans new Array List Physical Plan Set Physical Plan pl new Hash Set Physical Plan pl add All Demux node get Plans plans add All pl sb append plan String plans List original Predecessors m Plan get Predecessors node if original Predecessors null return sb to String List predecessors new Array List original Predecessors Collections sort predecessors int i for pred predecessors i String Str depth First pred if Str null sb append Sep if i predecessors size sb append shift String By Tabs Str else sb append shift String By Tabs Str return sb to String private String shift String By Tabs String Str int Tab Type String Builder sb new String Builder String spl Str split n String tab Tab Type More sb append spl n for int i i spl length i sb append tab sb append spl i sb append n return sb to String private void disp Tabs for int i i level Cntr i stream print public void visit Load Load op stream print op name public void visit Store Store op stream print op name public void visit Filter Filter op stream print op name public void visit Local Rearrange Local Rearrange op stream print op name public void visit Global Rearrange Global Rearrange op stream print op name public void visit Package Package op stream print op name public void visit Start Map Union op stream print op name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception Boolean and operator public class And extends Binary Comparison Operator private static final long serial Version public And Operator Key k this k public And Operator Key k int rp super k rp result Type Data Type Override public void visit Phy Plan Visitor v throws Visitor Exception v visit And this Override public String name return And Data Type find Type Name result Type m Key to String Override public Result get Next Boolean throws Exec Exception Result r accum Child null Data Type if r null return r Result left left lhs get Next Boolean pass on and and if left return Status Status return left truth table for t true n null f false t n f t t n f n n n f f f f f Short circuit if lhs is false return false above is handled with this boolean return Left false if left result null Boolean left result boolean Value if illustrator null return left illustrator Markup null left result return Left true Result right rhs get Next Boolean if return Left return left pass on and if right return Status Status return right if the lhs is null and rhs is true return null in all other cases we can just return the rhs and of table above if left result null right result null Boolean right result boolean Value return left No matter what what we get from the right side is what we ll return null true or false if right result null illustrator Markup null right result Boolean right result return right Override public And clone throws Clone Not Supported Exception And clone new And new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Array List import java util List import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Bin Cond extends Expression Operator private static final long serial Version Expression Operator cond Expression Operator lhs Expression Operator rhs private transient List Expression Operator child public Bin Cond Operator Key k super k public Bin Cond Operator Key k int rp super k rp public Bin Cond Operator Key k int rp Expression Operator cond Expression Operator lhs Expression Operator rhs super k rp this cond cond this lhs lhs this rhs rhs public Result generic Get Next byte data Type throws Exec Exception List Expression Operator list new Array List Expression Operator list add cond Result r accum Child list Data Type if r null if r return Status Status return r list clear list add lhs list add rhs r accum Child list data Type return r Result res cond get Next Boolean if res result null res return Status Status return res Result result Boolean res result true lhs get Next data Type rhs get Next data Type illustrator Markup null result result Boolean res result return result Override public Result get Next Boolean throws Exec Exception Result r accum Child null Data Type if r null return r Result res cond get Next Boolean if res result null res return Status Status return res return Boolean res result true lhs get Next Boolean rhs get Next Boolean Override public Result get Next Data Bag throws Exec Exception return generic Get Next Data Type Override public Result get Next Data Byte Array throws Exec Exception return generic Get Next Data Type Override public Result get Next Double throws Exec Exception return generic Get Next Data Type Override public Result get Next Float throws Exec Exception return generic Get Next Data Type Override public Result get Next Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Long throws Exec Exception return generic Get Next Data Type Override public Result get Next Date Time throws Exec Exception return generic Get Next Data Type Override public Result get Next Map throws Exec Exception return generic Get Next Data Type Override public Result get Next String throws Exec Exception return generic Get Next Data Type Override public Result get Next Tuple throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Integer throws Exec Exception return generic Get Next Data Type Override public Result get Next Big Decimal throws Exec Exception return generic Get Next Data Type Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Bin Cond this Override public String name return Bin Cond Data Type find Type Name result Type m Key to String Override public void attach Input Tuple t cond attach Input t lhs attach Input t rhs attach Input t public void set Cond Expression Operator cond Op this cond cond Op public void set Rhs Expression Operator rhs this rhs rhs public void set Lhs Expression Operator lhs this lhs lhs Get condition public Expression Operator get Cond return this cond Get right expression public Expression Operator get Rhs return this rhs Get left expression public Expression Operator get Lhs return this lhs Override public boolean supports Multiple Inputs return true Override public Bin Cond clone throws Clone Not Supported Exception Bin Cond clone new Bin Cond new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this clone cond cond clone clone lhs lhs clone clone rhs rhs clone return clone Get child expressions of this expression Override public List Expression Operator get Child Expressions if child null child new Array List Expression Operator child add cond child add lhs child add rhs return child Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java io Exception import java io Object Input Stream import java math Big Decimal import java math Big Integer import java util Array List import java util Iterator import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Eval Func import org apache pig Func Spec import org apache pig Load Caster import org apache pig Load Func import org apache pig Pig Exception import org apache pig Pig Warning import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig Stream To Pig import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig builtin To Date import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig impl util Cast Utils import org apache pig impl util Log Utils import org joda time Date Time This is just a cast that converts Data Byte Array into either String or Integer Just added it for testing the Union Need the full operator implementation public class Cast extends Expression Operator private final static Log log Log Factory get Log Cast class private static final String unknown Byte Array Error Message Received a bytearray from the or Union from two different Loaders Can not determine how to convert the bytearray to private Func Spec func Spec null transient private Load Caster caster private boolean cast Not Needed false private Byte real Type null private transient List Expression Operator child private Resource Field Schema field Schema null private static final long serial Version public Cast Operator Key k super k public Cast Operator Key k int rp super k rp private void instantiate Func throws Exception if caster null return if func Spec null Object obj Pig Context instantiate Func From Spec func Spec if obj instanceof Load Func caster Load Func obj get Load Caster else if obj instanceof Stream To Pig caster Stream To Pig obj get Load Caster else if obj instanceof Eval Func caster Eval Func obj get Load Caster else throw new Exception Invalid class type func Spec get Class Name private Result error Result res new Result res return Status Status return res public void set Func Spec Func Spec lf throws Exception this func Spec lf instantiate Func Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Cast this Override public String name if Data Type is Schema Type result Type return Cast Data Type find Type Name result Type field Schema calc Cast String m Key to String else return Cast Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return false Override public Result get Next Big Integer throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type and cache it real Type Data Type find Type res result try res result Data Type to Big Integer res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Big Integer res result real Type return res try if null caster res result caster bytes To Big Integer dba get else int err Code String msg unknown Byte Array Error Message Big Integer for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Big Integer return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result Big Integer else res result Big Integer return res case Data Type Result res in get Next Integer if res return Status Status res result null res result Big Integer value Of Integer res result long Value return res case Data Type Result res in get Next Double if res return Status Status res result null res result Big Integer value Of Double res result long Value return res case Data Type Result res in get Next Long if res return Status Status res result null res result Big Integer value Of Long res result long Value return res case Data Type Result res in get Next Float if res return Status Status res result null res result Big Integer value Of Float res result long Value return res case Data Type Result res in get Next String if res return Status Status res result null res result new Big Integer String res result return res case Data Type return in get Next Big Integer case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result Big Decimal res result to Big Integer return res return error Override public Result get Next Big Decimal throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type and cache it real Type Data Type find Type res result try res result Data Type to Big Decimal res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Big Decimal res result real Type return res try if null caster res result caster bytes To Big Decimal dba get else int err Code String msg unknown Byte Array Error Message Big Decimal for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Big Decimal return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result Big Decimal else res result Big Decimal return res case Data Type Result res in get Next Integer if res return Status Status res result null res result Big Decimal value Of Integer res result long Value return res case Data Type Result res in get Next Double if res return Status Status res result null res result Big Decimal value Of Double res result double Value return res case Data Type Result res in get Next Long if res return Status Status res result null res result Big Decimal value Of Long res result long Value return res case Data Type Result res in get Next Float if res return Status Status res result null res result Big Decimal value Of Float res result double Value return res case Data Type Result res in get Next String if res return Status Status res result null res result new Big Decimal String res result return res case Data Type Result res in get Next Big Integer if res return Status Status res result null res result new Big Decimal Big Integer res result return res case Data Type return in get Next Big Decimal return error Override public Result get Next Boolean throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type and cache it real Type Data Type find Type res result try res result Data Type to Boolean res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Boolean res result real Type return res try if null caster res result caster bytes To Boolean dba get else int err Code String msg unknown Byte Array Error Message boolean for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Boolean return res case Data Type Result res in get Next String if res return Status Status res result null res result Cast Utils string To Boolean String res result return res case Data Type return in get Next Boolean case Data Type Integer i null Result res in get Next Integer if res return Status Status res result null res result Boolean value Of Integer res result int Value return res case Data Type Result res in get Next Long if res return Status Status res result null res result Boolean value Of Long res result long Value return res case Data Type Result res in get Next Float if res return Status Status res result null res result Boolean value Of Float res result float Value return res case Data Type Result res in get Next Double if res return Status Status res result null res result Boolean value Of Double res result double Value return res case Data Type Big Integer bi null Result res in get Next Big Integer if res return Status Status res result null res result Boolean value Of Big Integer equals Big Integer res result return res case Data Type Big Decimal bd null Result res in get Next Big Decimal if res return Status Status res result null res result Boolean value Of Big Decimal equals Big Decimal res result return res return error Override public Result get Next Integer throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type and cache it real Type Data Type find Type res result try res result Data Type to Integer res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Integer res result real Type return res try if null caster res result caster bytes To Integer dba get else int err Code String msg unknown Byte Array Error Message int for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Integer return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result Integer value Of else res result Integer value Of return res case Data Type Result res in get Next Integer return res case Data Type Double d null Result res in get Next Double if res return Status Status res result null res result Data Type to Integer res result res result Integer value Of Double res result int Value return res case Data Type Result res in get Next Long if res return Status Status res result null res result Integer value Of Long res result int Value return res case Data Type Result res in get Next Float if res return Status Status res result null res result Integer value Of Float res result int Value return res case Data Type Result res in get Next Date Time if res return Status Status res result null res result Integer value Of Long value Of Date Time res result get Millis int Value return res case Data Type Result res in get Next String if res return Status Status res result null res result Cast Utils string To Integer String res result return res case Data Type Result res in get Next Big Integer if res return Status Status res result null res result Integer value Of Big Integer res result int Value return res case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result Integer value Of Big Decimal res result int Value return res return error Override public Result get Next Long throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type in first call and cache it real Type Data Type find Type res result try res result Data Type to Long res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Long res result real Type return res try if null caster res result caster bytes To Long dba get else int err Code String msg unknown Byte Array Error Message long for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Long return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result Long value Of else res result Long value Of return res case Data Type Result res in get Next Integer if res return Status Status res result null res result Long value Of Integer res result long Value return res case Data Type Result res in get Next Double if res return Status Status res result null res result Data Type to Integer res result res result Long value Of Double res result long Value return res case Data Type return in get Next Long case Data Type Result res in get Next Float if res return Status Status res result null res result Long value Of Float res result long Value return res case Data Type Result res in get Next Date Time if res return Status Status res result null res result Long value Of Date Time res result get Millis return res case Data Type Result res in get Next String if res return Status Status res result null res result Cast Utils string To Long String res result return res case Data Type Result res in get Next Big Integer if res return Status Status res result null res result Long value Of Big Integer res result long Value return res case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result Long value Of Big Decimal res result long Value return res return error Override public Result get Next Double throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type in first call and cache it real Type Data Type find Type res result try res result Data Type to Double res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Double res result real Type return res try if null caster res result caster bytes To Double dba get else int err Code String msg unknown Byte Array Error Message double for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Double return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result new Double else res result new Double return res case Data Type Result res in get Next Integer if res return Status Status res result null res result new Double Integer res result double Value return res case Data Type return in get Next Double case Data Type Result res in get Next Long if res return Status Status res result null res result new Double Long res result double Value return res case Data Type Result res in get Next Float if res return Status Status res result null res result new Double Float res result double Value return res case Data Type Result res in get Next Date Time if res return Status Status res result null res result new Double Long value Of Date Time res result get Millis double Value return res case Data Type Result res in get Next String if res return Status Status res result null res result Cast Utils string To Double String res result return res case Data Type Result res in get Next Big Integer if res return Status Status res result null res result Double value Of Big Integer res result double Value return res case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result Double value Of Big Decimal res result double Value return res return error Override public Result get Next Float throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type in first call and cache it real Type Data Type find Type res result try res result Data Type to Float res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Float res result real Type return res try if null caster res result caster bytes To Float dba get else int err Code String msg unknown Byte Array Error Message float for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Float return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result new Float else res result new Float return res case Data Type Result res in get Next Integer if res return Status Status res result null res result new Float Integer res result float Value return res case Data Type Result res in get Next Double if res return Status Status res result null res result Data Type to Integer res result res result new Float Double res result float Value return res case Data Type Result res in get Next Long if res return Status Status res result null res result new Float Long res result float Value return res case Data Type return in get Next Float case Data Type Date Time dt null Result res in get Next Date Time if res return Status Status res result null res result new Float Long value Of Date Time res result get Millis float Value return res case Data Type Result res in get Next String if res return Status Status res result null res result Cast Utils string To Float String res result return res case Data Type Result res in get Next Big Integer if res return Status Status res result null res result Float value Of Big Integer res result float Value return res case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result Float value Of Big Decimal res result float Value return res return error Override public Result get Next Date Time throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type in first call and cache it real Type Data Type find Type res result try res result Data Type to Date Time res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to Date Time res result real Type return res try if null caster res result caster bytes To Date Time dba get else int err Code String msg unknown Byte Array Error Message datetime for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Date Time return res case Data Type Result res in get Next Integer if res return Status Status res result null res result new Date Time Integer res result long Value return res case Data Type Result res in get Next Double if res return Status Status res result null res result new Date Time Double res result long Value return res case Data Type Result res in get Next Long if res return Status Status res result null res result new Date Time Long res result long Value return res case Data Type Result res in get Next Float if res return Status Status res result null res result new Date Time Float res result long Value return res case Data Type Result res in get Next Big Integer if res return Status Status res result null res result new Date Time Big Integer res result long Value return res case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result new Date Time Big Decimal res result long Value return res case Data Type return in get Next Date Time case Data Type Result res in get Next String if res return Status Status res result null res result To Date extract Date Time String res result return res return error Override public Result get Next String throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type switch result Type case Data Type case Data Type case Data Type return error case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e res result is not of type Byte Array But it can be one of the types from which cast is still possible if real Type null Find the type in first call and cache it real Type Data Type find Type res result try res result Data Type to String res result real Type catch Class Cast Exception cce Type has changed Need to find type again and try casting it again real Type Data Type find Type res result res result Data Type to String res result real Type return res try if null caster res result caster bytes To Char Array dba get else int err Code String msg unknown Byte Array Error Message string for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Char Array return res case Data Type Result res in get Next Boolean if res return Status Status res result null if Boolean res result res result res result Boolean to String else res result res result Boolean to String return res case Data Type Result res in get Next Integer if res return Status Status res result null res result Integer res result to String return res case Data Type Result res in get Next Double if res return Status Status res result null res result Data Type to Integer res result res result Double res result to String return res case Data Type Result res in get Next Long if res return Status Status res result null res result Long res result to String return res case Data Type Result res in get Next Float if res return Status Status res result null res result Float res result to String return res case Data Type Result res in get Next Date Time if res return Status Status res result null res result Date Time res result to String return res case Data Type return in get Next String case Data Type Big Integer bi null Result res in get Next Big Integer if res return Status Status res result null res result Big Integer res result to String return res case Data Type Result res in get Next Big Decimal if res return Status Status res result null res result Big Decimal res result to String return res return error Override public Result get Next Tuple throws Exec Exception Physical Operator in inputs get Byte cast To Type Data Type Byte result Type in get Result Type switch result Type case Data Type Result res in get Next Tuple if res return Status Status res result null try res result convert With Schema res result field Schema catch Exception e Log Utils warn this Unable to interpret value res result in field being converted to type tuple caught Parse Exception e get Message field discarded Pig Warning log res result null return res case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null res result new String Data Byte Array res result to String if cast Not Needed we examined the data once before and determined that the input is the same type as the type we are casting to so just send the input out as output return res try dba Data Byte Array res result catch Class Cast Exception e check if the type of res result is same as the type we are trying to cast to if Data Type find Type res result cast To Type remember this for future calls cast Not Needed true just return the output return res else the input is a differen type rethrow the exception int err Code String msg Can not cast to tuple Expected bytearray but received Data Type find Type Name res result throw new Exec Exception msg err Code Pig Exception e try if null caster res result caster bytes To Tuple dba get field Schema else int err Code String msg unknown Byte Array Error Message tuple for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Tuple return res case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type return error return error Suppress Warnings unchecked deprecation private Object convert With Schema Object obj Resource Field Schema fs throws Exception Object result null if fs null return obj if obj null handle Data Type return null switch fs get Type case Data Type if obj instanceof Data Bag Data Bag db Data Bag obj Get inner schema of a bag if fs get Schema null Resource Field Schema tuple Fs fs get Schema get Fields Iterator Tuple iter db iterator while iter has Next Tuple t iter next convert With Schema t tuple Fs result db else if obj instanceof Data Byte Array if null caster result caster bytes To Bag Data Byte Array obj get fs else int err Code String msg unknown Byte Array Error Message bag for this get Original Locations throw new Exec Exception msg err Code Pig Exception else throw new Exec Exception Can not cast obj to bag Pig Exception break case Data Type if obj instanceof Tuple try Tuple t Tuple obj Resource Schema inner Schema fs get Schema if inner Schema null return t if inner Schema get Fields length t size return null int i for Resource Field Schema field Schema inner Schema get Fields Object field convert With Schema t get i field Schema t set i field i result t catch Exception e throw new Exec Exception Can not convert obj to fs else if obj instanceof Data Byte Array if null caster result caster bytes To Tuple Data Byte Array obj get fs else int err Code String msg unknown Byte Array Error Message tuple for this get Original Locations throw new Exec Exception msg err Code Pig Exception else throw new Exec Exception Can not cast obj to tuple Pig Exception break case Data Type if obj instanceof Map if fs null fs get Schema null Resource Field Schema inner Field Schema fs get Schema get Fields Map m Map obj for Object entry m entry Set Object new Value convert With Schema Map Entry entry get Value inner Field Schema m put Map Entry entry get Key new Value result m else result obj else if obj instanceof Data Byte Array if null caster result caster bytes To Map Data Byte Array obj get fs else int err Code String msg unknown Byte Array Error Message tuple for this get Original Locations throw new Exec Exception msg err Code Pig Exception else throw new Exec Exception Can not cast obj to map Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Boolean Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message int for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type result obj break case Data Type result Boolean value Of Integer obj int Value break case Data Type result Boolean value Of Double obj double Value break case Data Type result Boolean value Of Long obj long Value break case Data Type result Boolean value Of Float obj float Value break case Data Type result Cast Utils string To Boolean String obj break case Data Type result Boolean value Of Big Integer equals Big Integer obj break case Data Type result Boolean value Of Big Decimal equals Big Decimal obj break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Integer Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message int for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result Integer value Of else result Integer value Of break case Data Type result obj break case Data Type result Integer value Of Double obj int Value break case Data Type result Integer value Of Long obj int Value break case Data Type result Integer value Of Float obj int Value break case Data Type result Integer value Of Long value Of Date Time obj get Millis int Value break case Data Type result Cast Utils string To Integer String obj break case Data Type result Integer value Of Big Integer obj int Value break case Data Type result Integer value Of Big Decimal obj int Value break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Double Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message double for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result new Double else result new Double break case Data Type result new Double Integer obj double Value break case Data Type result Double obj break case Data Type result new Double Long obj double Value break case Data Type result new Double Float obj double Value break case Data Type result new Double Long value Of Date Time obj get Millis double Value break case Data Type result Cast Utils string To Double String obj break case Data Type result Double value Of Big Integer obj double Value break case Data Type result Double value Of Big Decimal obj double Value break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Long Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message long for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result Long value Of else result Long value Of break case Data Type result Long value Of Integer obj long Value break case Data Type result Long value Of Double obj long Value break case Data Type result Long obj break case Data Type result Long value Of Float obj long Value break case Data Type result Long value Of Date Time obj get Millis break case Data Type result Cast Utils string To Long String obj break case Data Type result Long value Of Big Integer obj long Value break case Data Type result Long value Of Big Decimal obj long Value break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Float Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message float for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result new Float else result new Float break case Data Type result new Float Integer obj float Value break case Data Type result new Float Double obj float Value break case Data Type result new Float Long obj float Value break case Data Type result obj break case Data Type result new Float Long value Of Date Time obj get Millis float Value break case Data Type result Cast Utils string To Float String obj break case Data Type result Float value Of Big Integer obj float Value break case Data Type result Float value Of Big Decimal obj float Value break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Date Time Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message datetime for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type result new Date Time Integer obj long Value break case Data Type result new Date Time Double obj long Value break case Data Type result new Date Time Long obj long Value break case Data Type result new Date Time Float obj long Value break case Data Type result Date Time obj break case Data Type result To Date extract Date Time String obj break case Data Type result new Date Time Big Integer obj long Value break case Data Type result new Date Time Big Decimal obj long Value break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Char Array Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message float for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result result Boolean to String else result result Boolean to String break case Data Type result Integer obj to String break case Data Type result Double obj to String break case Data Type result Long obj to String break case Data Type result Float obj to String break case Data Type result Date Time obj to String break case Data Type result obj break case Data Type result Big Integer obj to String break case Data Type result Big Decimal obj to String break default throw new Exec Exception Can not convert obj to fs Pig Exception break case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Big Integer Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message Big Integer for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result Big Integer else result Big Integer break case Data Type result Big Integer value Of Integer obj long Value break case Data Type result Big Integer value Of Double obj long Value break case Data Type result Big Integer value Of Long obj long Value break case Data Type result Big Integer value Of Float obj long Value break case Data Type result new Big Integer String obj break case Data Type result Big Integer obj break case Data Type result Big Decimal obj to Big Integer break case Data Type result Big Integer value Of Date Time obj get Millis break default throw new Exec Exception Can not convert obj to fs Pig Exception case Data Type switch Data Type find Type obj case Data Type if null caster result caster bytes To Big Decimal Data Byte Array obj get else int err Code String msg unknown Byte Array Error Message Big Decimal for this get Original Locations throw new Exec Exception msg err Code Pig Exception break case Data Type if Boolean obj result Big Decimal else result Big Decimal break case Data Type result Big Decimal value Of Integer obj long Value break case Data Type result Big Decimal value Of Double obj double Value break case Data Type result Big Decimal value Of Long obj long Value break case Data Type result Big Decimal value Of Float obj double Value break case Data Type result new Big Decimal String obj break case Data Type result new Big Decimal Big Integer obj break case Data Type result Big Decimal obj break case Data Type result Big Decimal value Of Date Time obj get Millis break default throw new Exec Exception Can not convert obj to fs Pig Exception case Data Type no op result obj break default throw new Exec Exception Do n t know how to convert obj to fs Pig Exception return result Override public Result get Next Data Bag throws Exec Exception Physical Operator in inputs get Byte cast To Type Data Type Byte result Type in get Result Type switch result Type case Data Type res in get Next Data Bag if res return Status Status res result null try res result convert With Schema res result field Schema catch Exception e Log Utils warn this Unable to interpret value res result in field being converted to type bag caught Parse Exception e get Message field discarded Pig Warning log res result null return res case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null res result new String Data Byte Array res result to String if cast Not Needed we examined the data once before and determined that the input is the same type as the type we are casting to so just send the input out as output return res try dba Data Byte Array res result catch Class Cast Exception e check if the type of res result is same as the type we are trying to cast to if Data Type find Type res result cast To Type remember this for future calls cast Not Needed true just return the output return res else the input is a differen type rethrow the exception int err Code String msg Can not cast to bag Expected bytearray but received Data Type find Type Name res result throw new Exec Exception msg err Code Pig Exception e try if null caster res result caster bytes To Bag dba get field Schema else int err Code String msg unknown Byte Array Error Message bag for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Data Bag return res case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type return error return error Suppress Warnings deprecation Override public Result get Next Map throws Exec Exception Physical Operator in inputs get Byte cast To Type Data Type Byte result Type in get Result Type switch result Type case Data Type Result res in get Next Map if res return Status Status res result null try res result convert With Schema res result field Schema catch Exception e Log Utils warn this Unable to interpret value res result in field being converted to type map caught Parse Exception e get Message field discarded Pig Warning log res result null return res case Data Type Data Byte Array dba Result res in get Next Data Byte Array if res return Status Status res result null res result new String Data Byte Array res result to String if cast Not Needed we examined the data once before and determined that the input is the same type as the type we are casting to so just send the input out as output return res try dba Data Byte Array res result catch Class Cast Exception e check if the type of res result is same as the type we are trying to cast to if Data Type find Type res result cast To Type remember this for future calls cast Not Needed true just return the output return res else the input is a differen type rethrow the exception int err Code String msg Can not cast to map Expected bytearray but received Data Type find Type Name res result throw new Exec Exception msg err Code Pig Exception e try if null caster res result caster bytes To Map dba get field Schema else int err Code String msg unknown Byte Array Error Message map for this get Original Locations throw new Exec Exception msg err Code Pig Exception catch Exec Exception ee throw ee catch Exception e log error Error while casting from Byte Array to Map return res case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type return error return error Override public Result get Next Data Byte Array throws Exec Exception Physical Operator in inputs get Byte result Type in get Result Type if result Type Data Type return error Data Byte Array dba null Result res in get Next Data Byte Array if res return Status Status res result null try dba Data Byte Array res result catch Class Cast Exception e return error if dba null return res return res private void read Object Object Input Stream is throws Exception Class Not Found Exception is default Read Object instantiate Func Override public Cast clone throws Clone Not Supported Exception Cast clone new Cast new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this clone func Spec func Spec clone field Schema field Schema try clone instantiate Func catch Exception e Clone Not Supported Exception cnse new Clone Not Supported Exception cnse init Cause e throw cnse return clone Get child expression of this expression Override public List Expression Operator get Child Expressions if child null child new Array List Expression Operator if inputs get instanceof Expression Operator child add Expression Operator inputs get return child public void set Field Schema Resource Field Schema s field Schema s public Func Spec get Func Spec return func Spec Override public Tuple illustrator Markup Object in Object out int eq Class Index return Tuple out 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util List import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce Counter import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple This operator is part of the operator implementation It adds a local counter and a unique task id to each tuple There are modes of operations regular and dense The local counter is depends on the mode of operation With regular rank is considered duplicate rows while assigning numbers to distinct values groups With dense rank counts the number of distinct values without considering duplicate rows Depending on if it is considered the entire tuple row number or a by a set of columns rank by This Physical Operator relies on some specific class available at Pig Map Reduce Counter public class Counter extends Physical Operator private static final long serial Version public static final Long private List Physical Plan counter Plans private List Boolean m Asc Cols In case of it could by dense or not Being a dense rank means to assign consecutive ranks to different values private boolean is Dense Rank false In case of simple namely row number mode which is a consecutive number assigned to each tuple private boolean is Row Number false Local counter for tuples on the same task private Long local Count Task to label each tuple analyzed by the corresponding task private Integer task Unique identifier that links Counter and Rank through the global counter labeled with it private String operation public Counter Operator Key k this k null public Counter Operator Key k int rp this k rp null public Counter Operator Key k List Physical Operator inputs this k inputs public Counter Operator Key k int rp List Physical Operator inputs super k rp inputs public Counter Counter copy super copy this counter Plans copy counter Plans this m Asc Cols copy m Asc Cols this is Dense Rank copy is Dense Rank this is Row Number copy is Row Number this operation copy operation Suppress Warnings rawtypes unchecked public Counter Operator Key operator Key int requested Parallelism List inp List Physical Plan counter Plans List Boolean ascending Col super operator Key requested Parallelism inp this set Counter Plans counter Plans this set Ascending Columns ascending Col Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null return new Example Tuple Tuple out return Tuple out Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Counter this Override public Result get Next Tuple throws Exec Exception Result inp null while true inp process Input if inp return Status Status inp return Status Status break if inp return Status Status continue return add Counter Value inp return inp Add current task id and local counter value param input from the previous output return a tuple within two values prepended to the tuple the task identifier and the local counter value Local counter value could be incremented by one is a row number or dense rank or could be incremented by the size of the bag on the previous tuple processed protected Result add Counter Value Result input throws Exec Exception Tuple in Tuple input result Tuple out m Tuple Factory new Tuple in get All size Long size Bag int position Bag i Tuples are added by two stamps before the tuple content At position counter value At position last Current task Id On this case each tuple is analyzed independently of the tuples grouped if is Row Number is Dense Rank Only when is Dense Rank attached to a reduce phase it is incremented on this way Otherwise the increment is done at mapper automatically if is Dense Rank increment Reduce Counter Counter out set get Local Counter and the local incrementer is sequentially increased increment Local Counter else if is Dense Rank Standard rank On this case is important the number of tuples on the same group position Bag in get All size if in get Type position Bag Data Type size Bag org apache pig data Default Abstract Bag in get position Bag size This value the size of the tuples on the bag is used to increment the current global counter and increment Reduce Counter size Bag out set get Local Counter the value for the next tuple on the current task add To Local Counter size Bag for Object o in out set i o At position last Current task Id out set i get Task Id input result illustrator Markup in out return input protected void increment Reduce Counter Long increment Pig Map Reduce Counter Pig Reduce Counter increment Counter increment Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public String name return get Alias String Counter Data Type find Type Name result Type m Key to String public void set Counter Plans List Physical Plan counter Plans this counter Plans counter Plans public List Physical Plan get Counter Plans return counter Plans public void set Ascending Columns List Boolean m Asc Cols this m Asc Cols m Asc Cols public List Boolean get Ascending Columns return m Asc Cols Initialization step into the Counter is to set up local counter to public void reset Local Counter this local Count Sequential counter used at and mode protected Long increment Local Counter return local Count public void set Local Counter Long local Count this local Count local Count public Long get Local Counter return this local Count protected void add To Local Counter Long size Bag this local Count size Bag Task identifier of the task map or reducer public void set Task Id int task this task task public int get Task Id return this task Dense Rank flag public void set Is Dense Rank boolean is Dense Rank this is Dense Rank is Dense Rank public boolean is Dense Rank return is Dense Rank Row number flag public void set Is Row Number boolean is Row Number this is Row Number is Row Number public boolean is Row Number return is Row Number Operation identifier shared within the corresponding Rank public void set Operation String operation this operation operation public String get Operation return operation Override public Counter clone throws Clone Not Supported Exception Counter clone Counter super clone clone local Count new Long local Count clone task new Integer task counter Plans and m Asc Cols unused Not cloning them return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util Iterator import java util List import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple import org apache pig pen util Lineage Tracer Recover this class for nested cross operation public class Cross extends Physical Operator private static final long serial Version protected transient Data Bag input Bags protected transient Tuple data protected transient Iterator Tuple its protected transient Tuple tuple Of Last Bag public Cross Operator Key k super k public Cross Operator Key k int rp List Physical Operator inp super k rp inp public Cross Operator Key k int rp super k rp public Cross Operator Key k List Physical Operator inp super k inp Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Cross this Override public String name return get Alias String Cross Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return true Override public boolean supports Multiple Outputs return false Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null Example Tuple t Out new Example Tuple Tuple out illustrator add Data t Out illustrator get Equivalence Classes get eq Class Index add Tuple out Lineage Tracer lineage Tracer illustrator get Lineage lineage Tracer insert t Out for int i i data length i lineage Tracer union t Out data i return t Out else return Tuple out Override public Result get Next Tuple throws Exec Exception Result res new Result int no Items inputs size if input Bags null accumulate Data if load Last Bag res return Status Status clear Memory return res if its null we check if we are done with processing we do that by checking if all the iterators are used up boolean finished true boolean empty false for int i i its length i if input Bags i size empty true break finished its i has Next if empty if one bag is empty there does n t exist non null cross product simply clear all the input tuples of the first bag and finish int index inputs size for Result res Of Last Bag inputs get index get Next Tuple res Of Last Bag return Status Status res Of Last Bag inputs get index get Next Tuple res return Status Status clear Memory return res else if finished load Last Bag res return Status Status clear Memory return res if data null get Next being called for the first time or starting on new input data we instantiate the template array and start populating it with data data new Tuple no Items data no Items tuple Of Last Bag for int i i no Items i data i its i next res result create Tuple data res return Status Status return res else data no Items tuple Of Last Bag int length no Items for int index index length index if its index has Next data index its index next res result create Tuple data res return Status Status return res else reset this index s iterator so cross product can be achieved we would be resetting this way only for the indexes from the end when the first index which needs to be flattened has reached the last element in its iterator we wo n t come here instead we reset all iterators at the beginning of this method its index input Bags index iterator data index its index next res result create Tuple data res return Status Status return res Suppress Warnings unchecked private void accumulate Data throws Exec Exception int count int length inputs size input Bags new Data Bag length its new Iterator length for int i i length i Physical Operator op inputs get i Data Bag bag m Bag Factory new Default Bag input Bags count bag for Result res op get Next Tuple res return Status Status res op get Next Tuple if res return Status Status continue if res return Status Status throw new Exec Exception Error accumulating data in the local Cross operator if res return Status Status bag add Tuple res result its count bag iterator private Tuple create Tuple Tuple data throws Exec Exception Tuple out Tuple Factory get Instance new Tuple for int i i data length i Tuple t data i int size t size for int j j size j out append t get j return illustrator Markup out out private boolean load Last Bag throws Exec Exception Result res Of Last Bag null int index inputs size for res Of Last Bag inputs get index get Next Tuple res Of Last Bag return Status Status inputs get index get Next Tuple switch res Of Last Bag return Status case Status return false case Status each time when an tuple of last bag is ejected traverse all the combinations of the tuples from the other n bags to save the memory for one bag tuple Of Last Bag Tuple res Of Last Bag result return true case Status default throw new Exec Exception Error accumulating data in the local Cross operator private void clear Memory reset input Bags its data and tuple Of Last Bag to null so that in the next round of get Next the new input data will be loaded tuple Of Last Bag null input Bags null its null data null Override public void reset clear Memory 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util Iterator import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Configuration import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Internal Distinct Bag import org apache pig data Tuple import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception Find the distinct set of tuples in a bag This is a blocking operator All the input is put in the hashset implemented in Distinct Data Bag which also provides the other Data Bag interfaces public class Distinct extends Physical Operator implements Cloneable private static final Log log Log Factory get Log Distinct class private static final long serial Version private transient boolean inputs Accumulated private transient Data Bag distinct Bag private transient boolean initialized private transient boolean use Default Bag private transient Iterator Tuple it Since Global Rearrange is not used by Distinct passing the custom partioner through here protected String custom Partitioner public String get Custom Partitioner return custom Partitioner public void set Custom Partitioner String custom Partitioner this custom Partitioner custom Partitioner public Distinct Operator Key k int rp List Physical Operator inp super k rp inp public Distinct Operator Key k int rp super k rp public Distinct Operator Key k List Physical Operator inp super k inp public Distinct Operator Key k super k Override public boolean is Blocking return true Override public Result get Next Tuple throws Exec Exception if inputs Accumulated by default we create Internal Distinct Bag unless user configures explicitly to use old bag if initialized initialized true if Pig Map Reduce s Job Conf Internal get null String bag Type Pig Map Reduce s Job Conf Internal get get Pig Configuration if bag Type null bag Type equals Ignore Case default use Default Bag true distinct Bag use Default Bag m Bag Factory new Distinct Bag new Internal Distinct Bag Result in process Input while in return Status Status if in return Status Status log error Error in reading from inputs return in else if in return Status Status Ignore and read the next tuple in process Input continue else distinct Bag add Tuple in result illustrator Markup in result in result in process Input inputs Accumulated true if it null it distinct Bag iterator res result it next if res result null res return Status Status reset else res return Status Status return res Override public String name return get Alias String Distinct Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public void reset inputs Accumulated false distinct Bag null it null Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Distinct this non Javadoc see org apache pig backend hadoop executionengine physical Layer Physical Operator clone Override public Distinct clone throws Clone Not Supported Exception Auto generated method stub return new Distinct new Operator Key this m Key scope Node Id Generator get Generator get Next Node Id this m Key scope this requested Parallelism this inputs Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null illustrator get Equivalence Classes get eq Class Index add Tuple out illustrator add Data Tuple out return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java io Exception import java util Array List import java util Bit Set import java util Iterator import java util List import java util Map import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer End Of All Input Needed Visitor import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer expression Operators Relation To Expr Project import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Accumulative Bag import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data Schema Tuple Factory import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig data Tuple Maker import org apache pig data Unlimited Null Tuple import org apache pig impl logical Layer schema Schema import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple import org apache pig pen util Lineage Tracer We intentionally skip type checking in backend for performance reasons Suppress Warnings unchecked public class For Each extends Physical Operator private static final long serial Version private static final Result new Result Status new Unlimited Null Tuple protected List Physical Plan input Plans protected List Physical Operator ops To Be Reset protected Physical Operator plan Leaf Ops store result types of the plan leaves protected byte result Types array version of is To Be Flattened this is purely for optimization instead of calling is To Be Flattened get i we can do the quicker array access is To Be Flattened Array i Also we can store boolean values rather than Boolean objects so we can also save on the Boolean boolean Value calls protected boolean is To Be Flattened Array protected int no Items Since the plan has a generate this needs to be maintained as the generate can potentially return multiple tuples for same call protected transient boolean processing Plan its holds the iterators of the databags given by the input expressions which need flattening protected transient Iterator Tuple its null This holds the outputs given out by the input expressions of any datatype protected transient Object bags This is the template whcih contains tuples and is flattened out in create Tuple to generate the final output protected transient Object data store whether or not an accumulative has terminated early protected transient Bit Set early Termination protected transient Example Tuple t In protected transient Accumulative Tuple Buffer buffer protected transient Tuple inp Tuple protected transient boolean end Of All Input Processed Indicate the foreach statement can only in map side Currently only used in cross See protected boolean map Side Only false protected Boolean end Of All Input Processing false private Schema schema public For Each Operator Key k this k null null public For Each Operator Key k int rp this k rp null null public For Each Operator Key k List inp this k inp null public For Each Operator Key k int rp List Physical Plan inp List Boolean is To Be Flattened super k rp set Up Flattens is To Be Flattened this input Plans inp ops To Be Reset new Array List Physical Operator get Leaves public For Each Operator Key operator Key int requested Parallelism List Physical Plan inner Plans List Boolean flatten List Schema schema this operator Key requested Parallelism inner Plans flatten List this schema schema Override public void visit Phy Plan Visitor v throws Visitor Exception v visit For Each this Override public String name return get Alias String New For Each get Flat Str Data Type find Type Name result Type m Key to String String get Flat Str if is To Be Flattened Array null return String Builder sb new String Builder for Boolean b is To Be Flattened Array sb append b sb append if sb length sb delete Char At sb length return sb to String Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public void set Accumulative super set Accumulative for Physical Plan p input Plans Iterator Physical Operator iter p iterator while iter has Next Physical Operator po iter next if po instanceof Expression Operator po instanceof Distinct po set Accumulative Override public void set Accum Start super set Accum Start for Physical Plan p input Plans Iterator Physical Operator iter p iterator while iter has Next Physical Operator po iter next if po instanceof Expression Operator po instanceof Distinct po set Accum Start Override public void set Accum End super set Accum End for Physical Plan p input Plans Iterator Physical Operator iter p iterator while iter has Next Physical Operator po iter next if po instanceof Expression Operator po instanceof Distinct po set Accum End Calls get Next on the generate operator inside the nested physical plan and returns it maintaining an additional state to denote the begin and end of the nested plan processing Override public Result get Next Tuple throws Exec Exception try Result res null Result inp null The nested plan is under processing So return tuples that the generate oper returns if processing Plan while true res process Plan if res return Status Status return res if res return Status Status processing Plan false for Physical Plan plan input Plans plan detach Input break if res return Status Status return res if res return Status Status continue The nested plan processing is done or is yet to begin So process the input and start nested plan processing on the input tuple read while true inp process Input if inp return Status Status return inp if inp return Status Status continue if inp return Status Status if parent Plan null parent Plan end Of All Input end Of All Input Processed end Of All Input Processing continue pull one more output inp else return inp attach Input To Plans Tuple inp result inp Tuple Tuple inp result for Physical Operator po ops To Be Reset po reset if is Accumulative for int i i inp Tuple size i if inp Tuple get Type i Data Type we only need to check one bag because all the bags share the same buffer buffer Accumulative Bag inp Tuple get i get Tuplebuffer break set Accum Start while true if is Early Terminated buffer has Next Batch try buffer next Batch catch Exception e throw new Exec Exception e else if buffer instanceof Package Package Tuple Buffer inp Tuple Package Package Tuple Buffer buffer illustrator Markup null inp Tuple set Accum End res process Plan if res return Status Status attach same input again to process next batch attach Input To Plans Tuple inp result else if res return Status Status if this bubbled up then we just need to pass a null value through the pipe so that User Func will properly return the values attach Input To Plans null early Terminate else break buffer clear else res process Plan processing Plan true return res catch Runtime Exception e throw new Exec Exception Error while executing For Each at this get Original Locations e private boolean is Early Terminated false private Tuple Maker extends Tuple tuple Maker private boolean known Size false private boolean is Early Terminated return is Early Terminated private void early Terminate is Early Terminated true protected Result process Plan throws Exec Exception if schema null tuple Maker null Note here that if Schema Tuple is currently turned on then any s in the chain must follow good practices Namely they should not append to the Tuple that comes out of an iterator a practice which is fairly common but is not recommended tuple Maker Schema Tuple Factory get Instance schema false Gen Context if tuple Maker null known Size true if tuple Maker null tuple Maker Tuple Factory get Instance Result res new Result We check if all the databags have exhausted the tuples If so we enforce the reading of new data by setting data and its to null if its null boolean restart Its true for int i i no Items i if its i null is To Be Flattened Array i true restart Its its i has Next this means that all the databags have reached their last elements so we need to force reading of fresh databags if restart Its its null data null if its null if end Of All Input Processed return get Next being called for the first time starting with a set of new data from inputs its new Iterator no Items bags new Object no Items early Termination new Bit Set no Items for int i i no Items i Getting the iterators populate the input data Result input Data null switch result Types i case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type input Data plan Leaf Ops i get Next result Types i break default int err Code String msg Foreach currently does not handle type Data Type find Type Name result Types i throw new Exec Exception msg err Code Pig Exception we accrue information about what accumulators have early terminated in the case that they all do we can finish if input Data return Status Status if early Termination get i early Termination set i continue if input Data return Status Status continue if input Data return Status Status we are done with all the elements Time to return its null bags null return input Data if we see a error just return it if input Data return Status Status return input Data Object input null bags i input Data result if input Data result instanceof Data Bag is To Be Flattened Array i its i Data Bag bags i iterator else if input Data result instanceof Map is To Be Flattened Array i its i Map bags i entry Set iterator else its i null if parent Plan null parent Plan end Of All Input end Of All Input Processing end Of All Input Processed true if accumulating we have n t got data yet for some fields just return if is Accumulative is Accum Started if early Termination cardinality no Items res return Status Status else res return Status Status return res while true if data null get Next being called for the first time or starting on new input data we instantiate the template array and start populating it with data data new Object no Items for int i i no Items i if is To Be Flattened Array i bags i instanceof Data Bag bags i instanceof Map if its i has Next data i its i next else the input set is null so we return This is caught above and this function recalled with new inputs its null data null res return Status Status return res else data i bags i if get Reporter null get Reporter progress create Tuple data res result create Tuple data res return Status Status return res else we try to find the last expression which needs flattening and start iterating over it we also try to update the template array for int index no Items index index if its index null is To Be Flattened Array index if its index has Next data index its index next res result create Tuple data res return Status Status return res else reset this index s iterator so cross product can be achieved we would be resetting this way only for the indexes from the end when the first index which needs to be flattened has reached the last element in its iterator we wo n t come here instead we reset all iterators at the beginning of this method if bags index instanceof Data Bag its index Data Bag bags index iterator else if bags i instanceof Map its index Map bags index entry Set iterator data index its index next return null param data array that is the template for the final flattened tuple return the final flattened tuple protected Tuple create Tuple Object data throws Exec Exception Tuple out tuple Maker new Tuple int idx for int i i data length i Object in data i if is To Be Flattened Array i in instanceof Tuple Tuple t Tuple in int size t size for int j j size j if known Size out set idx t get j else out append t get j else if is To Be Flattened Array i in instanceof Map Entry Map Entry entry Map Entry in if known Size out set idx entry get Key out set idx entry get Value else out append entry get Key out append entry get Value else if known Size out set idx in else out append in if inp Tuple null return illustrator Markup inp Tuple out else return illustrator Markup data out protected void attach Input To Plans Tuple t super attach Input t for Physical Plan p input Plans p attach Input t public void get Leaves if input Plans null int i if is To Be Flattened Array null is To Be Flattened Array new boolean input Plans size plan Leaf Ops new Physical Operator input Plans size for Physical Plan p input Plans i Physical Operator leaf p get Leaves get plan Leaf Ops i leaf if leaf instanceof Project leaf get Result Type Data Type Project leaf is Project To End is To Be Flattened Array i true we are calculating plan leaves so lets reinitialize re Initialize private void re Initialize if plan Leaf Ops null no Items plan Leaf Ops length result Types new byte no Items for int i i result Types length i result Types i plan Leaf Ops i get Result Type else no Items result Types null if input Plans null for Physical Plan pp input Plans try Reset Finder lf new Reset Finder pp ops To Be Reset lf visit catch Visitor Exception ve String err Msg Internal Error Unexpected error looking for nested operators which need to be reset in throw new Runtime Exception err Msg ve public List Physical Plan get Input Plans return input Plans public void set Input Plans List Physical Plan plans input Plans plans plan Leaf Ops null get Leaves public void add Input Plan Physical Plan plan boolean flatten input Plans add plan add to plan Leaf Ops copy existing leaves Physical Operator new Plan Leaf Ops new Physical Operator plan Leaf Ops length for int i i plan Leaf Ops length i new Plan Leaf Ops i plan Leaf Ops i add to the end new Plan Leaf Ops plan Leaf Ops length plan get Leaves get plan Leaf Ops new Plan Leaf Ops add to is To Be Flattened Array copy existing values boolean new Is To Be Flattened Array new boolean is To Be Flattened Array length for int i i is To Be Flattened Array length i new Is To Be Flattened Array i is To Be Flattened Array i add to end new Is To Be Flattened Array is To Be Flattened Array length flatten is To Be Flattened Array new Is To Be Flattened Array we just added a leaf reinitialize re Initialize public void set To Be Flattened List Boolean flattens set Up Flattens flattens public List Boolean get To Be Flattened List Boolean result null if is To Be Flattened Array null result new Array List Boolean for int i i is To Be Flattened Array length i result add is To Be Flattened Array i return result Make a deep copy of this operator throws Clone Not Supported Exception Override public For Each clone throws Clone Not Supported Exception List Physical Plan plans new Array List Physical Plan input Plans size for Physical Plan plan input Plans plans add plan clone List Boolean flattens null if is To Be Flattened Array null flattens new Array List Boolean is To Be Flattened Array length for boolean b is To Be Flattened Array flattens add b For Each clone new For Each new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope requested Parallelism plans flattens clone set Result Type get Result Type clone add Original Location alias get Original Locations clone end Of All Input Processing end Of All Input Processing clone map Side Only map Side Only return clone public boolean in Processing return processing Plan protected void set Up Flattens List Boolean is To Be Flattened if is To Be Flattened null is To Be Flattened Array null else is To Be Flattened Array new boolean is To Be Flattened size int i for Iterator Boolean it is To Be Flattened iterator it has Next is To Be Flattened Array i it next Visits a pipeline and calls reset on all the nodes Currently only pays attention to limit nodes each of which need to be told to reset their limit private class Reset Finder extends Phy Plan Visitor Reset Finder Physical Plan plan List Physical Operator to Be Reset super plan new Dependency Order Walker Physical Operator Physical Plan plan Override public void visit Distinct Distinct d throws Visitor Exception add only if limit is present ops To Be Reset add d Override public void visit Limit Limit limit throws Visitor Exception ops To Be Reset add limit Override public void visit Sort Sort sort throws Visitor Exception add only if limit is present ops To Be Reset add sort Override public void visit Cross Cross c throws Visitor Exception add only if limit is present ops To Be Reset add c Override public void visit Project Project proj throws Visitor Exception if proj instanceof Relation To Expr Project ops To Be Reset add proj return the ops To Be Reset public List Physical Operator get Ops To Be Reset return ops To Be Reset param ops To Be Reset the ops To Be Reset to set public void set Ops To Be Reset List Physical Operator ops To Be Reset this ops To Be Reset ops To Be Reset private Tuple illustrator Markup Object in Object out if illustrator null Example Tuple t Out new Example Tuple Tuple out illustrator get Lineage insert t Out boolean synthetic false for Object t In in synthetic Example Tuple t In synthetic illustrator get Lineage union t Out Tuple t In illustrator add Data t Out int i for i i no Items i if Data Bag bags i size break if i no Items illustrator get Eq Classes Shared illustrator get Equivalence Classes get add t Out t Out synthetic synthetic return t Out else return Tuple out Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null Example Tuple t Out new Example Tuple Tuple out illustrator add Data t Out if illustrator get Eq Classes Shared illustrator get Equivalence Classes get add t Out Lineage Tracer lineage Tracer illustrator get Lineage lineage Tracer insert t Out t Out synthetic Example Tuple in synthetic lineage Tracer union Example Tuple in t Out return t Out else return Tuple out public Physical Operator get Plan Leaf Ops return plan Leaf Ops public void set Map Side Only boolean map Side Only this map Side Only map Side Only public boolean is Map Side Only return map Side Only public boolean need End Of All Input Processing throws Exec Exception try for Physical Plan inner Plan input Plans End Of All Input Needed Visitor end Of All Input Needed Visitor new End Of All Input Needed Visitor inner Plan end Of All Input Needed Visitor visit if end Of All Input Needed Visitor need End Of All Input Processing end Of All Input Processing true return true return false catch Exception e throw new Exec Exception e 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util List import org apache pig backend executionengine Exec Exception import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig impl plan Visitor Exception Dummy operator to test Compiler This will be a local operator and its get Next methods have to be implemented We intentionally skip type checking in backend for performance reasons Suppress Warnings unchecked public class Global Rearrange extends Physical Operator private static final long serial Version As Global Rearrange decides the map reduce boundary we add custom partitioner here protected String custom Partitioner private boolean cross false public String get Custom Partitioner return custom Partitioner public void set Custom Partitioner String custom Partitioner this custom Partitioner custom Partitioner public Global Rearrange Operator Key k this k null public Global Rearrange Operator Key k int rp this k rp null public Global Rearrange Operator Key k List inp this k null public Global Rearrange Operator Key k int rp List inp super k rp inp public Global Rearrange Global Rearrange copy throws Exec Exception super copy this cross copy cross this custom Partitioner copy custom Partitioner Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Global Rearrange this Override public String name return get Alias String Global Rearrange Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return true Override public boolean supports Multiple Outputs return false Override public boolean is Blocking return true Override public Tuple illustrator Markup Object in Object out int eq Class Index return null public void set Cross boolean cross this cross cross public boolean is Cross return cross 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl builtin import java io Exception import org apache hadoop conf Configuration import org apache hadoop mapreduce Record Reader import org apache pig Pig Configuration import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig data Tuple import org apache pig data Tuple Factory See Skewed Join sampler in http wiki apache org pig Pig Sampler public class Poisson Sample Loader extends Sample Loader marker string to mark the last sample row which has total number or rows seen by this map instance this string will be in the nd last column of the last sample row it is used by Get Mem Num Rows public static final String u u pig in Te Rnal sp Ecial ro num tuple kt Lehkblah num of rows sampled so far private int num Rows Sampled average size of tuple in memory for tuples sampled private long avg Tuple Mem Sz current row number private long row Num number of tuples to skip after each sample private long skip Interval bytes in input to skip after every sample divide this by avg Tuple Mem Size to get skip Interval private long mem To Skip Per Sample has the special row with row number information been returned private boolean num Row Spl Tuple Returned false is not a magic number It can be obtained by using a poisson cumulative distribution function with the mean set to emperically minimum number of samples and the confidence set to private static final int private int sample Rate total memory in bytes private long total Memory private double heap Perc Partition Skewed Keys new Sample tuple private Tuple new Sample null public Poisson Sample Loader String func Spec String ns super func Spec super set Num Samples Integer value Of ns will be overridden Override public Tuple get Next throws Exception if num Row Spl Tuple Returned row num special row has been returned after all inputs were read nothing more to read return null if skip Interval select first tuple as sample and calculate number of tuples to be skipped Tuple t loader get Next if t null since skip Interval is no previous sample and next sample is null the data set is empty return null long avail Red Mem long total Memory heap Perc avail Red Mem mem To Skip Per Sample avail Red Mem sample Rate update Skip Interval t row Num new Sample t skip tuples for long num Skipped num Skipped skip Interval num Skipped if skip Next return create Num Row Tuple new Sample row Num skipped enough get new sample Tuple t loader get Next if t null return create Num Row Tuple new Sample update Skip Interval t row Num Tuple current Sample new Sample new Sample t return current Sample Update the average tuple size base on newly sampled tuple t and recalculate skip Interval param t tuple private void update Skip Interval Tuple t avg Tuple Mem Sz avg Tuple Mem Sz num Rows Sampled t get Memory Size num Rows Sampled skip Interval mem To Skip Per Sample avg Tuple Mem Sz skipping fewer number of rows the first few times to reduce the probability of first tuples size if much smaller than rest resulting in very few samples being sampled Sampling a little extra is if num Rows Sampled skip Interval skip Interval num Rows Sampled num Rows Sampled param sample sample tuple return Tuple appended with special marker string column num rows column throws Exec Exception private Tuple create Num Row Tuple Tuple sample throws Exec Exception int sz sample null sample size Tuple Factory factory Tuple Factory get Instance Tuple t factory new Tuple sz if sample null for int i i sample size i t set i sample get i t set sz t set sz row Num num Row Spl Tuple Returned true return t Suppress Warnings rawtypes Override public void prepare To Read Record Reader reader Pig Split split throws Exception super prepare To Read reader split num Rows Sampled avg Tuple Mem Sz row Num skip Interval mem To Skip Per Sample num Row Spl Tuple Returned false new Sample null Configuration conf split get Conf sample Rate conf get Int Pig Configuration heap Perc conf get Float Pig Configuration Partition Skewed Keys total Memory conf get Long Pig Configuration if total Memory total Memory Runtime get Runtime max Memory 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util List import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple public class Limit extends Physical Operator private static final long serial Version Number of limited outputs private long m Limit The expression plan private Physical Plan expression Plan Counts for outputs processed private transient long so Far public Limit Operator Key k this k null public Limit Operator Key k int rp this k rp null public Limit Operator Key k List Physical Operator inputs this k inputs public Limit Operator Key k int rp List Physical Operator inputs super k rp inputs public void set Limit long limit m Limit limit public long get Limit return m Limit public Physical Plan get Limit Plan return expression Plan public void set Limit Plan Physical Plan expression Plan this expression Plan expression Plan Counts the number of tuples processed into static variable so Far if the number of tuples processed reach the limit return Otherwise return the tuple Override public Result get Next Tuple throws Exec Exception if it is the first time evaluate the expression Otherwise reuse the computed value if this get Limit expression Plan null Physical Operator expression expression Plan get Leaves get long variable Limit Result return Value switch expression get Result Type case Data Type return Value expression get Next Long if return Value return Status Status return Value result null throw new Runtime Exception Unable to evaluate Limit expression return Value variable Limit Long return Value result break case Data Type return Value expression get Next Integer if return Value return Status Status return Value result null throw new Runtime Exception Unable to evaluate Limit expression return Value variable Limit Integer return Value result break default throw new Runtime Exception Limit requires an integer parameter if variable Limit throw new Runtime Exception Limit requires a zero or a positive integer parameter this set Limit variable Limit Result inp null while true illustrator ignore before the post processing if illustrator null illustrator get Original Limit so Far m Limit inp break inp process Input if inp return Status Status inp return Status Status break illustrator Markup inp result null so Far break return inp Override public String name return get Alias String Limit m Key to String Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Limit this Override public void reset so Far Override public Limit clone throws Clone Not Supported Exception Limit clone Limit super clone if this expression Plan null clone expression Plan this expression Plan clone return clone Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null Example Tuple t In Example Tuple in illustrator get Equivalence Classes get eq Class Index add t In illustrator add Data Tuple in return Tuple in 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java io Exception import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Load Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl io Read To End Loader import org apache pig impl plan Operator Key import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple The load operator which is used in two ways As a local operator it can be used to load files In the Map Reduce setting it is used to create jobs from Map Reduce operators which keep the loads and stores in the Map and Reduce Plans till the job is created public class Load extends Physical Operator private static final Log log Log Factory get Log Load class private static final long serial Version The user defined load function or a default load function private transient Load Func loader null The filespec on which the operator is based File Spec l File Pig Context passed to us by the operator creator Pig Context pc Indicates whether the loader setup is done or not boolean set Up Done false Alias for the Load private String signature flag to distinguish user loads from Compiler loads private boolean is Tmp Load private long limit private transient List String cache Files null private transient List String ship Files null public Load Operator Key k this k null public Load Operator Key k File Spec l File this k l File public Load Operator Key k int rp File Spec l File super k rp this l File l File public Load Operator Key k Load Func lf this k this loader lf Set up the loader by Instantiating the load func Opening an input stream to the specified file and Binding to the input stream at the specified offset throws Exception public void set Up throws Exception loader new Read To End Loader Load Func Pig Context instantiate Func From Spec l File get Func Spec Configuration Util to Configuration pc get Properties l File get File Name signature At the end of processing the inputstream is closed using this method throws Exception public void tear Down throws Exception set Up Done false The main method used by this operator s successor to read tuples from the specified file using the specified load function return Whatever the loader returns null from the loader is indicative of and hence the tear Down of connection Override public Result get Next Tuple throws Exec Exception if set Up Done l File null try set Up catch Exception ioe int err Code String msg Unable to setup the load function throw new Exec Exception msg err Code Pig Exception ioe set Up Done true Result res new Result try res result loader get Next if res result null res return Status Status tear Down else res return Status Status if res return Status Status res result illustrator Markup res res result catch Exception e log error Received error from loader function e return res return res Override public String name return l File null get Alias String Load l File to String m Key to String get Alias String Load Dummy Fil Dummy Ldr m Key to String Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Load this public File Spec get File return l File public void set File File Spec file l File file public void set Is Tmp Load boolean tmp is Tmp Load tmp public boolean is Tmp Load return is Tmp Load public Pig Context get Pc return pc public void set Pc Pig Context pc this pc pc public String get Signature return signature public void set Signature String signature this signature signature public Load Func get Load Func if this loader null this loader Load Func Pig Context instantiate Func From Spec l File get Func Spec this loader set Context Signature signature return this loader public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null if illustrator ceiling Check Result in return Status Status return null if illustrator get Schema null illustrator get Schema size Tuple out size boolean has Null false for int i i Tuple out size i try if Tuple out get i null has Null true break catch Exec Exception e has Null true break if has Null Example Tuple t Out new Example Tuple Tuple out illustrator get Lineage insert t Out illustrator add Data Tuple t Out illustrator get Equivalence Classes get eq Class Index add t Out return t Out else return Tuple out else return Tuple out else return Tuple out public long get Limit return limit public void set Limit long limit this limit limit public List String get Cache Files return cache Files public void set Cache Files List String cf cache Files cf public List String get Ship Files return ship Files public void set Ship Files List String sf ship Files sf 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util Array List import java util Hash Map import java util List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Project import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple The local rearrange operator is a part of the co group implementation It has an embedded physical plan that generates tuples of the form grp Key indxed inp Tuple public class Local Rearrange extends Physical Operator private static final Log log Log Factory get Log Local Rearrange class protected static final long serial Version private static final Result new Result protected List Physical Plan plans protected List Physical Plan secondary Plans protected List Expression Operator leaf Ops protected List Expression Operator secondary Leaf Ops The position of this in the package operator protected byte index protected byte key Type protected byte main Key Type protected byte secondary Key Type protected boolean m Is Distinct false protected boolean is Cross false map to store mapping of projected columns to the position in the Key where these will be projected to We use this information to strip off these columns from the Value and in Package stitch the right Value tuple back by getting these columns from the key The goal is to reduce the amount of the data sent to Hadoop in the map Example a load bla b load bla c cogroup a by b by For the first input a the map would contain following key value corresponds to in cogroup a by and corresponds to st index in key corresponds to in cogroup a by and corresponds to nd index in key private final Map Integer Integer m Projected Cols Map private final Map Integer Integer m Secondary Projected Cols Map place holder Tuple used in distinct case where we really do n t have any value to pass through But hadoop gets cranky if we pass a null so we ll just create one instance of this empty tuple and pass it for every row We only get around to actually creating it if m Is Distinct is set to true protected Tuple m Fake Tuple null indicator whether the project in the inner plans is a project we set this when the project is the thing in the cogroup by private boolean m Project Star false private boolean m Secondary Project Star false marker to note that the key is a tuple this is required by Package to pick things off the key correctly to stitch together the value private boolean is Key Tuple false marker to note that the tuple key is compound in nature For example group a by a a The group key is a tuple of two fields is Key Compound is on group a by a a is a tuple The group key is a tuple of one field is Key Compound is off private boolean is Key Compound false private boolean is Secondary Key Tuple false private int m Projected Cols Map Size private int m Secondary Projected Cols Map Size private boolean use Secondary Key false By default we strip keys from the value private boolean strip Key From Value true protected transient Result inp public Local Rearrange Operator Key k this k null public Local Rearrange Operator Key k int rp this k rp null public Local Rearrange Operator Key k List Physical Operator inp this k inp public Local Rearrange Operator Key k int rp List Physical Operator inp super k rp inp index leaf Ops new Array List Expression Operator secondary Leaf Ops new Array List Expression Operator m Projected Cols Map new Hash Map Integer Integer m Secondary Projected Cols Map new Hash Map Integer Integer public Local Rearrange Local Rearrange copy super copy this plans copy plans this secondary Plans copy secondary Plans this leaf Ops copy leaf Ops this secondary Leaf Ops copy secondary Leaf Ops this index copy index this key Type copy key Type this main Key Type copy main Key Type this secondary Key Type copy secondary Key Type this m Is Distinct copy m Is Distinct this is Cross copy is Cross this m Projected Cols Map copy m Projected Cols Map this m Secondary Projected Cols Map copy m Secondary Projected Cols Map this m Fake Tuple copy m Fake Tuple this m Project Star copy m Project Star this m Secondary Project Star copy m Secondary Project Star this is Key Tuple copy is Key Tuple this is Key Compound copy is Key Compound this is Secondary Key Tuple copy is Secondary Key Tuple this m Projected Cols Map Size copy m Projected Cols Map Size this m Secondary Projected Cols Map Size copy m Secondary Projected Cols Map Size this use Secondary Key copy use Secondary Key this strip Key From Value copy strip Key From Value Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Local Rearrange this Override public String name return get Alias String Local Rearrange Data Type find Type Name result Type Data Type find Type Name key Type m Is Distinct m Key to String Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false public byte get Index return index Sets the co group index of this operator param index the position of this operator in a co group operation throws Exec Exception if the index value is bigger then x public void set Index int index throws Exec Exception set Index index false Sets the multi query index of this operator param index the position of the parent plan of this operator in the enclosed split operator throws Exec Exception if the index value is bigger then x public void set Multi Query Index int index throws Exec Exception set Index index true private void set Index int index boolean multi Query throws Exec Exception if index Pig Nullable Writable idx Space indices in group and cogroup should only be in the range x to x only possible inputs int err Code String msg multi Query Merge more than map reduce jobs not supported Cogroups with more than inputs not supported throw new Exec Exception msg err Code Pig Exception else We could potentially be sending the key value relating to multiple group by statements through one map reduce job in multiquery optimized execution In this case we want two keys which have the same content but coming from different group by operations to be treated differently so that they go to different invocations of the reduce To achieve this we let the index be outside the regular index space x to x by Ring with the mq Flag bitmask which will put the index above the x value In Pig Nullable Writable compare To if the index is in this multiquery space we also consider the index when comparing two Pig Nullable Writables and not just the contents Keys with same contents coming from different group by operations would have different indices and hence would go to different invocation of reduce this index multi Query byte index Pig Nullable Writable mq Flag byte index public boolean is Distinct return m Is Distinct public void set Distinct boolean is Distinct m Is Distinct is Distinct if m Is Distinct m Fake Tuple m Tuple Factory new Tuple Overridden since the attachment of the new input should cause the old processing to end Override public void attach Input Tuple t super attach Input t Calls get Next on the generate operator inside the nested physical plan Converts the generated tuple into the proper format i e key indexed Tuple value Override public Result get Next Tuple throws Exec Exception inp null Result res while true inp process Input if inp return Status Status inp return Status Status break if inp return Status Status continue for Physical Plan ep plans ep attach Input Tuple inp result List Result res Lst new Array List Result if secondary Plans null for Physical Plan ep secondary Plans ep attach Input Tuple inp result List Result secondary Res Lst null if secondary Leaf Ops null secondary Res Lst new Array List Result for Expression Operator op leaf Ops switch op get Result Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type res op get Next op get Result Type break default log error Invalid result type Data Type find Type op get Result Type break if res return Status Status return res res Lst add res if secondary Leaf Ops null for Expression Operator op secondary Leaf Ops switch op get Result Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type res op get Next op get Result Type break default log error Invalid result type Data Type find Type op get Result Type break allow null as group by key if res return Status Status res return Status Status return new Result secondary Res Lst add res If we are using secondary sort key our new key is nullable index key secondary key value res result construct Output res Lst secondary Res Lst Tuple inp result res return Status Status detach Plans plans if secondary Plans null detach Plans secondary Plans res result illustrator Markup inp result res result return res return inp private void detach Plans List Physical Plan plans for Physical Plan ep plans ep detach Input protected Object get Key From Result List Result res Lst byte type throws Exec Exception Object key if res Lst size Tuple t m Tuple Factory new Tuple res Lst size int i for Result res res Lst t set i res result key t else if res Lst size type Data Type We get here after merging multiple jobs that have different map key types into a single job during multi query optimization If the key is n t a tuple it must be wrapped in a tuple Object obj res Lst get result if obj instanceof Tuple key obj else Tuple t m Tuple Factory new Tuple t set res Lst get result key t else key res Lst get result return key protected Tuple construct Output List Result res Lst List Result secondary Res Lst Tuple value throws Exec Exception Tuple lr Output m Tuple Factory new Tuple lr Output set Byte value Of this index Construct key Object key Object secondary Key null if secondary Res Lst null secondary Res Lst size key get Key From Result res Lst main Key Type secondary Key get Key From Result secondary Res Lst secondary Key Type else key get Key From Result res Lst key Type if strip Key From Value lr Output set key lr Output set value return lr Output if m Is Distinct Put the key and the indexed tuple in a tuple and return lr Output set key if illustrator null lr Output set key else lr Output set m Fake Tuple return lr Output else if is Cross for int i i plans size i value get All remove Put the index key and value in a tuple and return lr Output set key lr Output set value return lr Output else Put the index key and value in a tuple and return if use Secondary Key Tuple compound Key m Tuple Factory new Tuple compound Key set key compound Key set secondary Key lr Output set compound Key else lr Output set key strip off the columns in the value which are present in the key if m Projected Cols Map Size m Project Star true Tuple minimal Value null if m Project Star minimal Value m Tuple Factory new Tuple look for individual columns that we are projecting for int i i value size i if m Projected Cols Map get i null this column was not found in the key so send it in the value minimal Value append value get i minimal Value illustrator Markup value minimal Value else for the project star case we would send out an empty tuple as the value since all elements are in the key minimal Value m Tuple Factory new Tuple lr Output set minimal Value else there were no columns in the key which we can strip off from the value so just send the value we got lr Output set value return lr Output public byte get Key Type return key Type public byte get Main Key Type return main Key Type public void set Key Type byte key Type if use Secondary Key this main Key Type key Type else this key Type key Type this main Key Type key Type public List Physical Plan get Plans return plans public void set Use Secondary Key boolean use Secondary Key this use Secondary Key use Secondary Key main Key Type key Type public void set Plans List Physical Plan plans throws Plan Exception this plans plans leaf Ops clear int key Index zero based index for fields in the key for Physical Plan plan plans Expression Operator leaf Expression Operator plan get Leaves get leaf Ops add leaf do n t optimize if is Cross Look for the leaf Ops which are Project operators get the the columns that these Project Operators are projecting They be projecting either a column or Keep track of the columns which are being projected and the position in the Key where these will be projected to Then we can use this information to strip off these columns from the Value and in Package stitch the right Value tuple back by getting these columns from the key The goal is reduce the amount of the data sent to Hadoop in the map if leaf instanceof Project Project project Project leaf if project is Star note that we have a project m Project Star true key will be a tuple in this case is Key Tuple true The number of columns from the project is unkown so position of remaining colums in key ca n t be determined stop optimizing here break else if project is Project To End List Physical Operator preds plan get Predecessors project if preds null preds size a sanity check should never come here throw new Assertion Error project range has predecessors The number of columns from the project to end is unkown so position of remaining colums in key ca n t be determined stop optimizing here break else try List Physical Operator preds plan get Predecessors leaf if preds null preds get instanceof Project m Projected Cols Map put project get Column key Index catch Exec Exception e int err Code String msg Problem in accessing column from project operator throw new Plan Exception msg err Code Pig Exception if project get Result Type Data Type is Key Tuple true key Index if key Index make a note that the key is a tuple this is required by Package to pick things off the key correctly to stitch together the value is Key Tuple true is Key Compound true m Projected Cols Map Size m Projected Cols Map size public void set Secondary Plans List Physical Plan plans throws Plan Exception this secondary Plans plans secondary Leaf Ops clear int key Index zero based index for fields in the key for Physical Plan plan plans Expression Operator leaf Expression Operator plan get Leaves get secondary Leaf Ops add leaf do n t optimize if is Cross Look for the leaf Ops which are Project operators get the the columns that these Project Operators are projecting They be projecting either a column or Keep track of the columns which are being projected and the position in the Key where these will be projected to Then we can use this information to strip off these columns from the Value and in Package stitch the right Value tuple back by getting these columns from the key The goal is reduce the amount of the data sent to Hadoop in the map if leaf instanceof Project Project project Project leaf if project is Star note that we have a project m Secondary Project Star true key will be a tuple in this case is Secondary Key Tuple true The number of columns from the project is unknown so position of remaining columns in key ca n t be determined stop optimizing here break else if project is Project To End List Physical Operator preds plan get Predecessors project if preds null preds size a sanity check should never come here throw new Assertion Error project range has predecessors The number of columns from the project to end is unknown so position of remaining columns in key ca n t be determined stop optimizing here break else try List Physical Operator preds plan get Predecessors leaf if preds null preds get instanceof Project m Secondary Projected Cols Map put project get Column key Index catch Exec Exception e int err Code String msg Problem in accessing column from project operator throw new Plan Exception msg err Code Pig Exception if project get Result Type Data Type is Secondary Key Tuple true key Index if key Index make a note that the key is a tuple this is required by Package to pick things off the key correctly to stitch together the value is Secondary Key Tuple true main Key Type key Type key Type Data Type if plans size secondary Key Type Data Type else secondary Key Type plans get get Leaves get get Result Type m Secondary Projected Cols Map Size m Secondary Projected Cols Map size Make a deep copy of this operator throws Clone Not Supported Exception Override public Local Rearrange clone throws Clone Not Supported Exception Local Rearrange clone Local Rearrange super clone Constructor clone leaf Ops new Array List Expression Operator clone secondary Leaf Ops new Array List Expression Operator Needs to be called as set Distinct so that the fake index tuple gets created clone set Distinct m Is Distinct Set the key Type to main Key Type set Secondary Plans will calculate based on that and set key Type to the final value if use Secondary Key clone key Type main Key Type try clone set Plans clone Plans plans if secondary Plans null clone set Secondary Plans clone Plans secondary Plans catch Plan Exception pe Clone Not Supported Exception cnse new Clone Not Supported Exception Problem with setting plans of this get Class get Simple Name cnse init Cause pe throw cnse return clone public boolean is Cross return is Cross public void set Cross boolean is Cross this is Cross is Cross return the m Projected Cols Map public Map Integer Integer get Projected Cols Map return m Projected Cols Map return the m Projected Cols Map public Map Integer Integer get Secondary Projected Cols Map return m Secondary Projected Cols Map return the m Project Star public boolean is Project Star return m Project Star return the m Project Star public boolean is Secondary Project Star return m Secondary Project Star return the key Tuple public boolean is Key Tuple return is Key Tuple return the is Key Compound public boolean is Key Compound return is Key Compound return the key Tuple public boolean is Secondary Key Tuple return is Secondary Key Tuple param plans throws Exec Exception public void set Plans From Combiner List Physical Plan plans throws Plan Exception this plans plans leaf Ops clear m Projected Cols Map clear int key Index zero based index for fields in the key for Physical Plan plan plans Expression Operator leaf Expression Operator plan get Leaves get leaf Ops add leaf do n t optimize if is Cross Look for the leaf Ops which are Project operators get the the columns that these Project Operators are projecting Keep track of the columns which are being projected and the position in the Key where these will be projected to Then we can use this information to strip off these columns from the Value and in Post Combiner Package stitch the right Value tuple back by getting these columns from the key The goal is reduce the amount of the data sent to Hadoop in the map if leaf instanceof Project Project project Project leaf if project is Project To End int err Code String msg Internal error Unexpected operator project or in local rearrange inner plan throw new Plan Exception msg err Code Pig Exception else try m Projected Cols Map put project get Column key Index catch Exec Exception e int err Code String msg Problem in accessing column from project operator throw new Plan Exception msg err Code Pig Exception if project get Result Type Data Type is Key Tuple true key Index if key Index make a note that the key is a tuple this is required by Package to pick things off the key correctly to stitch together the value is Key Tuple true m Projected Cols Map Size m Projected Cols Map size protected void set Strip Key From Value boolean strip Key From Value this strip Key From Value strip Key From Value Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null if out instanceof Example Tuple Example Tuple t Out new Example Tuple Tuple out illustrator get Lineage insert t Out illustrator add Data t Out illustrator get Lineage union t Out Tuple in t Out synthetic Example Tuple in synthetic return t Out return Tuple out 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util List import java util Map import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig impl plan Node Id Generator import org apache pig impl plan Visitor Exception public class Map Look Up extends Expression Operator private static final long serial Version private String key public Map Look Up Operator Key k super k public Map Look Up Operator Key k int rp super k rp public Map Look Up Operator Key k int rp String key super k rp this key key public void set Look Up Key String key this key key public String get Look Up Key return key Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Map Look Up this Override public String name return Map Look Up Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return false Override public Result process Input throws Exec Exception Result res new Result Map String Object inp Value null if input null inputs null inputs size log warn No inputs found Signaling End of Processing res return Status Status return res if is Input Attached return inputs get get Next Map else res result input res return Status Status detach Input return res Suppress Warnings unchecked private Result get Next throws Exec Exception Result res process Input if res result null res return Status Status res result Map String Object res result get key return res Override public Result get Next Boolean throws Exec Exception return get Next Override public Result get Next Data Bag throws Exec Exception return get Next Override public Result get Next Data Byte Array throws Exec Exception return get Next Override public Result get Next Double throws Exec Exception return get Next Override public Result get Next Float throws Exec Exception return get Next Override public Result get Next Integer throws Exec Exception return get Next Override public Result get Next Long throws Exec Exception return get Next Override public Result get Next Date Time throws Exec Exception return get Next Override public Result get Next Map throws Exec Exception return get Next Override public Result get Next String throws Exec Exception return get Next Override public Result get Next Tuple throws Exec Exception return get Next Override public Map Look Up clone throws Clone Not Supported Exception Map Look Up clone new Map Look Up new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope key clone clone Helper this return clone Override public List Expression Operator get Child Expressions return null Override public Tuple illustrator Markup Object in Object out int eq Class Index return Tuple out 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java io Exception import java io Object Input Stream import java util Array List import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop mapreduce Job import org apache pig Func Spec import org apache pig Indexable Load Func import org apache pig Load Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig data Schema Tuple import org apache pig data Schema Tuple Backend import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data Schema Tuple Factory import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig data Tuple Maker import org apache pig impl Pig Context import org apache pig impl builtin Default Indexable Loader import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map import org apache pig newplan logical relational Join This operator implements merge join algorithm to do map side joins Currently only two way joins are supported One input of join is identified as left and other is identified as right Left input tuples are the input records in map Right tuples are read from by opening right stream This join does n t support outer join Data is assumed to be sorted in ascending order It will fail if data is sorted in descending order public class Merge Join extends Physical Operator private static final Log log Log Factory get Log Merge Join class private static final long serial Version private static final String key Order Reminder Remember that you should not change the order of keys before a merge join in a or manipulate join keys in a in a way that would change the sort order Fs in a are allowed as long as they do not change the join key values in a way that would change the sort order n flag to indicate when get Next is called first private boolean first Time true The Local Rearrange operators modeling the join key private Local Rearrange Rs private transient Load Func right Loader private Operator Key op Key private Object prev Left Key private Result prev Left Inp private Object prev Right Key null private Result prev Right Inp boolean denoting whether we are generating joined tuples in this get Next call or do we need to read in more data private boolean doing Join private Func Spec right Loader Func Spec private String right Input File Name private String index File Buffer to hold accumulated left tuples private transient Tuples To Schema Tuple List left Tuples private Multi Map Physical Operator Physical Plan inp Plans private Physical Operator right Pipeline Leaf private Physical Operator right Pipeline Root private boolean no Inner Plan On Right Side private Object cur Join Key private Tuple cur Joining Right Tup private int counter of tuples on left side with same key private int left Tup Size private int right Tup Size private int array List Size private Join join Type private String signature private byte end Of Record Mark Status Only for Spark If current operator reaches at its end flag end Of Input is set as true The old flag parent Plan end Of All Input does n t work in spark mode because it is shared between operators in the same plan so it could be set by preceding operators even current operator does not reach at its end see private transient boolean end Of Input false public boolean is End Of Input return end Of Input public void set End Of Input boolean is End Of Input end Of Input is End Of Input Only for spark it means that current operator reaches at its end and the last left input was added into left Tuples ready for join private boolean left Input Consumed In Spark false This serves as the default Tuple Factory private transient Tuple Factory m Tuple Factory These Tuple Factories are used for more efficient Tuple generation This should decrease the amount of memory needed for a given map task to successfully perform a merge join private transient Tuple Maker merged Tuple Maker private transient Tuple Maker left Tuple Maker private Schema left Input Schema private Schema merged Input Schema param k param rp param inp param inp Plans there can only be inputs each being a List Physical Plan Ex join by by public Merge Join Operator Key k int rp List Physical Operator inp Multi Map Physical Operator Physical Plan inp Plans List List Byte key Types Join join Type Schema left Input Schema Schema right Input Schema Schema merged Input Schema throws Plan Exception super k rp inp this op Key k this doing Join false this inp Plans inp Plans Rs new Local Rearrange this create Join Plans inp Plans key Types this index File null this join Type join Type this left Input Schema left Input Schema this merged Input Schema merged Input Schema Configures the Local Rearrange operators to get keys out of tuple throws Exec Exception private void create Join Plans Multi Map Physical Operator Physical Plan inp Plans List List Byte key Types throws Plan Exception int i for Physical Operator inp Phy Op inp Plans key Set i Local Rearrange lr new Local Rearrange gen Key try lr set Index i catch Exec Exception e throw new Plan Exception e get Message e get Error Code e get Error Source e lr set Result Type Data Type lr set Key Type key Types get i size Data Type key Types get i get lr set Plans inp Plans get inp Phy Op Rs i lr This is a helper method that sets up all of the Tuple Factory members private void prepare Tuple Factories m Tuple Factory Tuple Factory get Instance if left Input Schema null left Tuple Maker Schema Tuple Backend new Schema Tuple Factory left Input Schema false Gen Context if left Tuple Maker null log debug No Schema Tuple Factory available for combined left merge join schema left Input Schema left Tuple Maker m Tuple Factory else log debug Using Schema Tuple Factory for left merge join schema left Input Schema if merged Input Schema null merged Tuple Maker Schema Tuple Backend new Schema Tuple Factory merged Input Schema false Gen Context if merged Tuple Maker null log debug No Schema Tuple Factory available for combined left right merge join schema merged Input Schema merged Tuple Maker m Tuple Factory else log debug Using Schema Tuple Factory for left right merge join schema merged Input Schema This provides a List to store Tuples in The implementation of that list depends on whether or not there is a Tuple Factory available return the list object to store Tuples in private Tuples To Schema Tuple List new Left Tuple Array return new Tuples To Schema Tuple List array List Size left Tuple Maker This is a class that extends Array List making it easy to provide on the fly conversion from Tuple to Schema Tuple This is necessary because we are not getting Schema Tuples from the source though in the future that is what we would like to do public static class Tuples To Schema Tuple List extends Array List Tuple private Schema Tuple Factory tf public Tuples To Schema Tuple List int ct Tuple Maker tf super ct if tf instanceof Schema Tuple Factory this tf Schema Tuple Factory tf public static Schema Tuple convert Tuple t Schema Tuple Factory tf if t instanceof Schema Tuple return Schema Tuple t Schema Tuple st tf new Tuple try return st set t catch Exec Exception e throw new Runtime Exception Unable to set Schema Tuple with schema st get Schema String with given Tuple in merge join Override public boolean add Tuple t if tf null t convert t tf return super add t Override public Tuple get int i return super get i Override public int size return super size Suppress Warnings unchecked Override public Result get Next Tuple throws Exec Exception Object cur Left Key Result cur Left Inp if first Time prepare Tuple Factories left Tuples new Left Tuple Array Do initial setup cur Left Inp process Input if cur Left Inp return Status Status return cur Left Inp Return because we want to fetch next left tuple cur Left Key extract Keys From Tuple cur Left Inp if null cur Left Key We drop the tuples which have null keys return new Result end Of Record Mark null try seek In Right Stream cur Left Key catch Exception e throw Processing Exception true e catch Class Cast Exception e throw Processing Exception true e left Tuples add Tuple cur Left Inp result first Time false prev Left Key cur Left Key return new Result end Of Record Mark null if doing Join We matched on keys Time to do the join if counter We have left tuples to join with current right tuple Tuple joining Left Tup left Tuples get counter left Tup Size joining Left Tup size Tuple joined Tup merged Tuple Maker new Tuple left Tup Size right Tup Size for int i i left Tup Size i joined Tup set i joining Left Tup get i for int i i right Tup Size i joined Tup set i left Tup Size cur Joining Right Tup get i return new Result Status joined Tup Join with current right input has ended But bag of left tuples may still join with next right tuple doing Join false while true Result right Inp get Next Right Inp if right Inp return Status Status prev Right Inp null return right Inp else Object right Key extract Keys From Tuple right Inp if null right Key If we see tuple having null keys in stream we drop them continue and fetch next tuple int cmpval Comparable right Key compare To cur Join Key if cmpval Matched the very next right tuple cur Joining Right Tup Tuple right Inp result right Tup Size cur Joining Right Tup size counter left Tuples size doing Join true return this get Next Tuple else if cmpval We got ahead on right side Store currently read right tuple if this parent Plan end Of All Input left Input Consumed In Spark prev Right Key right Key prev Right Inp right Inp There cant be any more join on this key left Tuples new Left Tuple Array left Tuples add Tuple prev Left Inp result return new Result end Of Record Mark null else This is end of all input and this is last join output Right loader in this case would n t get a chance to close input stream So we close it ourself try Indexable Load Func right Loader close catch Exception e Non fatal error We can continue log error Received exception while trying to close right side file e get Message return new Result Status null else At this point right side ca n t be behind int err Code String err Msg Data is not sorted on right side n key Order Reminder Last two tuples encountered were n cur Joining Right Tup n Tuple right Inp result throw new Exec Exception err Msg err Code cur Left Inp process Input switch cur Left Inp return Status case Status cur Left Key extract Keys From Tuple cur Left Inp if null cur Left Key We drop the tuples which have null keys return new Result end Of Record Mark null int cmp Val Comparable cur Left Key compare To prev Left Key if cmp Val Keep on accumulating left Tuples add Tuple cur Left Inp result return new Result end Of Record Mark null else if cmp Val Filled with left bag Move on cur Join Key prev Left Key break else Current key Prev Key int err Code String err Msg Data is not sorted on left side n key Order Reminder Last two tuples encountered were n prev Left Key n cur Left Key throw new Exec Exception err Msg err Code case Status if this parent Plan end Of All Input is End Of Input We hit the end on left input Tuples in bag may still possibly join with right side cur Join Key prev Left Key cur Left Key null if is End Of Input left Input Consumed In Spark true break else Fetch next left input return cur Left Inp default If encountered with on left side we send it down return cur Left Inp if null prev Right Key this parent Plan end Of All Input left Input Consumed In Spark Comparable prev Right Key compare To cur Left Key This will happen when we accumulated inputs on left side and moved on but are still behind the right side In that case throw away the tuples accumulated till now and add the one we read in this function call left Tuples new Left Tuple Array left Tuples add Tuple cur Left Inp result prev Left Inp cur Left Inp prev Left Key cur Left Key return new Result end Of Record Mark null Accumulated tuples with same key on left side But since we are reading ahead we still have n t checked the read ahead right tuple Accumulated left tuples may potentially join with that So lets check that first if null prev Right Key prev Right Key equals prev Left Key cur Joining Right Tup Tuple prev Right Inp result counter left Tuples size right Tup Size cur Joining Right Tup size doing Join true prev Left Inp cur Left Inp prev Left Key cur Left Key return this get Next Tuple We will get here only when cur Left Key prev Right Key boolean sliding To Next Record false while true Start moving on right stream to find the tuple whose key is same as with current left bag key Result right Inp if sliding To Next Record right Inp get Next Right Inp sliding To Next Record false else right Inp get Next Right Inp prev Left Key if right Inp return Status Status return right Inp Object extracted Right Key extract Keys From Tuple right Inp if null extracted Right Key If we see tuple having null keys in stream we drop them continue and fetch next tuple Comparable right Key Comparable extracted Right Key if prev Right Key null right Key compare To prev Right Key Sanity check int err Code String err Msg Data is not sorted on right side n key Order Reminder Last two tuples encountered were n prev Right Key n right Key throw new Exec Exception err Msg err Code int cmpval right Key compare To prev Left Key if cmpval still behind the left side do nothing fetch next right tuple sliding To Next Record true continue else if cmpval Found matching tuple Time to do join cur Joining Right Tup Tuple right Inp result counter left Tuples size right Tup Size cur Joining Right Tup size doing Join true prev Left Inp cur Left Inp prev Left Key cur Left Key return this get Next Tuple else We got ahead on right side Store currently read right tuple prev Right Key right Key prev Right Inp right Inp Since we did n t find any matching right tuple we throw away the buffered left tuples and add the one read in this function call left Tuples new Left Tuple Array left Tuples add Tuple cur Left Inp result prev Left Inp cur Left Inp prev Left Key cur Left Key if this parent Plan end Of All Input left Input Consumed In Spark This is end of all input and this is last time we will read right input Right loader in this case would n t get a chance to close input stream So we close it ourself try Indexable Load Func right Loader close catch Exception e Non fatal error We can continue log error Received exception while trying to close right side file e get Message return new Result end Of Record Mark null private void seek In Right Stream Object first Left Key throws Exception right Loader Load Func Pig Context instantiate Func From Spec right Loader Func Spec check if hadoop distributed cache is used if index File null right Loader instanceof Default Indexable Loader Default Indexable Loader loader Default Indexable Loader right Loader loader set Index File index File Pass signature of the loader to right Loader make a copy of the conf to use in calls to right Loader right Loader set Context Signature signature Job job new Job new Configuration Pig Map Reduce s Job Conf Internal get right Loader set Location right Input File Name job Indexable Load Func right Loader initialize job get Configuration Indexable Load Func right Loader seek Near first Left Key instanceof Tuple Tuple first Left Key m Tuple Factory new Tuple first Left Key private Result get Next Right Inp Object left Key throws Exec Exception Only call seek Near if the merge join is merge sparse Default Indexable Loader does not support more than a single call to seek Near per split so do n t call seek Near if join Type Join try Indexable Load Func right Loader seek Near left Key instanceof Tuple Tuple left Key m Tuple Factory new Tuple left Key prev Right Key null catch Exception e throw Processing Exception true e return this get Next Right Inp private Result get Next Right Inp throws Exec Exception try if no Inner Plan On Right Side Tuple t right Loader get Next if t null no more data on right side return new Result Status null else return new Result Status t else Result res right Pipeline Leaf get Next Tuple right Pipeline Leaf detach Input switch res return Status case Status return res case Status Tuple t right Loader get Next if t null no more data on right side return new Result Status null else run the tuple through the pipeline right Pipeline Root attach Input t return this get Next Right Inp default We do n t deal with just pass them down throw Processing Exception false null catch Exception e throw Processing Exception true e we should never get here return new Result Status null public void throw Processing Exception boolean with Cause Exception Exception e throws Exec Exception int err Code String err Msg Error processing right input during merge join if with Cause Exception throw new Exec Exception err Msg err Code Pig Exception e else throw new Exec Exception err Msg err Code Pig Exception private Object extract Keys From Tuple Result inp int lr Idx throws Exec Exception Separate Key Value of input using corresponding operator Local Rearrange lr Rs lr Idx lr attach Input Tuple inp result Result lr Out lr get Next Tuple lr detach Input if lr Out return Status Status int err Code String err Msg Local Rearrange used to extract keys from tuple is n t configured correctly throw new Exec Exception err Msg err Code Pig Exception return Tuple lr Out result get public void setup Right Pipeline Physical Plan right Pipeline throws Frontend Exception if right Pipeline null if right Pipeline get Leaves size right Pipeline get Roots size int err Code String err Msg Expected physical plan with exactly one root and one leaf throw new Frontend Exception err Msg err Code Pig Exception no Inner Plan On Right Side false this right Pipeline Leaf right Pipeline get Leaves get this right Pipeline Root right Pipeline get Roots get this right Pipeline Root set Inputs null else no Inner Plan On Right Side true private void read Object Object Input Stream is throws Exception Class Not Found Exception Exec Exception is default Read Object m Tuple Factory Tuple Factory get Instance private Operator Key gen Key return new Operator Key op Key scope Node Id Generator get Generator get Next Node Id op Key scope public void set Right Loader Func Spec Func Spec right Loader Func Spec this right Loader Func Spec right Loader Func Spec public List Physical Plan get Inner Plans Of int index return inp Plans get inputs get index Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Merge Join this Override public String name String name get Alias String Merge Join if join Type Join name sparse name Data Type find Type Name result Type m Key to String return name Override public boolean supports Multiple Inputs return true non Javadoc see org apache pig impl plan Operator supports Multiple Outputs Override public boolean supports Multiple Outputs return false param right Input File Name the right Input File Name to set public void set Right Input File Name String right Input File Name this right Input File Name right Input File Name public String get Signature return signature public void set Signature String signature this signature signature public void set Index File String index File this index File index File public String get Index File return index File Override public Tuple illustrator Markup Object in Object out int eq Class Index return null public Join get Join Type return join Type public Local Rearrange get Rs return Rs 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java math Big Decimal import java math Big Integer import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception public class Negative extends Unary Expression Operator private static final long serial Version public Negative Operator Key k int rp super k rp public Negative Operator Key k super k public Negative Operator Key k int rp Expression Operator input super k rp this expr input Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Negative this Override public String name Auto generated method stub return Negative Data Type find Type Name result Type m Key to String Override public Result get Next Double throws Exec Exception Result res expr get Next Double if res return Status Status res result null res result Double res result return res Override public Result get Next Float throws Exec Exception Result res expr get Next Float if res return Status Status res result null res result Float res result return res Override public Result get Next Integer throws Exec Exception Result res expr get Next Integer if res return Status Status res result null res result Integer res result return res Override public Result get Next Long throws Exec Exception Result res expr get Next Long if res return Status Status res result null res result Long res result return res Override public Result get Next Big Integer throws Exec Exception Result res expr get Next Big Integer if res return Status Status res result null res result Big Integer res result negate return res Override public Result get Next Big Decimal throws Exec Exception Result res expr get Next Big Decimal if res return Status Status res result null res result Big Decimal res result negate return res Override public Negative clone throws Clone Not Supported Exception Negative clone new Negative new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone Override public Tuple illustrator Markup Object in Object out int eq Class Index return Tuple out 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception Boolean or operator public class Or extends Binary Comparison Operator private static final long serial Version public Or Operator Key k this k public Or Operator Key k int rp super k rp result Type Data Type Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Or this Override public String name return Or Data Type find Type Name result Type m Key to String Override public Result get Next Boolean throws Exec Exception Result r accum Child null Data Type if r null return r Result left left lhs get Next Boolean pass on and if left return Status Status return left truth table for t true n null f false t n f t t t t n t n n f t n f Short circuit if lhs is true return true above is handled with this boolean return Left false if left result null Boolean left result boolean Value if illustrator null return left illustrator Markup null left result return Left true Result right rhs get Next Boolean if return Left return left pass on and if right return Status Status return right if the lhs is null and rhs is false return null in all other cases we can just return rhs and above if left result null right result null Boolean right result boolean Value return left No matter what what we get from the right side is what we ll return null true or false if right result null illustrator Markup null right result Boolean right result return right Override public Or clone throws Clone Not Supported Exception Or clone new Or new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone clone Helper this return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java io Exception import java util Array List import java util Iterator import java util List import org apache pig Pig Configuration import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine util Accumulator Optimizer Util import org apache pig data Accumulative Bag import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Internal Cached Bag import org apache pig data Read Once Bag import org apache pig data Tuple import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen Illustrator The package operator that packages the globally rearranged tuples into output format as required by co group This is last stage of processing co group This operator has a slightly different format than other operators in that it takes two things as input The key being worked on and the iterator of bags that contain indexed tuples that just need to be packaged into their appropriate output bags based on the index public class Package extends Physical Operator private static final long serial Version The iterator of indexed Tuples that is typically provided by Hadoop transient Iterator Nullable Tuple tup Iter The key being worked on protected Object key The number of inputs to this co group indicates a distinct which means there will only be a key no value protected int num Inputs private boolean last Bag Read Only true protected Packager pkgr protected Pig Nullable Writable key Writable private transient boolean initialized private transient boolean use Default Bag private transient int accumulative Batch Size public Package Operator Key k this k null public Package Operator Key k int rp this k rp null public Package Operator Key k List Physical Operator inp this k inp public Package Operator Key k int rp List Physical Operator inp this k rp inp new Packager public Package Operator Key k int rp List Physical Operator inp Packager pkgr super k rp inp num Inputs this pkgr pkgr public Package Package copy super copy this num Inputs copy num Inputs this pkgr copy pkgr this pkgr key Info copy pkgr key Info Override public void set Illustrator Illustrator illustrator super set Illustrator illustrator pkgr set Illustrator illustrator Override public String name return get Alias String Package pkgr name Data Type find Type Name result Type Data Type find Type Name pkgr get Key Type m Key to String Override public boolean supports Multiple Inputs return false Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Package this Override public boolean supports Multiple Outputs return false Attaches the required inputs param k the key being worked on param inp iterator of indexed tuples typically obtained from Hadoop public void attach Input Pig Nullable Writable k Iterator Nullable Tuple inp try tup Iter inp key pkgr get Key k key Writable k input Attached true catch Exception e throw new Runtime Exception Error attaching input for key k in name at location get Original Locations e attach Input s better half Override public void detach Input tup Iter null key null input Attached false public int get Num Inps return num Inputs public void set Num Inps int num Inps this num Inputs num Inps pkgr set Num Inputs num Inps From the inputs constructs the output tuple for this co group in the required format which is key bag of tuples from input bag of tuples from input Override public Result get Next Tuple throws Exec Exception if initialized initialized true if Pig Map Reduce s Job Conf Internal get null String bag Type Pig Map Reduce s Job Conf Internal get get Pig Configuration if bag Type null bag Type equals Ignore Case default use Default Bag true accumulative Batch Size Accumulator Optimizer Util get Accumulative Batch Size If multiquery the last bag is Internal Cached Bag and should not set Read Only flag otherwise we will materialize again to another Internal Cached Bag if pkgr instanceof Multi Query Packager last Bag Read Only false int num Inputs pkgr get Num Inputs key Writable get Index boolean read Once new boolean num Inputs for int i i num Inputs i read Once i false if is Input Attached Create num Inputs bags Data Bag dbs null dbs new Data Bag num Inputs if is Accumulative read Once num Inputs false create bag wrapper to pull tuples in many batches all bags have reference to the sample tuples buffer which contains tuples from one batch Package Tuple Buffer buffer new Package Tuple Buffer accumulative Batch Size key tup Iter for int i i num Inputs i dbs i new Accumulative Bag buffer i else if last Bag Read Only read Once num Inputs true We know the tuples will come sorted by index so we can wrap the last input in a Read Once Bag and let the Packager decide whether or not to read into memory create bag to pull all tuples out of iterator for int i i num Inputs i dbs i use Default Bag m Bag Factory new Default Bag In a very rare case if there is a Stream after this Package in the pipeline and is also blocking the pipeline constructor argument should be num Inputs But for one obscure case we do n t want to pay the penalty all the time new Internal Cached Bag num Inputs For each indexed tup in the inp sort them into their corresponding bags based on the index while tup Iter has Next Nullable Tuple ntup tup Iter next int index ntup get Index if index num Inputs if pkgr get Use Secondary Key if dbs index null dbs index use Default Bag m Bag Factory new Default Bag new Internal Cached Bag num Inputs else dbs index new Peeked Bag pkgr ntup tup Iter key Writable break Tuple copy pkgr get Value Tuple key Writable ntup index if num Inputs this is for multi query merge where the num Inputs is always but the index the position of the inner plan in the enclosed operator may not be dbs add copy else dbs index add copy if get Reporter null get Reporter progress Construct the output tuple by appending the key and all the above constructed bags and return it pkgr attach Input key dbs read Once detach Input return pkgr get Next public Packager get Pkgr return pkgr public void set Pkgr Packager pkgr this pkgr pkgr pkgr set Parent this pkgr set Illustrator illustrator Make a deep copy of this operator throws Clone Not Supported Exception Override public Package clone throws Clone Not Supported Exception Package clone Package super clone clone m Key new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope clone requested Parallelism requested Parallelism clone result Type result Type clone num Inputs num Inputs clone pkgr Packager this pkgr clone return clone public class Package Tuple Buffer implements Accumulative Tuple Buffer private List Tuple bags private Iterator Nullable Tuple iter private int batch Size private Object curr Key Suppress Warnings unchecked public Package Tuple Buffer int batch Size Object key Iterator Nullable Tuple iter this batch Size batch Size this curr Key key this iter iter this bags new List num Inputs for int i i num Inputs i this bags i new Array List Tuple batch Size Override public boolean has Next Batch return iter has Next Override public void next Batch throws Exception for int i i bags length i bags i clear key curr Key for int i i batch Size i if iter has Next Nullable Tuple ntup iter next int index ntup get Index Tuple copy pkgr get Value Tuple key Writable ntup index if num Inputs this is for multi query merge where the num Inputs is always but the index the position of the inner plan in the enclosed operator may not be bags add copy else bags index add copy else break Override public void clear for int i i bags length i bags i clear iter null Override public Iterator Tuple get Tuples int index return bags index iterator public Tuple illustrator Markup Object in Object out int eq Class Index return Package this illustrator Markup in out eq Class Index Override public Tuple illustrator Markup Object in Object out int eq Class Index return pkgr illustrator Markup in out eq Class Index public int number Of Equivalence Classes return pkgr number Of Equivalence Classes Read Once Bag that we ve already peeked at private static class Peeked Bag extends Read Once Bag private static final long serial Version Nullable Tuple head int index public Peeked Bag Packager pkgr Nullable Tuple head Iterator Nullable Tuple tup Iter Pig Nullable Writable key Writable super pkgr tup Iter key Writable this head head this index head get Index Override public Iterator Tuple iterator return new Iterator Tuple boolean head Returned false Override public boolean has Next if head Returned return true return tup Iter has Next Override public Tuple next if head Returned head Returned true try return pkgr get Value Tuple key Writable head head get Index catch Exec Exception e throw new Runtime Exception Peeked Bag failed to get value tuple e to String Nullable Tuple ntup tup Iter next Tuple ret null try ret pkgr get Value Tuple key Writable ntup index catch Exec Exception e throw new Runtime Exception Peeked Bag failed to get value tuple e to String if get Reporter null get Reporter progress return ret Override public void remove throw new Unsupported Operation Exception Peeked Bag does not support removal 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Array List import java util Iterator import java util List import org apache pig Pig Exception import org apache pig Pig Warning import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Single Tuple Bag import org apache pig data Tuple import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception Implements the overloaded form of the project operator Projects the specified column from the input tuple However if asked for tuples when the input is a bag the overloaded form is invoked and the project streams the tuples through instead of the bag public class Project extends Expression Operator private static final long serial Version private boolean result Single Tuple Bag false The column to project protected Array List Integer columns True if we are in the middle of streaming tuples in a bag private boolean processing Bag Of Tuples false The bag iterator used while straeming tuple private transient Iterator Tuple bag Iterator null Represents the fact that this instance of Project is overloaded to stream tuples in the bag rather than passing the entire bag It is the responsibility of the translator to set this protected boolean overloaded false protected boolean is Project To End false protected int start Col public Project Operator Key k this k public Project Operator Key k int rp this k rp public Project Operator Key k int rp int col super k rp columns new Array List Integer columns add col public Project Operator Key k int rp Array List Integer cols super k rp columns cols public void set Project To End int start Col this is Project To End true this start Col start Col columns new Array List Integer Override public String name String str Project Data Type find Type Name result Type if is Star str else if is Project To End str start Col else str columns str m Key to String return str Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Project this Overridden since the attachment of the new input should cause the old processing to end Override public void attach Input Tuple t super attach Input t processing Bag Of Tuples false Fetches the input tuple and returns the requested column return next value throws Exec Exception public Result get Next throws Exec Exception Result res process Input Tuple inp Value Tuple res result Object ret if res return Status Status return res if is Star illustrator Markup inp Value res result return res else if columns size if inp Value null the tuple is null so a dereference should also produce a null res return Status Status ret null else if inp Value size columns get ret inp Value get columns get else if pig Logger null pig Logger warn this Attempt to access field which was not found in the input Pig Warning res return Status Status ret null else if is Project To End ret get Range Tuple inp Value else Array List Object obj List new Array List Object columns size for int col columns add Column obj List inp Value col ret m Tuple Factory new Tuple No Copy obj List res result ret illustrator Markup inp Value res result return res private boolean is Range Invalid int last Col Idx if start Col last Col Idx this must be happening because tuple is smaller than start Col if pig Logger null pig Logger warn this Invalid range being projected start Col postition start Col greater than tuple size Pig Warning return true return false Add i th column from inp Value to obj List param obj List param inp Value param i throws Exec Exception private void add Column Array List Object obj List Tuple inp Value int i throws Exec Exception if inp Value null the tuple is null so a dereference should also produce a null obj List add null else if inp Value size i obj List add inp Value get i else if pig Logger null pig Logger warn this Attempt to access field i which was not found in the input Pig Warning obj List add null Override public Result get Next Data Bag throws Exec Exception Result res process Input Bag if res return Status Status return res return consume Input Bag res param input throws Exec Exception protected Result consume Input Bag Result input throws Exec Exception if is Input Attached is Star Result ret Val new Result ret Val result input result ret Val return Status Status detach Input return ret Val if input result instanceof Data Bag Data Bag inp Bag Data Bag input result Result ret Val new Result Data Bag out Bag if result Single Tuple Bag we have only one tuple in a bag so create Single Tuple Bag for the result and fill it appropriately from the input bag Tuple tuple inp Bag iterator next if is Project To End Array List Object obj List new Array List Object columns size for int col columns add Column obj List tuple col out Bag new Single Tuple Bag m Tuple Factory new Tuple No Copy obj List else Tuple tmp Tuple get Range Tuple tuple out Bag new Single Tuple Bag tmp Tuple else out Bag m Bag Factory new Default Bag for Tuple tuple inp Bag if is Project To End Array List Object obj List new Array List Object columns size for int col columns add Column obj List tuple col out Bag add m Tuple Factory new Tuple No Copy obj List else Tuple out Tuple get Range Tuple tuple out Bag add out Tuple ret Val result out Bag ret Val return Status Status return ret Val else if input result instanceof Tuple if input is tuple columns should only have one item Result ret Val new Result ret Val result Tuple input result get columns get ret Val return Status Status return ret Val else if input result null Result ret Val new Result ret Val result null ret Val return Status Status return ret Val else throw new Exec Exception Can not dereference a bag from input result get Class get Name private Tuple get Range Tuple Tuple tuple throws Exec Exception int last Col Idx tuple size Tuple out Tuple if is Range Invalid last Col Idx invalid range return empty tuple out Tuple m Tuple Factory new Tuple else Array List Object obj List new Array List Object last Col Idx start Col for int i start Col i last Col Idx i add Column obj List tuple i out Tuple m Tuple Factory new Tuple No Copy obj List return out Tuple Override public Result get Next Data Byte Array throws Exec Exception return get Next Override public Result get Next Double throws Exec Exception return get Next Override public Result get Next Float throws Exec Exception return get Next Override public Result get Next Integer throws Exec Exception return get Next Override public Result get Next Long throws Exec Exception return get Next Override public Result get Next Boolean throws Exec Exception return get Next Override public Result get Next Date Time throws Exec Exception return get Next Override public Result get Next Map throws Exec Exception return get Next Override public Result get Next String throws Exec Exception return get Next Override public Result get Next Big Integer throws Exec Exception return get Next Override public Result get Next Big Decimal throws Exec Exception return get Next Asked for Tuples Check if the input is a bag If so stream the tuples in the bag instead of the entire bag Override public Result get Next Tuple throws Exec Exception Result res new Result if processing Bag Of Tuples Tuple inp Value null res process Input if res return Status Status return res if is Star return res inp Value Tuple res result res result null Object ret if columns size if inp Value null the tuple is null so a dereference should also produce a null ret null else if inp Value size columns get ret inp Value get columns get else if pig Logger null pig Logger warn this Attempt to access field which was not found in the input Pig Warning ret null else if is Project To End ret get Range Tuple inp Value else Array List Object obj List new Array List Object columns size for int col columns if inp Value null the tuple is null so a dereference should also produce a null obj List add null else if inp Value size col obj List add inp Value get col else if pig Logger null pig Logger warn this Attempt to access field which was not found in the input Pig Warning obj List add null ret m Tuple Factory new Tuple obj List res result Tuple ret return res if overloaded if ret null Data Bag ret Bag Data Bag ret bag Iterator ret Bag iterator if bag Iterator has Next processing Bag Of Tuples true res result bag Iterator next If the bag contains no tuple set the return Status to if processing Bag Of Tuples res return Status Status else res return Status Status else res result Tuple ret return res if bag Iterator has Next res result bag Iterator next res return Status Status return res else done processing the bag of tuples processing Bag Of Tuples false return get Next Tuple public Array List Integer get Columns if is Project To End throw new Assertion Error Internal error Improper use of method get Columns in Project class get Simple Name return columns public int get Column throws Exec Exception if columns size is Project To End int err Code String msg Internal error Improper use of method get Column in Project class get Simple Name throw new Exec Exception msg err Code Pig Exception return columns get public int get Start Col return start Col public void set Columns Array List Integer cols if is Project To End throw new Assertion Error Columns should not be set for range projection this columns cols public void set Column int col is Project To End false if null columns columns new Array List Integer else columns clear columns add col public boolean is Overloaded return overloaded public void set Overloaded boolean overloaded this overloaded overloaded public boolean is Star return is Project To End start Col public boolean is Project To End return is Project To End public void set Star boolean star if star is Project To End true start Col else is Project To End false Override public Project clone throws Clone Not Supported Exception Array List Integer cols new Array List Integer columns size Can resuse the same Integer objects as they are immutable for Integer i columns cols add i Project clone new Project new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope requested Parallelism cols clone clone Helper this clone overloaded overloaded clone start Col start Col clone is Project To End is Project To End clone result Type result Type return clone protected Result process Input Bag throws Exec Exception Result res new Result if input null inputs null inputs size log warn No inputs found Signaling End of Processing res return Status Status return res Should be removed once the model is clear if get Reporter null get Reporter progress if is Input Attached if inputs get get Result Type Data Type return inputs get get Next Data Bag else return inputs get get Next Tuple else res result Data Bag input get columns get res return Status Status return res public void set Result Single Tuple Bag boolean result Single Tuple Bag this result Single Tuple Bag result Single Tuple Bag Override public List Expression Operator get Child Expressions return null Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util Array List import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Job Control Compiler import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple import org apache pig pen util Lineage Tracer This operator is part of the operator implementation Reads the output tuple from Counter and the cumulative sum previously calculated Here is read the task identifier in order to get the corresponding cumulative sum and the local counter at the tuple These values are summed and prepended to the tuple public class Rank extends Physical Operator private static final Log log Log Factory get Log Rank class private static final long serial Version private List Physical Plan rank Plans private List Boolean m Asc Cols private List Byte Expr Output Types Unique identifier that links Counter and Rank through the global counter labeled with it private String operation Counter used to set tuples into the equivalence classes private int local Count Illustrator public Rank Operator Key k this k null public Rank Operator Key k int rp this k rp null public Rank Operator Key k List Physical Operator inp this k inp public Rank Operator Key k int rp List Physical Operator inp super k rp inp public Rank Rank copy super copy this rank Plans copy rank Plans this m Asc Cols copy m Asc Cols this Expr Output Types copy Expr Output Types Suppress Warnings rawtypes unchecked public Rank Operator Key operator Key int requested Parallelism List inp List Physical Plan rank Plans List Boolean ascending Col super operator Key requested Parallelism inp this set Rank Plans rank Plans this set Ascending Cols ascending Col Expr Output Types new Array List Byte rank Plans size for Physical Plan plan rank Plans Expr Output Types add plan get Leaves get get Result Type Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null Example Tuple t Out new Example Tuple Tuple out illustrator add Data Tuple out illustrator get Equivalence Classes get eq Class Index add Tuple in Lineage Tracer lineage Tracer illustrator get Lineage lineage Tracer insert t Out return t Out return Tuple out Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Rank this Override public Result get Next Tuple throws Exec Exception Result inp null while true inp process Input if inp return Status Status inp return Status Status break if inp return Status Status continue return add Rank inp return inp Reads the output tuple from Counter and the cumulative sum previously calculated Here is read the task identifier in order to get the corresponding cumulative sum and the local counter at the tuple These values are summed and prepended to the tuple param input processed by Counter return input as Result The input result tuple owns the prepend rank value public Result add Rank Result input throws Exec Exception Tuple in Tuple input result Long local Counter Long in get Integer task Id Integer in get All remove in get All size Long rank get Rank Counter Offset task Id in set rank local Counter if local Count Illustrator local Count Illustrator input result illustrator Markup in in local Count Illustrator local Count Illustrator return input protected Long get Rank Counter Offset Integer task Id String name Counter Job Control Compiler get Operation Job Control Compiler String value Of task Id Long rank Pig Map Reduce s Job Conf Internal get get Long name Counter if illustrator null rank if rank log error Error on reading counter name Counter throw new Runtime Exception Unable to read counter name Counter return rank Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public String name return get Alias String Rank Data Type find Type Name result Type m Key to String public void set Rank Plans List Physical Plan rank Plans this rank Plans rank Plans public List Physical Plan get Rank Plans return rank Plans public void set Ascending Cols List Boolean m Asc Cols this m Asc Cols m Asc Cols public List Boolean get Ascending Cols return m Asc Cols Operation identifier shared within the corresponding Counter param operation public void set Operation String operation this operation operation public String get Operation return operation Override public Rank clone throws Clone Not Supported Exception Rank clone Rank super clone rank Plans m Asc Cols Expr Output Types are unused Not cloning them return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Array List import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig data Non Spillable Data Bag import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception Implements a specialized form of Project which is used in the following case This project is Project introduced after a relational operator to supply a bag as output as an expression This project is either providing the bag as input to a successor expression operator or is itself the leaf in a inner plan If the predecessor relational operator sends an then send an empty bag first to signal empty output and then send an Project of return type whose predecessor is from an outside plan i e not in the same inner plan as the project will lead us here So a query like a load baginp txt as b bag t tuple b foreach a generate dump b will go through a regular project without the following flag public class Relation To Expr Project extends Project private static final long serial Version boolean send Empty Bag On false public Relation To Expr Project Operator Key k this k public Relation To Expr Project Operator Key k int rp this k rp public Relation To Expr Project Operator Key k int rp int col super k rp col public Relation To Expr Project Operator Key k int rp Array List Integer cols super k rp cols Override public String name return Relation To Expression Project Data Type find Type Name result Type is Star columns m Key to String Override public void visit Phy Plan Visitor v throws Visitor Exception for now the specialization in this class does not affect the way visitors visit it so we can just use visit Project v visit Project this non Javadoc see org apache pig backend hadoop executionengine physical Layer Physical Operator reset Override public void reset the foreach in which this operator is present is starting with a new set of inputs if we see an from the predecessor first i e we do not see any other input before and and only see an this can happen if a Filter is the predecessor and it filters away all its input we should send an empty bag Set a flag which can be checked if an is encountered send Empty Bag On true Override public Result get Next Data Bag throws Exec Exception Result input process Input Bag if this is called during accumulation it is ok to have an empty bag we need to send so that the can be called if is Accumulative reset if input return Status Status if input return Status Status return input else if input return Status Status send Empty Bag On we received an from the predecessor since the successor in the pipeline is expecting a bag send an empty bag input result new Non Spillable Data Bag input return Status Status we should send the next time we are called if the foreach in which this operator is present calls this get Next bag with new inputs then this flag will be reset in this reset else since we are sending down some result empty bag or otherwise we should not be sending an empty bag on any more we are processing new inputs see reset send Empty Bag On false return input Result r consume Input Bag input since we are sending down some result empty bag or otherwise we should not be sending an empty bag on any more we are processing new inputs see reset send Empty Bag On false return r See Override public Relation To Expr Project clone throws Clone Not Supported Exception Array List Integer cols new Array List Integer columns size Can reuse the same Integer objects as they are immutable for Integer i columns cols add i Relation To Expr Project clone new Relation To Expr Project new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope requested Parallelism cols clone clone Helper this clone overloaded overloaded clone start Col start Col clone is Project To End is Project To End clone result Type result Type clone send Empty Bag On send Empty Bag On return clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java util Array List import java util List import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer schema Schema import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map The Physical Operator that represents a skewed join It must have two inputs This operator does not do any actually work it is only a place holder When it is translated into plan a Skewed Join is translated into a sampling job and a join job public class Skewed Join extends Physical Operator private static final long serial Version private boolean m Inner Flags The schema is used only by the Compiler to support outer join transient private List Schema input Schema new Array List Schema physical plans to retrive join keys the key of this code Multi Map code is the Physical Operator that corresponds to an input the value is a list of code Physical Plan code to retrieve each join key for this input private Multi Map Physical Operator Physical Plan m Join Plans public Skewed Join Operator Key k this k null null public Skewed Join Operator Key k int rp this k rp null null public Skewed Join Operator Key k List Physical Operator inp boolean flags this k inp flags public Skewed Join Operator Key k int rp List Physical Operator inp boolean flags super k rp inp if flags null copy the inner flags m Inner Flags new boolean flags length for int i i flags length i m Inner Flags i flags i public boolean get Inner Flags return m Inner Flags public Multi Map Physical Operator Physical Plan get Join Plans return m Join Plans public void set Join Plans Multi Map Physical Operator Physical Plan join Plans m Join Plans join Plans Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Skewed Join this Override public String name return get Alias String Skewed Join Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return true Override public boolean supports Multiple Outputs return false public void add Schema Schema s input Schema add s public Schema get Schema int i return input Schema get i Override public Tuple illustrator Markup Object in Object out int eq Class Index return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java io Serializable import java util Array List import java util Comparator import java util Iterator import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Map Reduce import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig backend hadoop executionengine physical Layer expression Operators User Comparison Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Internal Sorted Bag import org apache pig data Tuple import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception This implementation is applicable for both the physical plan and for the local backend as the conversion of physical to mapreduce would see the operator and take necessary steps to convert it to a quantile and a sort job This is a blocking operator The sorted Data Bag accumulates Tuples and sorts them only when there an iterator is started So all the tuples from the input operator should be accumulated and filled into the data Bag The attach Input method is not applicable here We intentionally skip type checking in backend for performance reasons Suppress Warnings unchecked public class Sort extends Physical Operator private static final Log log Log Factory get Log Sort class private static final long serial Version private List Integer m Sort Cols private List Physical Plan sort Plans private List Byte Expr Output Types private List Boolean m Asc Cols private User Comparison Func m Sort Func private Comparator Tuple m Comparator private long limit public boolean is Comparator Used false private transient boolean inputs Accumulated false private transient Data Bag sorted Bag private transient Iterator Tuple it private transient boolean initialized private transient boolean use Default Bag public Sort Operator Key k int rp List inp List Physical Plan sort Plans List Boolean m Asc Cols User Comparison Func m Sort Func super k rp inp this m Sort Cols m Sort Cols this sort Plans sort Plans this m Asc Cols m Asc Cols this limit set Sort Func m Sort Func private void set Sort Func User Comparison Func m Sort Func this m Sort Func m Sort Func if m Sort Func null m Comparator new Sort Comparator Expr Output Types new Array List Byte sort Plans size for Physical Plan plan sort Plans Expr Output Types add plan get Leaves get get Result Type else m Comparator new Sort Comparator is Comparator Used true public Sort Operator Key k int rp List inp super k rp inp public Sort Operator Key k int rp super k rp public Sort Operator Key k List inp super k inp public Sort Operator Key k super k public class Sort Comparator implements Comparator Tuple Serializable private static final long serial Version Override public int compare Tuple o Tuple o int count int ret if sort Plans null sort Plans size return for Physical Plan plan sort Plans try plan attach Input o Result res get Result plan Expr Output Types get count plan attach Input o Result res get Result plan Expr Output Types get count if res return Status Status res return Status Status log error Error processing the input in the expression plan plan to String else if m Asc Cols get count ret Data Type compare res result res result If they are not equal return Otherwise keep comparing the next one if ret return ret else ret Data Type compare res result res result if ret return ret catch Exec Exception e log error Invalid result while executing the expression plan plan to String n e get Message return ret private Result get Result Physical Plan plan byte result Type throws Exec Exception Expression Operator Op Expression Operator plan get Leaves get Result res null switch result Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type case Data Type res Op get Next result Type break default int err Code String msg Did not expect result of type Data Type find Type Name result Type throw new Exec Exception msg err Code Pig Exception return res public class Sort Comparator implements Comparator Tuple Serializable private static final long serial Version Override public int compare Tuple t Tuple t m Sort Func attach Input t t Integer i null Result res null try res m Sort Func get Next Integer catch Exec Exception e log error Input not ready Error on reading from input e get Message if res null return Integer res result else return Override public String name return get Alias String Sort Data Type find Type Name result Type m Sort Func null m Sort Func get Func Spec m Key to String Override public boolean is Blocking return true Override public Result get Next Tuple throws Exec Exception Result inp if inputs Accumulated inp process Input if initialized initialized true if Pig Map Reduce s Job Conf Internal get null String bag Type Pig Map Reduce s Job Conf Internal get get Pig Configuration if bag Type null bag Type equals Ignore Case default use Default Bag true if is Limited sorted Bag m Bag Factory new Limited Sorted Bag m Comparator limit else by default we create Internal Sorted Bag unless user configures explicitly to use old bag sorted Bag use Default Bag m Bag Factory new Sorted Bag m Comparator new Internal Sorted Bag m Comparator while inp return Status Status if inp return Status Status log error Error in reading from the inputs return inp else if inp return Status Status Ignore and read the next tuple inp process Input continue sorted Bag add Tuple inp result inp process Input inputs Accumulated true Result res new Result if it null it sorted Bag iterator if it has Next res result it next illustrator Markup res result res result res return Status Status else res return Status Status reset return res Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return false Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Sort this Override public void reset inputs Accumulated false sorted Bag null it null public List Physical Plan get Sort Plans return sort Plans public void set Sort Plans List Physical Plan sort Plans this sort Plans sort Plans public User Comparison Func get Sort Func return m Sort Func public void set Sort Func User Comparison Func sort Func m Sort Func sort Func public Comparator Tuple get Comparator return m Comparator public List Boolean get Asc Cols return m Asc Cols public void set Limit long l limit l public long get Limit return limit public boolean is Limited return limit Override public Sort clone throws Clone Not Supported Exception Sort clone Sort super clone clone sort Plans clone Plans sort Plans if m Sort Func null set Sort Func null else set Sort Func m Sort Func clone List Boolean clone Asc new Array List Boolean m Asc Cols size for Boolean b m Asc Cols clone Asc add b clone m Asc Cols clone Asc return clone Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null illustrator get Equivalence Classes get eq Class Index add Tuple in illustrator add Data Tuple out return Tuple out 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer relational Operators import java io Exception import java util List import org apache pig Pig Exception import org apache pig Sort Info import org apache pig Store Func Interface import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Store Impl import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl logical Layer schema Schema import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig pen util Example Tuple import org apache pig pen util Lineage Tracer import org apache pig tools pigstats Pig Stats Util The store operator which is used in two ways As a local operator it can be used to store files In the Map Reduce setting it is used to create jobs from Map Reduce operators which keep the loads and stores in the Map and Reduce Plans till the job is created public class Store extends Physical Operator private static final long serial Version transient private Store Func Interface storer transient private Store Func Decorator s Decorator transient private Store Impl impl transient private String counter Name null private File Spec s File private Schema schema flag to distinguish user stores from Compiler stores private boolean is Tmp Store flag to distinguish single store from multiquery store private boolean is Multi Store flag to indicate if the custom counter should be disabled private boolean disable Counter false the index of multiquery store to track counters private int index If we know how to reload the store here s how The l File File Spec is set in Pig Server post Process It can be used to reload this store if the optimizer has the need private File Spec l File if the predecessor of store is Sort order by then sort Info will have information of the sort column names and the asc dsc info private Sort Info sort Info private String signature private transient List String cache Files null private transient List String ship Files null public Store Operator Key k this k null public Store Operator Key k int rp this k rp null public Store Operator Key k int rp List Physical Operator inp super k rp inp public Store Store copy super copy this s File copy s File this schema copy schema this is Tmp Store copy is Tmp Store this is Multi Store copy is Multi Store this disable Counter copy disable Counter this index copy index this l File copy l File this sort Info copy sort Info this signature copy signature Set up the storer throws Exception public void set Up throws Exception if impl null try storer impl create Store Func this if is Tmp Store disable Counter impl instanceof Map Reduce Store Impl counter Name Pig Stats Util get Multi Store Counter Name this if counter Name null Create the counter This is needed because incr Counter may never be called in case of empty file Map Reduce Store Impl impl incr Record Counter counter Name catch Exception ioe int err Code String msg Unable to setup the store function throw new Exec Exception msg err Code Pig Exception ioe Called at the end of processing for clean up throws Exception public void tear Down throws Exception if impl null impl tear Down To perform cleanup when there is an error throws Exception public void clean Up throws Exception if impl null impl clean Up Override public Result get Next Tuple throws Exec Exception Result res process Input try switch res return Status case Status if illustrator null s Decorator put Next Tuple res result else illustrator Markup res result res result res if counter Name null Map Reduce Store Impl impl incr Record Counter counter Name break case Status break case Status case Status default break catch Exception ioe int err Code String msg Received error from store function ioe get Message throw new Exec Exception msg err Code ioe return res Override public String name return s File null get Alias String Store s File to String m Key to String get Alias String Store Dummy Fil Dummy Ldr m Key to String Override public boolean supports Multiple Inputs return false Override public boolean supports Multiple Outputs return true Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Store this public File Spec get File return s File public void set File File Spec s File this s File s File storer null public void set Input Spec File Spec l File this l File l File public File Spec get Input Spec return l File public void set Is Tmp Store boolean tmp is Tmp Store tmp public boolean is Tmp Store return is Tmp Store public void set Store Impl Store Impl impl this impl impl public void set Schema Schema schema this schema schema public Schema get Schema return schema public Store Func Interface get Store Func if storer null storer Store Func Interface Pig Context instantiate Func From Spec s File get Func Spec storer set Store Func Context Signature signature if s Decorator null Init the Decorator we use for writing Tuples set Store Func Decorator new Store Func Decorator storer signature return storer void set Store Func Decorator Store Func Decorator s Decorator this s Decorator s Decorator return The link Store Func Decorator used to write Tuples public Store Func Decorator get Store Func Decorator return s Decorator param sort Info the sort Info to set public void set Sort Info Sort Info sort Info this sort Info sort Info return the sort Info public Sort Info get Sort Info return sort Info public String get Signature return signature public void set Signature String signature this signature signature public void set Multi Store boolean is Multi Store this is Multi Store is Multi Store public boolean is Multi Store return is Multi Store Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null Example Tuple t In Example Tuple in Lineage Tracer lineage illustrator get Lineage lineage insert t In if is Tmp Store illustrator get Equivalence Classes get eq Class Index add t In illustrator add Data Tuple out return Tuple out public void set Index int index this index index public int get Index return index public void set Disable Counter boolean disable Counter this disable Counter disable Counter public boolean disable Counter return disable Counter public void set Store Func Store Func Interface store Func this storer store Func public List String get Cache Files return cache Files public void set Cache Files List String cf cache Files cf public List String get Ship Files return ship Files public void set Ship Files List String sf ship Files sf 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java io Exception import java io Object Input Stream import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Comparison Func import org apache pig Func Spec import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl Pig Context import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception We intentionally skip type checking in backend for performance reasons Suppress Warnings unchecked public class User Comparison Func extends Expression Operator private final static Log log Log Factory get Log User Comparison Func class private static final long serial Version Func Spec func Spec Tuple t t transient Comparison Func func public User Comparison Func Operator Key k int rp List inp Func Spec func Spec Comparison Func func super k rp super set Inputs inp this func Spec func Spec this func func if func null instantiate Func public User Comparison Func Operator Key k int rp List inp Func Spec func Spec this k rp inp func Spec null private void instantiate Func this func Comparison Func Pig Context instantiate Func From Spec this func Spec this func set Reporter get Reporter public Comparison Func get Comparator return func Override public Result get Next Integer throws Exec Exception Result result new Result result result func compare t t result return Status t null t null Status Status the two attached tuples are used up now So we set the input Attached flag to false input Attached false if result return Status Status illustrator Markup null result result Integer result result Integer result result return result private Result get Next Result res null log error get Next being called with non integer return res Override public Result get Next Boolean throws Exec Exception return get Next Override public Result get Next Data Bag throws Exec Exception return get Next Override public Result get Next Data Byte Array throws Exec Exception return get Next Override public Result get Next Double throws Exec Exception return get Next Override public Result get Next Float throws Exec Exception return get Next Override public Result get Next Long throws Exec Exception return get Next Override public Result get Next Date Time throws Exec Exception return get Next Override public Result get Next Map throws Exec Exception return get Next Override public Result get Next String throws Exec Exception return get Next Override public Result get Next Tuple throws Exec Exception return get Next Override public Result get Next Big Integer throws Exec Exception return get Next Override public Result get Next Big Decimal throws Exec Exception return get Next public void attach Input Tuple t Tuple t this t t this t t input Attached true private void read Object Object Input Stream is throws Exception Class Not Found Exception is default Read Object instantiate Func Override public void visit Phy Plan Visitor v throws Visitor Exception v visit Comparison Func this Override public String name return User Comparison Func func get Class get Name Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return false public Func Spec get Func Spec return func Spec Override public User Comparison Func clone throws Clone Not Supported Exception Func Spec clone Fs null if func Spec null clone Fs func Spec clone User Comparison Func clone new User Comparison Func new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope requested Parallelism null clone Fs clone clone Helper this return clone Get child expressions of this expression Override public List Expression Operator get Child Expressions return null Override public Tuple illustrator Markup Object in Object out int eq Class Index if illustrator null illustrator get Inputs add t illustrator get Equivalence Classes get eq Class Index add t illustrator get Inputs add t illustrator get Equivalence Classes get eq Class Index add t return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import static org apache pig Pig Configuration import static org apache pig Pig Configuration import static org apache pig Pig Constants import static org apache pig Pig Constants import java io Exception import java io Object Input Stream import java lang reflect Type import java util List import java util Properties import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache pig Accumulator import org apache pig Algebraic import org apache pig Eval Func import org apache pig Func Spec import org apache pig Pig Exception import org apache pig Terminating Accumulator import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer Status import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Result import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer util Monitored Executor import org apache pig builtin Monitored import org apache pig data Data Type import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data Schema Tuple Factory import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig data Tuple Maker import org apache pig impl Pig Context import org apache pig impl logical Layer schema Schema import org apache pig impl plan Node Id Generator import org apache pig impl plan Operator Key import org apache pig impl plan Visitor Exception import org apache pig impl util Context import org apache pig tools pigstats Pig Status Reporter public class User Func extends Expression Operator private static final long serial Version private static final Log Log Factory get Log User Func class private static final Tuple Factory tf Tuple Factory get Instance private transient String counter Group private transient Eval Func func private transient List String cache Files null private transient List String ship Files null Func Spec func Spec Func Spec orig Spec public static final byte public static final byte public static final byte private boolean initialized false private Monitored Executor executor null private Physical Operator referenced Operator null private boolean is Accumulation Done private String signature private boolean have Checked If Terminating Accumulator private long num Invocations private long timing Frequency private boolean do Timing false public Physical Operator get Referenced Operator return referenced Operator public void set Referenced Operator Physical Operator referenced Operator this referenced Operator referenced Operator public User Func Operator Key k int rp List Physical Operator inp this k rp inp null public User Func Operator Key k int rp List Physical Operator inp Func Spec func Spec this k rp inp func Spec null public User Func Operator Key k int rp List Physical Operator inp Func Spec func Spec Eval Func func super k rp super set Inputs inp this func Spec func Spec this orig Spec func Spec this func func instantiate Func func Spec public void set Func Input Schema set Func Input Schema signature private void instantiate Func Func Spec f Spec this func Eval Func Pig Context instantiate Func From Spec f Spec this set Signature signature this set Func Input Schema signature if func get Class is Annotation Present Monitored class executor new Monitored Executor func the next couple of initializations do not work as intended for the following reasons the reporter and pig Logger are member variables of Physical Operator when instanitate Func is invoked at deserialization time both reporter and pig Logger are null They are set during map and reduce calls making the initializations here basically useless Look at the process Input method where these variables are re initialized At that point the Physical Operator is set up correctly with the reporter and pig Logger references this func set Reporter get Reporter this func set Pig Logger pig Logger private transient Tuple Maker input Tuple Maker private boolean using Schema Tuple Factory Override public Result process Input throws Exec Exception Make sure the reporter is set because it is n t getting carried across in the serialization do n t know why suspect it s as cheap to call the set Reporter call everytime as to check whether have hopefully java will inline it if initialized func set Reporter get Reporter func set Pig Logger pig Logger Configuration job Conf Context get Context get Job Conf if job Conf null do Timing job Conf get Boolean false if do Timing counter Group func Spec to String timing Frequency job Conf get Long We initialize here instead of instantiate Func because this is called when actual processing has begun whereas a function can be instantiated on the frontend potentially mainly for optimization Schema tmp func get Input Schema if tmp null Currently get Instance For Schema returns null if no class was found This works fine if it is null the default will be used We pass the context because if it happens that the same Schema was generated elsewhere we do not want to override user expectations input Tuple Maker Schema Tuple Factory get Instance tmp false Gen Context if input Tuple Maker null debug No Schema Tuple Factory found for Schema tmp using default Tuple Factory using Schema Tuple Factory false else debug Using Schema Tuple Factory for Schema tmp using Schema Tuple Factory true In the future we could optionally use Schema Tuples for output as well if input Tuple Maker null input Tuple Maker Tuple Factory get Instance initialized true Result res new Result if input null inputs null inputs size res return Status Status return res Should be removed once the model is clear if get Reporter null get Reporter progress if is Input Attached res result input res return Status Status detach Input return res else we decouple this because there may be cases where the size is known and it is n t a schema tuple factory boolean known Size using Schema Tuple Factory int known Index res result input Tuple Maker new Tuple Result temp null for Physical Operator op inputs temp op get Next op get Result Type if temp return Status Status return temp if op instanceof Project op get Result Type Data Type Project proj Op Project op if proj Op is Project To End Tuple trslt Tuple temp result Tuple rslt Tuple res result for int i i trslt size i if known Size rslt set known Index trslt get i else rslt append trslt get i continue if known Size Tuple res result set known Index temp result else Tuple res result append temp result res return Status temp return Status return res private boolean is Early Terminating false private void set Is Early Terminating is Early Terminating true private boolean is Early Terminating return is Early Terminating private boolean is Terminated false private boolean has Been Terminated return is Terminated private void early Terminate is Terminated true private Result get Next throws Exec Exception Result result process Input long start Nanos boolean time This do Timing num Invocations timing Frequency if time This start Nanos System nano Time Pig Status Reporter get Instance incr Counter counter Group timing Frequency try if result return Status Status if is Accumulative if is Accum Started if have Checked If Terminating Accumulator have Checked If Terminating Accumulator true if func instanceof Terminating Accumulator set Is Early Terminating if has Been Terminated is Early Terminating Terminating Accumulator func is Finished early Terminate if has Been Terminated result return Status Status result result null is Accumulation Done false else Accumulator func accumulate Tuple result result result return Status Status result result null is Accumulation Done false else if is Accumulation Done Relation To Expr Project does not return so that udf gets called both when is Accum Started is first true and then set to false even when the input relation is empty so the has to be sent from User Func after the results have been sent result result null result return Status Status else result result Accumulator func get Value result return Status Status Accumulator func cleanup is Accumulation Done true else if parent Plan null parent Plan end Of All Input need End Of All Input Processing func set End Of All Input true if executor null result result executor monitor Exec Tuple result result else result result func exec Tuple result result if time This Pig Status Reporter get Instance incr Counter counter Group Math round System nano Time start Nanos timing Frequency return result catch Exec Exception ee throw ee catch Exception ioe int err Code String msg Caught error from func Spec get Class Name String footer ioe get Message if ioe instanceof Pig Exception int udf Error Code Pig Exception ioe get Error Code if udf Error Code err Code udf Error Code msg Pig Exception ioe get Message else msg Pig Exception ioe get Message else msg footer throw new Exec Exception msg err Code Pig Exception ioe catch Index Out Of Bounds Exception ie int err Code String msg Caught error from func Spec get Class Name Out of bounds access ie get Message throw new Exec Exception msg err Code Pig Exception ie Override public Result get Next Tuple throws Exec Exception return get Next Override public Result get Next Data Bag throws Exec Exception return get Next Override public Result get Next Integer throws Exec Exception return get Next Override public Result get Next Boolean throws Exec Exception return get Next Override public Result get Next Data Byte Array throws Exec Exception return get Next Override public Result get Next Double throws Exec Exception return get Next Override public Result get Next Big Integer throws Exec Exception return get Next Override public Result get Next Big Decimal throws Exec Exception return get Next Override public Result get Next Float throws Exec Exception return get Next Override public Result get Next Long throws Exec Exception return get Next Override public Result get Next Date Time throws Exec Exception return get Next Override public Result get Next Map throws Exec Exception return get Next Override public Result get Next String throws Exec Exception return get Next public void set Algebraic Function byte Function throws Exec Exception This will only be used by the optimizer for putting correct functions in the mapper combiner and reduce This helps in maintaining the physical plan as is without the optimiser having to replace any operators You would n t be able to make two calls to this function on the same algebraic Eval Func as func is being changed switch Function case func Spec new Func Spec get Initial break case func Spec new Func Spec get Intermed break case func Spec new Func Spec get Final break func Spec set Ctor Args orig Spec get Ctor Args instantiate Func func Spec set Result Type Data Type find Type Eval Func func get Return Type public String get Initial throws Exec Exception instantiate Func orig Spec if func instanceof Algebraic return Algebraic func get Initial else int err Code String msg Attempt to run a non algebraic function as an algebraic function throw new Exec Exception msg err Code Pig Exception public String get Intermed throws Exec Exception instantiate Func orig Spec if func instanceof Algebraic return Algebraic func get Intermed else int err Code String msg Attempt to run a non algebraic function as an algebraic function throw new Exec Exception msg err Code Pig Exception public String get Final throws Exec Exception instantiate Func orig Spec if func instanceof Algebraic return Algebraic func get Final else int err Code String msg Attempt to run a non algebraic function as an algebraic function throw new Exec Exception msg err Code Pig Exception public Type get Original Return Type throws Exec Exception instantiate Func orig Spec return func get Return Type public Type get Return Type return func get Return Type public void finish func finish if executor null executor terminate public Schema output Schema Schema input return func output Schema input public Boolean is Asynchronous return func is Asynchronous Override public String name return User Func func get Class get Name Data Type find Type Name result Type m Key to String Override public boolean supports Multiple Inputs return true Override public boolean supports Multiple Outputs return false Override public void visit Phy Plan Visitor v throws Visitor Exception v visit User Func this public Func Spec get Func Spec return func Spec public void set Func Spec Func Spec func Spec this func Spec func Spec instantiate Func func Spec public List String get Cache Files return cache Files public void set Cache Files List String cf cache Files cf public List String get Ship Files return ship Files public void set Ship Files List String sf ship Files sf public boolean combinable return func instanceof Algebraic Override public User Func clone throws Clone Not Supported Exception Inputs will be patched up later by Physical Plan clone User Func clone new User Func new Operator Key m Key scope Node Id Generator get Generator get Next Node Id m Key scope requested Parallelism null func Spec clone clone set Result Type result Type clone signature signature clone cache Files cache Files clone ship Files ship Files return clone private void read Object Object Input Stream is throws Exception Class Not Found Exception is default Read Object instantiate Func func Spec Get child expression of this expression Override public List Expression Operator get Child Expressions return null Suppress Warnings unchecked Override public void set Accum Start if is Accumulative is Accum Started super set Accum Start Accumulator func cleanup Override public void set Result Type byte result Type this result Type result Type Override public Tuple illustrator Markup Object in Object out int eq Class Index return Tuple out public Eval Func get Func return func public String get Signature return signature public void set Signature String signature this signature signature if this func null this func set Context Signature signature Sets Eval Func s inputschema based on the signature param signature public void set Func Input Schema String signature Properties props Context get Context get Properties func get Class Schema tmp Schema props get pig evalfunc inputschema signature if tmp null this func set Input Schema tmp public boolean need End Of All Input Processing return get Func need End Of All Input Processing 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import org apache pig data Tuple import java io Exception import java util List Base class for simple Pig Fs that are functions of primitive types to Handles marshalling objects basic error checking etc Extend this class and implement the pre exec input pre method when writting a that operates on only the first input of expected type from the Tuple public abstract class Primitive Eval Func extends Typed Output Eval Func protected Class in Type Class null public Class get Input Type Class return in Type Class Suppress Warnings unchecked public Primitive Eval Func List type Args get Type Arguments Primitive Eval Func class get Class in Type Class Class type Args get out Type Class Class type Args get Suppress Warnings unchecked public Primitive Eval Func Class in Type Class Class out Type Class this in Type Class in Type Class this out Type Class out Type Class Override Suppress Warnings unchecked public exec Tuple tuple throws Exception verify Udf Input get Counter Group tuple input tuple get if input null Default behavior of null input should be null output return null return exec input public abstract exec input throws Exception 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import java util List import org apache pig Pig Exception import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Plan Validation Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Source Location Projection of columns in an expression public class Project Expression extends Column Expression private int input Which input of the relational operator this project is projecting from Count is zero based So if this project is in a filter the input number will always be since filter has only one input If it is in a join cross cogroup or union it could be greater than private int col The column in the input which the project references Count is zero based private String alias The alias of the projected field In Foreach inner plan a projection can be made on a relational operator which may get reused However the expression needs to be sticky to the operator on which the expression is projected private Operator projected Operator private Logical Relational Operator attached Relational Op fields for range projection private boolean is Range Project false start and end columns in range end Col value of represents everything upto end private int start Col private int end Col private String start Alias private String end Alias Adds projection to the plan param plan Logical Expression Plan this projection will be a part of param input Num Input number this project references param col Num Column number this project references param attached Relational Op public Project Expression Operator Plan plan int input Num int col Num Logical Relational Operator attached Relational Op super Project plan this input input Num this col col Num plan add this this attached Relational Op attached Relational Op Adds projection to the plan param plan param input Num param alias param attached Relational Op throws Frontend Exception public Project Expression Operator Plan plan int input Num String alias Operator projected Op Logical Relational Operator attached Relational Op super Project plan this input input Num this alias alias this projected Operator projected Op plan add this this attached Relational Op attached Relational Op Constructor for range projection Adds projection to the plan The start and end alias column number should be set separately param plan param input Num param attached Relational Op public Project Expression Operator Plan plan int input Num Logical Relational Operator attached Relational Op super Project plan input input Num is Range Project true plan add this this attached Relational Op attached Relational Op like a copy constructor but with a plan argument param proj Expr param plan public Project Expression Project Expression proj Expr Operator Plan plan super Project plan this input proj Expr input this col proj Expr col this alias proj Expr alias this projected Operator proj Expr projected Operator this attached Relational Op proj Expr attached Relational Op this is Range Project proj Expr is Range Project this start Col proj Expr start Col this end Col proj Expr end Col this start Alias proj Expr start Alias this end Alias proj Expr end Alias plan add this If there is an alias finds the column number from it throws Frontend Exception if there is no such alias public void set Column Number From Alias throws Frontend Exception if is Range Project if start Alias null start Col find Col Num start Alias start Alias null if end Alias null end Col find Col Num end Alias end Alias null if start Col String msg Invalid start column position in range projection start Col throw new Plan Validation Exception this msg Pig Exception if end Col start Col end Col String msg start column appears after end column in range projection Start column position start Col End column position end Col throw new Plan Validation Exception this msg Pig Exception else set Col Num find Col Num alias private int find Col Num String alias throws Frontend Exception Logical Plan lp Logical Plan attached Relational Op get Plan List Operator inputs lp get Predecessors attached Relational Op Logical Relational Operator input Logical Relational Operator inputs get get Input Num Logical Schema input Schema input get Schema if alias null int col Num input Schema null input Schema get Field Position alias if col Num String msg Invalid field projection Projected field alias does not exist if input Schema null msg in schema input Schema to String false msg throw new Plan Validation Exception this msg return col Num else int col get Col Num if input Schema null col input Schema size throw new Plan Validation Exception this Out of bound access Trying to access non existent column col Schema input Schema to String false has input Schema size column s return col link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Input number this project references This is the input number for the relational operator that contains this expression The count is zero based return input number public int get Input Num return input public void set Input Num int input Num input input Num Column number this project references The column number is the column in the relational operator that contains this expression The count is zero based return column number public int get Col Num if is Range Project throw new Assertion Error get Col Num should not be called on range project return col public String get Col Alias return alias public Operator get Projected Operator return this projected Operator Set the column number for this project This should only be called by Projection Patcher Stupid Java needs friends param col Num new column number for projection public void set Col Num int col Num col col Num alias null Once the column number is set alias is no longer needed public boolean is Project Star return col public boolean is Range Project return is Range Project public boolean is Range Or Star Project return is Project Star is Range Project Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema Logical Relational Operator referent find Referent Logical Schema schema null if referent get Schema null schema referent get Schema deep Copy if attached Relational Op instanceof Generate plan get Successors this null if find Referent instanceof Inner Load Inner Load find Referent source Is Bag String alias find Referent get Alias Pair List Inner Load Boolean inner Loads Pair For Each find Reacheable Inner Load From Boundary Project this List Inner Load inner Loads inner Loads Pair first boolean need New Uid inner Loads Pair second pull tuple information from innerload if inner Loads get get Projection get Field Schema schema null inner Loads get get Projection get Field Schema type Data Type Logical Field Schema original Tuple Field Schema inner Loads get get Projection get Field Schema schema get Field Logical Field Schema new Tuple Field Schema new Logical Field Schema original Tuple Field Schema alias schema Data Type if need New Uid new Tuple Field Schema uid Logical Expression get Next Uid else new Tuple Field Schema uid original Tuple Field Schema uid Logical Schema new Tuple Schema new Logical Schema new Tuple Schema add Field new Tuple Field Schema field Schema new Logical Schema Logical Field Schema alias new Tuple Schema Data Type else field Schema new Logical Schema Logical Field Schema alias schema Data Type if need New Uid field Schema uid Logical Expression get Next Uid else field Schema uid inner Loads get get Projection get Field Schema uid else Inner Load and source is not bag if schema null if we get here it is range or star Project otherwise inner Load will convert schema to non null if is Range Project end Col Logical Schema inner Schema new Logical Schema for int i start Col i end Col i schema is null so null alias inner Schema add Field new Logical Field Schema null null Data Type field Schema new Logical Schema Logical Field Schema null inner Schema Data Type else field Schema null else field Schema schema get Field if field Schema null uid Only Field Schema field Schema merge Uid uid Only Field Schema else if schema null if is Range Or Star Project if is Range Project end Col Logical Schema inner Schema new Logical Schema for int i start Col i end Col i schema is null so null alias inner Schema add Field new Logical Field Schema null null Data Type field Schema new Logical Schema Logical Field Schema null inner Schema Data Type else field Schema null else field Schema new Logical Schema Logical Field Schema null null Data Type if field Schema null uid Only Field Schema field Schema merge Uid uid Only Field Schema else int index if is Range Or Star Project uid Only Field Schema null long uid uid Only Field Schema uid for int i i schema size i Logical Field Schema fs schema get Field i if fs uid uid index i if index if alias null index schema get Field Position alias if index index col if is Range Or Star Project if schema null schema size index field Schema schema get Field index else field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema clone Uid else Logical Schema new Tuple Schema null if is Project Star new Tuple Schema schema deep Copy else project range new Tuple Schema new Logical Schema for int i start Col i end Col i new Tuple Schema add Field schema get Field i deep Copy field Schema new Logical Schema Logical Field Schema null new Tuple Schema Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Find the Logical Relational Operator that this projection refers to return this projection refers to throws Frontend Exception public Logical Relational Operator find Referent throws Frontend Exception List Operator preds preds attached Relational Op get Plan get Predecessors attached Relational Op if preds null input preds size throw new Frontend Exception Projection with nothing to reference Logical Relational Operator pred Logical Relational Operator preds get input if pred null throw new Frontend Exception Can not find reference for this return pred Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Project Expression Project Expression po Project Expression other if po input input po col col return false Operator my Succ get Plan get Successors this null get Plan get Successors this get null Operator their Succ other get Plan get Successors other null other get Plan get Successors other get null if my Succ null their Succ null return my Succ is Equal their Succ if my Succ null their Succ null return true return false else return false public String to String String Builder msg new String Builder if field Schema null field Schema alias null msg append field Schema alias msg append Name name Type if field Schema null msg append Data Type find Type Name field Schema type else msg append null msg append Uid if field Schema null msg append field Schema uid else msg append null msg append Input input Column if alias null msg append alias else if is Project Star msg append else if is Range Project msg append append start Col append append end Col append else msg append col msg append return msg to String public Logical Relational Operator get Attached Relational Op return attached Relational Op public void set Attached Relational Op Logical Relational Operator attached Relational Op this attached Relational Op attached Relational Op Override public byte get Type throws Frontend Exception for boundary project if if get Field Schema null if attached Relational Op instanceof Generate find Referent instanceof Inner Load if Inner Load find Referent get Projection is Range Or Star Project return Data Type return Data Type return super get Type return the start Col public int get Start Col return start Col param start Col the start Col to set public void set Start Col int start Col this start Col start Col return the end Col public int get End Col return end Col param end Col the end Col to set public void set End Col int end Col this end Col end Col param start Alias throws Frontend Exception public void set Start Alias String start Alias throws Frontend Exception this start Alias start Alias param end Alias throws Frontend Exception public void set End Alias String end Alias throws Frontend Exception this end Alias end Alias Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Project Expression copy new Project Expression lg Exp Plan this get Input Num this get Col Num this get Attached Relational Op copy set Location new Source Location location copy alias alias copy projected Operator this projected Operator copy is Range Project this is Range Project copy start Col this start Col copy end Col this end Col copy start Alias this start Alias copy end Alias this end Alias return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical visitor import java util Array List import java util Hash Map import java util List import java util Map import java util Map Entry import org apache pig Pig Exception import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Multi Map import org apache pig impl util Pair import org apache pig newplan Dependency Order Walker import org apache pig newplan Depth First Walker import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cube import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import com google common primitives Booleans visitor to walk operators that contain a nested plan and translate project operators to a list of projection operators i e project project project project n project n If input schema is null project is not expanded It also expands project range eg It wo n t expand project range to end eg if the input schema is null public class Project Star Expander extends Logical Relational Nodes Visitor public Project Star Expander Operator Plan plan throws Frontend Exception super plan new Dependency Order Walker plan Override public void visit Sort sort throws Frontend Exception List Logical Expression Plan exp Plans sort get Sort Col Plans List Boolean asc Order sort get Ascending Cols new expressionplans and sort order list after star expansion List Logical Expression Plan new Exp Plans new Array List Logical Expression Plan List Boolean new Asc Order new Array List Boolean if exp Plans size asc Order size throw new Assertion Error Size of exp Plans and ascorder should be same for int i i exp Plans size i expand the plan Logical Expression Plan ith Exp Plan exp Plans get i List Logical Expression Plan expanded Plans expand Plan ith Exp Plan new Exp Plans add All expanded Plans add corresponding is Asc flags Boolean is Asc asc Order get i for int j j expanded Plans size j new Asc Order add is Asc check if there is a project star to end followed by another sort plan in the expanded plans can happen if there is no input schema for int i i new Exp Plans size i Project Expression proj get Project Star new Exp Plans get i if proj null proj is Range Project proj get End Col i new Exp Plans size because of order by sampler logic limitation this is not supported right now String msg Project range to end eg x is supported in order by only as last sort column throw new Frontend Exception msg Pig Exception sort set Sort Col Plans new Exp Plans sort set Ascending Cols new Asc Order Override public void visit Rank rank throws Frontend Exception List Logical Expression Plan exp Plans rank get Rank Col Plans List Boolean asc Order rank get Ascending Col List Logical Expression Plan new Exp Plans new Array List Logical Expression Plan List Boolean new Asc Order new Array List Boolean if exp Plans size asc Order size throw new Assertion Error Size of exp Plans and ascorder should be same for int i i exp Plans size i expand the plan Logical Expression Plan ith Exp Plan exp Plans get i List Logical Expression Plan expanded Plans expand Plan ith Exp Plan new Exp Plans add All expanded Plans add corresponding is Asc flags Boolean is Asc asc Order get i for int j j expanded Plans size j new Asc Order add is Asc check if there is a project star to end followed by another sort plan in the expanded plans can happen if there is no input schema for int i i new Exp Plans size i Project Expression proj get Project Star new Exp Plans get i if proj null proj is Range Project proj get End Col i new Exp Plans size String msg Project range to end eg x is supported in rank by only as last rank column throw new Frontend Exception msg Pig Exception rank set Rank Col Plan new Exp Plans rank set Ascending Col new Asc Order Expand plan into multiple plans if the plan contains a project star if there is no project star the returned list contains the plan argument param plan return throws Frontend Exception private List Logical Expression Plan expand Plan Logical Expression Plan plan int input Num throws Frontend Exception List Logical Expression Plan expanded Plans Project Expression proj Star get Project Star plan if proj Star null expand the plan into multiple plans return expand Plan plan proj Star input Num else no project star to expand expanded Plans new Array List Logical Expression Plan expanded Plans add plan return expanded Plans Override public void visit Cogroup cg throws Frontend Exception Multi Map Integer Logical Expression Plan inp Expr Plans cg get Expression Plans modify the plans if they have project star expand Plans inp Expr Plans do some validations List Operator inputs cg get Inputs Logical Plan cg get Plan check if after translation none of group by plans in a cogroup have a project if they still do it s because the input for the project did not have a schema in this case we should error out since we could have different number types of cogroup keys if inputs size only for cogroups for int i i inputs size i for Logical Expression Plan lp inp Expr Plans get i if get Project Star lp null String msg Cogroup Group by or x range of columns to the end is only allowed if the input has a schema throw new Visitor Exception cg msg Pig Exception check if after translation all group by plans have same arity int arity inp Expr Plans get size for int i i inputs size i if arity inp Expr Plans get i size String msg The arity of cogroup group by columns do not match throw new Visitor Exception cg msg Pig Exception Override public void visit Cube cu throws Frontend Exception Multi Map Integer Logical Expression Plan inp Expr Plans cu get Expression Plans modify the plans if they have project star expand Plans inp Expr Plans Override public void visit Join join throws Frontend Exception expand Plans join get Expression Plans Override public void visit For Each foreach throws Frontend Exception in case of Foreach expand when inner plan has a single project star and its input Inner Load also is a project star then Reset the input number in project expressions Logical Plan inner Plan foreach get Inner Plan visit the inner plan first Plan Walker new Walker current Walker spawn Child Walker inner Plan push Walker new Walker current Walker walk this pop Walker get the Generate List Operator fe Outputs inner Plan get Sinks Generate gen null for Operator op fe Outputs if op instanceof Generate if gen null String msg Expected single Generate output in innerplan of foreach throw new Visitor Exception foreach msg Pig Exception gen Generate op work on the generate plan flatten and user schema List Logical Expression Plan exp Plans gen get Output Plans List Logical Expression Plan new Exp Plans new Array List Logical Expression Plan List Operator lo Gen Preds inner Plan get Predecessors gen if lo Gen Preds null there are no Inner Loads must be working on just constants no project star expansion to be done return List Logical Schema user Schema gen get User Defined Schema List Logical Schema new User Schema null if user Schema null new User Schema new Array List Logical Schema boolean flattens gen get Flatten Flags List Boolean new Flattens new Array List Boolean flattens length get mapping of Generate predecessor current position to object Map Integer Logical Relational Operator old Pos Rel new Hash Map Integer Logical Relational Operator for int i i lo Gen Preds size i old Pos Rel put i Logical Relational Operator lo Gen Preds get i get schema of predecessor project star expansion needs a schema Logical Relational Operator pred Logical Relational Operator foreach get Plan get Predecessors foreach get Logical Schema inp Sch pred get Schema store mapping between the projection in inner plans of of Generate to the input relation object Map Project Expression Logical Relational Operator proj Inp Rel new Hash Map Project Expression Logical Relational Operator for int i i exp Plans size i Logical Expression Plan exp Plan exp Plans get i Project Expression proj Star get Project Lonely Star exp Plan old Pos Rel boolean found Expandable Project false if proj Star null there is a project star to be expanded Logical Schema user Star Sch null if user Schema null user Schema get i null user Star Sch user Schema get i the range values are set in the project in Inner Load Project Expression lo Inner Proj Inner Load old Pos Rel get proj Star get Input Num get Projection int first Proj Col int last Proj Col if lo Inner Proj is Range Project lo Inner Proj set Column Number From Alias first Proj Col lo Inner Proj get Start Col last Proj Col lo Inner Proj get End Col boolean is Project To End lo Inner Proj is Project Star lo Inner Proj is Range Project last Proj Col ca n t expand if there is no input schema and this is as project star or project range to end if inp Sch null is Project To End found Expandable Project true if is Project To End last Proj Col inp Sch size replacing the existing project star with new ones exp Plan remove proj Star remove the Inner Load with star Inner Load old Inner Load Inner Load old Pos Rel get proj Star get Input Num inner Plan disconnect old Inner Load gen inner Plan remove old Inner Load generate new exp plan inner load for each field in schema for int j first Proj Col j last Proj Col j add new Inner Load Inner Load new In Load new Inner Load inner Plan foreach j inner Plan add new In Load inner Plan connect new In Load gen new expression plan and proj Logical Expression Plan new Exp Plan new Logical Expression Plan new Exp Plans add new Exp Plan Project Expression new Proj new Project Expression new Exp Plan gen proj Inp Rel put new Proj new In Load new Flattens add flattens i if new User Schema null index into user specified schema int sch Idx j first Proj Col if user Star Sch null user Star Sch get Fields size sch Idx user Star Sch get Field sch Idx null if the project star field has user specified schema use the j th field for this column Logical Schema sch new Logical Schema sch add Field new Logical Field Schema user Star Sch get Field sch Idx new User Schema add sch else new User Schema add null if found Expandable Project no project star that could be expanded get all projects in here Find Projects find Projs new Find Projects exp Plan find Projs visit List Project Expression projs find Projs get Projs create a mapping of project expression to their inputs for Project Expression proj projs proj Inp Rel put proj old Pos Rel get proj get Input Num new Exp Plans add exp Plan new Flattens add flattens i if new User Schema null new User Schema add user Schema get i get mapping of Lo Generate input relation to current position Map Logical Relational Operator Integer rel pos new Hash Map Logical Relational Operator Integer List Operator new Gen Preds inner Plan get Predecessors gen int num New Gen Preds if new Gen Preds null num New Gen Preds new Gen Preds size for int i i num New Gen Preds i rel pos put Logical Relational Operator new Gen Preds get i i correct the input num for projects for Entry Project Expression Logical Relational Operator proj And Inp proj Inp Rel entry Set Project Expression proj proj And Inp get Key Logical Relational Operator rel proj And Inp get Value proj set Input Num rel pos get rel set the new lists gen set Output Plans new Exp Plans gen set Flatten Flags Booleans to Array new Flattens gen set User Defined Schema new User Schema gen reset Schema foreach reset Schema static class Find Projects extends Logical Expression Visitor private List Project Expression projs new Array List Project Expression protected Find Projects Logical Expression Plan plan throws Frontend Exception super plan new Depth First Walker plan Override public void visit Project Expression proj projs add proj public List Project Expression get Projs return projs Find project star in foreach statement The Inner Load corresponding to the project star also needs to have a project star param exp Plan expression plan param old Pos Rel inner relational plan of foreach return Project Expression that satisfies the conditions throws Frontend Exception private Project Expression get Project Lonely Star Logical Expression Plan exp Plan Map Integer Logical Relational Operator old Pos Rel throws Frontend Exception the expression plan should have just a single project if exp Plan size exp Plan size return null Operator output Op exp Plan get Operators next if output Op instanceof Project Expression Project Expression proj Project Expression output Op check if Project Expression is project Star if proj is Project Star now check if its input is a Inner Load and it is project Star or range project Logical Relational Operator input Rel old Pos Rel get proj get Input Num if input Rel instanceof Inner Load return null Project Expression inner Proj Inner Load input Rel get Projection if inner Proj is Range Or Star Project return proj return null private void expand Plans Multi Map Integer Logical Expression Plan inp Expr Plans throws Frontend Exception for each input relation expand any logical plan that has project star for int i i inp Expr Plans size i List Logical Expression Plan plans inp Expr Plans get i List Logical Expression Plan new Plans new Array List Logical Expression Plan for Logical Expression Plan plan plans new Plans add All expand Plan plan i inp Expr Plans remove Key i inp Expr Plans put i new Plans expand this plan containing project star to multiple plans each projecting a single column param exp Plan param proj return throws Frontend Exception private List Logical Expression Plan expand Plan Logical Expression Plan exp Plan Project Expression proj int input Num throws Frontend Exception Pair Integer Integer start And End Projs Project Star Expander Util get Project Start End Cols exp Plan proj List Logical Expression Plan new Plans new Array List Logical Expression Plan if start And End Projs null ca n t expand this project new Plans add exp Plan return new Plans expand from first Proj Col to last Proj Col int first Proj Col start And End Projs first int last Proj Col start And End Projs second Logical Relational Operator rel Op proj get Attached Relational Op for int i first Proj Col i last Proj Col i new Plans add create Exp Plan With Proj rel Op input Num i return new Plans Create new logical plan with a project that is attached to Logical Relation attach Rel and projects i th column from input param attach Rel param input Num param col Num return private Logical Expression Plan create Exp Plan With Proj Logical Relational Operator attach Rel int input Num int col Num Logical Expression Plan new Exp Plan new Logical Expression Plan Project Expression new Proj new Project Expression new Exp Plan input Num col Num attach Rel new Exp Plan add new Proj return new Exp Plan if Logical Expression Plan argument has a project star output then return it otherwise return null param exp Plan return throws Frontend Exception private Project Expression get Project Star Logical Expression Plan exp Plan throws Frontend Exception List Operator outputs exp Plan get Sources Project Expression proj Star null for Operator output Op outputs if output Op instanceof Project Expression Project Expression proj Project Expression output Op if proj is Range Or Star Project if outputs size String msg More than one operator in an expression plan containing project star project range throw new Visitor Exception proj msg Pig Exception proj Star proj return proj Star 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java util Array List import java util Hash Map import java util Hash Set import java util Iterator import java util List import java util Map import java util Set import java util Map Entry import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Operator Sub Plan import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical expression User Func Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Native import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan optimizer Rule import org apache pig newplan optimizer Transformer public class Push Up Filter extends Rule public Push Up Filter String n super n false Override public Transformer get New Transformer return new Push Up Filter Transformer public class Push Up Filter Transformer extends Transformer private Operator Sub Plan sub Plan Override public boolean check Operator Plan matched throws Frontend Exception check if it is inner join Operator current matched get Sources get Operator pred find Non Filter Predecessor current if pred null return false sort and union are always okay if pred instanceof Sort pred instanceof Union return true if the predecessor is one of Load Store Stream Limit Native if predecessor is For Each it is optimized by rule Filter Above Foreach return false if pred instanceof Load pred instanceof Store pred instanceof Stream pred instanceof Filter pred instanceof Split pred instanceof Split Output pred instanceof Limit pred instanceof Native pred instanceof For Each return false Filter filter Filter current List Operator preds current Plan get Predecessors pred Logical Expression Plan filter Plan filter get Filter Plan if Optimizer Utils plan Has Non Deterministic Udf filter Plan return false if there is no nondeterministic udf filter can be pushed above Distinct if pred instanceof Distinct return true collect all uids used in the filter plan Set Long uids collect Uid From Exp Plan filter Plan if pred instanceof Cogroup Cogroup cogrp Cogroup pred if preds size if has All Logical Relational Operator preds get uids Order by is ok if all Ds can be found from previous operator return true else if cogrp get Expression Plans get size contain filter Plan Optimization is possible if there is only a single key For regular cogroup we can not use Ds to determine if filter can be pushed up But if there is no it s okay as only can take bag field as input return true if the predecessor is a multi input operator then detailed checks are required if pred instanceof Cross pred instanceof Join boolean inner Flags null boolean is Full Outer true boolean is Inner true if pred instanceof Join inner Flags Join pred get Inner Flags If all inner Flag is false means a full outer join for boolean inner inner Flags if inner is Full Outer false else is Inner false if is Full Outer return false for int j j preds size j if has All Logical Relational Operator preds get j uids For Join inner Flag true indicate that branch is the outer join side which has the exact opposite semantics if pred instanceof Cross pred instanceof Join is Inner inner Flags j return true return false private boolean contain Logical Expression Plan filter Plan Iterator Operator it filter Plan get Operators while it has Next if it next instanceof User Func Expression return true return false Set Long collect Uid From Exp Plan Logical Expression Plan filter Plan throws Frontend Exception Set Long uids new Hash Set Long Iterator Operator iter filter Plan get Operators while iter has Next Operator op iter next if op instanceof Project Expression long uid Project Expression op get Field Schema uid uids add uid return uids Starting from current operator which is a filter search its successors until locating a non filter operator Null is returned if none is found private Operator find Non Filter Predecessor Operator current Operator op current do List Operator predecessors current Plan get Predecessors op if there are no predecessors return false if predecessors null predecessors size return null Operator pred predecessors get if pred instanceof Filter op pred continue else return pred while true Override public void transform Operator Plan matched throws Frontend Exception sub Plan new Operator Sub Plan current Plan Filter filter Filter matched get Sources get This is the one that we will insert filter btwn it and it s input Operator predecessor this find Non Filter Predecessor filter sub Plan add predecessor Disconnect the filter in the plan without removing it from the plan Operator predec current Plan get Predecessors filter get Operator succed if current Plan get Successors filter null succed current Plan get Successors filter get else succed null Pair Integer Integer p current Plan disconnect predec filter if succed null sub Plan add succed Pair Integer Integer p current Plan disconnect filter succed current Plan connect predec p first succed p second if predecessor instanceof Sort predecessor instanceof Distinct predecessor instanceof Cogroup current Plan get Predecessors predecessor size For sort put the filter in front of it Operator prev current Plan get Predecessors predecessor get insert Filter prev predecessor filter return Find the predecessor of join that contains all required uids Logical Expression Plan filter Plan filter get Filter Plan List Operator preds current Plan get Predecessors predecessor Map Integer Operator inputs find Inputs To Add Filter filter Plan predecessor preds Filter new Filter null for Entry Integer Operator entry inputs entry Set int input Index entry get Key Operator pred entry get Value Find projection field offset int column Offset if predecessor instanceof Join predecessor instanceof Cross for int i i input Index i column Offset Logical Relational Operator preds get i get Schema size Reuse the filter for the first match For others need to make a copy of the filter and add it between input and predecessor new Filter new Filter null filter new Filter Logical Plan current Plan current Plan add new Filter sub Plan add new Filter sub Plan add pred Logical Expression Plan f Plan filter Plan deep Copy List Operator sinks f Plan get Sinks List Project Expression proj Exprs new Array List Project Expression for Operator sink sinks if sink instanceof Project Expression proj Exprs add Project Expression sink if predecessor instanceof Cogroup for Project Expression proj Expr proj Exprs Need to merge filter condition and cogroup by expression Logical Expression Plan plan Cogroup predecessor get Expression Plans get input Index iterator next Logical Expression Plan copy plan deep Copy Logical Expression root Logical Expression copy get Sinks get List Operator predecessors f Plan get Predecessors proj Expr if predecessors null predecessors size f Plan remove proj Expr f Plan add root else f Plan add root Operator pred predecessors get Pair Integer Integer pair f Plan disconnect pred proj Expr f Plan connect pred pair first root pair second f Plan remove proj Expr Now reset the projection expressions in the new filter plan sinks f Plan get Sinks for Operator sink sinks if sink instanceof Project Expression Project Expression proj Project Expression sink proj set Attached Relational Op new Filter proj set Input Num proj set Col Num proj get Col Num column Offset new Filter set Filter Plan f Plan insert Filter pred predecessor new Filter check if a relational operator contains all of the specified uids private boolean has All Logical Relational Operator op Set Long uids throws Frontend Exception Logical Schema schema op get Schema if schema null return false for long uid uids if schema find Field uid return false return true Override public Operator Plan report Changes return current Plan Insert the filter in between the given two operators private void insert Filter Operator prev Operator predecessor Filter filter throws Frontend Exception Pair Integer Integer p current Plan disconnect prev predecessor current Plan connect prev p first filter current Plan connect filter predecessor p second Identify those among preds that will need to have a filter between it and the predecessor private Map Integer Operator find Inputs To Add Filter Logical Expression Plan filter Plan Operator predecessor List Operator preds throws Frontend Exception Map Integer Operator inputs new Hash Map Integer Operator if predecessor instanceof Union predecessor instanceof Cogroup for int i i preds size i inputs put i preds get i return inputs collect all uids used in the filter plan Set Long uids collect Uid From Exp Plan filter Plan boolean inner Flags null boolean is Inner true if predecessor instanceof Join inner Flags Join predecessor get Inner Flags for boolean inner inner Flags if inner is Inner false break Find the predecessor of join that contains all required uids for int j j preds size j Filter can push to Join outer branch but no inner branch if has All Logical Relational Operator preds get j uids predecessor instanceof Cross predecessor instanceof Join is Inner inner Flags j Operator input preds get j sub Plan add input inputs put j input return inputs Override protected Operator Plan build Pattern Logical Plan plan new Logical Plan Logical Relational Operator op new Filter plan plan add op return plan 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig parser import java io File import java io Exception import java net import java net Syntax Exception import java util Array List import java util Hash Set import java util List import java util Set import org antlr runtime Common Token Stream import org antlr runtime Recognition Exception import org antlr runtime tree Common Tree import org antlr runtime tree Tree import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache pig Func Spec import org apache pig Pig Configuration import org apache pig Store Func Interface import org apache pig backend datastorage Container Descriptor import org apache pig backend datastorage Data Storage import org apache pig backend datastorage Element Descriptor import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig backend hadoop executionengine shims Hadoop Shims import org apache pig builtin Pig Storage import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan logical relational Store import org apache pig newplan logical relational Logical Plan import org apache pig tools pigstats Script State public class Query Parser Utils public static String remove Quotes String str if str starts With u c str ends With u c return str substring str length else return str public static void attach Store Plan String scope Logical Plan lp String file Name String func Operator input String alias Pig Context pig Context throws Frontend Exception func func null pig Context get Properties get Property Pig Configuration Pig Storage class get Name func Func Spec func Spec new Func Spec func Store Func Interface sto Func Store Func Interface Pig Context instantiate Func From Spec func Spec file Name remove Quotes file Name File Spec file Spec new File Spec file Name func Spec String sig alias Logical Plan Builder new Operator Key scope boolean disambiguation Enabled Boolean parse Boolean pig Context get Properties get Property Pig Configuration Pig Configuration sto Func set Store Func Context Signature sig Store store new Store lp file Spec sto Func sig disambiguation Enabled store set Alias alias try sto Func rel To Abs Path For Store Location file Name get Current Dir pig Context catch Exception ioe Frontend Exception e new Frontend Exception ioe get Message ioe throw e lp add store lp connect input store static Path get Current Dir Pig Context pig Context throws Exception Data Storage dfs pig Context get Dfs Container Descriptor desc dfs get Active Container Element Descriptor el dfs as Element desc return new Path el to String static void set Hdfs Servers String absolute Path Pig Context pig Context throws Syntax Exception First check for the file system via the new property String default String pig Context get Properties get fs default if default null check the deprecated property if we must default String pig Context get Properties get fs default name default new default Configuration conf new Configuration true Configuration Util merge Conf conf Configuration Util to Configuration pig Context get Properties Set String remote Hosts get Remote Hosts absolute Path default conf String hdfs Servers String String pig Context get Properties get Configuration if hdfs Servers String null hdfs Servers String String hdfs Servers hdfs Servers String split for String remote Host remote Hosts boolean existing false for String hdfs Server hdfs Servers if hdfs Server equals remote Host existing true if existing if hdfs Servers String is Empty hdfs Servers String hdfs Servers String hdfs Servers String hdfs Servers String remote Host if hdfs Servers String is Empty pig Context get Properties set Property Configuration hdfs Servers String static Set String get Remote Hosts String absolute Path default Configuration conf String default Host default get Host null default get Host to Lower Case String default Scheme default get Scheme null default get Scheme to Lower Case Set String result new Hash Set String String fnames absolute Path split for String fname fnames remove leading trailing whitespace s Path path new Path fname trim uri path to Uri if uri is Absolute If it has scheme String this Host uri get Host null uri get Host to Lower Case String scheme uri get Scheme to Lower Case If host and scheme are same continue if scheme equals default Scheme this Host equals default Host this Host is Empty continue String authority uri get Authority null uri get Authority to Lower Case if scheme equals har String parts authority split scheme parts if parts length authority else authority parts if scheme is Empty scheme equals default Scheme authority equals default get Authority continue else if Hadoop Shims has File System Impl path conf continue result add scheme authority return result static String construct File Name Signature String file Name Func Spec func Spec return file Name func Spec to String static String generate Error Header Recognition Exception ex String filename return new Source Location filename ex line ex char Position In Line to String Suppress Warnings unchecked rawtypes static void replace Node With Node List Tree old Node Common Tree new Tree String file Name int idx old Node get Child Index Common Tree parent Common Tree old Node get Parent int count parent get Child Count List child List new Array List parent get Children List macro List new Tree get Children while parent get Child Count parent delete Child for int i i count i if i idx add only there is something to add if macro List null parent add Children macro List else parent add Child Tree child List get i static File get File From Import Search Path String script Path File f new File script Path if f exists f is Absolute script Path starts With script Path starts With return f Script State state Script State get if state null state get Pig Context null String srch Path state get Pig Context get Properties get Property pig import search path if srch Path null String paths srch Path split for String path paths File f new File path File separator script Path if f exists return f return null static Query Parser create Parser Common Token Stream tokens return create Parser tokens static Query Parser create Parser Common Token Stream tokens int line Offset Query Parser parser new Query Parser tokens Pig Parser Node Adaptor adaptor new Pig Parser Node Adaptor tokens get Source Name line Offset parser set Tree Adaptor adaptor return parser 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl builtin import java io Exception import java util Random import org apache pig data Tuple import org apache hadoop mapreduce Record Reader import org apache pig backend hadoop executionengine map Reduce Layer Pig Split loader that samples the data It randomly samples tuples from input The number of tuples to be sampled has to be set before the first call to get Next see documentation of get Next call public class Random Sample Loader extends Sample Loader array to store the sample tuples Tuple samples null index into samples array to the next sample to be returned protected int next Sample Idx Construct with a class of loader to use param func Spec func spec of the loader to use param ns Number of samples per map to collect Arguments are passed as strings instead of actual types Func Spec int because Func Spec only supports string arguments to constructors public Random Sample Loader String func Spec String ns instantiate the loader super func Spec set the number of samples super set Num Samples Integer value Of ns Allocate a buffer for num Samples elements populate it with the first num Samples tuples and continue scanning rest of the input For every ith next call we generate a random number r s t r i and if r num Samples we insert the new tuple into our buffer at position r This gives us a random sample of the tuples in the partition Override public Tuple get Next throws Exception if samples null return get Sample else collect samples samples new Tuple num Samples populate the samples array with first num Samples tuples Tuple t null for int i i num Samples i t loader get Next if t null break samples i t row Num that starts from int row Num num Samples Random rand Gen new Random if t null did not exhaust all tuples while true collect samples until input is exhausted int rand rand Gen next Int row Num if rand num Samples pick this as sample Tuple sample Tuple loader get Next if sample Tuple null break samples rand sample Tuple else skip tuple if skip Next break row Num return get Sample Override public void prepare To Read Record Reader reader Pig Split split throws Exception super prepare To Read reader split samples null next Sample Idx private Tuple get Sample if next Sample Idx samples length return samples next Sample Idx else return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Data Input import java io Data Output import java io Exception import java util Iterator import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine physical Layer relational Operators Packager import org apache pig impl io Nullable Tuple import org apache pig impl io Pig Nullable Writable This bag does not store the tuples in memory but has access to an iterator typically provided by Hadoop Use this when you already have an iterator over tuples and do not want to copy over again to a new bag public class Read Once Bag implements Data Bag The Packager that created this protected Packager pkgr The iterator of Tuples Marked transient because we will never serialize this protected transient Iterator Nullable Tuple tup Iter The key being worked on protected Pig Nullable Writable key Writable private static final long serial Version public Read Once Bag This constructor creates a bag out of an existing iterator of tuples by taking ownership of the iterator and copying the elements of the iterator param pkg Package Lite param tup Iter Iterator Nullable Tuple param key Object public Read Once Bag Packager pkgr Iterator Nullable Tuple tup Iter Pig Nullable Writable key Writable this pkgr pkgr this tup Iter tup Iter this key Writable key Writable non Javadoc see org apache pig impl util Spillable get Memory Size Override public long get Memory Size return non Javadoc see org apache pig impl util Spillable spill Override public long spill throw new Runtime Exception Read Once Bag does not support spill operation non Javadoc see org apache pig data Data Bag add org apache pig data Tuple Override public void add Tuple t throw new Runtime Exception Read Once Bag does not support add operation non Javadoc see org apache pig data Data Bag add All org apache pig data Data Bag Override public void add All Data Bag b throw new Runtime Exception Read Once Bag does not support add All operation non Javadoc see org apache pig data Data Bag clear Override public void clear throw new Runtime Exception Read Once Bag does not support clear operation non Javadoc see org apache pig data Data Bag is Distinct Override public boolean is Distinct throw new Runtime Exception Read Once Bag does not support is Distinct operation non Javadoc see org apache pig data Data Bag is Sorted Override public boolean is Sorted throw new Runtime Exception Read Once Bag does not support is Sorted operation non Javadoc see org apache pig data Data Bag iterator Override public Iterator Tuple iterator return new Read Once Bag Iterator non Javadoc see org apache pig data Data Bag mark Stale boolean Override public void mark Stale boolean stale throw new Runtime Exception Read Once Bag does not support mark Stale operation non Javadoc see org apache pig data Data Bag size Override public long size throw new Runtime Exception Read Once Bag does not support size operation non Javadoc see org apache hadoop io Writable read Fields java io Data Input Override public void read Fields Data Input in throws Exception throw new Runtime Exception Read Once Bag does not support read Fields operation non Javadoc see org apache hadoop io Writable write java io Data Output Override public void write Data Output out throws Exception int err Code String msg Read Once Bag should never be serialized throw new Exec Exception msg err Code Pig Exception non Javadoc see java lang Comparable compare To java lang Object This has to be defined since Data Bag implements Comparable although in this case we can not really compare Override public int compare To Object o throw new Runtime Exception Read Once Bags can not be compared Override public boolean equals Object other if other instanceof Read Once Bag if pkgr get Key Tuple if tup Iter Read Once Bag other tup Iter pkgr get Key Tuple Read Once Bag other pkgr get Key Tuple pkgr get Key As Tuple equals Read Once Bag other pkgr get Key As Tuple return true else return false else if tup Iter Read Once Bag other tup Iter pkgr get Key equals Read Once Bag other pkgr get Key return true else return false return false Override public int hash Code int hash if pkgr get Key Tuple hash hash pkgr get Key As Tuple hash Code else hash hash pkgr get Key hash Code return hash protected class Read Once Bag Iterator implements Iterator Tuple non Javadoc see java util Iterator has Next Override public boolean has Next return tup Iter has Next non Javadoc see java util Iterator next Override public Tuple next Nullable Tuple ntup tup Iter next int index ntup get Index Tuple ret null try ret pkgr get Value Tuple key Writable ntup index catch Exec Exception e throw new Runtime Exception Read Once Bag failed to get value tuple e to String return ret non Javadoc see java util Iterator remove Override public void remove throw new Runtime Exception Read Once Bag iterator remove is not allowed 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Regex Operator public class Regex Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Regex Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Regex plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Regex Expression Regex Expression ao Regex Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null Data Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Regex Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Exception import java io Serializable import java util Arrays import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org codehaus jackson annotate Json Property Order represenation of a schema used to communicate with load and store functions This is separate from link Schema which is an internal Pig representation of a schema since Pig Interface Audience Public Interface Stability Stable Json Property Order fields version sort Keys sort Key Orders public class Resource Schema implements Serializable private static final long serial Version private static Log log Log Factory get Log Resource Schema class Array Getters intentionally return mutable arrays instead of copies to simplify updates without unnecessary copying Setters make a copy of the arrays in order to prevent an array from being shared by two objects with modifications in one accidentally changing the other initializing arrays to empty so we do n t have to worry about Es setters wo n t set to null private Resource Field Schema fields new Resource Field Schema public enum Order private int sort Keys new int each entry is an offset into the fields array private Order sort Key Orders new Order private int version Json Property Order name type description schema public static class Resource Field Schema implements Serializable private static final long serial Version private String name values are constants from Data Type private byte type private String description nested tuples and bags will have their own schema private Resource Schema schema Construct an empty field schema public Resource Field Schema Construct using a link org apache pig impl logical Layer schema Schema Field Schema as the template param field Schema field Schema to copy from public Resource Field Schema Field Schema field Schema type field Schema type name field Schema alias description autogenerated from Pig Field Schema Schema inner field Schema schema allow partial schema if type Data Type type Data Type type Data Type inner null schema new Resource Schema inner else schema null Construct using a link org apache pig newplan logical relational Logical Schema Logical Field Schema as the template param field Schema field Schema to copy from public Resource Field Schema Logical Field Schema field Schema type field Schema type name field Schema alias description autogenerated from Pig Field Schema Logical Schema inner field Schema schema allow partial schema if Data Type is Schema Type type inner null schema new Resource Schema inner else schema null Get the name of this field return name public String get Name return name Set the name of this filed param name new name return this public Resource Field Schema set Name String name this name name return this Get the type of this field return type as a link Data Type static final byte public byte get Type return type Set the type of this field param type new type return this public Resource Field Schema set Type byte type this type type return this Get a free form text description of this field return description public String get Description return description Set the description param description new description return this public Resource Field Schema set Description String description this description description return this Get the schema for this field Type tuple bag map may have a schema return schema public Resource Schema get Schema return schema Set the schema for this field Type tuple bag map may have a schema param schema new schema return this public Resource Field Schema set Schema Resource Schema schema throws Exception validate Schema schema this schema schema return this private void validate Schema Resource Schema schema throws Exception if type Data Type schema null Resource Field Schema sub Fields schema get Fields if sub Fields length if sub Fields type Data Type throw Invalid Schema Exception else throw Invalid Schema Exception public static void throw Invalid Schema Exception throws Frontend Exception int err Code throw new Frontend Exception Invalid resource schema bag schema must have tuple as its field err Code Pig Exception Override public String to String return get Description true public String calc Cast String return get Description false private String get Description boolean print Alias String Builder sb new String Builder if print Alias this name null sb append this name append if Data Type is Atomic this type sb append Data Type find Type Name this type else if this schema null stringify Resource Schema sb this schema this type print Alias return sb to String Construct an empty Resource Schema public Resource Schema Construct a Resource Schema from a link Schema param pig Schema Schema to use public Resource Schema Schema pig Schema List Field Schema pig Schema Fields pig Schema get Fields fields new Resource Field Schema pig Schema Fields size for int i i fields length i fields i new Resource Field Schema pig Schema Fields get i Construct a Resource Schema from a link Logical Schema param pig Schema Schema to use public Resource Schema Logical Schema pig Schema List Logical Field Schema pig Schema Fields pig Schema get Fields fields new Resource Field Schema pig Schema Fields size for int i i fields length i fields i new Resource Field Schema pig Schema Fields get i Only for use by Pig internal code Construct a Resource Schema from a link Schema param pig Schema Schema to use param sort Info information on how data is sorted Interface Audience Private public Resource Schema Schema pig Schema Sort Info sort Info this pig Schema if sort Info null sort Info get Sort Col Info List size sort Keys new int sort Info get Sort Col Info List size sort Key Orders new Order sort Info get Sort Col Info List size for int i i sort Info get Sort Col Info List size i Sort Col Info col Info sort Info get Sort Col Info List get i int index col Info get Col Index Order order org apache pig Sort Col Info Order orig Order col Info get Sort Order if orig Order org apache pig Sort Col Info Order order Order else order Order sort Keys i index sort Key Orders i order Only for use by Pig internal code Construct a Resource Schema from a link Logical Schema param pig Schema Logical Schema to use param sort Info information on how data is sorted Interface Audience Private public Resource Schema Logical Schema pig Schema Sort Info sort Info this pig Schema if sort Info null sort Info get Sort Col Info List size sort Keys new int sort Info get Sort Col Info List size sort Key Orders new Order sort Info get Sort Col Info List size for int i i sort Info get Sort Col Info List size i Sort Col Info col Info sort Info get Sort Col Info List get i int index col Info get Col Index Order order org apache pig Sort Col Info Order orig Order col Info get Sort Order if orig Order org apache pig Sort Col Info Order order Order else order Order sort Keys i index sort Key Orders i order Get the version of this schema return version public int get Version return version public Resource Schema set Version int version this version version return this Get field schema for each field return array of field schemas public Resource Field Schema get Fields return fields Get all field names return array of field names public String field Names String names new String fields length for int i i fields length i names i fields i get Name return names Set all the fields If fields are not currently null the new fields will be silently ignored param fields to use as fields in this schema return this public Resource Schema set Fields Resource Field Schema fields if fields null this fields Arrays copy Of fields fields length return this Get the sort keys for this data return array of ints Each integer in the array represents the field number So if the schema of the data is a b c d and the data is sorted on c b the returned sort keys will be Field numbers are zero based If the data is not sorted a zero length array will be returned public int get Sort Keys return sort Keys Set the sort keys for htis data If sort keys are not currently null the new sort keys will be silently ignored param sort Keys Each integer in the array represents the field number So if the schema of the data is a b c d and the data is sorted on c b the sort keys should be Field numbers are zero based return this public Resource Schema set Sort Keys int sort Keys if sort Keys null this sort Keys Arrays copy Of sort Keys sort Keys length return this Get order for sort keys return array of Order This array will be the same length as the int array returned by link get Sort Keys public Order get Sort Key Orders return sort Key Orders Set the order for each sort key If order is not currently null new order will be silently ignored param sort Key Orders array of Order Should be the same length as int passed to link set Sort Keys return this public Resource Schema set Sort Key Orders Order sort Key Orders if sort Key Orders null this sort Key Orders Arrays copy Of sort Key Orders sort Key Orders length return this Test whether two Resource Schemas are the same Two schemas are said to be the same if they are both null or have the same number of fields and for each field the name type are the same For fields that have may have schemas i e tuples both schemas be equal Field descriptions are not used in testing equality return true if equal according to the above definition otherwise false public static boolean equals Resource Schema rs Resource Schema rs if rs null return rs null true false if rs null return false if rs get Version rs get Version Arrays equals rs get Sort Keys rs get Sort Keys Arrays equals rs get Sort Key Orders rs get Sort Key Orders return false Resource Field Schema rfs rs get Fields Resource Field Schema rfs rs get Fields if rfs length rfs length return false for int i i rfs length i if rfs i get Name null rfs i get Name null rfs i get Name null rfs i get Name null return false if rfs i get Name null rfs i get Name null if rfs i get Type rfs i get Type return true else return false if rfs i get Name equals rfs i get Name rfs i get Type rfs i get Type return false if equals rfs i get Schema rfs i get Schema return false return true Override public String to String String Builder sb new String Builder stringify Resource Schema sb this Data Type true return sb to String private static void stringify Resource Schema String Builder sb Resource Schema rs byte type boolean print Alias if type Data Type sb append else if type Data Type sb append else if type Data Type sb append if rs null for int i i rs get Fields length i sb append rs get Fields i get Description print Alias if i rs get Fields length sb append if type Data Type sb append else if type Data Type sb append else if type Data Type sb append 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Serializable import java util Arrays import org apache pig classification Interface Audience import org apache pig classification Interface Stability An class that represents statistics about data to be loaded or stored It is marked unstable because Pig does very little statistics collection at this point If and when that functionality is added it is expected that this interface will change since Pig Interface Audience Public Interface Stability Unstable public class Resource Statistics implements Cloneable Getters intentionally return mutable arrays instead of copies to simplify updates without unnecessary copying Setters make a copy of the arrays in order to prevent an array from being shared by two objects with modifications in one accidentally changing the other arrays are initialized to empty so we do n t have to worry about Es setters disallow setting them to null private static final long serial Version private Long num Records number of records private Long avg Record Size average record size in bytes private Resource Field Statistics fields new Resource Field Statistics private Long bytes Statistics for a given field in the data public static class Resource Field Statistics implements Serializable private static final long serial Version private int version private Long num Distinct Values number of distinct values represented in this field We need some way to represent a histogram of values in the field as those will be useful However we ca n t count on being able to hold such histograms in memory Have to figure out how they can be kept on disk and represented here for now do n t create so many buckets you ca n t hold them in memory an ordered array of the most common values in descending order of frequency private Object most Common Values new Object an array that matches the most Common Values array and lists the frequencies of those values as a fraction through of the total number of records private float most Common Values Freq new float an ordered array of values from min val to max val such that the number of records with values between value Histogram i and and value Histogram i is roughly equal for all values of i if most Common Values is non empty the values in that array should not be included in the histogram Adjust accordingly private Object value Histogram new Object public int get Version return version public Resource Field Statistics set Version int version this version version return this public Long get Num Distinct Values return num Distinct Values public Resource Field Statistics set Num Distinct Values Long num Distinct Values this num Distinct Values num Distinct Values return this public Object get Most Common Values return most Common Values public Resource Field Statistics set Most Common Values Object most Common Values if most Common Values null this most Common Values Arrays copy Of most Common Values most Common Values length return this public float get Most Common Values Freq return most Common Values Freq public Resource Field Statistics set Most Common Values Freq float most Common Values Freq if most Common Values Freq null this most Common Values Freq Arrays copy Of most Common Values Freq most Common Values Freq length return this public Object get Value Histogram return value Histogram public Resource Field Statistics set Value Histogram Object value Histogram if value Histogram null this value Histogram Arrays copy Of value Histogram value Histogram length return this equals and hash Code overridden mostly for ease of testing you should n t encounter a situation in which you need to equals two sets of statistics on different objects in the wild Override public boolean equals Object an Other if an Other null an Other get Class equals this get Class return false Resource Field Statistics other Resource Field Statistics an Other setters do not allow null values so no worries about Es here return Arrays equals most Common Values other most Common Values Arrays equals most Common Values Freq other most Common Values Freq Arrays equals value Histogram other value Histogram this num Distinct Values equals other num Distinct Values this version other version naive hash Code implementation following the example in s developerworks http www ibm com developerworks java library j jtp html Override public int hash Code int hash hash hash Arrays hash Code most Common Values hash hash Arrays hash Code most Common Values Freq hash hash num Distinct Values hash Code hash hash Arrays hash Code value Histogram hash hash version return Override public String to String String Builder sb new String Builder Resource Statistics Version version n sb append n for Object o most Common Values sb append o to String sb append n Vfreq n for Float f most Common Values Freq sb append f to String sb append n sb append num Dist Vals num Distinct Values sb append val Histogram n for Object o value Histogram sb append o to String sb append n return sb to String public Long getm Bytes return this bytes param m Bytes deprecated Use link Resource Statistics set Size In Bytes Long instead Deprecated public Resource Statistics setm Bytes Long m Bytes this bytes m Bytes return this Sets the size in bytes param bytes public void set Size In Bytes Long bytes this bytes bytes return size in bytes public Long get Size In Bytes return this bytes public Long get Num Records return num Records public Resource Statistics set Num Records Long num Records this num Records num Records return this returns average record size in bytes This number can be explicitly specified by statistics or if absent computed using totalbytes totalrecords Will return null if ca n t be computed public Long get Avg Record Size if avg Record Size null bytes null num Records null return bytes num Records else return avg Record Size Set average record size in bytes param size In Bytes public void set Avg Record Size Long size In Bytes avg Record Size size In Bytes public Resource Field Statistics get Fields return fields public Resource Statistics set Fields Resource Field Statistics fields if fields null this fields Arrays copy Of fields fields length return this equals and hash Code overridden mostly for ease of testing you should n t encounter a situation in which you need to equals two sets of statistics on different objects in the wild Override public boolean equals Object an Other if an Other null an Other get Class equals this get Class return false Resource Statistics other Resource Statistics an Other return Arrays equals fields other fields bytes null other bytes null bytes equals other bytes num Records null other num Records null num Records equals other num Records Override public int hash Code int hash hash hash Arrays hash Code fields hash hash bytes null bytes hash Code hash hash num Records null num Records hash Code return hash Probably more in here Override public String to String String Builder sb new String Builder Field Stats n for Resource Field Statistics f fields sb append f to String sb append bytes bytes sb append num Records num Records return sb to String public Object clone throws Clone Not Supported Exception return super clone 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util List import org apache pig Eval Func import org apache pig backend executionengine Exec Exception import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema import com google common collect Lists Produces a Data Bag with hierarchy of values from the most detailed level of aggregation to most general level of aggregation of the specified dimensions For example a b c will produce the following bag pre a b c a b null a null null null null null pre public class Rollup Dimensions extends Eval Func Data Bag private static Bag Factory bf Bag Factory get Instance private static Tuple Factory tf Tuple Factory get Instance private final String all Marker public Rollup Dimensions this null public Rollup Dimensions String all Marker super this all Marker all Marker Override public Data Bag exec Tuple tuple throws Exception List Tuple result Lists new Array List With Capacity tuple size Cube Dimensions convert Null To Unknown tuple result add tuple iteratively Rollup result tuple return bf new Default Bag result private void iteratively Rollup List Tuple result Tuple input throws Exec Exception Tuple temp Tup tf new Tuple input get All for int i input size i i temp Tup set i all Marker result add tf new Tuple temp Tup get All Override public Schema output Schema Schema input dimensions string is the default namespace assigned to the output schema this can be overridden by specifying user defined schema names in foreach operator if user defined schema names are not specified then the output schema of foreach operator using this will have dimensions namespace for all fields in the tuple try return new Schema new Field Schema dimensions input Data Type catch Frontend Exception e we are specifying explicitly so this should not happen throw new Runtime Exception e Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan optimizer import java util Array List import java util Deque import java util Hash Set import java util Iterator import java util Linked List import java util List import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Operator Sub Plan Rules describe a pattern of operators They also reference a Transformer If the pattern of operators is found one or more times in the provided plan then the optimizer will use the associated Transformer to transform the plan Note the pattern matching logic implemented here has a limitation that it assumes that all the leaves in the pattern are siblings See more detailed description here https issues apache org jira browse If new rules use patterns that do n t work with this limitation the pattern match logic will need to be updated public abstract class Rule protected String name null protected Operator Plan pattern transient protected Operator Plan current Plan protected static final Log log Log Factory get Log Rule class private transient Set Operator matched Nodes new Hash Set Operator private boolean mandatory private boolean skip Listener false Create this rule by using the default pattern that this rule provided param n Name of this rule param mandatory if it is set to false this rule can be disabled by user public Rule String n boolean mandatory name n pattern build Pattern this mandatory mandatory param n Name of this rule param p Pattern to look for public Rule String n Operator Plan p name n pattern p Build the pattern that this rule will look for return the pattern to look for by this rule abstract protected Operator Plan build Pattern Get the transformer for this rule Abstract because the rule may want to choose how to instantiate the transformer This should never return a cached transformer it should always return a fresh one with no state return Transformer to use with this rule abstract public Transformer get New Transformer Return the pattern to be matched for this rule return the pattern to be matched for this rule public Operator Plan get Pattern return pattern protected boolean is Skip Listener return this skip Listener protected void set Skip Listener boolean skip this skip Listener skip Search for all the sub plans that matches the pattern defined by this rule See class description above for limitations on the patterns supported return list of all matched sub plans The returned plans are partial views of the original Operator Plan Each is a sub set of the original plan and represents the same topology as the pattern but operators in the returned plan are the same objects as the original plan Therefore a call get Plan from any node in the return plan would return the original plan param plan the Operator Plan to look for matches to the pattern public List Operator Plan match Operator Plan plan throws Frontend Exception current Plan plan List Operator leaves pattern get Sinks Iterator Operator iter plan get Operators List Operator Plan matched List new Array List Operator Plan matched Nodes clear while iter has Next Operator op iter next find a node that matches the first leaf of the pattern if match op leaves get List Operator plan Ops new Array List Operator plan Ops add op if there is more than leaves in the pattern we check if other leaves match the siblings of this node if leaves size boolean matched true List Operator preds null preds plan get Predecessors op if this node has no predecessor it must be a root if preds null preds new Array List Operator preds add null for Operator s preds matched true List Operator siblings null if s null siblings plan get Successors s else for a root we get its siblings by getting all roots siblings plan get Sources int index siblings index Of op if siblings size index leaves size continue for int j j leaves size j if match siblings get index j leaves get j matched false break if matched for int j j leaves size j plan Ops add siblings get index j we have move on to next operator as this one does n t have siblings to match all the leaves if matched continue Pattern Match Operator Plan match new Pattern Match Operator Plan plan if match check plan Ops we find a matched pattern add the operators into matched Nodes Iterator Operator iter match get Operators while iter has Next Operator opt iter next matched Nodes add opt add pattern matched List add match return matched List public String get Name return name public boolean is Mandatory return mandatory Check if two operators match each other we want to find matches that do n t share nodes private boolean match Operator plan Node Operator pattern Node return plan Node get Class equals pattern Node get Class matched Nodes contains plan Node private class Pattern Match Operator Plan extends Operator Sub Plan public Pattern Match Operator Plan Operator Plan base Plan super base Plan protected boolean check List Operator plan Ops throws Frontend Exception List Operator pattern Ops pattern get Sinks if plan Ops size pattern Ops size return false for int i i plan Ops size i Deque Operator s new Linked List Operator if check plan Ops get i pattern Ops get i s return false Iterator Operator iter s iterator while iter has Next add iter next if size pattern size return true return false Check if the plan operator and its sub tree has a match to the pattern operator and its sub tree This algorithm only search and return one match It does n t recursively search for all possible matches For example for a plan that looks like join load load if we are looking for join load pattern only one match will be returned instead of two so that the matched subsets do n t share nodes private boolean check Operator plan Op Operator pattern Op Deque Operator opers throws Frontend Exception if match plan Op pattern Op return false check if their predecessors match List Operator preds get Base Plan get Predecessors plan Op List Operator preds pattern get Predecessors pattern Op if preds null preds null return false if preds null preds null preds size preds size return false we ve reached the root of the pattern so a match is found if preds null preds size opers push plan Op return true int index look for predecessors while index preds size boolean match true if match preds get index preds get if preds size index preds size return false int old Size opers size for int i i preds size i if check preds get index i preds get i opers for int j opers size j old Size j opers pop match false break if match opers push plan Op return true index return false 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl builtin import java io Exception import java util Array List import org apache hadoop fs Path import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Record Reader import org apache pig Load Caster import org apache pig Load Func import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig impl Pig Context import org apache pig impl io File Spec import org apache pig impl util Pair Abstract class that specifies the interface for sample loaders public abstract class Sample Loader extends Load Func number of samples to be sampled protected int num Samples protected Load Func loader Record Reader used by the underlying loader private Record Reader record Reader null public Sample Loader String func Spec func Spec func Spec replace All loader Load Func Pig Context instantiate Func From Spec func Spec public void set Num Samples int n num Samples n public int get Num Samples return num Samples Override public Input Format get Input Format throws Exception return loader get Input Format public boolean skip Next throws Exception try return record Reader next Key Value catch Interrupted Exception e throw new Exception Error getting input e public void compute Samples Array List Pair File Spec Boolean inputs Pig Context pc throws Exec Exception Override public Load Caster get Load Caster throws Exception return loader get Load Caster Override public String relative To Absolute Path String location Path cur Dir throws Exception return loader relative To Absolute Path location cur Dir Override public void prepare To Read Record Reader reader Pig Split split throws Exception loader prepare To Read reader split this record Reader reader Override public void set Location String location Job job throws Exception loader set Location location job Override public void set Context Signature String signature loader set Context Signature signature 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import java io Exception import java util Array List import java util List import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache pig Func Spec import org apache pig backend hadoop datastorage Configuration Util import org apache pig backend hadoop executionengine map Reduce Layer plans Op Plan Visitor import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Constant Expression import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig impl io File Spec import org apache pig impl plan Depth First Walker import org apache pig impl plan Plan Exception import org apache pig impl plan Plan Walker import org apache pig impl plan Visitor Exception import org apache pig impl util Utils import org apache pig impl Pig Context visitor to optimize plans that have a sample job that immediately follows a load store only job These kinds of plans are generated for order bys and will soon be generated for joins that need to sample their data first These can be changed so that the Random Sample Loader subsumes the loader used in the first job and then removes the first job public class Sample Optimizer extends Op Plan Visitor private static final Log log Log Factory get Log Sample Optimizer class private Pig Context pig Context public Sample Optimizer Oper Plan plan Pig Context pig Context super plan new Depth First Walker Map Reduce Oper Oper Plan plan this pig Context pig Context private List Map Reduce Oper ops To Remove new Array List Map Reduce Oper Override public void visit throws Visitor Exception super visit remove operators marked for removal for Map Reduce Oper op ops To Remove this m Plan remove op Override public void visit Op Map Reduce Oper mr throws Visitor Exception See if this is a sampling job List Physical Operator pos mr map Plan get Roots if pos null pos size log debug Map of operator empty return Physical Operator po pos get if po instanceof Load log debug Root operator of map is not load return Huh Load load Load po String load Func load get File get Func Name String load File load get File get File Name if org apache pig impl builtin Random Sample Loader equals load Func org apache pig impl builtin Poisson Sample Loader equals load Func log debug Not a sampling job return if load File null log debug No load file return Get this job s predecessor There should be exactly one List Map Reduce Oper preds m Plan get Predecessors mr if preds size log debug Too many predecessors to sampling job return Map Reduce Oper pred preds get The predecessor should be a root List Map Reduce Oper pred Preds m Plan get Predecessors pred if pred Preds null pred Preds size log debug Predecessor should be a root of the plan return The predecessor should have just a load and store in the map and nothing in the combine or reduce if pred reduce Plan is Empty pred combine Plan is Empty log debug Predecessor has a combine or reduce plan return The job should have one successor List Map Reduce Oper succs m Plan get Successors mr if succs size log debug Job has more than one successor return Map Reduce Oper succ succs get if pred map Plan null pred map Plan size log debug Predecessor has more than just load store in the map return List Physical Operator loads pred map Plan get Roots if loads size log debug Predecessor plan has more than one root return Physical Operator r loads get if r instanceof Load Huh log debug Predecessor s map plan root is not a load return Load pred Load Load r Find the load the correlates with the file the sampler is loading and check that it is using the temp file storage format if succ map Plan null Huh log debug Successor has no map plan return loads succ map Plan get Roots Load succ Load null for Physical Operator root loads if root instanceof Load Huh log debug Successor s roots are not loads return Load sl Load root if load File equals sl get File get File Name Utils get Tmp File Compressor Name pig Context equals sl get File get Func Name succ Load sl break if succ Load null log debug Could not find load that matched file we are sampling return Okay we re on First replace our Random Sample Loader with a Random Sample Loader that uses the load function from our predecessor String rslargs new String File Spec pred Fs pred Load get File First argument is Func Spec of loader function to subsume this we want to set for ourselves rslargs pred Fs get Func Spec to String Add the loader s funcspec to the list of udf s associated with this mr operator mr Fs add rslargs Second argument is the number of samples per block read this from the original rslargs load get File get Func Spec get Ctor Args File Spec fs new File Spec pred Fs get File Name new Func Spec load Func rslargs Load new Load new Load load get Operator Key load get Requested Parallelism fs new Load set Signature pred Load get Signature new Load set Limit pred Load get Limit try mr map Plan replace load new Load check if it has Partition Skewed Keys List Physical Operator ls mr reduce Plan get Leaves for Physical Operator op ls scan mr op fs get File Name catch Plan Exception e throw new Visitor Exception e Second replace the loader in our successor with whatever the originally used loader was fs new File Spec pred Fs get File Name pred Fs get Func Spec new Load new Load succ Load get Operator Key succ Load get Requested Parallelism fs new Load set Signature pred Load get Signature try succ map Plan replace succ Load new Load Add the loader s funcspec to the list of udf s associated with this mr operator succ Fs add new Load get File get Func Spec to String catch Plan Exception e throw new Visitor Exception e Can not delete the pred right now because we are still traversing the graph So mark the pred and remove it from the plan once the visit by this optimizer is complete ops To Remove add pred search for Partion Skewed Keys and update input file name it is always under a For Each operator in reduce plan private void scan Map Reduce Oper mr Physical Operator op String file Name if op instanceof User Func if User Func op get Func Spec get Class Name equals org apache pig impl builtin Partition Skewed Keys String ctor Args User Func op get Func Spec get Ctor Args ctor Args file Name return else if op instanceof For Each List Physical Plan pl For Each op get Input Plans for Physical Plan plan pl List Physical Operator list plan get Leaves for Physical Operator pp list scan mr pp file Name else List Physical Operator preds mr reduce Plan get Predecessors op if preds null return for Physical Operator p preds scan mr p file Name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl logical Layer schema import java io Serializable import java util Array List import java util Collection import java util Hash Map import java util Hash Set import java util Iterator import java util List import java util Map import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Exception import org apache pig Resource Schema import org apache pig Resource Schema Resource Field Schema import org apache pig data Data Type import org apache pig impl logical Layer Canonical Namer import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Multi Map The Schema class encapsulates the notion of a schema for a relational operator schema is a list of columns that describe the output of a relational operator Each column in the relation is represented as a Field Schema a static class inside the Schema column by definition has an alias a type and a possible schema if the column is a bag or a tuple In addition each column in the schema has a unique auto generated name used for tracking the lineage of the column in a sequence of statements The lineage of the column is tracked using a map of the predecessors columns to the operators that generate the predecessor columns The predecessor columns are the columns required in order to generate the column under consideration Similarly a reverse lookup of operators that generate the predecessor column to the predecessor column is maintained public class Schema implements Serializable Cloneable private static final long serial Version public static class Field Schema implements Serializable Cloneable private static final long serial Version Alias for this field public String alias Datatype using codes from link org apache pig data Data Type public byte type If this is a tuple itself it can have a schema Otherwise this field must be null public Schema schema Canonical name This name uniquely identifies a field throughout the query Unlike a an alias it can not be changed It will change when the field is transformed in some way such as being used in an arithmetic expression or passed to a udf At that point a new canonical name will be generated for the field public String canonical Name null Canonical namer object to generate new canonical names on request In order to ensure unique and consistent names across all field schema objects the object is made static public static final Canonical Namer canonical Namer new Canonical Namer private static Log log Log Factory get Log Schema Field Schema class Constructor for any type param a Alias if known If unknown leave null param t Type using codes from link org apache pig data Data Type public Field Schema String a byte t alias a type t schema null canonical Name Canonical Namer get New Name Constructor for tuple fields param a Alias if known If unknown leave null param s Schema of this tuple public Field Schema String a Schema s alias a type Data Type schema s canonical Name Canonical Namer get New Name Constructor for tuple fields param a Alias if known If unknown leave null param s Schema of this tuple param t Type using codes from link org apache pig data Data Type public Field Schema String a Schema s byte t throws Frontend Exception alias a schema s log debug t t Bag Data Type tuple Data Type if null s Data Type is Schema Type t int err Code throw new Frontend Exception Only a or can have schemas Got Data Type find Type Name t err Code Pig Exception type t canonical Name Canonical Namer get New Name Copy Constructor param fs Source Field Schema public Field Schema Field Schema fs if null fs alias fs alias if null fs schema schema new Schema fs schema else schema null type fs type else alias null schema null type Data Type canonical Name Canonical Namer get New Name Two field schemas are equal if types and schemas are equal in all levels In order to relax alias equivalent requirement instead use equals Field Schema fschema Field Schema fother boolean relax Inner boolean relax Alias Override public boolean equals Object other if other instanceof Field Schema return false Field Schema otherfs Field Schema other return Field Schema equals this otherfs false false Override public int hash Code return this type schema null schema hash Code alias null alias hash Code Recursively compare two schemas to check if the input schema can be cast to the cast schema param cast Fs schema of the cast operator param input Fs schema of the cast input return true or falsew public static boolean castable Schema Field Schema cast Fs Schema Field Schema input Fs if cast Fs null input Fs null return false if cast Fs null return false if input Fs null return false byte input Type input Fs type byte cast Type cast Fs type if Data Type is Schema Type cast Fs type if input Type Data Type good else if input Type cast Type Do n t do the comparison if both embedded schemas are null That will cause Schema equals to return false even though we want to view that as true if cast Fs schema null input Fs schema null compare recursively using schema if Schema castable cast Fs schema input Fs schema return false else return false else if input Type cast Type good else if input Type Data Type cast Type Data Type cast Type Data Type Data Type is Number Type cast Type good else if Data Type is Number Type input Type cast Type Data Type cast Type Data Type Data Type is Number Type cast Type cast Type Data Type cast Type Data Type good else if input Type Data Type cast Type Data Type cast Type Data Type Data Type is Number Type cast Type good else if input Type Data Type cast Type Data Type Data Type is Number Type cast Type cast Type Data Type cast Type Data Type good else if input Type Data Type good else return false return true Compare two field schema for equality param fschema param fother param relax Inner If true we do n t check inner tuple schemas param relax Alias If true we do n t check aliases return true if Field Schemas are equal false otherwise public static boolean equals Field Schema fschema Field Schema fother boolean relax Inner boolean relax Alias if fschema null return false if fother null return false if fschema type fother type return false if relax Alias if fschema alias null fother alias null good else if fschema alias null fother alias null return false else if fschema alias null fother alias null return false else if fschema alias equals fother alias return false if relax Inner Data Type is Schema Type fschema type Do n t do the comparison if both embedded schemas are null That will cause Schema equals to return false even though we want to view that as true if fschema schema null fother schema null compare recursively using schema if Schema equals fschema schema fother schema false relax Alias return false return true Override public String to String String Builder sb new String Builder if alias null sb append alias sb append sb append Data Type find Type Name type if schema null sb append sb append schema to String sb append if canonical Name null sb append cn sb append canonical Name return sb to String Make a deep copy of this Field Schema and return it return clone of the this Field Schema throws Clone Not Supported Exception Override public Field Schema clone throws Clone Not Supported Exception Strings are immutable so we do n t need to copy alias Schemas are mutable so we need to make a copy try Field Schema fs new Field Schema alias schema null null schema clone type fs canonical Name Canonical Namer get New Name return fs catch Frontend Exception fe throw new Runtime Exception Should never fail to clone a Field Schema fe Recursively prefix merge two schemas param other Fs the other field schema to be merged with return the prefix merged field schema this can be null if one schema is null and allow Incompatible Types is true throws Schema Merge Exception if they can not be merged public Schema Field Schema merge Prefix Field Schema Schema Field Schema other Fs throws Schema Merge Exception return merge Prefix Field Schema other Fs true false Recursively prefix merge two schemas param other Fs the other field schema to be merged with param other Takes Alias Precedence true if aliases from the other field schema take precedence return the prefix merged field schema this can be null if one schema is null and allow Incompatible Types is true throws Schema Merge Exception if they can not be merged public Schema Field Schema merge Prefix Field Schema Schema Field Schema other Fs boolean other Takes Alias Precedence throws Schema Merge Exception return merge Prefix Field Schema other Fs other Takes Alias Precedence false Recursively prefix merge two schemas param other Fs the other field schema to be merged with param other Takes Alias Precedence true if aliases from the other field schema take precedence param allow Mergeable Types true if mergeable types should be allowed Two types are mergeable if any of the following conditions is true of checks if either one has a type null or unknown and other has a type null or unknown the result type will be the latter non null unknown type If either type is bytearray then result type will be the other possibly non type If current type can be cast to the other type then the result type will be the other type return the prefix merged field schema this can be null throws Schema Merge Exception if they can not be merged public Schema Field Schema merge Prefix Field Schema Schema Field Schema other Fs boolean other Takes Alias Precedence boolean allow Mergeable Types throws Schema Merge Exception Schema Field Schema my Fs this Schema Field Schema merged Fs null byte merged Type Data Type if null other Fs return my Fs if is Null Or Unknown Type my Fs is Null Or Unknown Type other Fs int err Code String msg Type mismatch No useful type for merging Field Schema my Fs Other Field Schema other Fs throw new Schema Merge Exception msg err Code Pig Exception else if my Fs type other Fs type merged Type my Fs type else if is Null Or Unknown Type my Fs is Null Or Unknown Type other Fs merged Type my Fs type else if allow Mergeable Types if is Null Or Unknown Type my Fs is Null Or Unknown Type other Fs merged Type other Fs type else if other Fs type Data Type just set merge Type to my Fs s type could even be merged Type my Fs type else if castable other Fs my Fs merged Type other Fs type else int err Code String msg Type mismatch for merging schema prefix Field Schema my Fs Other Field Schema other Fs throw new Schema Merge Exception msg err Code Pig Exception else int err Code String msg Type mismatch merging schema prefix Field Schema my Fs Other Field Schema other Fs throw new Schema Merge Exception msg err Code Pig Exception String merged Alias merge Alias my Fs alias other Fs alias other Takes Alias Precedence if Data Type is Schema Type merged Type just normal merge merged Fs new Field Schema merged Alias merged Type else Schema merged Sub Schema null merge inner schemas because both sides have schemas if null my Fs schema merged Sub Schema my Fs schema merge Prefix Schema other Fs schema other Takes Alias Precedence allow Mergeable Types else merged Sub Schema other Fs schema set Schema Default Type merged Sub Schema Data Type create the merged field try merged Fs new Field Schema merged Alias merged Sub Schema merged Type catch Frontend Exception fee int err Code String msg Unable to create field schema throw new Schema Merge Exception msg err Code Pig Exception fee return merged Fs Recursively set type to the specifid type param fs the field schema whose type has to be set param t the specified type public static void set Field Schema Default Type Schema Field Schema fs byte t if null fs return if Data Type fs type fs type t if Data Type is Schema Type fs type set Schema Default Type fs schema t private boolean is Null Or Unknown Type Field Schema fs return fs type Data Type fs type Data Type Find a field schema instance in this Field Schema hierarchy including this that matches the given canonical name param canonical Name canonical name return the Field Schema instance found public Field Schema find Field Schema String canonical Name if this canonical Name equals canonical Name return this if this schema null return schema find Field Schema canonical Name return null private List Field Schema m Fields private Map String Field Schema m Aliases private Multi Map String String m Field Schemas private static Log log Log Factory get Log Schema class In bags which have a schema with a tuple which contains the fields present in it if we access the second field say we are actually trying to access the second field in the tuple in the bag This is currently true for two cases bag constants the schema of bag constant has a tuple which internally has the actual elements When bags are loaded from input data if the user specifies a schema with the bag type he has to specify the bag as containing a tuple with the actual elements in the schema declaration However in both the cases above the user can still say b i where b is the bag and i is an element in the bag s tuple schema So in these cases the access should translate to a lookup for i in the tuple schema present in the bag To indicate this the flag below is used It is false by default because currently we use bag as the type for relations However the schema of a relation does have a tuple fieldschema with items in it Instead the schema directly has the field schema of the items So for a relation b the above b i access would be a direct single level access of i in b s schema This is treated as the default case private boolean two Level Access Required false public Schema m Fields new Array List Field Schema m Aliases new Hash Map String Field Schema m Field Schemas new Multi Map String String param fields List of field schemas that describes the fields public Schema List Field Schema fields m Fields fields m Aliases new Hash Map String Field Schema fields size m Field Schemas new Multi Map String String for Field Schema fs fields if null fs if fs alias null m Aliases put fs alias fs m Field Schemas put fs canonical Name fs alias Create a schema with only one field param field Schema field to put in this schema public Schema Field Schema field Schema m Fields new Array List Field Schema m Fields add field Schema m Aliases new Hash Map String Field Schema m Field Schemas new Multi Map String String if null field Schema if field Schema alias null m Aliases put field Schema alias field Schema m Field Schemas put field Schema canonical Name field Schema alias Copy Constructor param s source schema public Schema Schema s if null s two Level Access Required s two Level Access Required m Fields new Array List Field Schema s size m Aliases new Hash Map String Field Schema m Field Schemas new Multi Map String String try for int i i s size i Field Schema fs new Field Schema s get Field i m Fields add fs if null fs if fs alias null m Aliases put fs alias fs m Field Schemas put fs canonical Name fs alias catch Frontend Exception pe m Fields new Array List Field Schema m Aliases new Hash Map String Field Schema m Field Schemas new Multi Map String String else m Fields new Array List Field Schema m Aliases new Hash Map String Field Schema m Field Schemas new Multi Map String String Given an alias name find the associated Field Schema param alias Alias to look up return Field Schema or null if no such alias is in this tuple public Field Schema get Field String alias throws Frontend Exception Field Schema fs m Aliases get alias if null fs String coco Prefix alias Map String Integer alias Matches new Hash Map String Integer build the map of aliases that have coco Prefix as the suffix for String key m Aliases key Set if key ends With coco Prefix Integer count alias Matches get key if null count alias Matches put key else alias Matches put key count process the map to check if are there multiple keys with count are there keys with count should never occur if thers is a single key with count we have our match if alias Matches key Set size return null if alias Matches key Set size Object keys alias Matches key Set to Array String key String keys if alias Matches get key int err Code throw new Frontend Exception Found duplicate aliases key err Code Pig Exception return m Aliases get key else check if the multiple aliases obtained actually point to the same field schema then just return that field schema Set Field Schema set new Hash Set Field Schema for String key alias Matches key Set set add m Aliases get key if set size return set iterator next boolean has Next false String Builder sb new String Builder Found more than one match for String key alias Matches key Set if has Next sb append else has Next true sb append key int err Code throw new Frontend Exception sb to String err Code Pig Exception else return fs Given an alias name find the associated Field Schema If exact name is not found see if any field matches the part of the namespaced alias eg if given alias is nm a and schema is a b It will return Field Schema of a if given alias is nm a and schema is nm a b it will return null param alias Alias to look up return Field Schema or null if no such alias is in this tuple public Field Schema get Field Sub Name Match String alias throws Frontend Exception if alias null return null Field Schema fs get Field alias if fs null return fs fs is null final String sep Array List Field Schema matched Field Schemas new Array List Field Schema if alias contains sep for Field Schema field m Fields if alias ends With sep field alias matched Field Schemas add field if matched Field Schemas size boolean has Next false String Builder sb new String Builder Found more than one sub alias name match for Field Schema match Fs matched Field Schemas if has Next sb append else has Next true sb append match Fs alias int err Code throw new Frontend Exception sb to String err Code Pig Exception else if matched Field Schemas size fs matched Field Schemas get return fs Given a field number find the associated Field Schema param field Num Field number to look up return Field Schema for this field throws Parse Exception if the field number exceeds the number of fields in the tuple public Field Schema get Field int field Num throws Frontend Exception if field Num m Fields size int err Code String detailed Msg Attempt to access field field Num from schema this String msg Attempt to fetch field field Num from schema of size m Fields size throw new Frontend Exception msg err Code Pig Exception false detailed Msg return m Fields get field Num Find the number of fields in the schema return number of fields public int size return m Fields size Reconcile this schema with another schema The schema being reconciled with should have the same number of columns The use case is where a schema already exists but may not have alias and or type information If an alias exists in this schema and a new one is given then the new one will be used Similarly with types though this needs to be used carefully as types should not be lightly changed param other Schema to reconcile with throws Parse Exception if this can not be reconciled public void reconcile Schema other throws Frontend Exception if other null if other size size int err Code String msg Can not reconcile schemas with different sizes This schema has size size other has size of other size String detailed Msg Schema size mismatch This schema this other schema other throw new Frontend Exception msg err Code Pig Exception false detailed Msg Iterator Field Schema i other m Fields iterator for int j i has Next j Field Schema other Fs i next Field Schema our Fs m Fields get j log debug our Fs our Fs other Fs other Fs if other Fs alias null log debug other Fs alias other Fs alias if our Fs alias null log debug Removing our Fs alias our Fs alias m Aliases remove our Fs alias Collection String aliases m Field Schemas get our Fs canonical Name if aliases null List String list Aliases new Array List String for String alias aliases list Aliases add alias for String alias list Aliases log debug Removing alias alias from multimap m Field Schemas remove our Fs canonical Name alias our Fs alias other Fs alias log debug Setting alias to other Fs alias m Aliases put our Fs alias our Fs if null our Fs alias m Field Schemas put our Fs canonical Name our Fs alias if other Fs type Data Type our Fs type other Fs type log debug Setting type to Data Type find Type Name other Fs type if other Fs schema null our Fs schema other Fs schema log debug Setting schema to other Fs schema For two schemas to be equal they have to be deeply equal Use Schema equals Schema schema Schema other boolean relax Inner boolean relax Alias if relaxation of aliases is a requirement Override public boolean equals Object other if other instanceof Schema return false Schema s Schema other return Schema equals this s false false Make a deep copy of a schema throws Clone Not Supported Exception Override public Schema clone throws Clone Not Supported Exception Schema s new Schema Build a map between old and new field schemas so we can properly construct the new alias and field schema maps Populate the field list with copies of the existing field schemas Map Field Schema Field Schema fs Map new Hash Map Field Schema Field Schema size Map String Field Schema fs Canonical Name Map new Hash Map String Field Schema size for Field Schema fs m Fields Field Schema copy fs clone s m Fields add copy fs Map put fs copy fs Canonical Name Map put fs canonical Name copy Build the aliases map for String alias m Aliases key Set Field Schema old Fs m Aliases get alias assert old Fs null Field Schema new Fs fs Map get old Fs assert new Fs null s m Aliases put alias new Fs Build the field schemas map for String old Fs Canonical Name m Field Schemas key Set Field Schema new Fs fs Canonical Name Map get old Fs Canonical Name assert new Fs null s m Field Schemas put new Fs canonical Name m Field Schemas get old Fs Canonical Name s two Level Access Required two Level Access Required return s static int prime List Override public int hash Code int idx int hash Code for Field Schema fs this m Fields hash Code fs hash Code prime List idx prime List length idx return hash Code Override public String to String return to Indented String Integer public String pretty Print return to Indented String private String to Indented String int indent Level String Builder sb new String Builder try stringify Schema sb this Data Type indent Level catch Frontend Exception fee throw new Runtime Exception return sb to String public static void stringify Schema String Builder sb Schema schema byte type throws Frontend Exception stringify Schema sb schema type This is used for building up output string type can only be or public static void stringify Schema String Builder sb Schema schema byte type int indent Level throws Frontend Exception if type Data Type sb append else if type Data Type sb append indent Level if schema null boolean is First true for int i i schema size i if is First sb append else is First false indent sb indent Level Field Schema fs schema get Field i if fs null continue if fs alias null sb append fs alias sb append if Data Type is Atomic fs type sb append Data Type find Type Name fs type else if fs type Data Type fs type Data Type safety net if schema fs schema stringify Schema sb fs schema fs type indent Level else throw new Assertion Error Schema refers to itself as inner schema else if fs type Data Type sb append Data Type find Type Name fs type if fs schema null stringify Schema sb fs schema fs type indent Level sb append else sb append Data Type find Type Name fs type indent Level indent sb indent Level if type Data Type sb append else if type Data Type sb append no op if indent Level is negative br otherwise print newline and indent Level spaces private static void indent String Builder sb int indent Level if indent Level sb append n while indent Level sb append spaces public void add Field Schema f m Fields add f if null f m Field Schemas put f canonical Name f alias if null f alias m Aliases put f alias f Given an alias find the associated position of the field schema param alias alias of the Field Schema return position of the Field Schema public int get Position String alias throws Frontend Exception return get Position alias false Given an alias find the associated position of the field schema It uses get Field Sub Name Match to look for sub Name matches as well param alias alias of the Field Schema return position of the Field Schema public int get Position Sub Name String alias throws Frontend Exception return get Position alias true private int get Position String alias boolean is Sub Name Match throws Frontend Exception if is Sub Name Match two Level Access Required should not happen int err Code String msg two Level Access Required true is not supported with and is Sub Name Match true throw new Frontend Exception msg err Code Pig Exception if two Level Access Required this is the case where this schema is that of a bag which has just one tuple fieldschema which in turn has a list of fieldschemas The alias supplied should be treated as an alias in the tuple s schema check that indeed we only have one field schema which is that of a tuple if m Fields size int err Code String msg Expected a bag schema with a single element of type Data Type find Type Name Data Type but got a bag schema with multiple elements throw new Frontend Exception msg err Code Pig Exception Schema Field Schema tuple m Fields get if tuple type Data Type int err Code String msg Expected a bag schema with a single element of type Data Type find Type Name Data Type but got an element of type Data Type find Type Name tuple type throw new Frontend Exception msg err Code Pig Exception check if the alias supplied is that of the tuple itself then disallow it since we do not allow access to the tuple itself we only allow access to the fields in the tuple if alias equals tuple alias int err Code String msg Access to the tuple alias of the bag is disallowed Only access to the elements of the tuple in the bag is allowed throw new Frontend Exception msg err Code Pig Exception all is good get the position from the tuple s schema return tuple schema get Position alias else Field Schema fs is Sub Name Match get Field Sub Name Match alias get Field alias if null fs return log debug fs fs int index for int i i m Fields size i log debug m Fields i m Fields get i alias m Fields get i alias if fs m Fields get i index i log debug index index return index return m Fields index Of fs public void add Alias String alias Field Schema fs if null alias m Aliases put alias fs if null fs m Field Schemas put fs canonical Name alias public Set String get Aliases return m Aliases key Set public void print Aliases Set String alias Names m Aliases key Set for String alias alias Names log debug Schema Alias alias public List Field Schema get Fields return m Fields Recursively compare two schemas to check if the input schema can be cast to the cast schema param cast schema of the cast operator param input schema of the cast input return true or falsew public static boolean castable Schema cast Schema input If both of them are null they are castable if cast null input null return false otherwise if cast null return false if input null return false if cast size input size return false Iterator Field Schema i cast m Fields iterator Iterator Field Schema j input m Fields iterator while i has Next iterate only for the number of fields in cast Field Schema cast Fs i next Field Schema input Fs j next Compare recursively using field schema if Field Schema castable cast Fs input Fs return false return true Recursively compare two schemas for equality param schema param other param relax Inner if true inner schemas will not be checked param relax Alias if true aliases will not be checked return true if schemas are equal false otherwise public static boolean equals Schema schema Schema other boolean relax Inner boolean relax Alias If both of them are null they are equal if schema null other null return true otherwise if schema null return false if other null return false Need to check for bags with schemas and bags with tuples that in turn have schemas Retrieve the tuple schema of the bag if two Level Access Required Assuming that only bags exhibit this behavior and two Level Access Required is used with the right intentions if schema is Two Level Access Required other is Two Level Access Required if schema is Two Level Access Required try schema schema get Field schema catch Frontend Exception fee return false if other is Two Level Access Required try other other get Field schema catch Frontend Exception fee return false return Schema equals schema other relax Inner relax Alias if schema size other size return false Iterator Field Schema i schema m Fields iterator Iterator Field Schema j other m Fields iterator while i has Next Field Schema my Fs i next Field Schema other Fs j next if relax Alias if my Fs alias null other Fs alias null good else if my Fs alias null other Fs alias null return false else if my Fs alias null other Fs alias null return false else if my Fs alias equals other Fs alias return false if my Fs type other Fs type return false if relax Inner Compare recursively using field schema if Field Schema equals my Fs other Fs false relax Alias return false return true Merge this schema with the other schema param other the other schema to be merged with param other Takes Alias Precedence true if aliases from the other schema take precedence return the merged schema null if they are not compatible public Schema merge Schema other boolean other Takes Alias Precedence return merge Schema this other other Takes Alias Precedence Recursively merge two schemas param schema the initial schema param other the other schema to be merged with param other Takes Alias Precedence true if aliases from the other schema take precedence return the merged schema null if they are not compatible public static Schema merge Schema Schema schema Schema other boolean other Takes Alias Precedence try Schema new Schema merge Schema schema other other Takes Alias Precedence false false return new Schema catch Schema Merge Exception sme just mean they are not compatible return null Recursively merge two schemas param schema the initial schema param other the other schema to be merged with param other Takes Alias Precedence true if aliases from the other schema take precedence param allow Different Size Merge allow merging of schemas of different types param allow Incompatible Types if types in schemas are not compatible they will be treated as Byte Array untyped if schemas in schemas are not compatible and allow Incompatible Types is true those inner schemas in the output will be null return the merged schema this can be null if one schema is null and allow Incompatible Types is true throws Schema Merge Exception if they can not be merged public static Schema merge Schema Schema schema Schema other boolean other Takes Alias Precedence boolean allow Different Size Merge boolean allow Incompatible Types throws Schema Merge Exception if schema null other null if both are null they are not incompatible return null if schema null if allow Incompatible Types return null else int err Code String msg One of the schemas is null for merging schemas Schema schema Other schema other throw new Schema Merge Exception msg err Code Pig Exception if other null if allow Incompatible Types return null else int err Code String msg One of the schemas is null for merging schemas Schema schema Other schema other throw new Schema Merge Exception msg err Code Pig Exception if schema size other size allow Different Size Merge int err Code String msg Different schema sizes for merging schemas Schema size schema size Other schema size other size throw new Schema Merge Exception msg err Code Pig Exception List Field Schema output List new Array List Field Schema List Field Schema mylist schema m Fields List Field Schema otherlist other m Fields We iterate up to the smaller one s size int iterate Limit schema m Fields size other m Fields size other m Fields size schema m Fields size int idx for idx iterate Limit idx Just for readability Field Schema my Fs mylist get idx Field Schema other Fs otherlist get idx byte merged Type Data Type merge Type my Fs type other Fs type If the types can not be merged if merged Type Data Type If treat Incompatible As Byte Array is true we will treat it as bytearray if allow Incompatible Types merged Type Data Type otherwise the schemas can not be merged else int err Code String msg Incompatible types for merging schemas Field schema type Data Type find Type Name my Fs type Other field schema type Data Type find Type Name other Fs type throw new Schema Merge Exception msg err Code Pig Exception String merged Alias merge Alias my Fs alias other Fs alias other Takes Alias Precedence Field Schema merged Fs null if Data Type is Schema Type merged Type just normal merge merged Fs new Field Schema merged Alias merged Type else merge inner tuple because both sides are tuples if inner schema are incompatible and allow Incompatible Types true an exception is thrown by merge Schema Schema merged Sub Schema merge Schema my Fs schema other Fs schema other Takes Alias Precedence allow Different Size Merge allow Incompatible Types create the merged field the merged Sub Schema can be true if allow Incompatible Types try merged Fs new Field Schema merged Alias merged Sub Schema merged Type catch Frontend Exception e int err Code String err Msg Internal Error Unexpected error creating field schema throw new Schema Merge Exception err Msg err Code Pig Exception e output List add merged Fs Handle different schema size if allow Different Size Merge if the first schema has leftover then append the rest for int i idx i mylist size i Field Schema fs mylist get i for non schema types if Data Type is Schema Type fs type output List add new Field Schema fs alias fs type for else Field Schema tmp new Field Schema fs alias fs schema tmp type fs type output List add tmp if the second schema has leftover then append the rest for int i idx i otherlist size i Field Schema fs otherlist get i for non schema types if Data Type is Schema Type fs type output List add new Field Schema fs alias fs type for else Field Schema tmp new Field Schema fs alias fs schema tmp type fs type output List add tmp Schema result new Schema output List if schema is Two Level Access Required other is Two Level Access Required int err Code String err Msg Can not merge schema schema and other One with two Lever Access flag the other does n t throw new Schema Merge Exception err Msg err Code Pig Exception if schema is Two Level Access Required result set Two Level Access Required true return result Merge two aliases If one of aliases is null return the other Otherwise check the precedence condition param alias param other param other Takes Precedence return private static String merge Alias String alias String other boolean other Takes Precedence if alias null return other else if other null return alias else if other Takes Precedence return other else return alias Merges collection of schemas using their column aliases unlike merge Schema functions which merge using positions Schema will not be merged if types are incompatible as per Data Type merge Type For Tuples and Bags Sub Schemas have to be equal be considered compatible param schemas list of schemas to be merged using their column alias return merged schema throws Schema Merge Exception public static Schema merge Schemas By Alias Collection Schema schemas throws Schema Merge Exception Schema merged Schema null list of schemas that have currently been merged used in error message Array List Schema merged Schemas new Array List Schema schemas size for Schema sch schemas if merged Schema null merged Schema new Schema sch merged Schemas add sch continue try merged Schema merge Schema By Alias merged Schema sch merged Schemas add sch catch Schema Merge Exception e String msg Error merging schema sch with merged schema merged Schema of schemas merged Schemas Schema Merge Exception sme new Schema Merge Exception msg e get Error Code e sme set Marked As Show To User true throw sme return merged Schema Merges two schemas using their column aliases unlike merge Schema functions which merge using positions Schema will not be merged if types are incompatible as per Data Type merge Type For Tuples and Bags Sub Schemas have to be equal be considered compatible param schema param schema return Merged Schema throws Schema Merge Exception if schemas can not be merged public static Schema merge Schema By Alias Schema schema Schema schema throws Schema Merge Exception Schema merged Schema new Schema Hash Set Field Schema schema cols Added new Hash Set Field Schema add merge fields present in first schema for Field Schema fs schema get Fields check Null Alias fs schema Field Schema fs get Field Sub Name Match Throw Schema Merge Exception schema fs alias if fs null if schema cols Added contains fs alias corresponds to multiple fields in schema just do a lookup on schema that will throw the appropriate error get Field Sub Name Match Throw Schema Merge Exception schema fs alias schema cols Added add fs Field Schema merged Fs merge Field Schema First Level Same Alias fs fs merged Schema add merged Fs add schemas from nd schema that are not already present in merged schema for Field Schema fs schema get Fields check Null Alias fs schema if schema cols Added contains fs try merged Schema add fs clone catch Clone Not Supported Exception e throw new Schema Merge Exception Error encountered while merging schemas e return merged Schema private static void check Null Alias Field Schema fs Schema schema throws Schema Merge Exception if fs alias null throw new Schema Merge Exception Schema having field with null alias can not be merged using alias Schema schema Schema will not be merged if types are incompatible as per Data Type merge Type For Tuples and Bags Sub Schemas have to be equal be considered compatible Aliases are assumed to be same for both param fs param fs return throws Schema Merge Exception private static Field Schema merge Field Schema First Level Same Alias Field Schema fs Field Schema fs throws Schema Merge Exception if fs null return fs if fs null return fs Schema inner Schema null String alias merge Name Spaced Alias fs alias fs alias byte merged Type Data Type merge Type fs type fs type If the types can not be merged if merged Type Data Type int err Code String msg Incompatible types for merging schemas Field schema fs Other field schema fs throw new Schema Merge Exception msg err Code Pig Exception if Data Type is Schema Type merged Type if one of them is a bytearray pick inner schema of other one if fs type Data Type inner Schema fs schema else if fs type Data Type inner Schema fs schema else in case of types with inner schema such as bags and tuples the inner schema has to be same if equals fs schema fs schema false false int err Code String msg Incompatible types for merging inner schemas of Field schema type fs Other field schema type fs throw new Schema Merge Exception msg err Code Pig Exception inner Schema fs schema try return new Field Schema alias inner Schema merged Type catch Frontend Exception e this exception is not expected int err Code throw new Schema Merge Exception Error in creating field Schema err Code Pig Exception If one of the aliases is of form nm str and other is of the form str this returns str param alias param alias return merged alias throws Schema Merge Exception private static String merge Name Spaced Alias String alias String alias throws Schema Merge Exception if alias equals alias return alias if alias ends With alias return alias if alias ends With alias return alias the aliases are different alias can not be merged return null Utility function that calls schema get Filed alias and converts link Frontend Exception to link Schema Merge Exception param schema param alias return Field Schema throws Schema Merge Exception private static Field Schema get Field Sub Name Match Throw Schema Merge Exception Schema schema String alias throws Schema Merge Exception Field Schema fs null try fs schema get Field Sub Name Match alias catch Frontend Exception e String msg Caught exception finding Field Schema for alias alias throw new Schema Merge Exception msg e get Error Code e return fs param top Level Type Data Type type of the top level element param inner Types Data Type types of the inner level element return nested schema representing type of top level element at first level and inner schema representing types of inner element s public static Schema generate Nested Schema byte top Level Type byte inner Types throws Frontend Exception Schema inner Schema new Schema for int i i inner Types length i inner Schema add new Schema Field Schema null inner Types i Schema Field Schema outer Schema new Schema Field Schema null inner Schema top Level Type return new Schema outer Schema Recursively prefix merge two schemas param other the other schema to be merged with param other Takes Alias Precedence true if aliases from the other schema take precedence return the prefix merged schema this can be null if one schema is null and allow Incompatible Types is true throws Schema Merge Exception if they can not be merged public Schema merge Prefix Schema Schema other boolean other Takes Alias Precedence throws Schema Merge Exception return merge Prefix Schema other other Takes Alias Precedence false Recursively prefix merge two schemas param other the other schema to be merged with param other Takes Alias Precedence true if aliases from the other schema take precedence param allow Mergeable Types true if mergeable types should be allowed Two types are mergeable if any of the following conditions is true of checks if either one has a type null or unknown and other has a type null or unknown the result type will be the latter non null unknown type If either type is bytearray then result type will be the other possibly non type If current type can be cast to the other type then the result type will be the other type return the prefix merged schema this can be null if one schema is null and allow Incompatible Types is true throws Schema Merge Exception if they can not be merged public Schema merge Prefix Schema Schema other boolean other Takes Alias Precedence boolean allow Mergeable Types throws Schema Merge Exception Schema schema this if other null return this if schema size other size int err Code String msg Schema size mismatch for merging schemas Other schema size greater than schema size Schema this Other schema other throw new Schema Merge Exception msg err Code Pig Exception List Field Schema output List new Array List Field Schema List Field Schema mylist schema m Fields List Field Schema otherlist other m Fields We iterate up to the smaller one s size int iterate Limit other m Fields size int idx for idx iterate Limit idx Just for readability Field Schema my Fs mylist get idx Field Schema other Fs otherlist get idx Field Schema merged Fs my Fs merge Prefix Field Schema other Fs other Takes Alias Precedence allow Mergeable Types output List add merged Fs if the first schema has leftover then append the rest for int i idx i mylist size i Field Schema fs mylist get i for non schema types if Data Type is Schema Type fs type output List add new Field Schema fs alias fs type for else try Field Schema tmp new Field Schema fs alias fs schema fs type output List add tmp catch Frontend Exception fee int err Code String msg Unable to create field schema throw new Schema Merge Exception msg err Code Pig Exception fee Schema s new Schema output List s set Two Level Access Required other two Level Access Required return s Recursively set type to the specifid type in a schema param s the schema whose type has to be set param t the specified type public static void set Schema Default Type Schema s byte t if null s return for Schema Field Schema fs s get Fields Field Schema set Field Schema Default Type fs t return the two Level Access deprecated two Level Access is no longer needed Deprecated public boolean is Two Level Access Required return two Level Access Required param two Level Access the two Level Access to set deprecated two Level Access is no longer needed Deprecated public void set Two Level Access Required boolean two Level Access this two Level Access Required two Level Access public static Schema get Pig Schema Resource Schema r Schema throws Frontend Exception if r Schema null return null List Field Schema fs List new Array List Field Schema for Resource Field Schema rfs r Schema get Fields Field Schema fs new Field Schema rfs get Name rfs get Schema null null get Pig Schema rfs get Schema rfs get Type if rfs get Type Data Type if fs schema null allow partial schema if fs schema size Field Schema inner Fs fs schema get Field if inner Fs type Data Type Resource Field Schema throw Invalid Schema Exception else Resource Field Schema throw Invalid Schema Exception fs List add fs return new Schema fs List Look for a Field Schema instance in the schema hierarchy which has the given canonical name param canonical Name canonical name return the Field Schema instance found public Field Schema find Field Schema String canonical Name for Field Schema fs m Fields if fs canonical Name equals canonical Name return fs if fs schema null Field Schema result fs schema find Field Schema canonical Name if result null return result return null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical visitor import java util Hash Set import java util Set import org apache pig Pig Exception import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Plan Validation Exception import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator Plan import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Native import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema public class Schema Alias Visitor extends Logical Relational Nodes Visitor public Schema Alias Visitor Operator Plan plan throws Frontend Exception super plan new Dependency Order Walker plan The logic here is to check if we have duplicate alias in each schema throws Frontend Exception protected void validate Logical Relational Operator op throws Frontend Exception Logical Schema schema op get Schema Set String seen Aliases new Hash Set String if schema null for int i i schema size i if schema get Field i null schema get Field i alias null String alias schema get Field i alias if seen Aliases contains alias int err Code String msg Duplicate schema alias schema get Field i alias if op get Alias null msg msg in op get Alias throw new Plan Validation Exception op msg err Code Pig Exception seen Aliases add alias Override public void visit Store store throws Frontend Exception validate store Override public void visit Filter filter throws Frontend Exception validate filter Override public void visit Join join throws Frontend Exception validate join Override public void visit For Each foreach throws Frontend Exception new Schema Alias Visitor foreach get Inner Plan visit Override public void visit Generate gen throws Frontend Exception validate gen Override public void visit Inner Load load throws Frontend Exception validate load Override public void visit Cogroup group throws Frontend Exception validate group Override public void visit Split split throws Frontend Exception validate split Override public void visit Split Output split Output throws Frontend Exception validate split Output Override public void visit Union union throws Frontend Exception validate union Override public void visit Sort sort throws Frontend Exception validate sort Override public void visit Rank rank throws Frontend Exception validate rank Override public void visit Distinct distinct throws Frontend Exception validate distinct Override public void visit Limit limit throws Frontend Exception validate limit Override public void visit Cross cross throws Frontend Exception validate cross Override public void visit Stream stream throws Frontend Exception validate stream Override public void visit Native native throws Frontend Exception validate native 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical optimizer import java util Collection import java util Hash Set import java util List import java util Set import org apache pig Pig Exception import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Plan Validation Exception import org apache pig impl util Multi Map import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical expression All Same Expression Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Cube import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema public class Schema Resetter extends Logical Relational Nodes Visitor uid duplicates are removed only after optimizer rule Duplicate For Each Column Rewrite has run So disable it in calls before that boolean skip Duplicate Uid Check true private void visit All Collection Logical Expression Plan lexp Plans throws Frontend Exception for Logical Expression Plan exp Plan lexp Plans Field Schema Resetter fs Resetter new Field Schema Resetter exp Plan fs Resetter visit public Schema Resetter Operator Plan plan throws Frontend Exception this plan false public Schema Resetter Operator Plan plan boolean skip Duplicate Uid Check throws Frontend Exception super plan new Dependency Order Walker plan this skip Duplicate Uid Check skip Duplicate Uid Check Override public void visit Load load throws Frontend Exception load reset Schema validate load get Schema Override public void visit Filter filter throws Frontend Exception filter reset Schema Field Schema Resetter fs Resetter new Field Schema Resetter filter get Filter Plan fs Resetter visit validate filter get Schema Override public void visit Store store throws Frontend Exception store reset Schema validate store get Schema Override public void visit Join join throws Frontend Exception join reset Schema visit All join get Expression Plan Values validate join get Schema Override public void visit For Each foreach throws Frontend Exception foreach reset Schema Operator Plan inner Plan foreach get Inner Plan Plan Walker new Walker current Walker spawn Child Walker inner Plan push Walker new Walker current Walker walk this pop Walker validate foreach get Schema Override public void visit Generate gen throws Frontend Exception gen reset Schema visit All gen get Output Plans validate gen get Schema Override public void visit Inner Load load throws Frontend Exception load reset Schema load get Projection reset Field Schema load get Schema Override public void visit Cube lo Cube throws Frontend Exception lo Cube reset Schema visit All lo Cube get Expression Plans values validate lo Cube get Schema Override public void visit Cogroup lo Cogroup throws Frontend Exception lo Cogroup reset Schema visit All lo Cogroup get Expression Plans values validate lo Cogroup get Schema Override public void visit Split lo Split throws Frontend Exception lo Split reset Schema validate lo Split get Schema Override public void visit Split Output lo Split Output throws Frontend Exception lo Split Output reset Schema Field Schema Resetter fs Resetter new Field Schema Resetter lo Split Output get Filter Plan fs Resetter visit validate lo Split Output get Schema Override public void visit Union lo Union throws Frontend Exception lo Union reset Schema validate lo Union get Schema Override public void visit Sort lo Sort throws Frontend Exception lo Sort reset Schema visit All lo Sort get Sort Col Plans validate lo Sort get Schema Override public void visit Rank lo Rank throws Frontend Exception lo Rank reset Schema visit All lo Rank get Rank Col Plans validate lo Rank get Schema Override public void visit Distinct lo Distinct throws Frontend Exception lo Distinct reset Schema validate lo Distinct get Schema Override public void visit Limit lo Limit throws Frontend Exception lo Limit reset Schema if lo Limit get Limit Plan null Field Schema Resetter fs Resetter new Field Schema Resetter lo Limit get Limit Plan fs Resetter visit validate lo Limit get Schema Override public void visit Cross lo Cross throws Frontend Exception lo Cross reset Schema validate lo Cross get Schema Override public void visit Stream lo Stream throws Frontend Exception lo Stream reset Schema validate lo Stream get Schema Check if schema is valid ready to be part of a final logical plan param schema throws Plan Validation Exception if the if any field in schema has uid or skip Duplicate Uid Check is true and there are duplicate uids in schema public void validate Logical Schema schema throws Plan Validation Exception if schema null return Set Long uids Seen new Hash Set Long for Logical Field Schema fs schema get Fields if skip Duplicate Uid Check check duplicate uid if uids Seen add fs uid uid already seen String msg Logical plan invalid state duplicate uid in schema schema throw new Plan Validation Exception msg Pig Exception if fs uid String msg Logical plan invalid state invalid uid fs uid in schema schema throw new Plan Validation Exception msg Pig Exception class Field Schema Resetter extends All Same Expression Visitor protected Field Schema Resetter Operator Plan p throws Frontend Exception super p new Reverse Dependency Order Walker p Override protected void execute Logical Expression op throws Frontend Exception op reset Field Schema op get Field Schema 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import static org apache pig Pig Configuration import static org apache pig Pig Constants import java io File import java io File Input Stream import java io File Output Stream import java io Exception import java net Malformed Exception import java net import java net Class Loader import java util Map import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache pig Pig Constants import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data utils Structures Helper Schema Key import org apache pig data utils Structures Helper Triple import org apache pig impl Pig Context import org apache pig impl logical Layer schema Schema import org apache pig impl util Utils import com google common collect Maps import com google common collect Sets import com google common io Files public class Schema Tuple Backend private static final Log Log Factory get Log Schema Tuple Backend class private Set String files To Resolve Sets new Hash Set We use the Class Loader to resolve the generated classes because we can simply give it the directory we put all of the compiled class files into and it will handle the dynamic loading private Class Loader class Loader private Map Triple Schema Key Boolean Gen Context Schema Tuple Factory schema Tuple Factories By Triple Maps new Hash Map private Map Integer Schema Tuple Factory schema Tuple Factories By Id Maps new Hash Map private Configuration j Conf private File code Dir private boolean is Local private boolean abort false The only information this class needs is a directory of generated code to resolve classes in param j Conf param directory of generated code private Schema Tuple Backend Configuration j Conf boolean is Local if is Local String local Code Dir j Conf get Pig Constants if local Code Dir null debug No local code dir set in local mode Aborting code gen resolution abort true return code Dir new File j Conf get Pig Constants else code Dir Files create Temp Dir code Dir delete On Exit try class Loader new Class Loader new code Dir to to catch Malformed Exception e throw new Runtime Exception Unable to make Class Loader for temp Dir code Dir get Absolute Path this j Conf j Conf this is Local is Local This method fetches the Schema Tuple Factory that can create Tuples of the given Schema ignoring aliases and appendability if no such Schema Tuple Factory is available this returns null param schema param true if it should be appendable param the context in which this Schema Tuple Factory is being requested return generating Schema Tuple Factory null otherwise private Schema Tuple Factory internal New Schema Tuple Factory Schema s boolean is Appendable Gen Context context return new Schema Tuple Factory Triple make new Schema Key s is Appendable context This method fetches the Schema Tuple Factory that generates the Schema Tuple registered with the given identifier if no such Schema Tuple Factory is available this returns null param identifier return generating schema Tuple Factory null otherwise private Schema Tuple Factory internal New Schema Tuple Factory int id Schema Tuple Factory stf schema Tuple Factories By Id get id if stf null debug No Schema Tuple Factory present for given identifier id return stf This method fetches the Schema Tuple Factory that can create Tuples of the given Schema and appendability if no such Schema Tuple Factory is available this returns null param Schema Key appendability pair return generating Schema Tuple Factory null otherwise private Schema Tuple Factory new Schema Tuple Factory Triple Schema Key Boolean Gen Context trip Schema Tuple Factory stf schema Tuple Factories By Triple get trip if stf null debug No Schema Tuple Factory present for given Schema Key Boolean Context combination trip return stf This method copies all of the generated classes from the distributed cache to a local directory and then seeks to resolve them and cache their respective Schema Tuple Factories param configuration param true if the job is local throws Exception private void copy And Resolve throws Exception if abort debug Nothing to resolve on the backend return Step one is to see if there are any classes in the distributed cache if j Conf get Boolean info Key was not set will not generate code return Step two is to copy everything from the distributed cache if we are in distributed mode if is Local copy All From Distributed Cache Step three is to see if the file needs to be resolved If there is a in the name we know that it is an inner class and thus does n t need to be instantiated directly for File f code Dir list Files String name f get Name split if name contains files To Resolve add name info Added class to list of class to resolve name Step four is to actually try and resolve the classes resolve Classes private void copy All From Distributed Cache throws Exception String to Deserialize j Conf get Pig Constants if to Deserialize null info No classes in in key Pig Constants to copy from distributed cache return info Copying files in key Pig Constants from distributed cache to Deserialize for String s to Deserialize split info Attempting to read file s The string is the symlink into the distributed cache File src new File s File Input Stream fin null File Output Stream fos null try fin new File Input Stream src fos new File Output Stream new File code Dir s fin get Channel transfer To src length fos get Channel info Successfully copied file to local directory finally if fin null fin close if fos null fos close Once all of the files are copied from the distributed cache to the local temp directory this will attempt to resolve those files and add their information Suppress Warnings unchecked private void resolve Classes for String s files To Resolve Schema Tuple Factory info Attempting to resolve class s Step one is to simply attempt to get the class object from the classloader that includes the generated code Class clazz try clazz class Loader load Class s catch Class Not Found Exception e throw new Runtime Exception Unable to find class s e Step three is to check if the class is a Schema Tuple If it is n t we do not attempt to resolve it because it is support code such as anonymous classes if Schema Tuple class is Assignable From clazz return Class Schema Tuple st Class Class Schema Tuple clazz Step four is to actually try to create the Schema Tuple instance Schema Tuple st try st st Class new Instance catch Instantiation Exception e throw new Runtime Exception Error instantiating file s e catch Illegal Access Exception e throw new Runtime Exception Error accessing file s e Step five is to get information about the class boolean is Appendable st instanceof Appendable Schema Tuple int id st get Schema Tuple Identifier Schema schema st get Schema Schema Tuple Factory stf new Schema Tuple Factory st Class st get Quick Generator for Gen Context context Gen Context values if context Gen Context context should Generate st Class Schema Tuple Factory debug Context context not present for class skipping continue the Schema Key Schema sans alias and appendability are how we will uniquely identify a Schema Tuple Factory Triple Schema Key Boolean Gen Context trip Triple make new Schema Key schema is Appendable context schema Tuple Factories By Triple put trip stf Schema Tuple Factory info Successfully resolved class for schema schema and appendability is Appendable in context context schema Tuple Factories By Id put id stf public static void reset stb null private static Schema Tuple Backend stb public static void initialize Configuration j Conf Pig Context pig Context throws Exception if stb null Schema Tuple Frontend lazy Reset pig Context initialize j Conf pig Context get Exec Type is Local public static void initialize Configuration j Conf throws Exception initialize j Conf Utils is Local j Conf public static void initialize Configuration j Conf boolean is Local throws Exception if stb null warn Schema Tuple Backend has already been initialized else Schema Tuple Frontend reset Schema Tuple Backend stb Instance new Schema Tuple Backend j Conf is Local stb Instance copy And Resolve stb stb Instance public static Schema Tuple Factory new Schema Tuple Factory Schema s boolean is Appendable Gen Context context if stb null It is possible though ideally should be avoided for this to be called on the frontend if the Tuple processing path of the Plan is invoked perhaps for optimization purposes throw new Runtime Exception initialize was not called Even when Schema Tuple feature is not set it should be called return stb internal New Schema Tuple Factory s is Appendable context protected static Schema Tuple Factory new Schema Tuple Factory int id if stb null It is possible though ideally should be avoided for this to be called on the frontend if the Tuple processing path of the Plan is invoked perhaps for optimization purposes throw new Runtime Exception initialize was not called Even when Schema Tuple feature is not set it should be called return stb internal New Schema Tuple Factory id 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import static org apache pig Pig Configuration import static org apache pig Pig Constants import static org apache pig Pig Constants import static org apache pig Pig Constants import java io File import java io Exception import java net import java net Syntax Exception import java util Map import java util Properties import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop filecache Distributed Cache import org apache hadoop fs File System import org apache hadoop fs Path import org apache pig backend hadoop executionengine map Reduce Layer Configuration import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data utils Structures Helper Pair import org apache pig data utils Structures Helper Schema Key import org apache pig impl Pig Context import org apache pig impl io File Localizer import org apache pig impl logical Layer schema Schema import com google common collect Maps import com google common collect Sets import com google common io Files This class is to be used at job creation time It provides the that lets code register Schemas with pig to be generated It is necessary to register these Schemas and reducers public class Schema Tuple Frontend private static final Log Log Factory get Log Schema Tuple Frontend class private static Schema Tuple Frontend stf Schemas registered for generation are held here private static Map Pair Schema Key Boolean Pair Integer Set Gen Context schemas To Generate Maps new Hash Map private int internal Register To Generate If Possible Schema udf Schema boolean is Appendable Gen Context type Pair Schema Key Boolean key Pair make new Schema Key udf Schema is Appendable Pair Integer Set Gen Context pr schemas To Generate get key if pr null pr get Second add type return pr get First if Schema Tuple Factory is Generatable udf Schema debug Given Schema is not generatable udf Schema return int id Schema Tuple Class Generator get Next Global Class Identifier Set Gen Context contexts Sets new Hash Set contexts add Gen Context contexts add type schemas To Generate put key Pair make Integer value Of id contexts debug Registering is Appendable Appendable Schema for generation udf Schema with id id and context type return id private Map Pair Schema Key Boolean Pair Integer Set Gen Context get Schemas To Generate return schemas To Generate private static class Schema Tuple Frontend Gen Helper private File code Dir private Pig Context pig Context private Configuration conf public Schema Tuple Frontend Gen Helper Pig Context pig Context Configuration conf code Dir Files create Temp Dir code Dir delete On Exit debug Temporary directory for generated code created code Dir get Absolute Path this pig Context pig Context this conf conf This method copies all class files present in the local temp directory to the distributed cache All copied files will have a symlink of their name No files will be copied if the current job is being run from local mode param pig Context param conf private void internal Copy All Generated To Distributed Cache info Starting process to move generated code to distributed cacche if pig Context get Exec Type is Local String code Path code Dir get Absolute Path info Distributed cache not supported or needed in local mode Setting key with code temp directory code Path conf set code Path return else This let s us avoid in some of the non traditional pipelines String code Path code Dir get Absolute Path conf set code Path Distributed Cache create Symlink conf we will read using symlinks String Builder serialized new String Builder boolean first true We attempt to copy over every file in the generated code temp directory for File f code Dir list Files if first first false else serialized append String symlink f get Name the class name will also be the symlink serialized append symlink Path src new Path f to Path dst try dst File Localizer get Temporary Path pig Context catch Exception e throw new Runtime Exception Error getting temporary path in e File System fs try fs dst get File System conf catch Exception e throw new Runtime Exception Unable to get File System e try fs copy From Local File src dst fs set Replication dst short conf get Int Configuration catch Exception e throw new Runtime Exception Unable to copy from local filesystem to src src dst dst e String destination dst to String symlink try Distributed Cache add Cache File new destination conf catch Syntax Exception e throw new Runtime Exception Unable to add file to distributed cache destination e info File successfully added to the distributed cache symlink String to Ser serialized to String info Setting key with classes to deserialize to Ser we must set a key in the job conf so individual jobs know to resolve the shipped classes conf set to Ser This sets into motion the generation of all registered Schemas All code will be generated into the temporary directory return true of false depending on if there are any files to copy to the distributed cache private boolean generate All Map Pair Schema Key Boolean Pair Integer Set Gen Context schemas To Generate boolean files To Ship false if conf get Boolean info Key is false will not generate code return false info Generating all registered Schemas for Map Entry Pair Schema Key Boolean Pair Integer Set Gen Context entry schemas To Generate entry Set Pair Schema Key Boolean key Pair entry get Key Schema s key Pair get First get Pair Integer Set Gen Context value Pair entry get Value Set Gen Context contexts To Include Sets new Hash Set boolean is Shipping false for Gen Context context value Pair get Second if context should Generate conf info Skipping generation of Schema s as key value context key was false else is Shipping true contexts To Include add context if is Shipping continue int id value Pair get First boolean is Appendable key Pair get Second Schema Tuple Class Generator generate Schema Tuple s is Appendable id code Dir contexts To Include to Array new Gen Context files To Ship true return files To Ship This allows the frontend backend process to be repeated if on the same as in testing public static void reset stf null schemas To Generate clear This method registers a Schema to be generated It allows a portions of the code to register a Schema for generation without knowing whether code generation is enabled unique will be passed back that can be used internally to refer to generated Schema Tuples such as in the case of serialization and deserialization The context is necessary to allow the client to restrict where generated code can be used param udf Schema This is the Schema of a Tuple that we will potentially generate param is Appendable This specifies whether or not we want the Schema Tuple to be appendable param context This is the context in which users should be able to access the Schema Tuple return identifier public static int register To Generate If Possible Schema udf Schema boolean is Appendable Gen Context context if stf null if pig Context To Reset null Properties prop pig Context To Reset get Properties prop remove prop remove pig Context To Reset null Schema Tuple Backend reset Schema Tuple Class Generator reset Global Class Identifier stf new Schema Tuple Frontend if udf Schema null return try udf Schema udf Schema clone catch Clone Not Supported Exception e throw new Runtime Exception Unable to clone Schema udf Schema e strip Aliases udf Schema return stf internal Register To Generate If Possible udf Schema is Appendable context private static void strip Aliases Schema s for Schema Field Schema fs s get Fields fs alias null if fs schema null strip Aliases fs schema This must be called when the code has been generated and the generated code needs to be shipped to the cluster so that it may be used by the mappers and reducers param pig Context param conf public static void copy All Generated To Distributed Cache Pig Context pig Context Configuration conf if stf null debug Nothing registered to generate return Schema Tuple Frontend Gen Helper stfgh new Schema Tuple Frontend Gen Helper pig Context conf stfgh generate All stf get Schemas To Generate stfgh internal Copy All Generated To Distributed Cache Properties prop pig Context get Properties String value conf get if value null prop set Property value else prop remove value conf get if value null prop set Property value else prop remove private static Pig Context pig Context To Reset null This is a method which caches a Pig Context object that has had relevant key values set by Schema Tuple Backend This is necessary because in some cases multiple cycles of jobs might run in the but the Pig Context object may be shared so we want to make sure to undo any changes we have made to it protected static void lazy Reset Pig Context pig Context pig Context To Reset pig Context 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig scripting import java io Buffered Reader import java io File import java io File Input Stream import java io File Not Found Exception import java io File Reader import java io Exception import java io Input Stream import java net import java util Array List import java util Collections import java util Hash Map import java util Hash Set import java util List import java util Map import java util Set import java util regex Pattern import org apache hadoop util Shell import org apache pig backend executionengine Exec Exception import org apache pig impl Pig Context import org apache pig impl util Context import org apache pig tools pigstats Pig Stats Base class for various scripting implementations public abstract class Script Engine public static enum Supported Script Lang jruby new String ruby jruby new String rb org apache pig scripting jruby Jruby Script Engine jython new String python jython new String py org apache pig scripting jython Jython Script Engine javascript new String new String js org apache pig scripting js Js Script Engine groovy new String new String groovy org apache pig scripting groovy Groovy Script Engine streaming python new String streaming python new String org apache pig scripting streaming python Python Script Engine private static Set String supported Script Langs static supported Script Langs new Hash Set String for Supported Script Lang value Supported Script Lang values supported Script Langs add value name supported Script Langs Collections unmodifiable Set supported Script Langs public static boolean contains String supported Script Lang return supported Script Langs contains supported Script Lang private String shebangs private String extensions Class implementing the engine As a string as dependencies are possibly not on the class path private String engine Class Name private Supported Script Lang String shebangs String extensions String engine Class Name this shebangs shebangs this extensions extensions this engine Class Name engine Class Name If other discovery mechanisms come up they can also override accepts param file the path of the file param first Line The first line of the file possibly containing public boolean accepts String file String first Line if first Line null return false for String shebang shebangs Pattern p Pattern compile shebang s if p matcher first Line matches return true for String ext extensions if file ends With ext return true return false public String get Engine Class Name return engine Class Name private static final Pattern shebang Pattern Pattern compile private static boolean declares Shebang String first Line if first Line null return false return shebang Pattern matcher first Line matches open a stream load a script locally or in the classpath param script Path the path of the script return a stream it is the responsibility of the caller to close it throws Illegal State Exception if we could not open a stream public static Input Stream get Script As Stream String script Path protected static Input Stream get Script As Stream String script Path Input Stream is null File file new File script Path In the frontend give preference to the local file In the backend try the jar first if Context get Context is Frontend file exists try is new File Input Stream file catch File Not Found Exception e throw new Illegal State Exception could not find existing file script Path e else if Shell script Path char At script Path script Path char At script Path substring Try system current and context classloader is Script Engine class get Resource As Stream script Path if is null is get Resource Using Class Loader script Path Script Engine class get Class Loader if is null is get Resource Using Class Loader script Path Thread current Thread get Context Class Loader if is null file is Absolute String path script Path is Script Engine class get Resource As Stream path if is null is get Resource Using Class Loader path Script Engine class get Class Loader if is null is get Resource Using Class Loader path Thread current Thread get Context Class Loader if is null file exists try is new File Input Stream file catch File Not Found Exception e throw new Illegal State Exception could not find existing file script Path e discuss if we want to add logic here to load a script from if is null throw new Illegal State Exception Could not initialize interpreter from file system or classpath with script Path return is private static Input Stream get Resource Using Class Loader String full Filename Class Loader loader if loader null return loader get Resource As Stream full Filename return null public static final String param file the file to inspect return the Supported Script Lang if this is a supported script language throws Exception if there was an error reading the file or if the file defines explicitly an unknown public static Supported Script Lang get Supported Script Lang String file throws Exception Buffered Reader br new Buffered Reader new File Reader file String first Line try first Line br read Line finally br close for Supported Script Lang supported Script Lang Supported Script Lang values if supported Script Lang accepts file first Line return supported Script Lang if declares Shebang first Line throw new Exception Unsupported script type is specified first Line return null private Map String List Pig Stats stats Map new Hash Map String List Pig Stats Registers scripting language functions as Pig functions with given namespace param path path of the script param namespace namespace for the functions param pig Context pigcontext to register functions to pig in the given namespace throws Exception public abstract void register Functions String path String namespace Pig Context pig Context throws Exception Actually runs the script file This method will be implemented by individual script engines param context link Script Pig Context to run the script file param script File the file throws Exception protected abstract Map String List Pig Stats main Pig Context context String script File throws Exception Not needed as a general abstraction so far Loads the script in the interpreter param script protected abstract void load Input Stream script throws Exception Gets Script Engine classname or keyword for the scripting language protected abstract String get Scripting Lang Returns a map from local variable names to their values throws Exception protected abstract Map String Object get Params From Variables throws Exception Figures out the jar location from the class param clazz class in the jar file return the jar file location null if the class was not loaded from a jar throws File Not Found Exception public static String get Jar Path Class clazz throws File Not Found Exception resource clazz get Class Loader get Resource clazz get Canonical Name replace class if resource get Protocol equals jar return resource get Path substring resource get Path index Of resource get Path index Of throw new File Not Found Exception Jar for clazz get Name class is not found Gets instance of the script Engine for the given scripting language param scripting Lang Script Engine classname or keyword for the scripting Lang return scriptengine for the given scripting language throws Exception public static Script Engine get Instance String scripting Lang throws Exception String scripting Engine scripting Lang try if Supported Script Lang contains scripting Lang Supported Script Lang supported Script Lang Supported Script Lang value Of scripting Lang scripting Engine supported Script Lang get Engine Class Name return Script Engine Class for Name scripting Engine new Instance catch Exception e throw new Exception Could not load Script Engine scripting Engine for scripting Lang Supported langs Supported Script Lang supported Script Langs e e Runs a script file param pig Context link Pig Context to run the script file param script File the file return a collection of link Pig Stats objects One for each runtime instance of link Pig in the script For named pipelines the map key is the name of the pipeline For unnamed pipeline the map key is the script id associated with the instance throws Exec Exception throws Exception public Map String List Pig Stats run Pig Context pig Context String script File throws Exec Exception Exception Script Pig Context set pig Context this return main pig Context script File Gets the collection of link Pig Stats after the script is run protected Map String List Pig Stats get Pig Stats Map return stats Map void set Pig Stats String key Pig Stats stats List Pig Stats lst stats Map get key if lst null lst new Array List Pig Stats stats Map put key lst lst add stats void set Pig Stats String key List Pig Stats stats stats Map put key stats 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats import java io Buffered Reader import java io File import java io File Not Found Exception import java io File Reader import java io Exception import java util Bit Set import java util Hash Set import java util List import java util Map import java util import java util jar Attributes import java util jar Jar File import java util jar Manifest import org apache commons codec binary Base import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop util Version Info import org apache pig Pig Configuration import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer Physical Operator Original Location import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer relational Operators Collected Group import org apache pig backend hadoop executionengine physical Layer relational Operators Demux import org apache pig backend hadoop executionengine physical Layer relational Operators Distinct import org apache pig backend hadoop executionengine physical Layer relational Operators Join import org apache pig backend hadoop executionengine physical Layer relational Operators Filter import org apache pig backend hadoop executionengine physical Layer relational Operators For Each import org apache pig backend hadoop executionengine physical Layer relational Operators Limit import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Local Rearrange import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Cogroup import org apache pig backend hadoop executionengine physical Layer relational Operators Merge Join import org apache pig backend hadoop executionengine physical Layer relational Operators Partial Agg import org apache pig backend hadoop executionengine physical Layer relational Operators Skewed Join import org apache pig backend hadoop executionengine physical Layer relational Operators Sort import org apache pig backend hadoop executionengine physical Layer relational Operators Split import org apache pig backend hadoop executionengine physical Layer relational Operators Stream import org apache pig backend hadoop executionengine physical Layer relational Operators Union import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl plan Depth First Walker import org apache pig impl plan Operator Plan import org apache pig impl plan Visitor Exception import org apache pig impl util Jar Manager import org apache pig impl util Object Serializer import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Join import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Native import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig tools pigstats mapreduce Script State import com google common collect Lists Script States encapsulates settings for a Pig script that runs on a hadoop cluster These settings are added to all jobs spawned by the script and in turn are persisted in the hadoop job xml With the properties already in the job xml users who want to know the relations between the script and jobs can derive them from the job xmls public abstract class Script State Keys of Pig settings added to Jobs protected enum pig script id pig script pig command line pig hadoop version pig version pig input dirs pig map output dirs pig reduce output dirs pig parent jobid pig job feature pig script features pig alias pig alias location private String display Str private String s display Str s Override public String to String return display Str Features used in a Pig script public static enum private static final Log Log Factory get Log Script State class Each thread should have its own copy of Script State We initialize the Script State for new threads with the Script State of its parent thread using Inheritable Thread Local Used eg in running in separate thread private static Inheritable Thread Local Script State tss new Inheritable Thread Local Script State protected String id protected String serialized Script protected String truncated Script protected String command Line protected String file Name protected String pig Version protected String hadoop Version protected long script Features protected Pig Context pig Context protected List Pig Progress Notification Listener listeners Lists new Array List protected Script State String id this id id this serialized Script this truncated Script public static Script State get return tss get public static Script State start Script State state tss set state return tss get deprecated use link org apache pig tools pigstats Script State start Script State instead Deprecated public static Script State start String command Line Pig Context pig Context Script State ss new Script State random to String ss set Command Line command Line ss set Pig Context pig Context tss set ss return ss public void register Listener Pig Progress Notification Listener listener listeners add listener public List Pig Progress Notification Listener get All Listeners return listeners public void emit Initial Plan Notification Operator Plan plan for Pig Progress Notification Listener listener listeners try listener initial Plan Notification id plan catch No Such Method Error e warn Pig Progress Notification Listener implementation does n t implement initial Plan Notification method listener get Class get Name e public void emit Launch Started Notification int num Jobs To Launch for Pig Progress Notification Listener listener listeners listener launch Started Notification id num Jobs To Launch public void emit Jobs Submitted Notification int num Jobs Submitted for Pig Progress Notification Listener listener listeners listener jobs Submitted Notification id num Jobs Submitted public void emit Job Started Notification String assigned Job Id for Pig Progress Notification Listener listener listeners listener job Started Notification id assigned Job Id public void emitjob Finished Notification Job Stats job Stats for Pig Progress Notification Listener listener listeners listener job Finished Notification id job Stats public void emit Job Failed Notification Job Stats job Stats for Pig Progress Notification Listener listener listeners listener job Failed Notification id job Stats public void emit Output Completed Notification Output Stats output Stats for Pig Progress Notification Listener listener listeners listener output Completed Notification id output Stats public void emit Progress Updated Notification int progress for Pig Progress Notification Listener listener listeners listener progress Updated Notification id progress public void emit Launch Completed Notification int num Jobs Succeeded for Pig Progress Notification Listener listener listeners listener launch Completed Notification id num Jobs Succeeded public void set Script File file throws Exception Buffered Reader reader null try reader new Buffered Reader new File Reader file set Script reader catch File Not Found Exception e warn unable to find the file e finally if reader null try reader close catch Exception ignored public void set Script String script throws Exception if script null return Retain the truncated script set Truncated Script script Serialize and encode the string this serialized Script Object Serializer serialize script private void set Truncated Script String script restrict the size of the script to be stored in job conf int max Script Size if pig Context null String prop pig Context get Properties get Property Pig Configuration if prop null max Script Size Integer value Of prop this truncated Script script length max Script Size script substring max Script Size script public void set Script Features Logical Plan plan Bit Set bs new Bit Set try new Logical Plan Feature Visitor plan bs visit catch Frontend Exception e warn unable to get script feature e script Features bit Set To Long bs info Pig features used in the script feature Long To String script Features public String get Hadoop Version if hadoop Version null hadoop Version Version Info get Version return hadoop Version null hadoop Version public String get Pig Version if pig Version null String find Containing Jar Jar Manager find Containing Jar Script State class if find Containing Jar null try Jar File jar new Jar File find Containing Jar final Manifest manifest jar get Manifest final Map String Attributes attrs manifest get Entries Attributes attr attrs get org apache pig pig Version attr get Value Implementation Version catch Exception e warn unable to read pigs manifest file else warn unable to read pigs manifest file Not running from the Pig jar return pig Version null pig Version public String get File Name return file Name null file Name public void set File Name String file Name this file Name file Name public String get Id return id public void set Command Line String command Line this command Line new String Base encode Base command Line get Bytes public String get Command Line return command Line null command Line public String get Serialized Script return serialized Script null serialized Script public String get Script return truncated Script null truncated Script protected void set Script Buffered Reader reader throws Exception String Builder sb new String Builder try String line reader read Line while line null sb append line append n line reader read Line catch Exception e warn unable to parse the script e set Script sb to String protected long bit Set To Long Bit Set bs long ret for int i bs next Set Bit i i bs next Set Bit i ret i return ret protected String feature Long To String long l if l return name String Builder sb new String Builder for int i i values length i if l i x if sb length sb append sb append values i name return sb to String public void set Pig Context Pig Context pig Context this pig Context pig Context public Pig Context get Pig Context return pig Context public String get Script Features return feature Long To String script Features static class Logical Plan Feature Visitor extends Logical Relational Nodes Visitor private Bit Set feature protected Logical Plan Feature Visitor Logical Plan plan Bit Set feature throws Frontend Exception super plan new org apache pig newplan Depth First Walker plan this feature feature Override public void visit Cogroup op if op get Group Type feature set ordinal else if op get Group Type feature set ordinal else if op get Group Type if op get Expression Plans size feature set ordinal else feature set ordinal Override public void visit Cross op feature set ordinal Override public void visit Distinct op feature set ordinal Override public void visit Filter op feature set ordinal Override public void visit For Each op Override public void visit Join op if op get Join Type feature set ordinal else if op get Join Type feature set ordinal feature set ordinal feature set ordinal else if op get Join Type feature set ordinal else if op get Join Type feature set ordinal else if op get Join Type feature set ordinal else if op get Join Type feature set ordinal Override public void visit Limit op feature set ordinal Override public void visit Rank op feature set ordinal Override public void visit Sort op feature set ordinal Override public void visit Stream op feature set ordinal Override public void visit Split op Override public void visit Union op feature set ordinal Override public void visit Native n feature set ordinal protected static class Feature Visitor extends Phy Plan Visitor private Bit Set feature public Feature Visitor Physical Plan plan Bit Set feature super plan new Depth First Walker Physical Operator Physical Plan plan this feature feature Override public void visit Join Join join throws Visitor Exception feature set ordinal Override public void visit Merge Join Merge Join join throws Visitor Exception if join get Join Type Join feature set ordinal else feature set ordinal Override public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception feature set ordinal Override public void visit Collected Group Collected Group mg throws Visitor Exception feature set ordinal Override public void visit Distinct Distinct distinct throws Visitor Exception feature set ordinal Override public void visit Stream Stream stream throws Visitor Exception feature set ordinal Override public void visit Split Split split throws Visitor Exception feature set ordinal Override public void visit Demux Demux demux throws Visitor Exception feature set ordinal Override public void visit Partial Agg Partial Agg part Agg feature set ordinal protected static class Alias Visitor extends Phy Plan Visitor private Hash Set String alias Set private List String alias private final List String alias Location public Alias Visitor Physical Plan plan List String alias List String alias Location super plan new Depth First Walker Physical Operator Physical Plan plan this alias alias this alias Location alias Location alias Set new Hash Set String if alias is Empty for String s alias alias Set add s Override public void visit Load Load load throws Visitor Exception set Alias load super visit Load load Override public void visit Join Join join throws Visitor Exception set Alias join super visit Join join Override public void visit Merge Join Merge Join join throws Visitor Exception set Alias join super visit Merge Join join Override public void visit Merge Co Group Merge Cogroup merge Co Grp throws Visitor Exception set Alias merge Co Grp super visit Merge Co Group merge Co Grp Override public void visit Collected Group Collected Group mg throws Visitor Exception set Alias mg super visit Collected Group mg Override public void visit Distinct Distinct distinct throws Visitor Exception set Alias distinct super visit Distinct distinct Override public void visit Stream Stream stream throws Visitor Exception set Alias stream super visit Stream stream Override public void visit Filter Filter fl throws Visitor Exception set Alias fl super visit Filter fl Override public void visit Local Rearrange Local Rearrange lr throws Visitor Exception set Alias lr super visit Local Rearrange lr Override public void visit For Each For Each nfe throws Visitor Exception set Alias nfe super visit For Each nfe Override public void visit Union Union un throws Visitor Exception set Alias un super visit Union un Override public void visit Sort Sort sort throws Visitor Exception set Alias sort super visit Sort sort Override public void visit Limit Limit lim throws Visitor Exception set Alias lim super visit Limit lim Override public void visit Skewed Join Skewed Join sk throws Visitor Exception set Alias sk super visit Skewed Join sk private void set Alias Physical Operator op String s op get Alias if s null if alias Set contains s alias add s alias Set add s List Original Location original Locations op get Original Locations for Original Location original Location original Locations alias Location add original Location to String 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig tools pigstats mapreduce import java text Simple Date Format import java util Date import java util Hash Map import java util Hash Set import java util Iterator import java util List import java util Map import java util Set import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop mapred Job Client import org apache hadoop mapred Job import org apache hadoop mapred jobcontrol Job import org apache pig Exec Type import org apache pig Pig Runner Return Code import org apache pig backend hadoop executionengine map Reduce Layer Job Control Compiler import org apache pig backend hadoop executionengine map Reduce Layer Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer Native Map Reduce Oper import org apache pig backend hadoop executionengine map Reduce Layer plans Op Plan Visitor import org apache pig backend hadoop executionengine map Reduce Layer plans Oper Plan import org apache pig impl Pig Context import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Visitor Exception import org apache pig tools pigstats Input Stats import org apache pig tools pigstats mapreduce Job Stats import org apache pig tools pigstats Output Stats import org apache pig tools pigstats Pig Stats import org apache pig tools pigstats Job Stats Simple Pig Stats encapsulates the statistics collected from a running script It includes status of the execution the of its jobs as well as information about outputs and inputs of the script public final class Simple Pig Stats extends Pig Stats private static final Log Log Factory get Log Simple Pig Stats class private Job Client job Client private Job Control Compiler jcc private Map Job Map Reduce Oper job Mro Map private Map Map Reduce Oper Job Stats mro Job Map successful jobs so far private Set Job job Seen new Hash Set Job This class builds the job from a plan private class Job Graph Builder extends Op Plan Visitor public Job Graph Builder Oper Plan plan super plan new Dependency Order Walker Map Reduce Oper Oper Plan plan job Plan new Job Graph mro Job Map new Hash Map Map Reduce Oper Job Stats Override public void visit Op Map Reduce Oper mr throws Visitor Exception Job Stats js new Job Stats mr get Operator Key to String job Plan job Plan add js List Map Reduce Oper preds get Plan get Predecessors mr if preds null for Map Reduce Oper pred preds Job Stats jpred mro Job Map get pred if job Plan is Connected jpred js job Plan connect jpred js mro Job Map put mr js Override public List String get All Error Messages throw new Unsupported Operation Exception Override public Map String List Pig Stats get All Stats throw new Unsupported Operation Exception Override public boolean is Embedded return false Override public long get Spill Count Iterator Job Stats it job Plan iterator long ret while it has Next ret Job Stats it next get Spill Count return ret Override public long get Proactive Spill Count Objects Iterator Job Stats it job Plan iterator long ret while it has Next ret Job Stats it next get Proactive Spill Count Objects return ret Override public long get Proactive Spill Count Records Iterator Job Stats it job Plan iterator long ret while it has Next ret Job Stats it next get Proactive Spill Count Recs return ret public Simple Pig Stats job Mro Map new Hash Map Job Map Reduce Oper job Plan new Job Graph void initialize Pig Context pig Context Job Client job Client Job Control Compiler jcc Oper Plan mr Plan super start if pig Context null job Client null jcc null warn invalid params pig Context job Client jcc return this pig Context pig Context this job Client job Client this jcc jcc build job with job ids assigned to null try new Job Graph Builder mr Plan visit catch Visitor Exception e warn unable to build job plan e void finish super stop boolean is Initialized return start Time Deprecated Override public Job Client get Job Client return job Client Job Control Compiler get Job Control Compiler return jcc Job Stats add Job Stats Job job Map Reduce Oper mro job Mro Map get job if mro null warn unable to get oper for job job to String return null Job Stats js mro Job Map get mro Job job Id job get Assigned Job js set Id job Id js set Alias mro js set Conf job get Job Conf return js Job Stats add Job Stats For Native Native Map Reduce Oper mr Job Stats js mro Job Map get mr js set Id new Job mr get Job Id Native Map Reduce Oper get Job Number js set Alias mr return js void display info get Display String Override public String get Display String if return Code Return Code warn unknown return code ca n t display the results return if pig Context null warn unknown exec type do n t display the results return Simple Date Format sdf new Simple Date Format String Builder sb new String Builder sb append n Hadoop Version t Pig Version t User Id t Started At t Finished At t Features n sb append get Hadoop Version append t append get Pig Version append t append user Id append t append sdf format new Date start Time append t append sdf format new Date end Time append t append get Features append n sb append n if return Code Return Code sb append Success n else if return Code Return Code sb append Some jobs have failed Stop running all dependent jobs n else sb append Failed n sb append n if return Code Return Code return Code Return Code sb append Job Stats time in seconds n sb append Job Stats append n List Job Stats arr job Plan get Successful Jobs for Job Stats js arr sb append js get Display String sb append n if return Code Return Code return Code Return Code sb append Failed Jobs n sb append Job Stats append n List Job Stats arr job Plan get Failed Jobs for Job Stats js arr sb append js get Display String sb append n sb append Input s n for Input Stats is get Input Stats sb append is get Display String sb append n sb append Output s n for Output Stats ds get Output Stats sb append ds get Display String sb append n Counters n sb append Total records written get Record Written append n sb append Total bytes written get Bytes Written append n sb append Spillable Memory Manager spill count get Spill Count append n sb append Total bags proactively spilled get Proactive Spill Count Objects append n sb append Total records proactively spilled get Proactive Spill Count Records append n sb append n Job n append job Plan to String return Script Statistics n sb to String void map Oper To Job Map Reduce Oper mro Job job if mro null warn null operator else Job Stats js mro Job Map get mro if js null warn null job stats for mro mro get Operator Key else job Mro Map put job mro boolean is Job Seen Job job return job Seen add job 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util List import org apache pig Eval Func import org apache pig Func Spec import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the size of the argument passed to it For bytearrays this means the number of bytes For charrays the number of characters For bags the number of tuples for tuples the number of fields and for maps the number of keyvalue pairs For all other types the value of is always returned public class extends Eval Func Long Override public Long exec Tuple input throws Exception try Data Byte Array dba Data Byte Array input get return dba null null Long value Of dba size catch Exec Exception exp throw exp catch Exception e int err Code String msg Error while computing size in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec Schema s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec this get Class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec String Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Tuple Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Bag Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Map Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Constant Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Constant Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Constant Size class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec Constant Size class get Name s return func List Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Buffered Input Stream import java io Data Input Stream import java io Data Output Stream import java io Exception import java io File import java io File Input Stream import java io File Not Found Exception import java io Exception import java util Array List import java util Collections import java util Comparator import java util Iterator import java util Linked List import java util List Iterator import java util Priority Queue import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Pig Counters import org apache pig Pig Warning An ordered collection of Tuples possibly with multiples Data is stored unsorted as it comes in and only sorted when it is time to dump it to a file or when the first iterator is requested Experementation found this to be the faster than storing it sorted to begin with We allow a user defined comparator but provide a default comparator in cases where the user does n t specify one public class Sorted Data Bag extends Default Abstract Bag private static final long serial Version private static final Inter Sedes Inter Sedes Factory get Inter Sedes Instance private static final Log log Log Factory get Log Sorted Data Bag class transient private Comparator Tuple m Comp private boolean m Read Started false private static class Default Comparator implements Comparator Tuple Override Suppress Warnings unchecked public int compare Tuple t Tuple t return t compare To t Override public boolean equals Object o return false Override public int hash Code return param comp Comparator to use to do the sorting If null Default Comparator will be used public Sorted Data Bag Comparator Tuple comp m Comp comp null new Default Comparator comp m Contents new Array List Tuple Override public boolean is Sorted return true Override public boolean is Distinct return false Override public Iterator Tuple iterator return new Sorted Data Bag Iterator Override public long spill Make sure we have something to spill Do n t create empty files as that will make a mess if m Contents size return Lock the container before spill so that iterators are n t trying to read while m mucking with the container long spilled synchronized m Contents Data Output Stream out null try out get Spill File catch Exception ioe Do not remove last file from spilled array It was not added as File create Tmp File threw an Exception warn Unable to create tmp file to spill to disk Pig Warning ioe return try Have to sort the data before we can dump it It s bogus that we have to do this under the lock but there s no way around it If the reads alread started then we ve already sorted it No reason to do it again Do n t set m Read Started because we could still be in the add phase in which case more unsorted will be added later if m Read Started Collections sort Array List Tuple m Contents m Comp Iterator Tuple i m Contents iterator while i has Next write Datum out i next Data Type spilled This will spill every records if spilled x fff report Progress out flush out close out null m Contents clear catch Throwable e Remove the last file from the spilled array since we failed to write to it m Spill Files remove m Spill Files size warn Unable to spill contents to disk Pig Warning e return finally if out null try out close catch Exception e warn Error closing spill Pig Warning e Increment the spill count inc Spill Count Pig Counters return spilled An iterator that handles getting the next tuple from the bag This iterator has a couple of issues to deal with First data can be stored in a combination of in memory and on disk Second the bag may be asked to spill while the iterator is reading it This means that it will be pointing to someplace in memory and suddenly it will need to switch to a disk file private class Sorted Data Bag Iterator implements Iterator Tuple container to hold tuples in a priority queue Stores the file number the tuple came from so that when the tuple is read out of the queue we know which file to read its replacement tuple from private class Container implements Comparable Container public Tuple tuple public int file Num Override public int compare To Container other return m Comp compare tuple other tuple Override public boolean equals Object other if other instanceof Container return tuple equals Container other tuple else return false Override public int hash Code return tuple hash Code We have to buffer a tuple because there s no easy way for next to tell whether or not there s another tuple available other than to read it private Tuple m Buf null private int m Memory Ptr private Priority Queue Container m Merge null private Array List Data Input Stream m Streams null private int m Cntr Sorted Data Bag Iterator If this is the first read we need to sort the data synchronized m Contents if m Read Started pre Merge Collections sort Array List Tuple m Contents m Comp m Read Started true Override public boolean has Next See if we can find a tuple If so buffer it m Buf next return m Buf null Override public Tuple next This will report progress every times through next This should be much faster than using mod if m Cntr x ff report Progress If there s one in the buffer use that one if m Buf null Tuple t m Buf m Buf null return t Check to see if we just need to read from memory boolean spilled false synchronized m Contents if m Spill Files null m Spill Files size return read From Memory Check to see if we were reading from memory but we spilled if m Memory Ptr m Contents size spilled true if spilled Data Input Stream in We need to open the new file and then fast forward past all of the tuples we ve already read Then we need to place the first tuple from that file in the priority queue Whatever tuples from memory that were already in the queue will be fine as they re guaranteed to be ahead of the point we fast foward to We re guaranteed that the file we want to read from for the fast forward is the last element in m Spill Files because we do n t support calls to add after calls to iterator and spill wo n t create empty files try in new Data Input Stream new Buffered Input Stream new File Input Stream m Spill Files get m Spill Files size if m Streams null We did n t have any files before this spill m Merge new Priority Queue Container m Streams new Array List Data Input Stream m Streams add in catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Fast foward past the tuples we ve already put in the queue for int i i m Memory Ptr i try read Datum in catch Exception eof This should never happen it means we did n t dump all of our tuples to disk String msg Ran out of tuples to read prematurely log fatal msg eof throw new Runtime Exception msg eof catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe m Memory Ptr Add the next tuple from this file to the queue add To Queue null m Spill Files size Fall through to read the next entry from the priority queue We have spill files so we need to read the next tuple from one of those files or from memory return read From Priority Not implemented Override public void remove private Tuple read From Priority if m Merge null First read we need to set up the queue and the array of file streams Add one to the size for the list in memory m Merge new Priority Queue Container m Spill Files size Add one to the size in case we spill later m Streams new Array List Data Input Stream m Spill Files size Iterator File i m Spill Files iterator while i has Next try Data Input Stream in new Data Input Stream new Buffered Input Stream new File Input Stream i next m Streams add in Add the first tuple from this file into the merge queue add To Queue null m Streams size catch File Not Found Exception fnfe We ca n t find our own spill file That should never happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Prime one from memory too if m Contents size add To Queue null Pop the top one off the queue Container c m Merge poll if c null return null Add the next tuple from whereever we read from into the queue Buffer the tuple we re returning as we ll be reusing c Tuple t c tuple add To Queue c c file Num return t private void add To Queue Container c int file Num if c null c new Container c file Num file Num if file Num Need to read from memory We may have spilled since this tuple was put in the queue and hence memory might be empty But do n t care as then just wo n t add any more from memory synchronized m Contents c tuple read From Memory if c tuple null m Merge add c return Read the next tuple from the indicated file Data Input Stream in m Streams get file Num if in null There s still data in this file try c tuple Tuple read Datum in m Merge add c catch Exception eof Out of tuples in this file Set our slot in the array to null so we do n t keep trying to read from this file try in close catch Exception e log warn Failed to close spill file e m Streams set file Num null catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe Function assumes that the reader lock is already held before we enter this function private Tuple read From Memory if m Contents size return null if m Memory Ptr m Contents size return Array List Tuple m Contents get m Memory Ptr else return null Pre merge if there are too many spill files This avoids the issue of having too large a fan out in our merge Experimentation by the hadoop team has shown that is about the optimal number of spill files This function modifies the m Spill Files array and assumes the write lock is already held It will not unlock it Tuples are reconstituted as tuples evaluated and rewritten as tuples This is expensive but need to do this in order to use the sort spec that was provided to me private void pre Merge if m Spill Files null m Spill Files size return While there are more than max spill files gather max spill files together and merge them into one file Then remove the others from m Spill Files The new spill files are attached at the end of the list so can just keep going until get a small enough number without too much concern over uneven size merges Convert m Spill Files to a linked list since we ll be removing pieces from the middle and we want to do it efficiently try Linked List File ll new Linked List File m Spill Files Linked List File files To Delete new Linked List File while ll size List Iterator File i ll list Iterator m Streams new Array List Data Input Stream m Merge new Priority Queue Container for int j j j try File f i next Data Input Stream in new Data Input Stream new Buffered Input Stream new File Input Stream f m Streams add in add To Queue null m Streams size i remove files To Delete add f catch File Not Found Exception fnfe We ca n t find our own spill file That should neer happen String msg Unable to find our spill file log fatal msg fnfe throw new Runtime Exception msg fnfe Get a new spill file This adds one to the end of the spill files list So need to append it to my linked list as well so that it s still there when move my linked list back to the spill files Data Output Stream out null try out get Spill File ll add m Spill Files get m Spill Files size Tuple t while t read From Priority null t write out out flush catch Exception ioe String msg Unable to find our spill file log fatal msg ioe throw new Runtime Exception msg ioe finally try out close catch Exception e warn Error closing spill Pig Warning e delete files that have been merged into new files for File f files To Delete if f delete false log warn Failed to delete spill file f get Path clear the list so that finalize does not delete any files when m Spill Files is assigned a new value m Spill Files clear Now move our new list back to the spill files array m Spill Files new File List ll finally Reset m Streams and m Merge so that they ll be allocated properly for regular merging m Streams null m Merge null 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Serializable import java util Array List import java util List import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig impl util Utils Class to communicate sort column information based on order by statment s sort columns and schema Interface Audience Private Interface Stability Unstable public class Sort Info implements Serializable private static final long serial Version boolean is Globally Sorted true in pig this is the default List Sort Col Info sort Col Info List param sort Col Info List list of sort Col Info one for each field in the data public Sort Info List Sort Col Info sort Col Info List this sort Col Info List sort Col Info List return the sort Col Info List the list of sort Col Info for this data public List Sort Col Info get Sort Col Info List return new Array List Sort Col Info sort Col Info List non Javadoc see java lang Object hash Code Override public int hash Code final int prime int result result prime result sort Col Info List null sort Col Info List hash Code result prime result is Globally Sorted return result return the is Globally Sorted true if the data is globally sorted false if it is sorted only within each part file public boolean is Globally Sorted return is Globally Sorted non Javadoc see java lang Object equals java lang Object Override public boolean equals Object obj if Utils check Null And Class this obj return false Sort Info other Sort Info obj return is Globally Sorted other is Globally Sorted Utils check Null Equals sort Col Info List other sort Col Info List true non Javadoc see java lang Object to String Override public String to String return Global Sort is Globally Sorted sort column info list sort Col Info List 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin mock import static org apache hadoop mapreduce lib output File Output Format get Unique File import java io Data Input import java io Data Output import java io Exception import java io Serializable import java util Array List import java util Arrays import java util Collection import java util Hash Map import java util Iterator import java util List import java util Map import java util Map Entry import java util Properties import java util Set import java util Tree Map import org apache hadoop fs Path import org apache hadoop io Writable import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Job Context import org apache hadoop mapreduce Output Committer import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce Task Attempt Context import org apache log j Logger import org apache pig Expression import org apache pig Load Caster import org apache pig Load Func import org apache pig Load Metadata import org apache pig Pig Server import org apache pig Resource Schema import org apache pig Resource Statistics import org apache pig Store Func Interface import org apache pig Store Metadata import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig data Data Bag import org apache pig data Non Spillable Data Bag import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl Pig Context import org apache pig impl logical Layer schema Schema import org apache pig impl util Utils import org apache pig parser Parser Exception convenient mock Storage for unit tests pre Pig Server pig Server new Pig Server Exec Type Data data reset Data pig Server data set foo tuple a tuple b tuple c tuple map d e f g tuple bag tuple h tuple i pig Server register Query foo mock Storage pig Server register Query bar mock Storage List Tuple out data get bar assert Equals tuple a out get assert Equals tuple b out get assert Equals tuple c out get assert Equals tuple map f g d e out get assert Equals tuple bag tuple h tuple i out get pre With Schema pre Pig Server pig Server new Pig Server Exec Type Data data reset Data pig Server data set foo blah chararray tuple a tuple b tuple c pig Server register Query foo mock Storage pig Server register Query blah as a blah as b pig Server register Query bar mock Storage assert Equals schema a chararray b chararray data get Schema bar List Tuple out data get bar assert Equals tuple a a out get assert Equals tuple b b out get assert Equals tuple c c out get pre public class Storage extends Load Func implements Store Func Interface Load Metadata Store Metadata private static final String pig mock storage id private static final Logger Logger get Logger Storage class private static Map Integer Data id To Data new Hash Map Integer Data private static Tuple Factory Tuple Factory get Instance private static int next Id param objects return a tuple containing the provided objects public static Tuple tuple Object objects return new Tuple Arrays as List objects param tuples return a bag containing the provided objects public static Data Bag bag Tuple tuples return new Non Spillable Data Bag Arrays as List tuples param input These params are alternating key value So the number of params be even Implementation is very similar to the So map generates a map return a map containing the provided objects public static Map String Object map Object input if input null input length return null try Map String Object output new Hash Map String Object for int i i input length i i String key String input i Object val input i output put key val return output catch Class Cast Exception e throw new Illegal Argument Exception Map key must be a String catch Array Index Out Of Bounds Exception e throw new Illegal Argument Exception Function input must have even number of parameters catch Exception e throw new Runtime Exception Error while creating a map e param schema return the schema represented by the string throws Parser Exception if the schema is invalid public static Schema schema String schema throws Parser Exception return Utils get Schema From String schema reset the store and get the Data object to access it param pig Server return Data public static Data reset Data Pig Server pig Server return reset Data pig Server get Pig Context reset the store and get the Data object to access it param context return data as Data public static Data reset Data Pig Context context Properties properties context get Properties cleaning up previous data try if properties contains Integer previous Id new Integer properties get Property id To Data remove previous Id catch Runtime Exception e warn invalid id in context properties for e setting new Store int id next Id properties set Property String value Of id Data data new Data id To Data put id data return data private Data get Data Job job throws Exception String string Id job get Configuration get if string Id null throw new Exception no Data prepared for this Script You need to call Storage reset Data pig Server get Pig Context first Data data id To Data get new Integer string Id if data null throw new Exception no Data anymore for this Script Has data been reset by another Storage reset Data pig Server get Pig Context return data private static class Parts final String location Tree Map to read part files in order final Map String Collection Tuple parts new Tree Map String Collection Tuple public Parts String location super this location location public void set String part File Collection Tuple data if parts put part File data null throw new Runtime Exception the part part File for location location already exists public List Tuple get All List Tuple all new Array List Tuple Set Entry String Collection Tuple entry Set parts entry Set for Entry String Collection Tuple entry entry Set all add All entry get Value return all An isolated data store to avoid side effects public static class Data implements Serializable private static final long serial Version private Map String Parts location To Data new Hash Map String Parts private Map String Schema location To Schema new Hash Map String Schema to set the data in a location with a known schema param location where to store the tuples param schema the schema of the data param data the tuples to store throws Parser Exception if schema is invalid public void set String location String schema Collection Tuple data throws Parser Exception set location Utils get Schema From String schema data to set the data in a location with a known schema param location where to store the tuples param schema param data the tuples to store throws Parser Exception if schema is invalid public void set String location String schema Tuple data throws Parser Exception set location Utils get Schema From String schema Arrays as List data to set the data in a location with a known schema param location where to store the tuples param schema param data the tuples to store public void set String location Schema schema Collection Tuple data set location data if location To Schema put location schema null throw new Runtime Exception schema already set for location location to set the data in a location with a known schema param location where to store the tuples param schema param data the tuples to store public void set String location Schema schema Tuple data set location schema Arrays as List data to set the data in a location param location where to store the tuples param data the tuples to store private void set Internal String location String part Collection Tuple data Parts parts location To Data get location if part null if parts null part mock else throw new Runtime Exception Can not set location location twice if parts null parts new Parts location location To Data put location parts parts set part data to set the data in a location param location where to store the tuples param data the tuples to store public void set String location Collection Tuple data set Internal location null data to set the data in a location param location where to store the tuples param data the tuples to store public void set String location Tuple data set location Arrays as List data param location return the data in this location public List Tuple get String location if location To Data contains Key location throw new Runtime Exception No data for location location return location To Data get location get All param location return the schema stored in this location public Schema get Schema String location return location To Schema get location to set the schema for a given location param location param schema public void set Schema String location Schema schema location To Schema put location schema private String location private Data data private Schema schema private Iterator Tuple data Being Read private Mock Record Writer mock Record Writer private void init String location Job job throws Exception this data get Data job this location location this schema data get Schema location Load Func Override public String relative To Absolute Path String location Path cur Dir throws Exception this location location return location Override public void set Location String location Job job throws Exception init location job this data Being Read data get location iterator Override public Input Format get Input Format throws Exception return new Mock Input Format location Override public Load Caster get Load Caster throws Exception return super get Load Caster Override public void prepare To Read Suppress Warnings rawtypes Record Reader reader Pig Split split throws Exception Override public Tuple get Next throws Exception if data Being Read null throw new Exception data was not correctly initialized in Mock Loader return data Being Read has Next data Being Read next null Override public void set Context Signature String signature super set Context Signature signature Load Meta Data Override public Resource Schema get Schema String location Job job throws Exception init location job return schema null null new Resource Schema schema Override public Resource Statistics get Statistics String location Job job throws Exception init location job return null Override public String get Partition Keys String location Job job throws Exception init location job return null Override public void set Partition Filter Expression partition Filter throws Exception Store Func Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception this location location return location Override public Output Format get Output Format throws Exception return new Mock Output Format Override public void set Store Location String location Job job throws Exception init location job Override public void check Schema Resource Schema s throws Exception Override public void prepare To Write Suppress Warnings rawtypes Record Writer writer throws Exception mock Record Writer Mock Record Writer writer this data set Internal location mock Record Writer part mock Record Writer data Being Written Override public void put Next Tuple t throws Exception mock Record Writer data Being Written add new Tuple t get All Override public void set Store Func Context Signature String signature Override public void cleanup On Failure String location Job job throws Exception init location job Override public void cleanup On Success String location Job job throws Exception init location job Store Meta Data Override public void store Statistics Resource Statistics stats String location Job job throws Exception init location job Override public void store Schema Resource Schema schema String location Job job throws Exception init location job data set Schema location Schema get Pig Schema schema Mocks for Load Func private static class Mock Record Reader extends Record Reader Object Object Override public void close throws Exception Override public Object get Current Key throws Exception Interrupted Exception return mock Key Override public Object get Current Value throws Exception Interrupted Exception return mock Value Override public float get Progress throws Exception Interrupted Exception return f Override public void initialize Input Split split Task Attempt Context arg throws Exception Interrupted Exception Override public boolean next Key Value throws Exception Interrupted Exception return true private static class Mock Input Split extends Input Split implements Writable private String location used through reflection by Hadoop Suppress Warnings unused public Mock Input Split public Mock Input Split String location this location location Override public String get Locations throws Exception Interrupted Exception return new String location Override public long get Length throws Exception Interrupted Exception return Override public boolean equals Object arg return arg this Override public int hash Code return location hash Code Override public void read Fields Data Input arg throws Exception location arg read Override public void write Data Output arg throws Exception arg write location private static class Mock Input Format extends Input Format Object Object private final String location public Mock Input Format String location this location location Override public Record Reader Object Object create Record Reader Input Split arg Task Attempt Context arg throws Exception Interrupted Exception return new Mock Record Reader Override public List Input Split get Splits Job Context job Context throws Exception Interrupted Exception return Arrays Input Split as List new Mock Input Split location mocks for Store Func private static final class Mock Record Writer extends Record Writer Object Object private final List Tuple data Being Written new Array List Tuple private final String part public Mock Record Writer String part super this part part Override public void close Task Attempt Context task Attempt Context throws Exception Interrupted Exception Override public void write Object arg Object arg throws Exception Interrupted Exception private static class Mock Output Committer extends Output Committer Override public void abort Task Task Attempt Context arg throws Exception Override public void commit Task Task Attempt Context arg throws Exception Override public boolean needs Task Commit Task Attempt Context arg throws Exception return true Override public void setup Job Job Context arg throws Exception Override public void setup Task Task Attempt Context arg throws Exception private static final class Mock Output Format extends Output Format Object Object Override public void check Output Specs Job Context arg throws Exception Interrupted Exception Override public Output Committer get Output Committer Task Attempt Context arg throws Exception Interrupted Exception return new Mock Output Committer Override public Record Writer Object Object get Record Writer Task Attempt Context arg throws Exception Interrupted Exception if arg get Configuration get mapreduce output basename null return new Mock Record Writer arg get Configuration get mapreduce output basename arg get Task Attempt get Task get Id return new Mock Record Writer get Unique File arg part mock 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import java io Exception import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Writer import org apache pig backend hadoop executionengine map Reduce Layer Pig Hadoop Logger import org apache pig classification Interface Audience import org apache pig classification Interface Stability import org apache pig data Tuple import org apache pig impl util Context Store Funcs take records from Pig s processing and store them into a data store Most frequently this is an file but it could also be an Base instance etc Interface Audience Public Interface Stability Stable public abstract class Store Func implements Store Func Interface This method is called by the Pig runtime in the front end to convert the output location to an absolute path if the location is relative The Store Func implementation is free to choose how it converts a relative location to an absolute location since this may depend on what the location string represent hdfs path or some other data source param location location as provided in the store statement of the script param cur Dir the current working direction based on any cd statements in the script before the store statement If there are no cd statements in the script this would be the home directory pre user username pre return the absolute location based on the arguments passed throws Exception if the conversion is not possible Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir Return the Output Format associated with Store Func This will be called on the front end during planning and on the backend during execution return the link Output Format associated with Store Func throws Exception if an exception occurs while constructing the Output Format Override public abstract Output Format get Output Format throws Exception Communicate to the storer the location where the data needs to be stored The location string passed to the link Store Func here is the return value of link Store Func rel To Abs Path For Store Location String Path This method will be called in the frontend and backend multiple times Implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls link check Schema Resource Schema will be called before any call to link set Store Location String Job param location Location returned by link Store Func rel To Abs Path For Store Location String Path param job The link Job object throws Exception if the location is not valid Override public abstract void set Store Location String location Job job throws Exception Set the schema for data to be stored This will be called on the front end during planning if the store is associated with a schema Store function should implement this function to check that a given schema is acceptable to it For example it can check that the correct partition keys are included a storage function to be written directly to an Output Format can make sure the schema will translate in a well defined way Default implementation is a no op param s to be checked throws Exception if this schema is not acceptable It should include a detailed error message indicating what is wrong with the schema Override public void check Schema Resource Schema s throws Exception default implementation is a no op Initialize Store Func to write data This will be called during execution on the backend before the call to put Next param writer Record Writer to use throws Exception if an exception occurs during initialization Override public abstract void prepare To Write Record Writer writer throws Exception Write a tuple to the data store param t the tuple to store throws Exception if an exception occurs during the write Override public abstract void put Next Tuple t throws Exception This method will be called by Pig both in the front end and back end to pass a unique signature to the link Store Func which it can use to store information in the link Context which it needs to store between various method invocations in the front end and back end This method will be called before other methods in link Store Func This is necessary because in a Pig Latin script with multiple stores the different instances of store functions need to be able to find their and only their data in the Context object The default implementation is a no op param signature a unique signature to identify this Store Func Override public void set Store Func Context Signature String signature default implementation is a no op This method will be called by Pig if the job which contains this store fails Implementations can clean up output locations in this method to ensure that no incorrect incomplete results are left in the output location The default implementation deletes the output location if it is a link File System location param location Location returned by link Store Func rel To Abs Path For Store Location String Path param job The link Job object this should be used only to obtain cluster properties through link Job get Configuration and not to set query any runtime job information Override public void cleanup On Failure String location Job job throws Exception cleanup On Failure Impl location job This method will be called by Pig if the job which contains this store is successful and some cleanup of intermediate resources is required Implementations can clean up output locations in this method to ensure that no incorrect incomplete results are left in the output location param location Location returned by link Store Func rel To Abs Path For Store Location String Path param job The link Job object this should be used only to obtain cluster properties through link Job get Configuration and not to set query any runtime job information Override public void cleanup On Success String location Job job throws Exception user defined overrides can call cleanup On Failure Impl location job or Default implementation for link cleanup On Failure String Job and link cleanup On Success String Job This removes a file from param location file name or of file to remove param job Hadoop job used to access the appropriate file system throws Exception public static void cleanup On Failure Impl String location Job job throws Exception Path path new Path location File System fs path get File System job get Configuration if fs exists path fs delete path true When dropping support for move this as a default method to Store Func Interface execution engines like Tez support optimizing union by writing to output location in parallel from tasks of different vertices Commit is called once all the vertices in the union are complete This eliminates need to have a separate phase to read data output from previous phases union them and write out again Enabling the union optimization requires the Output Format to Support creation of different part file names for tasks of different vertices Conflicting filenames can create data corruption and loss For eg If task of vertex and vertex both create filename as part r then one of the files will be overwritten when promoting from temporary to final location leading to data loss File Output Format has mapreduce output basename config which enables naming files differently in different vertices Classes extending File Output Format and those prefixing file names with mapreduce output basename value will not encounter conflict Cases like Base Storage which write to key value store and do not produce files also should not face any conflict Support calling of commit once at the end takes care of promoting temporary files of the different vertices into the final location For eg File Output Format commit algorithm handles promoting of files produced by tasks of different vertices into final output location without issues if there is no file name conflict In cases like Base Storage the Table Output Committer does nothing on commit If custom Output Format used by the Store Func does not support the above two criteria then false should be returned Union optimization will be disabled for the Store Func Default implementation returns null and in that case planner falls back to link Pig Configuration and link Pig Configuration settings to determine if the Store Func supports it public Boolean supports Parallel Write To Store Location return null Issue a warning Warning messages are aggregated and reported to the user param msg String message of the warning param warning Enum type of warning public final void warn String msg Enum warning Enum Pig Hadoop Logger get Instance warn this msg warning Enum 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import org apache hadoop fs Path import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Writer import org apache pig data Tuple import org apache pig impl util Utils import java io Exception Convenience class to extend when decorating a Store Func It s not abstract so that it will fail to compile if new methods get added to Store Func Interface Subclasses must call the set Store Func with an instance of Store Func Interface before other methods can be called Not doing so will result in an Illegal Argument Exception when the method is called public class Store Func Wrapper implements Store Func Interface private Store Func Interface store Func protected Store Func Wrapper The wrapped Store Func Interface object must be set before method calls are made on this object Typically this is done with via constructor but often times the wrapped object can not be properly initialized until later in the lifecycle of the wrapper object param store Func protected void set Store Func Store Func Interface store Func this store Func store Func Override public String rel To Abs Path For Store Location String location Path path throws Exception return store Func rel To Abs Path For Store Location location path Override public Output Format get Output Format throws Exception return store Func get Output Format Override public void set Store Location String location Job job throws Exception store Func set Store Location location job Override public void check Schema Resource Schema resource Schema throws Exception store Func check Schema resource Schema Override public void prepare To Write Record Writer record Writer throws Exception store Func prepare To Write record Writer Override public void put Next Tuple tuple throws Exception store Func put Next tuple Override public void set Store Func Context Signature String signature store Func set Store Func Context Signature signature Override public void cleanup On Failure String location Job job throws Exception store Func cleanup On Failure location job Override public void cleanup On Success String location Job job throws Exception store Func cleanup On Success location job private Store Func Interface store Func if this store Func null Pig does not re throw the exception with a stack trace in the parse phase throw new Illegal Argument Exception Method calls can not be made on the Store Func Wrapper object before the wrapped Store Func Interface object has been set Failed on method call get Method Name return store Func Returns a method in the call stack at the given depth Depth will return the method that called this get Method Name depth the method that called it etc param depth return method name as String protected String get Method Name final int depth final Stack Trace Element ste Thread current Thread get Stack Trace int index if Utils is Vendor index depth else index depth return ste index get Method Name 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer schema Schema This method should never be used directly use link public class String Concat extends Eval Func String Override public String exec Tuple input throws Exception try if input null input size return null String Builder sb new String Builder for int i i input size i if input get i null return null sb append String value Of input get i return sb to String catch Exec Exception exp throw exp catch Exception e int err Code String msg Error while computing concat in this get Class get Simple Name throw new Exec Exception msg err Code Pig Exception e Override public Schema output Schema Schema input return new Schema new Schema Field Schema null Data Type Override public Schema Type get Schema Type return Schema Type Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import static java lang String format import java io Exception import java util Hash Set import java util Iterator import java util Set import org apache pig Eval Func import org apache pig backend executionengine Exec Exception import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Tuple takes two bags as arguments and returns a new bag composed of tuples of first bag not in the second bag br If null bag arguments are replaced by empty bags p The implementation assumes that both bags being passed to this function will fit entirely into memory simultaneously br If that is not the case the will still function but it will be strong very strong slow public class extends Eval Func Data Bag Compares the two bag fields from input Tuple and returns a new bag composed of elements of first bag not in the second bag param input a tuple with exactly two bag fields throws Exception if there are not exactly two fields in a tuple or if they are not link Data Bag Override public Data Bag exec Tuple input throws Exception if input size throw new Exec Exception expected two inputs but received input size inputs Data Bag bag to Data Bag input get Data Bag bag to Data Bag input get return subtract bag bag private static String class Name Of Object o return o null null o get Class get Simple Name private static Data Bag to Data Bag Object o throws Exec Exception if o null return Bag Factory get Instance new Default Bag if o instanceof Data Bag return Data Bag o throw new Exec Exception format Expecting input to be Data Bag only but was s class Name Of o private static Data Bag subtract Data Bag bag Data Bag bag Data Bag subtract Bag From Bag Bag Factory get Instance new Default Bag convert each bag to Set this does make the assumption that the sets will fit in memory Set Tuple set to Set bag remove elements of bag from set Iterator Tuple bag Iterator bag iterator while bag Iterator has Next set remove bag Iterator next set now contains all elements of bag not in bag we can build the resulting Data Bag for Tuple tuple set subtract Bag From Bag add tuple return subtract Bag From Bag private static Set Tuple to Set Data Bag bag Set Tuple set new Hash Set Tuple Iterator Tuple iterator bag iterator while iterator has Next set add iterator next return set Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical relational Logical Schema import org apache pig parser Source Location Subtract Operator public class Subtract Expression extends Binary Expression Will add this operator to the plan and connect it to the left and right hand side operators param plan plan this operator is part of param lhs expression on its left hand side param rhs expression on its right hand side public Subtract Expression Operator Plan plan Logical Expression lhs Logical Expression rhs super Subtract plan lhs rhs link org apache pig newplan Operator accept org apache pig newplan Plan Visitor Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception if other null other instanceof Subtract Expression Subtract Expression ao Subtract Expression other return ao get Lhs is Equal get Lhs ao get Rhs is Equal get Rhs else return false Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema field Schema new Logical Schema Logical Field Schema null null get Lhs get Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception Logical Expression copy new Subtract Expression lg Exp Plan this get Lhs deep Copy lg Exp Plan this get Rhs deep Copy lg Exp Plan copy set Location new Source Location location return copy 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java util Array List import java util List import org apache pig Func Spec import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema Generates the sum of a set of values This class implements link org apache pig Algebraic so if possible the execution will performed in a distributed fashion p can operate on any numeric type It can also operate on bytearrays which it will cast to doubles It expects a bag of tuples of one record each If Pig knows from the schema that this function will be passed a bag of integers or longs it will use a specially adapted version of that uses integer arithmetic for summing the data The return type of is double for float double or bytearray arguments and long for int or long arguments p implements the link org apache pig Accumulator interface as well While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation public class extends Algebraic Byte Array Math Base public set Op public static class Intermediate extends Algebraic Byte Array Math Base Intermediate Override public get Op return public static class Final extends Algebraic Byte Array Math Base Final Override public get Op return non Javadoc see org apache pig Eval Func get Arg To Func Mapping Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec func List add new Func Spec this get Class get Name Schema generate Nested Schema Data Type Data Type Double Sum works for both Floats and Doubles func List add new Func Spec Double Sum class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Double Sum class get Name Schema generate Nested Schema Data Type Data Type Long Sum works for both Ints and Longs func List add new Func Spec Long Sum class get Name Schema generate Nested Schema Data Type Data Type func List add new Func Spec Long Sum class get Name Schema generate Nested Schema Data Type Data Type Adding Big Decimal func List add new Func Spec Big Decimal Sum class get Name Schema generate Nested Schema Data Type Data Type dding Big Integer func List add new Func Spec Big Integer Sum class get Name Schema generate Nested Schema Data Type Data Type return func List 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java math Big Decimal import java math Big Integer import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop io Text import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Job import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce lib input File Input Format import org apache pig Load Caster import org apache pig Load Func import org apache pig Pig Configuration import org apache pig Pig Exception import org apache pig Resource Schema Resource Field Schema import org apache pig backend executionengine Exec Exception import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig backend hadoop executionengine map Reduce Layer Pig Text Input Format import org apache pig bzip r Bzip Text Input Format import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Tuple import org apache pig data Tuple Factory import org joda time Date Time This load function simply creates a tuple for each line of text that has a single chararray field that contains the line of text public class Text Loader extends Load Func implements Load Caster protected Record Reader in null private Tuple Factory m Tuple Factory Tuple Factory get Instance private String load Location protected final Log m Log Log Factory get Log get Class it determines whether to depend on pig s own Bzip Text Input Format or to simply depend on hadoop for handling bzip inputs private boolean bzipinput usehadoops Override public Tuple get Next throws Exception try boolean not Done in next Key Value if not Done return null Text value Text in get Current Value byte ba value get Bytes make a copy of the bytes representing the input since Text Input Format will reuse the byte array return m Tuple Factory new Tuple new Data Byte Array ba value get Length catch Interrupted Exception e throw new Exception Error getting input Text Loader does not support conversion to Boolean throws Exception if the value can not be cast Override public Boolean bytes To Boolean byte b throws Exception int err Code String msg Text Loader does not support conversion to Boolean throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Integer throws Exception if the value can not be cast Override public Integer bytes To Integer byte b throws Exception int err Code String msg Text Loader does not support conversion to Integer throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Long throws Exception if the value can not be cast Override public Long bytes To Long byte b throws Exception int err Code String msg Text Loader does not support conversion to Long throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Float throws Exception if the value can not be cast Override public Float bytes To Float byte b throws Exception int err Code String msg Text Loader does not support conversion to Float throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Double throws Exception if the value can not be cast Override public Double bytes To Double byte b throws Exception int err Code String msg Text Loader does not support conversion to Double throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Date Time throws Exception if the value can not be cast Override public Date Time bytes To Date Time byte b throws Exception int err Code String msg Text Loader does not support conversion to Date Time throw new Exec Exception msg err Code Pig Exception Cast data from bytes to chararray value param b byte array to be cast return String value throws Exception if the value can not be cast Override public String bytes To Char Array byte b throws Exception return new String b Override public Map String Object bytes To Map byte b Resource Field Schema schema throws Exception int err Code String msg Text Loader does not support conversion to Map throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Tuple throws Exception if the value can not be cast Override public Tuple bytes To Tuple byte b Resource Field Schema schema throws Exception int err Code String msg Text Loader does not support conversion to Tuple throw new Exec Exception msg err Code Pig Exception Text Loader does not support conversion to Bag throws Exception if the value can not be cast public Data Bag bytes To Bag byte b Resource Field Schema schema throws Exception int err Code String msg Text Loader does not support conversion to Bag throw new Exec Exception msg err Code Pig Exception public byte to Bytes Data Bag bag throws Exception int err Code String msg Text Loader does not support conversion from Bag throw new Exec Exception msg err Code Pig Exception public byte to Bytes String s throws Exception return s get Bytes public byte to Bytes Double d throws Exception int err Code String msg Text Loader does not support conversion from Double throw new Exec Exception msg err Code Pig Exception public byte to Bytes Float f throws Exception int err Code String msg Text Loader does not support conversion from Float throw new Exec Exception msg err Code Pig Exception public byte to Bytes Boolean b throws Exception int err Code String msg Text Loader does not support conversion from Boolean throw new Exec Exception msg err Code Pig Exception public byte to Bytes Integer i throws Exception int err Code String msg Text Loader does not support conversion from Integer throw new Exec Exception msg err Code Pig Exception public byte to Bytes Long l throws Exception int err Code String msg Text Loader does not support conversion from Long throw new Exec Exception msg err Code Pig Exception public byte to Bytes Date Time dt throws Exception int err Code String msg Text Loader does not support conversion from Date Time throw new Exec Exception msg err Code Pig Exception public byte to Bytes Map String Object m throws Exception int err Code String msg Text Loader does not support conversion from Map throw new Exec Exception msg err Code Pig Exception public byte to Bytes Tuple t throws Exception int err Code String msg Text Loader does not support conversion from Tuple throw new Exec Exception msg err Code Pig Exception Override public Big Integer bytes To Big Integer byte b throws Exception int err Code String msg Text Loader does not support conversion to Big Integer throw new Exec Exception msg err Code Pig Exception Override public Big Decimal bytes To Big Decimal byte b throws Exception int err Code String msg Text Loader does not support conversion to Big Decimal throw new Exec Exception msg err Code Pig Exception Override public Input Format get Input Format if load Location ends With bz load Location ends With bz bzipinput usehadoops m Log info Using Bzip Text Input Format return new Bzip Text Input Format else m Log info Using Pig Text Input Format return new Pig Text Input Format Override public Load Caster get Load Caster return this Override public void prepare To Read Record Reader reader Pig Split split in reader Override public void set Location String location Job job throws Exception load Location location File Input Format set Input Paths job location bzipinput usehadoops job get Configuration get Boolean Pig Configuration true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl io import java io Exception import java util Iterator import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache hadoop fs Path import org apache hadoop io Text import org apache hadoop io Writable Comparable import org apache hadoop mapreduce Input Format import org apache hadoop mapreduce Input Split import org apache hadoop mapreduce Job import org apache hadoop mapreduce Output Format import org apache hadoop mapreduce Record Reader import org apache hadoop mapreduce Record Writer import org apache hadoop mapreduce Task Attempt Context import org apache hadoop mapreduce lib input File Input Format import org apache hadoop mapreduce lib output File Output Format import org apache pig Expression import org apache pig File Input Load Func import org apache pig Load Func import org apache pig Load Metadata import org apache pig Pig Configuration import org apache pig Resource Schema import org apache pig Resource Statistics import org apache pig Store Func import org apache pig Store Func Interface import org apache pig backend hadoop executionengine map Reduce Layer Pig File Input Format import org apache pig backend hadoop executionengine map Reduce Layer Pig Split import org apache pig classification Interface Audience import org apache pig data Tuple import org apache pig impl util Utils This load function is used for storing intermediate data between jobs of a pig query The serialization format of this load function can change in newer versions of pig so this should be used to store any persistent data Interface Audience Private public class File Storage extends File Input Load Func implements Store Func Interface Load Metadata private static final Log m Log Log Factory get Log File Storage class public static final String use Log File storage in use private File Record Reader rec Reader null private File Record Writer rec Writer null Simple binary nested reader format public File Storage throws Exception m Log debug use Log Override public Tuple get Next throws Exception if rec Reader next Key Value return rec Reader get Current Value else return null Override public void put Next Tuple t throws Exception try rec Writer write null t catch Interrupted Exception e throw new Exception e public static class File Input Format extends Pig File Input Format Text Tuple non Javadoc see org apache hadoop mapreduce Input Format create Record Reader org apache hadoop mapreduce Input Split org apache hadoop mapreduce Task Attempt Context Override public Record Reader Text Tuple create Record Reader Input Split split Task Attempt Context context throws Exception Interrupted Exception return new File Record Reader Override public Input Format get Input Format return new File Input Format Override public int hash Code return Override public void prepare To Read Record Reader reader Pig Split split rec Reader File Record Reader reader Override public void set Location String location Job job throws Exception File Input Format set Input Paths job location public static class File Output Format extends File Output Format org apache hadoop io Writable Comparable Tuple non Javadoc see org apache hadoop mapreduce lib output File Output Format get Record Writer org apache hadoop mapreduce Task Attempt Context Override public Record Writer Writable Comparable Tuple get Record Writer Task Attempt Context job throws Exception Interrupted Exception Configuration conf job get Configuration String codec conf get Pig Configuration if codec equals lzo codec equals gz codec equals gzip throw new Exception Invalid temporary file compression codec codec Expected compression codecs are gz gzip and lzo if codec equals gzip codec gz m Log info codec compression codec in use Path file get Default Work File job return new File Record Writer file codec conf Override public Output Format get Output Format return new File Output Format Override public void prepare To Write Record Writer writer this rec Writer File Record Writer writer Override public void set Store Location String location Job job throws Exception File Output Format set Output Path job new Path location Override public void check Schema Resource Schema s throws Exception Override public String rel To Abs Path For Store Location String location Path cur Dir throws Exception return Load Func get Absolute Path location cur Dir Override public String get Partition Keys String location Job job throws Exception return null Override public Resource Schema get Schema String location Job job throws Exception return Utils get Schema this location true job Override public Resource Statistics get Statistics String location Job job throws Exception return null Override public void set Partition Filter Expression plan throws Exception throw new Unsupported Operation Exception Override public void set Store Func Context Signature String signature Override public void cleanup On Failure String location Job job throws Exception Store Func cleanup On Failure Impl location job Override public void cleanup On Success String location Job job throws Exception do nothing 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Non Spillable Data Bag import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema This class takes a list of items and puts them into a bag foreach generate It s like saying this foreach generate All arguments that are not of tuple type are inserted into a tuple before being added to the bag This is because bag is always a bag of tuples Output schema The output schema for this udf depends on the schema of its arguments If all the arguments have same type and same inner schema for bags tuple columns then the udf output schema would be a bag of tuples having a column of the type and inner schema if any of the arguments If the arguments are of type tuple bag then their inner schemas should match though schema field aliases may differ If these conditions are not met the output schema will be a bag with null inner schema example grunt describe a a a int a int grunt b foreach a generate a a grunt describe b b int example grunt describe a a a x int a x int grunt b foreach a generate a a grunt describe b b x int example grunt describe a a a x int a y int note that the inner schemas have matching types but different field aliases the aliases of the first argument a will be used in output schema grunt b foreach a generate a a grunt describe b b x int example grunt describe a a a x int a x chararray here the inner schemas do not match so output schema is not well defined grunt b foreach a generate a a grunt describe b b public class extends Eval Func Data Bag Override public Data Bag exec Tuple input throws Exception try The assumption is that if the bag contents fits into an input tuple it will not need to be spilled Data Bag bag new Non Spillable Data Bag input size for int i i input size i final Object object input get i if object instanceof Tuple bag add Tuple object else Tuple tp Tuple Factory get Instance new Tuple tp set object bag add tp return bag catch Exception ee throw new Runtime Exception Error while creating a bag ee non Javadoc see org apache pig Eval Func output Schema org apache pig impl logical Layer schema Schema If all the columns in the tuple are of same type then set the bag schema to bag of tuple with column of this type Override public Schema output Schema Schema input Sch byte type Data Type Schema inner Schema null if input Sch null for Field Schema fs input Sch get Fields if type Data Type type fs type inner Schema fs schema else if type fs type null Equals inner Schema fs schema invalidate the type type Data Type break try if type Data Type return Schema generate Nested Schema Data Type Data Type Field Schema inner Fs new Schema Field Schema null inner Schema type Schema inner Sch new Schema inner Fs Schema bag Schema new Schema new Field Schema null inner Sch Data Type return bag Schema catch Frontend Exception e This should not happen throw new Runtime Exception Bug exception thrown while creating output schema for udf e private boolean null Equals Schema current Schema Schema new Schema if current Schema null if new Schema null return false return true return Schema equals current Schema new Schema false true Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util List import org apache pig Eval Func import org apache pig Func Spec import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org joda time Date Time import org joda time Date Time Zone import org joda time format Date Time Formatter import org joda time format Date Time Format p To Date converts the or the customized string or the Unix timestamp to the Date Time object p p To Date is overloaded p dl dt b Syntax b dt dd code Date Time To Date Long millis code dd dt b Input b dt dd code the milliseconds code dd dt b Output b dt dd code the Date Time object code dd dl dl dt b Syntax b dt dd code Date Time To Date String dt Str code dd dt b Input b dt dd code the format date time string code dd dt b Output b dt dd code the Date Time object code dd dl dl dt b Syntax b dt dd code Date Time To Date String dt Str String format code dd dt b Input b dt dd code dt Str the string that represents a date time code dd dd code format the format string code dd dt b Output b dt dd code the Date Time object code dd dl dl dt b Syntax b dt dd code Date Time To Date String dt Str String format String timezone code dd dt b Input b dt dd code dt Str the string that represents a date time code dd dd code format the format string code dd dd code timezone the timezone string code dd dt b Output b dt dd code the Date Time object code dd dl public class To Date extends Eval Func Date Time private static final Date Time Formatter iso Date Time Formatter Date Time Format date Optional Time Parser with Offset Parsed public Date Time exec Tuple input throws Exception if input null input size input get null return null return new Date Time Data Type to Long input get Override public Schema output Schema Schema input return new Schema new Schema Field Schema get Schema Name this get Class get Name to Lower Case input Data Type Override public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec Schema s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec this get Class get Name s s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec To Date class get Name s s new Schema s add new Schema Field Schema null Data Type s add new Schema Field Schema null Data Type func List add new Func Spec To Date class get Name s s new Schema s add new Schema Field Schema null Data Type s add new Schema Field Schema null Data Type s add new Schema Field Schema null Data Type func List add new Func Spec To Date class get Name s return func List public static Date Time Zone extract Date Time Zone String dt Str return iso Date Time Formatter parse Date Time allow Iso Space dt Str get Zone public static Date Time extract Date Time String dt Str return iso Date Time Formatter parse Date Time allow Iso Space dt Str format and timestamp format are similar but not the same Strict specifies a between the date portion and the time portion allows a space instead of a as a looser variant This variant is often adopted because it increases human readability The timestamp format uses the space variant Hive Impala are database oriented and generate timestamps with a space We would like to accept both and space formats org joda time format Date Time Formatter requires the The cleanest way to get joda time to accept both is to convert the space to a a before feeding the string to the Date Time Formatter private static String allow Iso Space String dt Str if dt Str null dt Str length dt Str char At return dt Str return dt Str substring dt Str substring Override public boolean allow Compile Time Calculation return true 
Generated By Java Do not edit this line Token java Version Java Options null true package org apache pig tools parameters Describes the input token stream public class Token implements java io Serializable The version identifier for this Serializable class Increment only if the i serialized i form of the class changes private static final long serial Version An integer that describes the kind of this token This numbering system is determined by Java Parser and a table of these numbers is stored in the file Constants java public int kind The line number of the first character of this Token public int begin Line The column number of the first character of this Token public int begin Column The line number of the last character of this Token public int end Line The column number of the last character of this Token public int end Column The string image of the token public String image reference to the next regular non special token from the input stream If this is the last token from the input stream or if the token manager has not read tokens beyond this one this field is set to null This is true only if this token is also a regular token Otherwise see below for a description of the contents of this field public Token next This field is used to access special tokens that occur prior to this token but after the immediately preceding regular non special token If there are no such special tokens this field is set to null When there are more than one such special token this field refers to the last of these special tokens which in turn refers to the next previous special token through its special Token field and so on until the first special token whose special Token field is null The next fields of special tokens refer to other special tokens that immediately follow it without an intervening regular token If there is no such token this field is null public Token special Token An optional attribute value of the Token Tokens which are not used as syntactic sugar will often contain meaningful values that will be used later on by the compiler or interpreter This attribute value is often different from the image Any subclass of Token that actually wants to return a non null value can override this method as appropriate public Object get Value return null No argument constructor public Token Constructs a new token for the specified Image public Token int kind this kind null Constructs a new token for the specified Image and Kind public Token int kind String image this kind kind this image image Returns the image public String to String return image Returns a new Token object by default However if you want you can create and return subclass objects based on the value of of Kind Simply add the cases to the switch for all those special cases For example if you have a subclass of Token called Token that you want to create if of Kind is simply add something like case My Parser Constants return new Token of Kind image to the following switch statement Then you can cast matched Token variable to the appropriate type and use sit in your lexical actions public static Token new Token int of Kind String image switch of Kind default return new Token of Kind image public static Token new Token int of Kind return new Token of Kind null Java Original Checksum f a c f ad c f a ce do not edit this line 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Array List import java util List import java util String Tokenizer import org apache pig Eval Func import org apache pig Pig Exception import org apache pig backend executionengine Exec Exception import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig Func Spec Given a chararray as an argument this method will split the chararray and return a bag with a tuple for each chararray that results from the split The string is split on space double quote comma open parend close parend and asterisk star public class extends Eval Func Data Bag Tuple Factory m Tuple Factory Tuple Factory get Instance Bag Factory m Bag Factory Bag Factory get Instance Override public Data Bag exec Tuple input throws Exception try if input null return null if input size return null Object o input get if o null return null Data Bag output m Bag Factory new Default Bag if o instanceof String int err Code String msg Expected input to be chararray but got o get Class get Name throw new Exec Exception msg err Code Pig Exception String delim if input size Object d input get if d instanceof String int err Code String msg Expected delim to be chararray but got d get Class get Name throw new Exec Exception msg err Code Pig Exception delim String d String Tokenizer tok new String Tokenizer String o delim false while tok has More Tokens output add m Tuple Factory new Tuple tok next Token return output catch Exec Exception ee throw ee Suppress Warnings deprecation Override public Schema output Schema Schema input try Schema Field Schema token Fs new Schema Field Schema token Data Type Schema tuple Schema new Schema token Fs Schema Field Schema tuple Fs tuple Fs new Schema Field Schema tuple of tokens tuple Schema Data Type Schema bag Schema new Schema tuple Fs bag Schema set Two Level Access Required true Schema Field Schema bag Fs new Schema Field Schema bag of token Tuples from input get Field alias bag Schema Data Type return new Schema bag Fs catch Frontend Exception e throwing because above schema creation is not expected to throw an exception and also because superclass does not throw exception throw new Runtime Exception Unable to compute schema public List Func Spec get Arg To Func Mapping throws Frontend Exception List Func Spec func List new Array List Func Spec Schema s new Schema s add new Schema Field Schema null Data Type func List add new Func Spec this get Class get Name s s new Schema s add new Schema Field Schema null Data Type s add new Schema Field Schema null Data Type func List add new Func Spec this get Class get Name s return func List Override public boolean allow Compile Time Calculation return true 
Generated By Java Do not edit this line Token Mgr Error java Version Java Options package org apache pig tools parameters Token Manager Error public class Token Mgr Error extends Error The version identifier for this Serializable class Increment only if the i serialized i form of the class changes private static final long serial Version Ordinals for various reasons why an Error of this type can be thrown Lexical error occurred static final int An attempt was made to create a second instance of a static token manager static final int Tried to change to an invalid lexical state static final int Detected and bailed out of an infinite loop in the token manager static final int Indicates the reason why the exception is thrown It will have one of the above values int error Code Replaces unprintable characters by their escaped or unicode escaped equivalents in the given string protected static final String add Escapes String str String Buffer retval new String Buffer char ch for int i i str length i switch str char At i case continue case b retval append b continue case t retval append t continue case n retval append n continue case f retval append f continue case r retval append r continue case retval append continue case retval append continue case retval append continue default if ch str char At i x ch x e String s Integer to String ch retval append u s substring s length s length else retval append ch continue return retval to String Returns a detailed message for the Error when it is thrown by the token manager to indicate a lexical error Parameters Seen indicates if caused the lexical error cur Lex State lexical state in which this error occurred error Line line number when the error occurred error Column column number when the error occurred error After prefix that was seen before this error occurred curchar the offending character Note You can customize the lexical error message by modifying this method protected static String Lexical Error boolean Seen int lex State int error Line int error Column String error After char cur Char return Lexical error at line error Line column error Column Encountered Seen add Escapes String value Of cur Char int cur Char after add Escapes error After You can also modify the body of this method to customize your error messages For example cases like and are not of end users concern so you can return something like Internal Error Please file a bug report from this method for such cases in the release version of your parser public String get Message return super get Message Constructors of various flavors follow No arg constructor public Token Mgr Error Constructor with message and reason public Token Mgr Error String message int reason super message error Code reason Full Constructor public Token Mgr Error boolean Seen int lex State int error Line int error Column String error After char cur Char int reason this Lexical Error Seen lex State error Line error Column error After cur Char reason Java Original Checksum aa b c bba c f b b a do not edit this line 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import java util Map import java util Hash Map import org apache pig Eval Func import org apache pig data Data Bag import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer schema Schema This class makes a map out of the parameters passed to it foreach generate It generates a map This also accepts a bag with pair tuples i e tuples with a key and a value public class extends Eval Func Map Override public Map exec Tuple input throws Exception if input null input size return null Map String Object output new Hash Map String Object try Is this a single bag with all the values if input size if input get instanceof Data Bag Data Bag bag Of Pairs Data Bag input get if bag Of Pairs size return output for Tuple tuple bag Of Pairs if tuple size throw new Runtime Exception All input tuples in the bag have exactly fields String key String tuple get Object val tuple get output put key val return output else return null If only value then it must be a bag for int i i input size i i String key String input get i Object val input get i output put key val return output catch Class Cast Exception e throw new Runtime Exception Map key must be a String catch Array Index Out Of Bounds Exception e throw new Runtime Exception Function input must have even number of parameters catch Exception e throw new Runtime Exception Error while creating a map e Override public Schema output Schema Schema input Byte value Type null if input size If input is bag with pair tuples Schema bag Schema input get Fields get schema if bag Schema null bag Schema size Schema tuple Schema bag Schema get Fields get schema if tuple Schema null value Type tuple Schema get Fields get type else if input null input get Fields null for int i i input size i if i if value Type null value Type input get Fields get i type else if value Type input get Fields get i type value Type Data Type break Schema s new Schema new Schema Field Schema null Data Type if value Type null value Type Data Type s get Fields get schema new Schema new Schema Field Schema null value Type return s return s Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Exception import org apache pig Eval Func import org apache pig data Data Type import org apache pig data Tuple import org apache pig impl logical Layer schema Schema This class makes a tuple out of the parameter foreach generate It generates a tuple containing and public class extends Eval Func Tuple Override public Tuple exec Tuple input throws Exception return input Override public Schema output Schema Schema input try Schema tuple Schema new Schema for int i i input size i tuple Schema add input get Field i return new Schema new Schema Field Schema get Schema Name this get Class get Name to Lower Case input tuple Schema Data Type catch Exception e return null Override public boolean allow Compile Time Calculation return true 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan optimizer import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator Plan public abstract class Transformer check if the transform should be done If this is being called then the pattern matches but there may be other criteria that must be met as well param matched the sub set of the plan that matches the pattern This subset has the same graph as the pattern but the operators point to the same objects as the plan to be matched return true if the transform should be done throws Transformer public abstract boolean check Operator Plan matched throws Frontend Exception Transform the tree param matched the sub set of the plan that matches the pattern This subset has the same graph as the pattern but the operators point to the same objects as the plan to be matched throws Transformer public abstract void transform Operator Plan matched throws Frontend Exception Report what parts of the tree were transformed This is so that listeners can know which part of the tree to visit and modify schemas annotations etc So any nodes that were removed need will not be in this plan only nodes that were added or moved return Operator Plan that describes just the changed nodes public abstract Operator Plan report Changes 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java net import java net Class Loader import java util List import org apache pig backend hadoop executionengine map Reduce Layer Pig Tuple Default Raw Comparator import org apache pig classification Interface Audience import org apache pig classification Interface Stability factory to construct tuples This class is abstract so that users can override the tuple factory if they desire to provide their own that returns their implementation of a tuple If the property pig data tuple factory name is set to a class name and pig data tuple factory jar is set to a pointing to a jar that contains the above named class then link get Instance will create a an instance of the named class using the indicated jar Otherwise it will create an instance of link Default Tuple Factory Interface Audience Public Interface Stability Stable public abstract class Tuple Factory implements Tuple Maker Tuple private static Tuple Factory g Self null Get a reference to the singleton factory return The Tuple Factory to use to construct tuples public static Tuple Factory get Instance if g Self null String factory Name System get Property pig data tuple factory name String factory Jar System get Property pig data tuple factory jar if factory Name null factory Jar null try urls new urls new factory Jar Class Loader loader new Class Loader urls Tuple Factory class get Class Loader Class c Class for Name factory Name true loader Object o c new Instance if o instanceof Tuple Factory throw new Runtime Exception Provided factory factory Name does not extend Tuple Factory g Self Tuple Factory o catch Exception e if e instanceof Runtime Exception We just threw this Runtime Exception re Runtime Exception e throw re throw new Runtime Exception Unable to instantiate tuple factory factory Name e else if factory Name null try Class c Class for Name factory Name Object o c new Instance if o instanceof Tuple Factory throw new Runtime Exception Provided factory factory Name does not extend Tuple Factory g Self Tuple Factory o catch Exception e if e instanceof Runtime Exception We just threw this Runtime Exception re Runtime Exception e throw re throw new Runtime Exception Unable to instantiate tuple factory factory Name e else g Self new Bin Sedes Tuple Factory return g Self Create an empty tuple This should be used as infrequently as possible use new Tuple int instead return Empty new tuple public abstract Tuple new Tuple Create a tuple with size fields Whenever possible this is preferred over the null constructor as the constructor can preallocate the size of the container holding the fields Once this is called it is legal to call Tuple set x object where x lt size param size Number of fields in the tuple return Tuple with size fields public abstract Tuple new Tuple int size Create a tuple from the provided list of objects The underlying list will be copied param c List of objects to use as the fields of the tuple return tuple with the list objects as its fields public abstract Tuple new Tuple List c Create a tuple from a provided list of objects keeping the provided list The new tuple will take over ownership of the provided list param list List of objects that will become the fields of the tuple return tuple with the list objects as its fields public abstract Tuple new Tuple No Copy List list Create a tuple with a single element This is useful because of the fact that bags currently only take tuples we often end up sticking a single element in a tuple in order to put it in a bag param datum Datum to put in the tuple return tuple with one field public abstract Tuple new Tuple Object datum Return the actual class representing a tuple that the implementing factory will be returning This is needed because Hadoop needs to know the exact class we will be using for input and output return Class that implements tuple public abstract Class extends Tuple tuple Class protected Tuple Factory Provided for testing purposes only This function should never be called by anybody but the unit tests public static void reset Self g Self null Return the actual class implementing the raw comparator for tuples that the factory will be returning Ovverride this to allow Hadoop to speed up tuple sorting The actual returned class should know the serialization details for the tuple The default implementation Pig Tuple Default Raw Comparator will serialize the data before comparison return Class that implements tuple raw comparator public Class extends Tuple Raw Comparator tuple Raw Comparator Class return Pig Tuple Default Raw Comparator class This method is used to inspect whether the Tuples created by this factory will be of a fixed size when they are created In practical terms this means whether they support append or not return where the Tuple is fixed or not public abstract boolean is Fixed Size 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical rules import java util Array List import java util List import org apache pig Func Spec import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl streaming Streaming Command import org apache pig impl streaming Streaming Command Handle Spec import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical expression Cast Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Load import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan optimizer Rule import org apache pig newplan optimizer Transformer public abstract class Type Cast Inserter extends Rule public Type Cast Inserter String n super n true protected abstract Logical Schema determine Schema Logical Relational Operator op throws Frontend Exception Override public Transformer get New Transformer return new Type Cast Inserter Transformer public class Type Cast Inserter Transformer extends Transformer Override public boolean check Operator Plan matched throws Frontend Exception Logical Relational Operator op Logical Relational Operator matched get Sources get Logical Schema s op get Schema if s null return false only process each node once if is Cast Adjusted op return false if op instanceof Load if Load op get Script Schema null return false else if Stream op get Script Schema null return false Now that we ve narrowed it down to an operation that can have casts added because the user specified some types which might not match the data let s see if they re actually needed Logical Schema determined Schema determine Schema op if determined Schema null determined Schema size s size we do n t know what the data looks like but the user has specified that they want a certain number of fields loaded return true if at Least One Cast Needed determined Schema s return true return false private boolean at Least One Cast Needed Logical Schema determined Schema Logical Schema s for int i i s size i Logical Schema Logical Field Schema fs s get Field i if fs type Data Type fs is Equal determined Schema get Field i we have to cast this field from the default type to whatever the user specified in the clause of the statement the fs type return true return false Override public void transform Operator Plan matched throws Frontend Exception Logical Relational Operator op Logical Relational Operator matched get Sources get Logical Schema s op get Schema Logical Schema determined Schema determine Schema op if current Plan get Successors op null the output of this s not going anywhere so we do n t need to bother about tidying up the output return For every field build a logical plan If the field has a type other than byte array then the plan will be cast project Else it will just be project Logical Plan inner Plan new Logical Plan For Each foreach new For Each current Plan foreach set Inner Plan inner Plan foreach set Alias op get Alias Insert the foreach into the plan and patch up the plan Operator next current Plan get Successors op get current Plan insert Between op foreach next List Logical Expression Plan exps new Array List Logical Expression Plan Generate gen new Generate inner Plan exps new boolean s size inner Plan add gen for int i i s size i Logical Schema Logical Field Schema fs s get Field i Inner Load inner Load new Inner Load inner Plan foreach i inner Plan add inner Load inner Plan connect inner Load gen Logical Expression Plan exp new Logical Expression Plan Project Expression prj new Project Expression exp i gen exp add prj if fs type Data Type determined Schema null fs is Equal determined Schema get Field i Either no schema was determined by loader the type from the determined Schema is different from the type specified so we need to cast Cast Expression cast new Cast Expression exp prj new Logical Schema Logical Field Schema fs exp add cast Func Spec load Func Spec null if op instanceof Load load Func Spec Load op get File Spec get Func Spec else if op instanceof Stream Streaming Command command Stream op get Streaming Command Handle Spec stream Output Spec command get Output Spec load Func Spec new Func Spec stream Output Spec get Spec else String msg Type Cast Inserter invoked with an invalid operator class name inner Plan get Class get Simple Name throw new Frontend Exception msg cast set Func Spec load Func Spec exps add exp mark Cast Inserted op Override public Operator Plan report Changes return current Plan protected abstract void mark Cast Inserted Logical Relational Operator op protected abstract void mark Cast No Need Logical Relational Operator op protected abstract boolean is Cast Adjusted Logical Relational Operator op 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical visitor import java util Array List import java util Arrays import java util Collections import java util Comparator import java util Hash Map import java util Iterator import java util List import java util Map import org apache pig Eval Func import org apache pig Eval Func Schema Type import org apache pig Func Spec import org apache pig Pig Exception import org apache pig Pig Warning import org apache pig data Data Type import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl logical Layer schema Schema Field Schema import org apache pig impl logical Layer validators Type Checker Exception import org apache pig impl plan Compilation Message Collector import org apache pig impl plan Compilation Message Collector Message Type import org apache pig impl plan Plan Exception import org apache pig impl plan Visitor Exception import org apache pig impl util Pair import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical Util import org apache pig newplan logical expression Add Expression import org apache pig newplan logical expression All Same Expression Visitor import org apache pig newplan logical expression And Expression import org apache pig newplan logical expression Bin Cond Expression import org apache pig newplan logical expression Binary Expression import org apache pig newplan logical expression Cast Expression import org apache pig newplan logical expression Constant Expression import org apache pig newplan logical expression Dereference Expression import org apache pig newplan logical expression Divide Expression import org apache pig newplan logical expression Equal Expression import org apache pig newplan logical expression Greater Than Equal Expression import org apache pig newplan logical expression Greater Than Expression import org apache pig newplan logical expression Less Than Equal Expression import org apache pig newplan logical expression Less Than Expression import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Visitor import org apache pig newplan logical expression Map Lookup Expression import org apache pig newplan logical expression Mod Expression import org apache pig newplan logical expression Multiply Expression import org apache pig newplan logical expression Negative Expression import org apache pig newplan logical expression Not Equal Expression import org apache pig newplan logical expression Not Expression import org apache pig newplan logical expression Or Expression import org apache pig newplan logical expression Regex Expression import org apache pig newplan logical expression Subtract Expression import org apache pig newplan logical expression User Func Expression import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema public class Type Checking Exp Visitor extends Logical Expression Visitor private Compilation Message Collector msg Collector private Logical Relational Operator current Rel Op private static final int public Type Checking Exp Visitor Operator Plan exp Plan Compilation Message Collector msg Collector Logical Relational Operator rel Op throws Frontend Exception super exp Plan new Reverse Dependency Order Walker exp Plan this msg Collector msg Collector this current Rel Op rel Op reset field schema of all expression operators because it needs to be re evaluated after correct types are set Field Schema Resetter sr new Field Schema Resetter exp Plan sr visit Override public void visit Add Expression bin Op throws Frontend Exception add Casts To Numeric Bin Expression bin Op Override public void visit Subtract Expression bin Op throws Frontend Exception add Casts To Numeric Bin Expression bin Op Override public void visit Multiply Expression bin Op throws Frontend Exception add Casts To Numeric Bin Expression bin Op Override public void visit Divide Expression bin Op throws Frontend Exception add Casts To Numeric Bin Expression bin Op Add casts to promote numeric type to larger of two input numeric types of the link Binary Expression bin Op If one of the inputs is numeric and other bytearray cast the bytearray type to other numeric type If both inputs are bytearray cast them to double param bin Op throws Frontend Exception private void add Casts To Numeric Bin Expression Binary Expression bin Op throws Frontend Exception Logical Expression lhs bin Op get Lhs Logical Expression rhs bin Op get Rhs byte lhs Type lhs get Type byte rhs Type rhs get Type if Data Type is Number Type lhs Type Data Type is Number Type rhs Type return the bigger type byte bigger Type lhs Type rhs Type lhs Type rhs Type Cast smaller type to the bigger type if lhs Type bigger Type insert Cast bin Op bigger Type bin Op get Lhs else if rhs Type bigger Type insert Cast bin Op bigger Type bin Op get Rhs else if lhs Type Data Type Data Type is Number Type rhs Type insert Cast bin Op rhs Type bin Op get Lhs else if rhs Type Data Type Data Type is Number Type lhs Type insert Cast bin Op lhs Type bin Op get Rhs else if lhs Type Data Type rhs Type Data Type Cast both operands to double insert Cast bin Op Data Type bin Op get Lhs insert Cast bin Op Data Type bin Op get Rhs else int err Code String msg generate Incompatible Types Message bin Op msg Collector collect msg Message Type Error throw new Type Checker Exception bin Op msg err Code Pig Exception Override public void visit Mod Expression bin Op throws Frontend Exception Logical Expression lhs bin Op get Lhs Logical Expression rhs bin Op get Rhs byte lhs Type lhs get Type byte rhs Type rhs get Type boolean error false if lhs Type Data Type if rhs Type Data Type do nothing else if rhs Type Data Type rhs Type Data Type insert Cast bin Op rhs Type bin Op get Lhs else error true else if lhs Type Data Type if rhs Type Data Type insert Cast bin Op lhs Type bin Op get Rhs else if rhs Type Data Type insert Cast bin Op rhs Type bin Op get Lhs else if rhs Type Data Type do nothing else error true else if lhs Type Data Type if rhs Type Data Type rhs Type Data Type insert Cast bin Op lhs Type bin Op get Rhs else if rhs Type Data Type do nothing else error true else if lhs Type Data Type if rhs Type Data Type rhs Type Data Type rhs Type Data Type insert Cast bin Op rhs Type bin Op get Lhs else error true else error true if error int err Code String msg generate Incompatible Types Message bin Op msg Collector collect msg Message Type Error throw new Type Checker Exception bin Op msg err Code Pig Exception private String generate Incompatible Types Message Binary Expression bin Op throws Frontend Exception String msg bin Op to String if current Rel Op null current Rel Op get Alias null msg In alias current Rel Op get Alias Logical Field Schema lhs Fs bin Op get Lhs get Field Schema Logical Field Schema rhs Fs bin Op get Rhs get Field Schema msg msg incompatible types in bin Op get Name Operator left hand side Data Type find Type Name lhs Fs type lhs Fs schema null lhs Fs schema to String false right hand side Data Type find Type Name rhs Fs type rhs Fs schema null rhs Fs schema to String false return msg Override public void visit Negative Expression neg Exp throws Frontend Exception byte type neg Exp get Expression get Type if Data Type is Number Type type do nothing else if type Data Type cast bytearray to double insert Cast neg Exp Data Type neg Exp get Expression else int err Code String msg can be used with numbers or Bytearray only msg Collector collect msg Message Type Error throw new Type Checker Exception neg Exp msg err Code Pig Exception Override public void visit Not Expression not Exp throws Frontend Exception if not Exp get Expression instanceof Constant Expression Constant Expression not Exp get Expression get Value null insert Cast not Exp Data Type not Exp get Expression byte type not Exp get Expression get Type if type Data Type int err Code String msg can be used with boolean only msg Collector collect msg Message Type Error throw new Type Checker Exception not Exp msg err Code Pig Exception Override public void visit Or Expression or Exp throws Frontend Exception visit Boolean Binary or Exp Override public void visit And Expression and Exp throws Frontend Exception visit Boolean Binary and Exp private void visit Boolean Binary Binary Expression bool Exp throws Frontend Exception if lhs or rhs is null constant then cast it to boolean insert Casts For Null To Boolean bool Exp Logical Expression lhs bool Exp get Lhs Logical Expression rhs bool Exp get Rhs byte lhs Type lhs get Type byte rhs Type rhs get Type if lhs Type Data Type rhs Type Data Type int err Code String msg Operands of can be boolean only msg Collector collect msg Message Type Error throw new Type Checker Exception bool Exp msg err Code Pig Exception Override public void visit Less Than Expression bin Op throws Frontend Exception add Casts To Compare Binary Exp bin Op false not equality op Override public void visit Less Than Equal Expression bin Op throws Frontend Exception add Casts To Compare Binary Exp bin Op false not equality op Override public void visit Greater Than Expression bin Op throws Frontend Exception add Casts To Compare Binary Exp bin Op false not equality op Override public void visit Greater Than Equal Expression bin Op throws Frontend Exception add Casts To Compare Binary Exp bin Op false not equality op Override public void visit Equal Expression bin Op throws Frontend Exception add Casts To Compare Binary Exp bin Op true equality op Override public void visit Not Equal Expression bin Op throws Frontend Exception add Casts To Compare Binary Exp bin Op true equality op private void add Casts To Compare Binary Exp Binary Expression bin Op boolean is Equality throws Frontend Exception Logical Expression lhs bin Op get Lhs Logical Expression rhs bin Op get Rhs byte lhs Type lhs get Type byte rhs Type rhs get Type if Data Type is Number Type lhs Type Data Type is Number Type rhs Type If not the same type we cast them to the same byte bigger Type lhs Type rhs Type lhs Type rhs Type Cast smaller type to the bigger type if lhs Type bigger Type insert Cast bin Op bigger Type bin Op get Lhs else if rhs Type bigger Type insert Cast bin Op bigger Type bin Op get Rhs else if lhs Type Data Type rhs Type Data Type good else if lhs Type Data Type rhs Type Data Type good else if lhs Type Data Type rhs Type Data Type good else if lhs Type Data Type rhs Type Data Type Data Type is Number Type rhs Type rhs Type Data Type rhs Type Data Type Cast byte array to the type on rhs insert Cast bin Op rhs Type bin Op get Lhs else if rhs Type Data Type lhs Type Data Type Data Type is Number Type lhs Type lhs Type Data Type lhs Type Data Type Cast byte array to the type on lhs insert Cast bin Op lhs Type bin Op get Rhs else if is Equality in case of equality condition allow boolean tuples and maps as args if lhs Type Data Type rhs Type Data Type good else if lhs Type Data Type rhs Type Data Type good else if lhs Type Data Type rhs Type Data Type good else if lhs Type Data Type rhs Type Data Type rhs Type Data Type Cast byte array to the type on lhs insert Cast bin Op rhs Type bin Op get Lhs else if rhs Type Data Type lhs Type Data Type lhs Type Data Type Cast byte array to the type on lhs insert Cast bin Op lhs Type bin Op get Rhs else throw Incompatible Type Error bin Op else throw Incompatible Type Error bin Op private void throw Incompatible Type Error Binary Expression bin Op throws Frontend Exception int err Code String msg generate Incompatible Types Message bin Op msg Collector collect msg Message Type Error throw new Type Checker Exception bin Op msg err Code Pig Exception private void insert Casts For Null To Boolean Binary Expression bin Op throws Frontend Exception if bin Op get Lhs instanceof Constant Expression Constant Expression bin Op get Lhs get Value null insert Cast bin Op Data Type bin Op get Lhs if bin Op get Rhs instanceof Constant Expression Constant Expression bin Op get Rhs get Value null insert Cast bin Op Data Type bin Op get Rhs add cast to convert the input of exp link Logical Expression arg to type to Type param exp param to Type param arg throws Frontend Exception private void insert Cast Logical Expression exp byte to Type Logical Expression arg throws Frontend Exception Logical Field Schema to Fs new Logical Schema Logical Field Schema null null to Type insert Cast exp to Fs arg private void insert Cast Logical Expression node Logical Field Schema to Fs Logical Expression arg throws Frontend Exception collect Cast Warning node arg get Type to Fs type msg Collector Cast Expression cast new Cast Expression plan arg to Fs cast set Location node get Location try disconnect cast and arg because the connection is already added by cast constructor and insert Between call is going to do it again plan disconnect cast arg plan insert Between node cast arg catch Plan Exception pe int err Code String msg Problem with inserting cast operator for node in plan throw new Type Checker Exception arg msg err Code Pig Exception pe this visit cast For Basic Types Casting to itself is always ok Casting from number to number is always ok Byte Array to anything is ok number to chararray is ok For Composite Types Recursively traverse the schemas till you get a basic type throws Frontend Exception Override public void visit Cast Expression cast throws Frontend Exception byte in Type cast get Expression get Type byte out Type cast get Type if out Type Data Type in Type out Type int err Code String msg Can not cast from Data Type find Type Name in Type to bytearray msg Collector collect msg Message Type Error throw new Type Checker Exception cast msg err Code Pig Exception Logical Field Schema in Fs cast get Expression get Field Schema Logical Field Schema out Fs cast get Field Schema if in Fs null replace null schema with bytearray schema in Fs new Logical Field Schema null null Data Type check if the field schemas are castable boolean castable Logical Field Schema castable in Fs out Fs if castable int err Code String msg Can not cast Data Type find Type Name in Type Data Type is Schema Type in Type with schema in Fs to String false to Data Type find Type Name out Type Data Type is Schema Type out Type with schema out Fs to String false msg Collector collect msg Message Type Error throw new Type Checker Exception cast msg err Code Pig Exception link Regex Expression expects Char Array as input Itself always returns Boolean param rg throws Frontend Exception Override public void visit Regex Expression rg throws Frontend Exception We allow to be converted to if rg get Lhs get Type Data Type insert Cast rg Data Type rg get Lhs if rg get Rhs get Type Data Type insert Cast rg Data Type rg get Rhs Other than that if it s not Char Array just say goodbye if rg get Lhs get Type Data Type rg get Rhs get Type Data Type int err Code String msg Operands of Regex can be Char Array only rg msg Collector collect msg Message Type Error throw new Type Checker Exception rg msg err Code Pig Exception Override public void visit Bin Cond Expression bin Cond throws Frontend Exception high level type checking if bin Cond get Condition get Type Data Type int err Code String msg Condition in Bin Cond must be boolean msg Collector collect msg Message Type Error throw new Type Checker Exception bin Cond msg err Code Pig Exception byte lhs Type bin Cond get Lhs get Type byte rhs Type bin Cond get Rhs get Type If both sides are number we can convert the smaller type to the bigger type if Data Type is Number Type lhs Type Data Type is Number Type rhs Type byte bigger Type lhs Type rhs Type lhs Type rhs Type if bigger Type lhs Type insert Cast bin Cond bigger Type bin Cond get Lhs else if bigger Type rhs Type insert Cast bin Cond bigger Type bin Cond get Rhs else if lhs Type Data Type rhs Type Data Type Data Type is Number Type rhs Type rhs Type Data Type need to add boolean as well Cast byte array to the type on rhs insert Cast bin Cond rhs Type bin Cond get Lhs else if rhs Type Data Type lhs Type Data Type Data Type is Number Type lhs Type rhs Type Data Type need to add boolean as well Cast byte array to the type on lhs insert Cast bin Cond lhs Type bin Cond get Rhs constant null is always bytearray so cast it to rhs type else if bin Cond get Lhs instanceof Constant Expression Constant Expression bin Cond get Lhs get Value null try insert Cast bin Cond bin Cond get Rhs get Field Schema bin Cond get Lhs catch Frontend Exception e int err Code String msg Problem getting field Schema for bin Cond get Rhs throw new Type Checker Exception bin Cond msg err Code Pig Exception e else if bin Cond get Rhs instanceof Constant Expression Constant Expression bin Cond get Rhs get Value null try insert Cast bin Cond bin Cond get Lhs get Field Schema bin Cond get Rhs catch Frontend Exception e int err Code String msg Problem getting field Schema for bin Cond get Rhs throw new Type Checker Exception bin Cond msg err Code Pig Exception e else if lhs Type rhs Type Matching schemas if we re working with tuples bags if Data Type is Schema Type lhs Type try if Logical Field Schema is Equal Unless Unknown bin Cond get Lhs get Field Schema bin Cond get Rhs get Field Schema int err Code String msg Two inputs of Bin Cond must have compatible schemas left hand side bin Cond get Lhs get Field Schema right hand side bin Cond get Rhs get Field Schema msg Collector collect msg Message Type Error throw new Type Checker Exception bin Cond msg err Code Pig Exception We may have to merge the schema here if the previous check is not exact match catch Frontend Exception fe int err Code String msg Problem during evaluaton of Bin Cond output type msg Collector collect msg Message Type Error throw new Type Checker Exception bin Cond msg err Code Pig Exception fe else int err Code String msg Unsupported input type for Bin Cond left hand side Data Type find Type Name lhs Type right hand side Data Type find Type Name rhs Type msg Collector collect msg Message Type Error throw new Type Checker Exception bin Cond msg err Code Pig Exception Override public void visit Map Lookup Expression map throws Frontend Exception if map get Map get Type Data Type insert cast if the predecessor does not return map insert Cast map Data Type map get Map Override public void visit Dereference Expression deref throws Frontend Exception byte input Type deref get Referred Expression get Type switch input Type case Data Type case Data Type case Data Type ideally determine type at runtime allowed types break default int err Code String msg Referring to column s within a column of type Data Type find Type Name input Type is not allowed throw new Type Checker Exception deref msg err Code Pig Exception Override public void visit User Func Expression func throws Frontend Exception List Logical Expression list func get Arguments If the dependency graph is right all the inputs must already know the types Schema current Arg Schema new Schema for Logical Expression op list if Data Type is Usable Type op get Type int err Code String msg Problem with input op of User defined function func msg Collector collect msg Message Type Error throw new Type Checker Exception func msg err Code Pig Exception try current Arg Schema add Util translate Field Schema op get Field Schema catch Frontend Exception e int err Code String msg Unable to retrieve field schema throw new Type Checker Exception func msg err Code Pig Exception e Eval Func ef Eval Func Pig Context instantiate Func From Spec func get Func Spec ask the Eval Func what types of inputs it can handle List Func Spec func Specs null try func Specs ef get Arg To Func Mapping if func Specs null for Func Spec func Spec func Specs Schema s func Spec get Input Args Schema Logical Schema ls Util translate Schema s ls normalize func Spec set Input Args Schema Util translate Schema ls catch Exception e int err Code String msg Unable to get list of overloaded methods throw new Type Checker Exception func msg err Code Pig Exception e Eval Func s schema type Schema Type udf Schema Type ef get Schema Type Here is an explanation of the way the matching funcspec will be chosen based on actual types in the input schema First an exact match is tried for each of the fields in the input schema with the corresponding fields in the candidate funcspecs schemas If exact match fails then first a check if made if the input schema has any bytearrays in it If there are bytearrays in the input schema then a best fit match is attempted for the different fields Essential a permissible cast from one type to another is given a score based on its position in the cast Lookup table final score for a candidate funcspec is deduced as score of particular cast no Of Casts So Far If no permissible casts are possible the score for the candidate is Among the non score candidates the candidate with the lowest score is chosen If there are bytearrays in the input schema a modified exact match is tried In this matching bytearrays in the input schema are not considered As a result of ignoring the bytearrays we could get multiple candidate funcspecs which match exactly for the other columns if this is the case we notify the user of the ambiguity and error out Else if all other non byte array fields matched exactly then we can cast bytearray s to the corresponding type s in the matched udf schema If this modified exact match fails the above best fit algorithm is attempted by initially coming up with scores and candidate func Specs with bytearray s being ignored in the scoring process Then a check is made to ensure that the positions which have bytearrays in the input schema have the same type for a given position in the corresponding positions in all the candidate func Specs If this is not the case it indicates a conflict and the user is notified of the error because we have more than one choice for the destination type of the cast for the bytearray If this is the case the candidate with the lowest score is chosen Func Spec matching Spec null boolean not Exact Match false if func Specs null func Specs size Some function mappings found Trying to see if one of them fits the input schema if matching Spec exact Match func Specs current Arg Schema func udf Schema Type null Oops no exact match found Trying to see if we have mappings that we can fit using casts not Exact Match true if byte Array Found func current Arg Schema try exact matching all other fields except the byte array fields and if they all exact match and we have only one candidate for the byte array cast then that s the matching one if matching Spec exact Match With Byte Arrays func Specs current Arg Schema func udf Schema Type null exact match with byte arrays did not work try best fit match if matching Spec best Fit Match With Byte Arrays func Specs current Arg Schema func udf Schema Type null int err Code String msg Could not infer the matching function for func get Func Spec as multiple or none of them fit Please use an explicit cast msg Collector collect msg Message Type Error throw new Type Checker Exception func msg err Code Pig Exception else if matching Spec best Fit Match func Specs current Arg Schema udf Schema Type null Either no byte arrays found or there are byte arrays but only one mapping exists However we could not find a match as there were either none fitting the input schema or it was ambiguous Throw exception that we ca n t infer a fit int err Code String msg Could not infer the matching function for func get Func Spec as multiple or none of them fit Please use an explicit cast msg Collector collect msg Message Type Error throw new Type Checker Exception func msg err Code Pig Exception if matching Spec null Voila We have a fitting match Lets insert casts and make it work notify the user about the match we picked if it was not an exact match if not Exact Match String msg Function func get Func Spec get Class Name will be called with following argument types matching Spec get Input Args Schema If you want to use different input argument types please use explicit casts msg Collector collect msg Message Type Warning Pig Warning if func is Via Define matching Spec set Ctor Args func get Func Spec get Ctor Args func set Func Spec matching Spec insert Casts For func current Arg Schema matching Spec get Input Args Schema udf Schema Type Tries to find the schema supported by one of func Specs which can be obtained by inserting a set of casts to the input schema param func Specs mappings provided by udf param s input schema param func udf expression param udf Schema Type schema type of the udf return the func Spec that supports the schema that is best suited to s The best suited schema is one that has the lowest score as returned by fit Possible throws Visitor Exception private Func Spec best Fit Match With Byte Arrays List Func Spec func Specs Schema s User Func Expression func Schema Type udf Schema Type throws Visitor Exception List Pair Long Func Spec score Func Spec List new Array List Pair Long Func Spec for Iterator Func Spec iterator func Specs iterator iterator has Next Func Spec fs iterator next long score fit Possible s fs get Input Args Schema udf Schema Type if score score Func Spec List add new Pair Long Func Spec score fs if no candidates found return null if score Func Spec List size return null if score Func Spec List size sort the candidates based on score Collections sort score Func Spec List new Score Func Spec List Comparator if there are two or more candidates with the same lowest score we can not choose one of them notify the user if score Func Spec List get first score Func Spec List get first int err Code String msg Multiple matching functions for func get Func Spec with input schemas score Func Spec List get second get Input Args Schema score Func Spec List get second get Input Args Schema Please use an explicit cast msg Collector collect msg Message Type Error throw new Type Checker Exception func msg err Code Pig Exception now consider the bytearray fields List Integer byte Array Positions get Byte Array Positions func s make sure there is only one type to cast to for the byte array positions among the candidate func Specs Map Integer Pair Func Spec Byte cast To Map new Hash Map Integer Pair Func Spec Byte for Iterator Pair Long Func Spec it score Func Spec List iterator it has Next Func Spec func Spec it next second Schema sch func Spec get Input Args Schema for Iterator Integer iter byte Array Positions iterator iter has Next Integer i iter next try if cast To Map contains Key i first candidate cast To Map put i new Pair Func Spec Byte func Spec sch get Field i type else make sure the existing type from an earlier candidate matches Pair Func Spec Byte existing Pair cast To Map get i if sch get Field i type existing Pair second int err Code String msg Multiple matching functions for func get Func Spec with input schema existing Pair first get Input Args Schema func Spec get Input Args Schema Please use an explicit cast msg Collector collect msg Message Type Error throw new Type Checker Exception func msg err Code Pig Exception catch Frontend Exception fee int err Code String msg Unalbe to retrieve field schema throw new Type Checker Exception func msg err Code Pig Exception fee if we reached here it means we have candidates and these candidates have the same type for position which have bytearray in the input Also the candidates are stored sorted by score in a list we can now just return the first candidate the one with the lowest score return score Func Spec List get second private static class Score Func Spec List Comparator implements Comparator Pair Long Func Spec non Javadoc see java util Comparator compare java lang Object java lang Object Override public int compare Pair Long Func Spec o Pair Long Func Spec o if o first o first return else if o first o first return else return Finds if there is an exact match between the schema supported by one of the func Specs and the input schema s Here first exact match for all non byte array fields is first attempted and if there is exactly one candidate it is chosen since the bytearray s can just be cast to corresponding type s in the candidate param func Specs mappings provided by udf param s input schema param func User Func Expression for which matching is requested param udf Schema Type schema type of the udf return the matching spec if found else null throws Frontend Exception private Func Spec exact Match With Byte Arrays List Func Spec func Specs Schema s User Func Expression func Schema Type udf Schema Type throws Frontend Exception exact match all fields except byte array fields ignore byte array fields for matching return exact Match Helper func Specs s func udf Schema Type true Finds if there is an exact match between the schema supported by one of the func Specs and the input schema s Here an exact match for all fields is attempted param func Specs mappings provided by udf param s input schema param func User Func Expression for which matching is requested param udf Schema Type schema type of the user defined function return the matching spec if found else null throws Frontend Exception private Func Spec exact Match List Func Spec func Specs Schema s User Func Expression func Schema Type udf Schema Type throws Frontend Exception exact match all fields do n t ignore byte array fields return exact Match Helper func Specs s func udf Schema Type false Tries to find the schema supported by one of func Specs which can be obtained by inserting a set of casts to the input schema param func Specs mappings provided by udf param s input schema param udf Schema Type schema type of the udf return the func Spec that supports the schema that is best suited to s The best suited schema is one that has the lowest score as returned by fit Possible private Func Spec best Fit Match List Func Spec func Specs Schema s Schema Type udf Schema Type Func Spec matching Spec null long score long prev Best Score Long long best Score Long for Iterator Func Spec iterator func Specs iterator iterator has Next Func Spec fs iterator next score fit Possible s fs get Input Args Schema udf Schema Type if score score best Score matching Spec fs prev Best Score best Score best Score score if matching Spec null best Score prev Best Score return matching Spec return null Checks to see if any field of the input schema is a byte array param func param s input schema return true if found else false throws Visitor Exception private boolean byte Array Found User Func Expression func Schema s throws Visitor Exception for int i i s size i try Field Schema fs s get Field i if fs null return false if fs type Data Type return true catch Frontend Exception fee int err Code String msg Unable to retrieve field schema throw new Type Checker Exception func msg err Code Pig Exception fee return false Gets the positions in the schema which are byte arrays param func param s input schema throws Visitor Exception private List Integer get Byte Array Positions User Func Expression func Schema s throws Visitor Exception List Integer result new Array List Integer for int i i s size i try Field Schema fs s get Field i if fs type Data Type result add i catch Frontend Exception fee int err Code String msg Unable to retrieve field schema throw new Type Checker Exception func msg err Code Pig Exception fee return result Finds if there is an exact match between the schema supported by one of the func Specs and the input schema s param func Specs mappings provided by udf param s input schema param func user defined function param udf Schema Type schema type of the user defined function param ignore Byte Arrays flag for whether the exact match is to computed after ignoring bytearray if true or without ignoring bytearray if false return the matching spec if found else null throws Frontend Exception private Func Spec exact Match Helper List Func Spec func Specs Schema s User Func Expression func Schema Type udf Schema Type boolean ignore Byte Arrays throws Frontend Exception List Func Spec matching Specs new Array List Func Spec for Iterator Func Spec iterator func Specs iterator iterator has Next Func Spec fs iterator next if schema Equals For Matching s fs get Input Args Schema udf Schema Type ignore Byte Arrays matching Specs add fs if matching Specs size return null if matching Specs size int err Code String msg Multiple matching functions for func get Func Spec with input schema matching Specs get get Input Args Schema matching Specs get get Input Args Schema Please use an explicit cast msg Collector collect msg Message Type Error throw new Type Checker Exception func msg err Code Pig Exception exactly one matching spec return it return matching Specs get Compare two schemas for equality for argument matching purposes This is a more relaxed form of Schema equals wherein first the Datatypes of the field schema are checked for equality Then if a field schema in the udf schema is for a complex type if the inner schema is null check for schema equality of the inner schemas of the field schema and input field schema param input Schema param udf Schema param ignore Byte Arrays return true if Field Schemas are equal for argument matching false otherwise throws Frontend Exception public static boolean schema Equals For Matching Schema input Schema Schema udf Schema Schema Type udf Schema Type boolean ignore Byte Arrays throws Frontend Exception If both of them are null they are equal if input Schema null udf Schema null return true otherwise if input Schema null return false if udf Schema null return false the old udf schemas might not have tuple inside bag fix that udf Schema Util fix Schema Add Tuple In Bag udf Schema if udf Schema Type Schema Type input Schema size udf Schema size return false if udf Schema Type Schema Type input Schema size udf Schema size return false Iterator Field Schema i input Schema get Fields iterator Iterator Field Schema j udf Schema get Fields iterator Field Schema udf Field Schema null while i has Next Field Schema input Field Schema i next if input Field Schema null return false if there s no more field take the last one which is the vararg field udf Field Schema j has Next j next udf Field Schema if ignore Byte Arrays input Field Schema type Data Type continue if input Field Schema type udf Field Schema type return false if a field schema in the udf schema is for a complex type if the inner schema is null check for schema equality of the inner schemas of the field schema and input field schema If the field schema in the udf schema is for a complex type if the inner schema null it means the udf is applicable for all input which has the same type for that field irrespective of inner schema if it is a bag with empty tuple then just rely on the field type if Data Type is Schema Type udf Field Schema type udf Field Schema schema null is Not Bag With Empty Tuple udf Field Schema Compare recursively using field schema if Field Schema equals input Field Schema udf Field Schema false true try modifying any empty tuple to type of bytearray and see if that matches Need to do this for backward compatibility User might have specified tuple with a bytearray and this should also match an empty tuple Field Schema input With Bytearrayin Tuple new Field Schema input Field Schema convert Empty Tuple To Bytearray Tuple input With Bytearrayin Tuple if Field Schema equals input With Bytearrayin Tuple udf Field Schema false true return false return true Check if the field Sch is a bag with empty tuple schema param field Sch return throws Frontend Exception private static boolean is Not Bag With Empty Tuple Field Schema field Sch throws Frontend Exception boolean is Bag With Empty Tuple false if field Sch type Data Type field Sch schema null field Sch schema get Field null field Sch schema get Field type Data Type field Sch schema get Field schema null is Bag With Empty Tuple true return is Bag With Empty Tuple private static void convert Empty Tuple To Bytearray Tuple Field Schema fs if fs type Data Type fs schema null fs schema size fs schema add new Field Schema null Data Type return if fs schema null for Field Schema in Fs fs schema get Fields convert Empty Tuple To Bytearray Tuple in Fs static final Hash Map Byte List Byte cast Lookup new Hash Map Byte List Byte static Ordering here decides the score for the best fit function Do not change the order Conversions to a smaller type is preferred over conversion to a bigger type where ordering of types is from small to big List Byte bool To Types Arrays as List Data Type Data Type Data Type Data Type Data Type Data Type maybe more bigger types cast Lookup put Data Type bool To Types List Byte int To Types Arrays as List Data Type Data Type Data Type Data Type Data Type cast Lookup put Data Type int To Types List Byte long To Types Arrays as List Data Type Data Type Data Type Data Type cast Lookup put Data Type long To Types List Byte float To Types Arrays as List Data Type Data Type Data Type cast Lookup put Data Type float To Types List Byte double To Types Arrays as List Data Type Data Type cast Lookup put Data Type double To Types List Byte big Integer To Types Arrays as List Data Type cast Lookup put Data Type big Integer To Types List Byte byte Array To Types Arrays as List Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type Data Type cast Lookup put Data Type byte Array To Types Computes a modified version of manhattan distance between the two schemas s s Here the value on the same axis are preferred over values that change axis as this means that the number of casts required will be lesser on the same axis However this function ceases to be a metric as the triangle inequality does not hold Each schema is an s size dimensional vector The ordering for each axis is as defined by cast Lookup Unallowed casts are returned a dist of param s param s param s Type return private long fit Possible Schema s Schema s Schema Type s Type if s null s null return List Field Schema s Fields s get Fields List Field Schema fs Fields s get Fields if s Type Schema Type s Fields size fs Fields size return if s Type Schema Type s Fields size fs Fields size return long score int cast Cnt for int i i s Fields size i Field Schema s s Fields get i if s null return if we have a byte array do not include it in the computation of the score bytearray fields will be looked at separately outside of this function if s type Data Type continue if we get to the vararg field if defined take it repeatedly Field Schema fs s Type Schema Type i s size fs Fields get s size fs Fields get i if Data Type is Schema Type s type if Field Schema equals s fs false true return if Field Schema equals s fs true true continue if cast Lookup contains Key s type return if cast Lookup get s type contains fs type return score cast Lookup get s type index Of fs type cast Cnt return score cast Cnt private void insert Casts For User Func Expression func Schema from Sch Schema to Sch Schema Type to Sch Type throws Frontend Exception List Field Schema fs Lst from Sch get Fields List Field Schema ts Lst to Sch get Fields List Logical Expression args func get Arguments int i for Field Schema f Sch fs Lst i if we get to the vararg field if defined take it repeatedly Field Schema t Sch to Sch Type Schema Type i ts Lst size ts Lst get ts Lst size ts Lst get i if f Sch type t Sch type continue insert Cast func Util translate Field Schema t Sch args get i Helper for collecting warning when casting is inserted to the plan implicit casting param node param original Type param to Type static void collect Cast Warning Operator node byte original Type byte to Type Compilation Message Collector msg Collector String original Type Name Data Type find Type Name original Type String to Type Name Data Type find Type Name to Type String op Name node get Class get Simple Name Pig Warning kind null switch to Type case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break case Data Type kind Pig Warning break msg Collector collect original Type Name is implicitly cast to to Type Name under op Name Operator Message Type Warning kind static class Field Schema Resetter extends All Same Expression Visitor protected Field Schema Resetter Operator Plan p throws Frontend Exception super p new Reverse Dependency Order Walker p Override protected void execute Logical Expression op throws Frontend Exception op reset Field Schema 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical visitor import java util Array List import java util Collection import java util List import org apache pig Pig Exception import org apache pig data Data Type import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer validators Type Checker Exception import org apache pig impl plan Compilation Message Collector import org apache pig impl plan Visitor Exception import org apache pig impl plan Compilation Message Collector Message Type import org apache pig impl util Multi Map import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan logical expression Cast Expression import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema All the get Type of these operators always return We just have to Check types of inputs expression plans Compute output schema with type information At the moment the parser does only return Get Schema with correct aliases Insert casting if necessary public class Type Checking Rel Visitor extends Logical Relational Nodes Visitor private Compilation Message Collector msg Collector public Type Checking Rel Visitor Operator Plan plan Compilation Message Collector msg Collector throws Frontend Exception super plan new Dependency Order Walker plan this msg Collector msg Collector Override public void visit Load load do nothing Override public void visit Store store throws Frontend Exception store reset Schema store get Schema The schema of filter output will be the same as filter input throws Frontend Exception Override public void visit Filter filter throws Frontend Exception filter reset Schema Logical Expression Plan comparison Plan filter get Filter Plan Check that the inner plan has only output port if comparison Plan get Sources size int err Code String msg Filter s cond plan can only have one output msg Collector collect msg Message Type Error throw Type Checker Exception filter msg err Code Pig Exception null visit the filter expression visit Expression Plan comparison Plan filter check filter expression type byte inner Cond Type Logical Expression comparison Plan get Sources get get Type if inner Cond Type Data Type int err Code String msg Filter s condition must evaluate to boolean Found Data Type find Type Name inner Cond Type msg Collector collect msg Message Type Error throw Type Checker Exception filter msg err Code Pig Exception null try re compute the schema filter reset Schema filter get Schema catch Frontend Exception fe int err Code String msg Problem while reconciling output schema of Filter msg Collector collect msg Message Type Error throw Type Checker Exception filter msg err Code Pig Exception fe private void throw Type Checker Exception Operator op String msg int err Code byte input Frontend Exception fe throws Type Checker Exception if fe null throw new Type Checker Exception op msg err Code Pig Exception throw new Type Checker Exception op msg err Code Pig Exception fe Override public void visit Generate gen throws Frontend Exception for int i i gen get Output Plans size i Logical Expression Plan exp Plan gen get Output Plans get i Check that the inner plan has only output port if exp Plan get Sources size int err Code String msg Generate expression plan can only have one output msg Collector collect msg Message Type Error throw Type Checker Exception gen msg err Code Pig Exception null visit the filter expression visit Expression Plan exp Plan gen gen reset Schema gen get Schema Override public void visit Inner Load inner Load throws Frontend Exception inner Load reset Schema inner Load get Schema Override public void visit For Each for Each throws Frontend Exception try visit inner plan new Type Checking Rel Visitor for Each get Inner Plan msg Collector visit re compute the schema for Each reset Schema for Each get Schema catch Frontend Exception fe int err Code String msg Problem while reconciling output schema of For Each msg Collector collect msg Message Type Error throw Type Checker Exception for Each msg err Code Pig Exception fe private void visit Expression Plan Logical Expression Plan expl Plan Logical Relational Operator rel Op throws Frontend Exception Type Checking Exp Visitor exp Type Check new Type Checking Exp Visitor expl Plan msg Collector rel Op exp Type Check visit non Javadoc see org apache pig newplan logical relational Logical Relational Nodes Visitor visit org apache pig newplan logical relational Union The output schema of Union is the merge of all input schemas Operands on left side always take precedance on aliases We allow type promotion here Override public void visit Union u throws Frontend Exception u reset Schema Have to make a copy because as we insert operators this list will change under us List Operator inputs new Array List Operator u get Inputs There is no point to union only one operand it should be a problem in the parser if inputs size throw new Assertion Error Union with Count Operand Logical Schema schema null try Compute the schema schema u get Schema catch Frontend Exception fee int err Code String msg Problem while reading schemas from inputs of Union msg Collector collect msg Message Type Error throw Type Checker Exception u msg err Code Pig Exception fee Do cast insertion only if we are typed and if its not union onschema In case of union onschema the foreach with cast is added in Union On Schema Setter if schema null u is On Schema Insert casting to inputs if necessary for int i i inputs size i For Each inserted Op insert Cast For Each In Between If Necessary Logical Relational Operator inputs get i u We may have to compute the schema of the input again because we have just inserted if inserted Op null if inserted Op get Alias null inserted Op set Alias Logical Relational Operator inputs get i get Alias try this visit inserted Op catch Frontend Exception fee int err Code String msg Problem while casting inputs of Union msg Collector collect msg Message Type Error throw Type Checker Exception u msg err Code Pig Exception fee u reset Schema u get Schema For casting insertion for relational operators only if it s necessary Currently this only does shallow casting param from Op param to Op return the inserted operator null is no insertion throws Frontend Exception private For Each insert Cast For Each In Between If Necessary Logical Relational Operator from Op Logical Relational Operator to Op throws Frontend Exception Make sure that they are adjacent and the direction is from from Op to to Op List Operator pre List plan get Predecessors to Op boolean found false for Operator tmp Op pre List compare by reference if tmp Op from Op found true break if found int err Code String msg Two operators that require a cast in between are not adjacent throw Type Checker Exception from Op msg err Code Pig Exception null retrieve input schema to be casted this will be used later Logical Schema from Schema null Logical Schema to Schema null try from Schema from Op get Schema to Schema to Op get Schema catch Frontend Exception fe int err Code String msg Problem while reading schema from input of from Op get Class get Simple Name throw Type Checker Exception from Op msg err Code Pig Exception fe make sure the supplied target Schema has the same number of members as number of output fields from from Op if from Schema size to Schema size int err Code String msg Schema size mismatch for casting Input schema size from Schema size Target schema size to Schema size throw Type Checker Exception to Op msg err Code Pig Exception null Plans inside Generate Fields that do not need casting will only have Project Fields that need casting will have Project Cast Array List Logical Expression Plan generate Plans new Array List Logical Expression Plan Logical Plan inner Plan new Logical Plan create Generate for foreach Generate lo Gen new Generate inner Plan generate Plans new boolean to Schema size inner Plan add lo Gen Create For Each to be inserted For Each foreach new For Each plan foreach set Inner Plan inner Plan int cast Needed Counter for int i i from Schema size i Inner Load inner Load new Inner Load inner Plan foreach i inner Plan add inner Load inner Plan connect inner Load lo Gen Logical Expression Plan gen Plan new Logical Expression Plan Project Expression project new Project Expression gen Plan i lo Gen gen Plan add project add casting if necessary by comparing target types to the input schema Logical Field Schema fs null fs from Schema get Field i This only does shallow checking Logical Field Schema out Field Schema out Field Schema to Schema get Field i if out Field Schema type fs type cast Needed Counter Cast Expression castexp new Cast Expression gen Plan project out Field Schema castexp set Location to Op get Location generate Plans add gen Plan if we really need casting if cast Needed Counter Flatten List This is just cast insertion so we do n t have any flatten Array List Boolean flatten List new Array List Boolean for int i i to Schema size i flatten List add Boolean value Of false Manipulate the plan structure plan add foreach plan insert Between from Op foreach to Op return foreach else plan remove foreach return null Override public void visit Split Output op throws Frontend Exception op reset Schema Operator Plan lp op get Plan Split Output can only have input List Operator list lp get Predecessors op if list size int err Code String msg Split Output can not have more than one input Found list size input s throw Type Checker Exception op msg err Code Pig Exception null Logical Expression Plan cond Plan op get Filter Plan Check that the inner plan has only output port if cond Plan get Sources size int err Code String msg Split s inner plan can only have one output leaf msg Collector collect msg Message Type Error throw Type Checker Exception op msg err Code Pig Exception null visit Expression Plan cond Plan op byte inner Cond Type Logical Expression cond Plan get Sources get get Type if inner Cond Type Data Type int err Code String msg Split s condition must evaluate to boolean Found Data Type find Type Name inner Cond Type msg Collector collect msg Message Type Error throw Type Checker Exception op msg err Code Pig Exception null try Compute the schema op get Schema catch Frontend Exception fe int err Code String msg Problem while reading schemas from inputs of Split Output msg Collector collect msg Message Type Error throw Type Checker Exception op msg err Code Pig Exception fe Distinct output schema should be the same as input param op throws Visitor Exception Override public void visit Distinct op throws Visitor Exception op reset Schema try Compute the schema op get Schema catch Frontend Exception fe int err Code String msg Problem while reading schemas from inputs of Distinct msg Collector collect msg Message Type Error throw Type Checker Exception op msg err Code Pig Exception fe Override public void visit Limit limit throws Frontend Exception limit reset Schema Logical Expression Plan expression Plan limit get Limit Plan if expression Plan null Check that the inner plan has only output port if expression Plan get Sources size int err Code String msg Limit s expression plan can only have one output msg Collector collect msg Message Type Error throw Type Checker Exception limit msg err Code Pig Exception null visit the limit expression visit Expression Plan expression Plan limit check limit expression type byte inner Cond Type Logical Expression expression Plan get Sources get get Type cast to long if it is a bytearray if inner Cond Type Data Type insert Atomic Cast For Inner Plan expression Plan limit Data Type else it must be an int or a long else if inner Cond Type Data Type inner Cond Type Data Type int err Code String msg Limit s expression must evaluate to Long or Integer Found Data Type find Type Name inner Cond Type msg Collector collect msg Message Type Error throw Type Checker Exception limit msg err Code Pig Exception null try Compute the schema limit get Schema catch Frontend Exception fe int err Code String msg Problem while reading schemas from inputs of Limit msg Collector collect msg Message Type Error throw Type Checker Exception limit msg err Code Pig Exception fe Return concatenated of all fields from all input operators If one of the inputs have no schema then we can not construct the output schema param cs throws Visitor Exception public void visit Cross cs throws Visitor Exception cs reset Schema try Compute the schema cs get Schema catch Frontend Exception fe int err Code String msg Problem while reading schemas from inputs of Cross msg Collector collect msg Message Type Error throw Type Checker Exception cs msg err Code Pig Exception fe The schema of sort output will be the same as sort input throws Frontend Exception public void visit Sort sort throws Frontend Exception sort reset Schema Type checking internal plans for int i i sort get Sort Col Plans size i Logical Expression Plan sort Col Plan sort get Sort Col Plans get i Check that the inner plan has only output port if sort Col Plan get Sources size int err Code String msg Sort s inner plan can only have one output leaf msg Collector collect msg Message Type Error throw Type Checker Exception sort msg err Code Pig Exception null visit Expression Plan sort Col Plan sort try Compute the schema sort get Schema catch Frontend Exception fee int err Code String msg Problem while reconciling output schema of Sort msg Collector collect msg Message Type Error throw Type Checker Exception sort msg err Code Pig Exception fee The schema of rank output will be the same as input plus a rank field throws Frontend Exception public void visit Rank rank throws Frontend Exception rank reset Schema Type checking internal plans List Logical Expression Plan rank Col Plans rank get Rank Col Plans for int i i rank Col Plans size i Logical Expression Plan rank Col Plan rank Col Plans get i Check that the inner plan has only output port if rank Col Plan get Sources size int err Code String msg Rank s inner plan can only have one output leaf msg Collector collect msg Message Type Error throw Type Checker Exception rank msg err Code Pig Exception null visit Expression Plan rank Col Plan rank try Compute the schema rank get Schema catch Frontend Exception fee int err Code String msg Problem while reconciling output schema of Rank msg Collector collect msg Message Type Error throw Type Checker Exception rank msg err Code Pig Exception fee The schema of split output will be the same as split input public void visit Split split throws Visitor Exception Operator Plan lp split get Plan List Operator input List lp get Predecessors split if input List size int err Code String msg Split can not have more than one input Found input List size input s throw Type Checker Exception split msg err Code Pig Exception null split reset Schema try Compute the schema split get Schema catch Frontend Exception fe int err Code String msg Problem while reconciling output schema of Split msg Collector collect msg Message Type Error throw Type Checker Exception split msg err Code Pig Exception fe Join visitor throws Frontend Exception public void visit Join join throws Frontend Exception try join reset Schema join get Schema catch Frontend Exception fe int err Code String msg Can not resolve Join output schema msg Collector collect msg Message Type Error throw Type Checker Exception join msg err Code Pig Exception fe Multi Map Integer Logical Expression Plan join Col Plans join get Expression Plans List Operator inputs join get Inputs Logical Plan plan Type checking internal plans for int i i inputs size i Array List Logical Expression Plan inner Plans new Array List Logical Expression Plan join Col Plans get i for int j j inner Plans size j Logical Expression Plan inner Plan inner Plans get j Check that the inner plan has only output port if inner Plan get Sources size int err Code String msg Join s inner plans can only have one output leaf msg Collector collect msg Message Type Error throw Type Checker Exception join msg err Code Pig Exception null visit Expression Plan inner Plan join try if is Join On Multi Cols join merge all the inner plan outputs so we know what type our group column should be byte group Type get Atomic Join Col Type join go through all inputs again to add cast if necessary for int i i inputs size i Collection Logical Expression Plan expr Plans join get Join Plan i there should be one and only expression plan that gets checked in get Atomic Join Col Type Logical Expression Plan expr Plan expr Plans iterator next Checking inner Plan size already done above byte inner Type Logical Expression expr Plan get Sources get get Type if inner Type group Type insert Atomic Cast For Inner Plan expr Plan join group Type else schema of the group by key Logical Schema group By Schema get Schema From Inner Plans join get Expression Plans join go through all inputs again to add cast if necessary for int i i inputs size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan join get Join Plan i for int j j inner Plans size j Logical Expression Plan inner Plan inner Plans get j Logical Expression output Exp Logical Expression inner Plan get Sources get byte inner Type output Exp get Type byte expected Type group By Schema get Field j type if Data Type is Atomic inner Type Data Type inner Type int err Code String msg Join s inner plans can only have one output leaf msg Collector collect msg Message Type Error throw Type Checker Exception join msg err Code Pig Exception null if inner Type expected Type insert Atomic Cast For Inner Plan inner Plan join expected Type catch Frontend Exception fe int err Code String msg Can not resolve Join output schema msg Collector collect msg Message Type Error throw Type Checker Exception join msg err Code Pig Exception fe try join reset Schema join get Schema catch Frontend Exception fe int err Code String msg Can not resolve Join output schema msg Collector collect msg Message Type Error throw Type Checker Exception join msg err Code Pig Exception fe param join return true if there is more than one join column for an input private boolean is Join On Multi Cols Join join Multi Map Integer Logical Expression Plan expr Plans join get Expression Plans if expr Plans null expr Plans size throw new Assertion Error Join is Join On Multi Cols can only be called after it has an join expression plans return expr Plans get size This can be used to get the merged type of output join col only when the join col is of atomic type return The type of the join col throws Frontend Exception private byte get Atomic Join Col Type Join join throws Frontend Exception if is Join On Multi Cols join int err Code String msg get Atomic Join Col Type is used only when dealing with atomic group col throw new Frontend Exception msg err Code Pig Exception false null byte group Type Data Type merge all the inner plan outputs so we know what type our group column should be for int i i plan get Predecessors join size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan join get Join Plan i if inner Plans size int err Code String msg Each Group input has to have the same number of inner plans throw new Frontend Exception msg err Code Pig Exception false null byte inner Type Logical Expression inner Plans get get Sources get get Type byte new Group Type Data Type merge Type group Type inner Type if new Group Type int err Code String msg Can not merge join keys incompatible types Outer Data Type find Type Name group Type inner Data Type find Type Name inner Type throw new Frontend Exception msg err Code Pig Exception else group Type new Group Type return group Type This can be used to get the merged type of output join col only when the join cogroup col is of atomic type return The type of the join col throws Frontend Exception private byte get Atomic Col Type Multi Map Integer Logical Expression Plan all Expr Plans throws Frontend Exception if is Multi Expr Plan Per Input all Expr Plans int err Code String msg get Atomic Join Col Type is used only when dealing with atomic group col throw new Frontend Exception msg err Code Pig Exception false null byte group Type Data Type merge all the inner plan outputs so we know what type our group column should be for int i i all Expr Plans size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan all Expr Plans get i if inner Plans size int err Code String msg Each Group input has to have the same number of inner plans throw new Frontend Exception msg err Code Pig Exception false null byte inner Type Logical Expression inner Plans get get Sources get get Type byte new Group Type Data Type merge Type group Type inner Type if new Group Type int err Code String msg Can not merge join keys incompatible types Outer Data Type find Type Name group Type inner Data Type find Type Name inner Type throw new Frontend Exception msg err Code Pig Exception else group Type new Group Type return group Type private boolean is Multi Expr Plan Per Input Multi Map Integer Logical Expression Plan expr Plans if expr Plans null expr Plans size throw new Assertion Error Join is Join On Multi Cols can only be called after it has an join expression plans return expr Plans get size Cast the single output operator of inner Plan to to Type param inner Plan param rel Op join or cogroup param to Type throws Frontend Exception private void insert Atomic Cast For Inner Plan Logical Expression Plan inner Plan Logical Relational Operator rel Op byte to Type throws Frontend Exception if Data Type is Usable Type to Type int err Code String msg Can not cast to Data Type find Type Name to Type throw Type Checker Exception rel Op msg err Code Pig Exception null List Operator outputs inner Plan get Sources if outputs size int err Code String msg Expected one output Found outputs size outputs throw Type Checker Exception rel Op msg err Code Pig Exception null Logical Expression current Output Logical Expression outputs get Type Checking Exp Visitor collect Cast Warning rel Op current Output get Type to Type msg Collector Logical Field Schema new new Logical Field Schema current Output get Field Schema alias null to Type add cast new Cast Expression inner Plan current Output new visit modified inner plan visit Expression Plan inner Plan rel Op Create combined group by join column schema based on join cogroup expression plans for all inputs This implementation is based on the assumption that all the inputs have the same join col tuple arity param expr Plans return throws Frontend Exception private Logical Schema get Schema From Inner Plans Multi Map Integer Logical Expression Plan expr Plans Logical Relational Operator op throws Frontend Exception this fs List represents all the columns in group tuple List Logical Field Schema fs List new Array List Logical Field Schema int output Schema Size expr Plans get size by default they are all bytearray for type checking we do n t care about aliases for int i i output Schema Size i fs List add new Logical Field Schema null null Data Type merge all the inner plan outputs so we know what type our group column should be for int i i expr Plans size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan expr Plans get i for int j j inner Plans size j Logical Expression e Op Logical Expression inner Plans get j get Sources get byte inner Type e Op get Type if e Op instanceof Project Expression if Project Expression e Op is Project Star there is a project star and there is more than one expression plan int err Code String msg Grouping attributes can either be star or a list of expressions but not both msg Collector collect msg Message Type Error throw new Frontend Exception msg err Code Pig Exception false null merge the type Logical Field Schema group Fs fs List get j group Fs type Data Type merge Type group Fs type inner Type if group Fs type Data Type String col Type join if op instanceof Cogroup col Type group String msg col Type column no j in relation no i of col Type statement has datatype Data Type find Type Name inner Type which is incompatible with type of corresponding column in earlier relation s in the statement msg Collector collect msg Message Type Error Type Checker Exception ex new Type Checker Exception op msg Pig Exception ex set Marked As Show To User true throw ex create schema from field schemas Logical Schema tuple Schema new Logical Schema for Logical Field Schema fs fs List tuple Schema add Field fs return tuple Schema Group All group by cols from all inputs have to be of the same type throws Frontend Exception Override public void visit Cogroup cg throws Frontend Exception try cg reset Schema cg get Schema catch Frontend Exception fe int err Code String msg Can not resolve Group output schema msg Collector collect msg Message Type Error throw Type Checker Exception cg msg err Code Pig Exception fe Multi Map Integer Logical Expression Plan group By Plans cg get Expression Plans List Operator inputs cg get Inputs Logical Plan plan Type checking internal plans for int i i inputs size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan cg get Expression Plans get i for int j j inner Plans size j Logical Expression Plan inner Plan inner Plans get j Check that the inner plan has only output port if inner Plan get Sources size int err Code String msg Group s inner plans can only have one output leaf msg Collector collect msg Message Type Error throw Type Checker Exception cg msg err Code Pig Exception null visit Expression Plan inner Plan cg try if is Co Group On Multi Cols cg merge all the inner plan outputs so we know what type our group column should be byte group Type get Atomic Col Type cg get Expression Plans go through all inputs again to add cast if necessary for int i i inputs size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan cg get Expression Plans get i Checking inner Plan size already done above byte inner Type Logical Expression inner Plans get get Sources get get Type if inner Type group Type insert Atomic Cast For Inner Plan inner Plans get cg group Type else Logical Schema group By Schema get Schema From Inner Plans cg get Expression Plans cg go through all inputs again to add cast if necessary for int i i inputs size i List Logical Expression Plan inner Plans new Array List Logical Expression Plan group By Plans get i for int j j inner Plans size j Logical Expression Plan inner Plan inner Plans get j byte inner Type Logical Expression inner Plan get Sources get get Type byte expected Type Data Type if Data Type is Atomic inner Type Data Type inner Type int err Code String msg Sorry group by complex types will be supported soon msg Collector collect msg Message Type Error throw Type Checker Exception cg msg err Code Pig Exception null expected Type group By Schema get Field j type if inner Type expected Type insert Atomic Cast For Inner Plan inner Plan cg expected Type catch Frontend Exception fe int err Code String msg Can not resolve Group output schema msg Collector collect msg Message Type Error throw Type Checker Exception cg msg err Code Pig Exception fe try cg reset Schema cg get Schema catch Frontend Exception fe int err Code String msg Can not resolve Group output schema msg Collector collect msg Message Type Error throw Type Checker Exception cg msg err Code Pig Exception fe param co Group return true if there is more than one join column for an input private boolean is Co Group On Multi Cols Cogroup co Group Multi Map Integer Logical Expression Plan expr Plans co Group get Expression Plans if expr Plans null expr Plans size throw new Assertion Error Co Group is Join On Multi Cols can only becalled after it has an join expression plans return expr Plans get size 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig import com google common collect Maps import org apache pig data Tuple import org apache pig tools pigstats Pig Status Reporter import java io Exception import java lang reflect import java util Array List import java util List import java util Map Base class for Pig Fs that are functions from Tuples to generic type Handles marshalling objects basic error checking etc Also infers output Schema and provides a function to verify the input Tuple Extend this class and implement the pre exec Tuple input pre method when writting a that operates on multiple inputs from the Tuple public abstract class Typed Output Eval Func extends Eval Func Used to implement output Schema below protected Class out Type Class null public Class get Output Type Class return out Type Class Suppress Warnings unchecked public Typed Output Eval Func out Type Class Class get Type Arguments Typed Output Eval Func class get Class get Increment Hadoop counters for bad inputs which are either null or too small protected void verify Input Tuple input int minimum Size throws Exception verify Udf Input get Counter Group input minimum Size Incremented counters will use this as the counter group Typically this works fine since the subclass name is enough to identify the In some cases though i e a wrapper that is a facade to a number of different transformation functions a more specific group name is needed protected String get Counter Group return get Class get Name Get the actual type arguments a child class has used to extend a generic base class param base Class the base class param child Class the child class return a list of the raw classes for the actual type arguments Suppress Warnings unchecked protected static List Class get Type Arguments Class base Class Class extends child Class Map Type Type resolved Types Maps new Hash Map Type type child Class start walking up the inheritance hierarchy until we hit base Class while get Class type equals base Class if type instanceof Class there is no useful information for us in raw types so just keep going type Class type get Generic Superclass else Parameterized Type parameterized Type Parameterized Type type Class raw Type Class parameterized Type get Raw Type Type actual Type Arguments parameterized Type get Actual Type Arguments Type Variable type Parameters raw Type get Type Parameters for int i i actual Type Arguments length i resolved Types put type Parameters i actual Type Arguments i if raw Type equals base Class type raw Type get Generic Superclass finally for each actual type argument provided to base Class determine if possible the raw class for that type argument Type actual Type Arguments if type instanceof Class actual Type Arguments Class type get Type Parameters else actual Type Arguments Parameterized Type type get Actual Type Arguments List Class type Arguments As Classes new Array List Class resolve types by chasing down type variables for Type base Type actual Type Arguments while resolved Types contains Key base Type base Type resolved Types get base Type type Arguments As Classes add get Class base Type return type Arguments As Classes Get the underlying class for a type or null if the type is a variable type param type the type return the underlying class Suppress Warnings unchecked private static Class get Class Type type if type instanceof Class return Class type else if type instanceof Parameterized Type return get Class Parameterized Type type get Raw Type else if type instanceof Generic Array Type Type component Type Generic Array Type type get Generic Component Type Class component Class get Class component Type if component Class null return Array new Instance component Class get Class else return null else return null Increment Hadoop counters for bad inputs which are either null or too small param klass the name of the calling class for recording purposes param input the tuple passed to the param minimum Size the minimum size required of the tuple protected static void verify Udf Input String klass Tuple input int minimum Size throws Exception if input null safe Incr Counter klass Null Input throw new Exception Null input to klass else if input size minimum Size String reason Too Few Arguments Got input size Needed At Least minimum Size safe Incr Counter klass reason throw new Exception Not enough arguments to klass got input size expected at least minimum Size else safe Incr Counter klass Valid Input protected static void safe Incr Counter String group String name Long increment Pig Status Reporter get Instance incr Counter group name increment 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer plans import java util Hash Set import java util Set import org apache pig Load Func import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators Cast import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer relational Operators Load import org apache pig backend hadoop executionengine physical Layer relational Operators Store import org apache pig impl Pig Context import org apache pig impl plan Depth First Walker import org apache pig impl plan Visitor Exception public class Udf Cache Ship Files Visitor extends Phy Plan Visitor private Set String cache Files new Hash Set String private Set String ship Files new Hash Set String public Udf Cache Ship Files Visitor Physical Plan plan super plan new Depth First Walker Physical Operator Physical Plan plan Override public void visit Load Load ld throws Visitor Exception if ld get Cache Files null cache Files add All ld get Cache Files if ld get Ship Files null ship Files add All ld get Ship Files Override public void visit Store Store st throws Visitor Exception if st get Cache Files null cache Files add All st get Cache Files if st get Ship Files null ship Files add All st get Ship Files public void visit User Func User Func udf throws Visitor Exception if udf get Cache Files null cache Files add All udf get Cache Files if udf get Ship Files null ship Files add All udf get Ship Files Override public void visit Cast Cast cast if cast get Func Spec null Object obj Pig Context instantiate Func From Spec cast get Func Spec if obj instanceof Load Func Load Func load Func Load Func obj if load Func get Cache Files null cache Files add All load Func get Cache Files if load Func get Ship Files null ship Files add All load Func get Ship Files public Set String get Cache Files return cache Files public Set String get Ship Files return ship Files 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig impl util import java io Exception import java io Serializable import java util Arrays import java util Hash Map import java util Properties import org apache commons logging Log import org apache commons logging Log Factory import org apache hadoop conf Configuration import org apache pig backend hadoop executionengine map Reduce Layer Configuration public class Context private static final Log Log Factory get Log Context class private Configuration jconf null private Hash Map Context Key Properties udf Confs private Properties client Sys Props static final String pig client sys props static final String pig udf context private static Thread Local Context tss new Thread Local Context Override public Context initial Value return new Context private Context udf Confs new Hash Map Context Key Properties return a Thread Local link Context public static Context get Context return tss get internal pig use only should be called from user code public static void set Udf Context Context udf Context tss set udf Context internal pig use only should be called from user code Static Data Cleanup public static void static Data Cleanup tss new Thread Local Context Override public Context initial Value return new Context internal pig use only should be called from user code public Hash Map Context Key Properties get Udf Confs return udf Confs internal pig use only should be called from user code public void set Client System Props Properties properties client Sys Props properties Get the System Properties Read only as on the client machine from where Pig was launched This will include command line properties passed at launch time return client side System Properties including command line properties public Properties get Client System Props return client Sys Props Adds the Job Conf to this singleton Will be called on the backend by the Map and Reduce functions so that Fs can obtain the Job Conf on the backend public void add Job Conf Configuration conf jconf conf Get the Job Conf This should only be called on the backend It will return null on the frontend return Job Conf for this job This is a copy of the Job Conf Nothing written here will be kept by the system get Conf should be used for recording specific information public Configuration get Job Conf if jconf null return new Configuration jconf else return null Get a properties object that is specific to this Note that if a given is called multiple times in a script and each instance passes different arguments then each will be provided with different configuration object This can be used by loaders to pass their input object path or and separate themselves from other instances of the same loader Constructor arguments could also be used as they are available on both the front and back end Note that this can only be used to share information across instantiations of the same function in the front end and between front end and back end It can not be used to share information between instantiations that is between map and or reduce instances on the back end at runtime param c of the obtaining the properties object param args String arguments that make this instance of the unique return reference to the properties object specific to the calling This is a reference not a copy Any changes to this object will automatically be propogated to other instances of the calling this function Suppress Warnings rawtypes public Properties get Properties Class c String args Context Key k generate Key c args Properties p udf Confs get k if p null p new Properties udf Confs put k p return p Get a properties object that is specific to this Note that if a given is called multiple times in a script they will all be provided the same configuration object It is up to the to make sure the multiple instances do not stomp on each other It is guaranteed that this properties object will be separate from that provided to any other Note that this can only be used to share information across instantiations of the same function in the front end and between front end and back end It can not be used to share information between instantiations that is between map and or reduce instances on the back end at runtime param c of the obtaining the properties object return reference to the properties object specific to the calling This is a reference not a copy Any changes to this object will automatically be propogated to other instances of the calling this function Suppress Warnings rawtypes public Properties get Properties Class c Context Key k generate Key c null Properties p udf Confs get k if p null p new Properties udf Confs put k p return p Serialize the specific information into an instance of Job Conf This function is intended to be called on the front end in preparation for sending the data to the backend param conf Job Conf to serialize into throws Exception if underlying serialization throws it public void serialize Configuration conf throws Exception conf set Object Serializer serialize udf Confs conf set Object Serializer serialize client Sys Props Internal pig use public String serialize try return Object Serializer serialize udf Confs catch Exception e error Context serialize throws error e return null Populate the udf Confs field This function is intended to be called by Map configure or Reduce configure on the backend It assumes that add Job Conf has already been called throws Exception if underlying deseralization throws it Suppress Warnings unchecked public void deserialize throws Exception udf Confs Hash Map Context Key Properties Object Serializer deserialize jconf get client Sys Props Properties Object Serializer deserialize jconf get public void deserialize For Spark String udf Confs Str String client Sys Props Str throws Exception if udf Confs Str null client Sys Props Str null udf Confs Hash Map Context Key Properties Object Serializer deserialize udf Confs Str client Sys Props Properties Object Serializer deserialize client Sys Props Str private Context Key generate Key Class c String args return new Context Key c get Name args public void reset udf Confs clear public boolean is Conf Empty return udf Confs is Empty Convenience method for code to check where it runs see return boolean type value public boolean is Frontend mapred task id is for mapreduce job application attempt id is for return this jconf null jconf get Configuration null jconf get Configuration null Make a shallow copy of the context Override public Context clone Context other new Context other client Sys Props this client Sys Props other jconf this jconf other udf Confs this udf Confs return other Class that acts as key for hashmap in Context it holds the class and args of the udf and implements equals and hash Code static class Context Key implements Serializable private static final long serial Version private String class Name private String args Context Key String class Name String args this class Name class Name this args args String get Class Name return class Name String get Args return args Override public String to String return Context Key class Name class Name args Arrays to String args Override public int hash Code final int prime int result result prime result Arrays hash Code args result prime result class Name null class Name hash Code return result Override public boolean equals Object obj if this obj return true if obj null return false if get Class obj get Class return false Context Key other Context Key obj if Arrays equals args other args return false if class Name null if other class Name null return false else if class Name equals other class Name return false return true public Properties get Client Sys Props return client Sys Props 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine map Reduce Layer import org apache pig backend hadoop executionengine physical Layer Physical Operator import org apache pig backend hadoop executionengine physical Layer expression Operators User Func import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig impl plan Dependency Order Walker import org apache pig impl plan Visitor Exception public class End Of All Input Needed Visitor extends Phy Plan Visitor private boolean needed false public End Of All Input Needed Visitor Physical Plan plan super plan new Dependency Order Walker Physical Operator Physical Plan plan Override public void visit User Func User Func user Func throws Visitor Exception super visit User Func user Func if user Func need End Of All Input Processing needed true public boolean need End Of All Input Processing return needed 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical optimizer import java util Collection import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig impl util Multi Map import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator Plan import org apache pig newplan Plan Walker import org apache pig newplan Reverse Dependency Order Walker import org apache pig newplan logical expression All Same Expression Visitor import org apache pig newplan logical expression Logical Expression import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Relational Nodes Visitor public class Uid Resetter extends Logical Relational Nodes Visitor public Uid Resetter Operator Plan plan throws Frontend Exception super plan new Dependency Order Walker plan Override public void visit Load load throws Frontend Exception load reset Uid Override public void visit Filter filter throws Frontend Exception filter reset Uid Expression Uid Resetter uid Resetter new Expression Uid Resetter filter get Filter Plan uid Resetter visit Override public void visit Store store throws Frontend Exception store reset Uid Override public void visit Join join throws Frontend Exception join reset Uid Collection Logical Expression Plan join Plans join get Expression Plan Values for Logical Expression Plan join Plan join Plans Expression Uid Resetter fs Resetter new Expression Uid Resetter join Plan fs Resetter visit Override public void visit For Each foreach throws Frontend Exception foreach reset Uid Operator Plan inner Plan foreach get Inner Plan Plan Walker new Walker current Walker spawn Child Walker inner Plan push Walker new Walker current Walker walk this pop Walker Override public void visit Generate gen throws Frontend Exception gen reset Uid List Logical Expression Plan gen Plans gen get Output Plans for Logical Expression Plan gen Plan gen Plans Expression Uid Resetter fs Resetter new Expression Uid Resetter gen Plan fs Resetter visit Override public void visit Inner Load load throws Frontend Exception load reset Uid load get Projection reset Uid Override public void visit Cogroup lo Cogroup throws Frontend Exception lo Cogroup reset Uid Multi Map Integer Logical Expression Plan exp Plans lo Cogroup get Expression Plans for Logical Expression Plan exp Plan exp Plans values Expression Uid Resetter uid Resetter new Expression Uid Resetter exp Plan uid Resetter visit Override public void visit Split lo Split throws Frontend Exception lo Split reset Uid Override public void visit Split Output lo Split Output throws Frontend Exception lo Split Output reset Uid Expression Uid Resetter uid Resetter new Expression Uid Resetter lo Split Output get Filter Plan uid Resetter visit Override public void visit Union lo Union throws Frontend Exception lo Union reset Uid Override public void visit Sort lo Sort throws Frontend Exception lo Sort reset Uid List Logical Expression Plan sort Plans lo Sort get Sort Col Plans for Logical Expression Plan sort Plan sort Plans Expression Uid Resetter uid Resetter new Expression Uid Resetter sort Plan uid Resetter visit Override public void visit Rank lo Rank throws Frontend Exception lo Rank reset Uid List Logical Expression Plan rank Plans lo Rank get Rank Col Plans for Logical Expression Plan rank Plan rank Plans Expression Uid Resetter uid Resetter new Expression Uid Resetter rank Plan uid Resetter visit Override public void visit Distinct lo Distinct throws Frontend Exception lo Distinct reset Uid Override public void visit Limit lo Limit throws Frontend Exception lo Limit reset Uid if lo Limit get Limit Plan null Expression Uid Resetter uid Resetter new Expression Uid Resetter lo Limit get Limit Plan uid Resetter visit Override public void visit Cross lo Cross throws Frontend Exception lo Cross reset Uid Override public void visit Stream lo Stream throws Frontend Exception lo Stream reset Uid class Expression Uid Resetter extends All Same Expression Visitor protected Expression Uid Resetter Operator Plan p throws Frontend Exception super p new Reverse Dependency Order Walker p Override protected void execute Logical Expression op throws Frontend Exception op reset Uid 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import java util List import org apache pig impl logical Layer Frontend Exception import org apache pig newplan Operator import org apache pig newplan Operator Plan Superclass for all unary expressions public abstract class Unary Expression extends Logical Expression Will add this operator to the plan and connect it to the left and right hand side operators param name of the operator param plan plan this operator is part of param exp expression that this expression operators on public Unary Expression String name Operator Plan plan Logical Expression exp super name plan plan add this plan connect this exp Get the expression that this unary expression operators on return expression on the left hand side throws Frontend Exception public Logical Expression get Expression throws Frontend Exception List Operator preds plan get Successors this if preds null return null return Logical Expression preds get 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop executionengine physical Layer expression Operators import java util Array List import java util List import org apache pig impl plan Operator Key import org apache pig backend hadoop executionengine physical Layer plans Phy Plan Visitor import org apache pig backend hadoop executionengine physical Layer plans Physical Plan import org apache pig backend hadoop executionengine physical Layer expression Operators Expression Operator import org apache pig impl plan Visitor Exception public abstract class Unary Expression Operator extends Expression Operator Expression Operator expr private transient List Expression Operator child public Unary Expression Operator Operator Key k int rp super k rp public Unary Expression Operator Operator Key k super k Override public boolean supports Multiple Inputs Auto generated method stub return false Set the contained expression to the be the input value public void set Input As Expr Physical Plan plan expr Expression Operator plan get Predecessors this get Set the contained expression explicitly This is mostly for testing param e Expression to contain public void set Expr Expression Operator e expr e Get the contained expression return contained expression public Expression Operator get Expr return expr protected void clone Helper Unary Expression Operator op Do n t clone this as it is just a reference to something already in the plan expr op expr result Type op get Result Type Get child expression of this expression Override public List Expression Operator get Child Expressions if child null child new Array List Expression Operator child add expr return child 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig data import java io Data Input import java io Data Output import java io Exception import java util List import org apache pig backend executionengine Exec Exception public class Unlimited Null Tuple extends Abstract Tuple Override public int size return Integer Override public Object get int field Num throws Exec Exception return null Override public List Object get All throw new Runtime Exception Unimplemented Override public void set int field Num Object val throws Exec Exception throw new Exec Exception Unimplemented Override public void append Object val throw new Runtime Exception Unimplemented Override public long get Memory Size throw new Runtime Exception Unimplemented Override public void read Fields Data Input arg throws Exception throw new Exception Unimplemented Override public void write Data Output arg throws Exception throw new Exception Unimplemented Override public int compare To Object o throw new Runtime Exception Unimplemented 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical expression import java io Exception import java lang reflect Method import java util Array List import java util List import java util Properties import java util regex Matcher import java util regex Pattern import org apache pig Algebraic import org apache pig Eval Func import org apache pig Func Spec import org apache pig builtin Invoker Generator import org apache pig builtin Nondeterministic import org apache pig data Data Type import org apache pig data Schema Tuple Class Generator Gen Context import org apache pig data Schema Tuple Frontend import org apache pig impl Pig Context import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig impl util Context import org apache pig newplan Operator import org apache pig newplan Operator Plan import org apache pig newplan Plan Visitor import org apache pig newplan logical Util import org apache pig newplan logical relational Logical Schema import org apache pig newplan logical relational Logical Schema Logical Field Schema import org apache pig parser Logical Plan Builder import org apache pig parser Source Location import com google common base Joiner import com google common collect Lists public class User Func Expression extends Logical Expression private Func Spec m Func Spec private Eval Func ef null private String signature private static int sig Seq private boolean via Define false this represents whether the function was instantiate via a statement or not public User Func Expression Operator Plan plan Func Spec func Spec super User Func plan m Func Spec func Spec plan add this if signature null signature Integer to String sig Seq public User Func Expression Operator Plan plan Func Spec func Spec List Logical Expression args this plan func Spec for Logical Expression arg args plan connect this arg public User Func Expression Operator Plan plan Func Spec func Spec boolean via Define this plan func Spec this via Define via Define public User Func Expression Operator Plan plan Func Spec func Spec List Logical Expression args boolean via Define this plan func Spec args this via Define via Define private boolean lazily Initialize Invoker Function false private List Logical Expression save Args For Later null private boolean invoker Is Static false private String func Name null private String package Name null public User Func Expression Operator Plan plan Func Spec func Spec List Logical Expression args boolean via Define boolean lazily Initialize Invoker Function boolean invoker Is Static String package Name String func Name this plan func Spec args via Define this save Args For Later args this lazily Initialize Invoker Function lazily Initialize Invoker Function this package Name package Name this func Name func Name this invoker Is Static invoker Is Static public Func Spec get Func Spec return m Func Spec Override public void accept Plan Visitor v throws Frontend Exception if v instanceof Logical Expression Visitor throw new Frontend Exception Expected Logical Expression Visitor Logical Expression Visitor v visit this Override public boolean is Equal Operator other throws Frontend Exception For the purpose of optimization rules specially Logical Expression Simplifier a non deterministic udf is not equal to another So returning false for such cases Note that the function is also invoked by implementations of Operator Plan is Equal that function is called from test cases to compare logical plans and it will return false even if the plans are clones if this is Deterministic return false if other instanceof User Func Expression User Func Expression exp User Func Expression other if m Func Spec equals exp m Func Spec return false List Operator my Succs get Plan get Successors this List Operator their Succs other get Plan get Successors other if my Succs null their Succs null if my Succs null their Succs null return true else only one of the udfs has null successors return false if my Succs size their Succs size return false for int i i my Succs size i if my Succs get i is Equal their Succs get i return false return true else return false public boolean is Deterministic throws Frontend Exception Class udf Class try udf Class Pig Context resolve Class Name get Func Spec get Class Name catch Exception ioe throw new Frontend Exception Can not instantiate get Func Spec ioe if udf Class get Annotation Nondeterministic class null return true return false public List Logical Expression get Arguments throws Frontend Exception List Operator successors null List Logical Expression args new Array List Logical Expression try successors plan get Successors this if successors null return args for Operator lo successors args add Logical Expression lo catch Frontend Exception e return args return args param func Spec the Func Spec to set public void set Func Spec Func Spec func Spec m Func Spec func Spec ef Eval Func Pig Context instantiate Func From Spec m Func Spec Override public Logical Schema Logical Field Schema get Field Schema throws Frontend Exception if field Schema null return field Schema Logical Schema input Schema new Logical Schema List Operator succs plan get Successors this if succs null for Operator lo succs if Logical Expression lo get Field Schema null input Schema null break input Schema add Field Logical Expression lo get Field Schema if lazily Initialize Invoker Function initialize Invoker Function Since ef only set one time we never change its value so we can optimize it by instantiate only once This significantly optimize the performance of frontend if ef null ef Eval Func Pig Context instantiate Func From Spec m Func Spec ef set Context Signature signature Schema translated Input Schema Util translate Schema input Schema if translated Input Schema null Properties props Context get Context get Properties ef get Class props put pig evalfunc inputschema signature translated Input Schema if ef instanceof Algebraic In case of Algebraic func set original input Schema to Initial Intermed Final for String func new String Algebraic ef get Initial Algebraic ef get Intermed Algebraic ef get Final Class c Pig Context instantiate Func From Spec new Func Spec func get Class props Context get Context get Properties c props put pig evalfunc inputschema signature translated Input Schema Store input Schema into the context ef set Input Schema translated Input Schema Schema udf Schema ef output Schema translated Input Schema if udf Schema null udf Schema size throw new Frontend Exception Given returns an improper Schema Schema should only contain one field of a Tuple Bag or a single type Returns udf Schema appendability should come from a setting Schema Tuple Frontend register To Generate If Possible translated Input Schema false Gen Context Schema Tuple Frontend register To Generate If Possible udf Schema false Gen Context if udf Schema null Schema Field Schema fs if udf Schema size fs new Schema Field Schema null null Data Type find Type ef get Return Type else if udf Schema size fs new Schema Field Schema udf Schema get Field else fs new Schema Field Schema null udf Schema Data Type field Schema Util translate Field Schema fs field Schema normalize else field Schema new Logical Schema Logical Field Schema null null Data Type find Type ef get Return Type uid Only Field Schema field Schema merge Uid uid Only Field Schema return field Schema private void initialize Invoker Function List Logical Field Schema field Schemas Lists new Array List for Logical Expression le save Args For Later try field Schemas add le get Field Schema catch Frontend Exception e throw new Runtime Exception e Class func Class if invoker Is Static try func Class Pig Context resolve Class Name package Name catch Exception e throw new Runtime Exception Invoker function name not found package Name e else func Class Data Type find Type Class field Schemas get type if func Class is Primitive func Class Logical Plan Builder type To Class func Class Class parameter Types new Class field Schemas size invoker Is Static int idx for int i invoker Is Static i field Schemas size i parameter Types idx Data Type find Type Class field Schemas get i type List Integer primitive Parameters Lists new Array List for int i i parameter Types length i if parameter Types i is Primitive primitive Parameters add i int tries primitive Parameters size Method m null for int i i tries i Class tmp Parameter Types new Class parameter Types length for int j j parameter Types length j tmp Parameter Types j parameter Types j int tmp i int idx while tmp if tmp int to Flip primitive Parameters get idx tmp Parameter Types to Flip Logical Plan Builder type To Class tmp Parameter Types to Flip tmp idx try m func Class get Method func Name parameter Types if m null parameter Types tmp Parameter Types break catch Security Exception e throw new Runtime Exception Not allowed to access method func Name on class func Class e catch No Such Method Exception e we just continue as we are searching for a match post boxing if m null throw new Runtime Exception Given method func Name does not exist on class func Class String ctor Args new String ctor Args func Class get Name ctor Args func Name ctor Args List String params Lists new Array List for Class param parameter Types params add param get Name ctor Args Joiner on join params need to allow them to define such a function so it can be cached etc esp if they reuse m Func Spec new Func Spec Invoker Generator class get Name ctor Args lazily Initialize Invoker Function false need to fix this to use the updated code it currently wo n t copy properly if called before it s done maybe that s ok Override public Logical Expression deep Copy Logical Expression Plan lg Exp Plan throws Frontend Exception User Func Expression copy null try copy new User Func Expression lg Exp Plan this get Func Spec clone via Define copy signature signature Deep copy the input expressions List Operator inputs plan get Successors this if inputs null for Operator op inputs Logical Expression input Logical Expression op Logical Expression input Copy input deep Copy lg Exp Plan lg Exp Plan add input Copy lg Exp Plan connect copy input Copy catch Clone Not Supported Exception e e print Stack Trace copy set Location new Source Location location return copy public String to String String Builder msg new String Builder msg append Name name get Func Spec Type if field Schema null msg append Data Type find Type Name field Schema type else msg append null msg append Uid if field Schema null msg append field Schema uid else msg append null msg append return msg to String public String get Signature return signature public boolean is Via Define return via Define public Eval Func get Eval Func if ef null ef Eval Func Pig Context instantiate Func From Spec m Func Spec return ef 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig builtin import java io Byte Array Input Stream import java io Byte Array Output Stream import java io Exception import java io Pushback Input Stream import java math Big Decimal import java math Big Integer import java util Arrays import java util Deque import java util Hash Map import java util Linked List import java util Map import org apache commons logging Log import org apache commons logging Log Factory import org apache pig Load Store Caster import org apache pig Pig Warning import org apache pig Resource Schema Resource Field Schema import org apache pig data Bag Factory import org apache pig data Data Bag import org apache pig data Data Byte Array import org apache pig data Data Type import org apache pig data Default Bag Factory import org apache pig data Tuple import org apache pig data Tuple Factory import org apache pig impl util Log Utils import org joda time Date Time This abstract class provides standard conversions between utf encoded data and pig data types It is intended to be extended by load and store functions such as link Pig Storage public class Utf Storage Converter implements Load Store Caster protected Bag Factory m Bag Factory Bag Factory get Instance protected Tuple Factory m Tuple Factory Tuple Factory get Instance protected final Log m Log Log Factory get Log get Class private static final Integer m Max Int Integer value Of Integer private static final Integer m Min Int Integer value Of Integer private static final Long m Max Long Long value Of Long private static final Long m Min Long Long value Of Long private static final int public Utf Storage Converter private char find Start Char char start throws Exception switch start case return case return case return default throw new Exception Unknown start character private Data Bag consume Bag Pushback Input Stream in Resource Field Schema field Schema throws Exception if field Schema null throw new Exception Schema is null Resource Field Schema fss field Schema get Schema get Fields Tuple t int buf while buf in read if buf throw new Exception Unexpect end of bag if fss length throw new Exception Only tuple is allowed inside bag schema Resource Field Schema fs fss Data Bag db Default Bag Factory get Instance new Default Bag while true t consume Tuple in fs if t null db add t while buf in read buf if buf throw new Exception Unexpect end of bag if buf break return db private Tuple consume Tuple Pushback Input Stream in Resource Field Schema field Schema throws Exception if field Schema null throw new Exception Schema is null int buf Byte Array Output Stream m Out while buf in read buf if buf throw new Exception Unexpect end of tuple if buf in unread buf return null Tuple t Tuple Factory get Instance new Tuple if field Schema get Schema null field Schema get Schema get Fields length Resource Field Schema fss field Schema get Schema get Fields Interpret item inside tuple one by one based on the inner schema for int i i fss length i Object field Resource Field Schema fs fss i int delimit if i fss length delimit if Data Type is Complex fs get Type field consume Complex Type in fs while buf in read delimit if buf throw new Exception Unexpect end of tuple else m Out new Byte Array Output Stream while buf in read delimit if buf throw new Exception Unexpect end of tuple if buf delimit break m Out write buf field parse Simple Type m Out to Byte Array fs t append field else No inner schema treat everything inside tuple as bytearray Deque Character level new Linked List Character keep track of nested tuple bag map We do not interpret save them as bytearray m Out new Byte Array Output Stream while true buf in read if buf throw new Exception Unexpect end of tuple if buf buf buf level push char buf m Out write buf else if buf level is Empty End of tuple Data Byte Array value new Data Byte Array m Out to Byte Array t append value break else if buf level is Empty Data Byte Array value new Data Byte Array m Out to Byte Array t append value m Out reset else if buf buf buf if level peek find Start Char char buf level pop else throw new Exception Malformed tuple m Out write buf else m Out write buf return t private Map String Object consume Map Pushback Input Stream in Resource Field Schema field Schema throws Exception int buf boolean empty Map true while buf in read if buf throw new Exception Unexpect end of map Hash Map String Object m new Hash Map String Object Byte Array Output Stream m Out new Byte Array Output Stream while true Read key assume key can not contains special character such as while buf in read end of map if empty Map buf return m if buf throw new Exception Unexpect end of map empty Map false m Out write buf String key bytes To Char Array m Out to Byte Array if key length throw new Exception Map key can not be null Read value m Out reset Deque Character level new Linked List Character keep track of nested tuple bag map We do not interpret save them as bytearray while true buf in read if buf throw new Exception Unexpect end of map if buf buf buf level push char buf else if buf level is Empty End of map break else if buf buf buf if level is Empty throw new Exception Malformed map if level peek find Start Char char buf level pop else if buf level is Empty Current map item complete break m Out write buf Object value null if field Schema null field Schema get Schema null m Out size value bytes To Object m Out to Byte Array field Schema get Schema get Fields else if m Out size untyped map value new Data Byte Array m Out to Byte Array m put key value m Out reset if buf break return m private Object bytes To Object byte b Resource Field Schema fs throws Exception Object field if Data Type is Complex fs get Type Byte Array Input Stream bis new Byte Array Input Stream b Pushback Input Stream in new Pushback Input Stream bis field consume Complex Type in fs else field parse Simple Type b fs return field private Object consume Complex Type Pushback Input Stream in Resource Field Schema complex Field Schema throws Exception Object field switch complex Field Schema get Type case Data Type field consume Bag in complex Field Schema break case Data Type field consume Tuple in complex Field Schema break case Data Type field consume Map in complex Field Schema break default throw new Exception Unknown complex data type return field private Object parse Simple Type byte b Resource Field Schema simple Field Schema throws Exception Object field switch simple Field Schema get Type case Data Type field bytes To Integer b break case Data Type field bytes To Long b break case Data Type field bytes To Float b break case Data Type field bytes To Double b break case Data Type field bytes To Char Array b break case Data Type field new Data Byte Array b break case Data Type field bytes To Boolean b break case Data Type field bytes To Big Integer b break case Data Type field bytes To Big Decimal b break case Data Type field bytes To Date Time b break default throw new Exception Unknown simple data type return field Override public Data Bag bytes To Bag byte b Resource Field Schema schema throws Exception if b null return null Data Bag db try Byte Array Input Stream bis new Byte Array Input Stream b Pushback Input Stream in new Pushback Input Stream bis db consume Bag in schema catch Exception e Log Utils warn this Unable to interpret value Arrays to String b in field being converted to type bag caught Parse Exception e get Message field discarded Pig Warning m Log return null return db Override public String bytes To Char Array byte b throws Exception if b null return null return new String b Override public Double bytes To Double byte b if b null b length return null try return Double value Of new String b catch Number Format Exception nfe Log Utils warn this Unable to interpret value Arrays to String b in field being converted to double caught Number Format Exception nfe get Message field discarded Pig Warning m Log return null Override public Float bytes To Float byte b throws Exception if b null b length return null String s if b length b b length b b length f s new String b b length else s new String b try return Float value Of s catch Number Format Exception nfe Log Utils warn this Unable to interpret value Arrays to String b in field being converted to float caught Number Format Exception nfe get Message field discarded Pig Warning m Log return null Override public Boolean bytes To Boolean byte b throws Exception if b null return null String s new String b if s equals Ignore Case true return Boolean else if s equals Ignore Case false return Boolean else return null Sanity check of whether this number is a valid integer or long param number the number to check return true if it does n t contain any invalid characters i e only contains digits and private static boolean sanity Check Integer Long String number for int i i number length i if number char At i number char At i i number char At i valid one else contains invalid characters must not be a integer or long return false return true Override public Integer bytes To Integer byte b throws Exception if b null b length return null String s new String b s s trim Integer ret null See Using exception handling to check if it s a double is very expensive So we write our sanity check if sanity Check Integer Long s try ret Integer value Of s catch Number Format Exception nfe if ret null It s possible that this field can be interpreted as a double Unfortunately Java does n t handle this in Integer value Of So we need to try to convert it to a double and if that works then go to an int try Double d Double value Of s Need to check for an overflow error if Double compare d double Value m Max Int double Value Double compare d double Value m Min Int double Value Log Utils warn this Value d too large for integer Pig Warning m Log return null return Integer value Of d int Value catch Number Format Exception nfe Log Utils warn this Unable to interpret value Arrays to String b in field being converted to int caught Number Format Exception nfe get Message field discarded Pig Warning m Log return null return ret Override public Long bytes To Long byte b throws Exception if b null b length return null String s new String b trim if s ends With l s ends With s s substring s length See Using exception handling to check if it s a double is very expensive So we write our sanity check Long ret null if sanity Check Integer Long s try ret Long value Of s catch Number Format Exception nfe if ret null It s possible that this field can be interpreted as a double Unfortunately Java does n t handle this in Long value Of So we need to try to convert it to a double and if that works then go to an long try Double d Double value Of s Need to check for an overflow error if Double compare d double Value m Max Long double Value Double compare d double Value m Min Long double Value Log Utils warn this Value d too large for long Pig Warning m Log return null return Long value Of d long Value catch Number Format Exception nfe Log Utils warn this Unable to interpret value Arrays to String b in field being converted to long caught Number Format Exception nfe get Message field discarded Pig Warning m Log return null return ret Override public Date Time bytes To Date Time byte b throws Exception if b null return null try String dt Str new String b return To Date extract Date Time dt Str catch Illegal Argument Exception e Log Utils warn this Unable to interpret value Arrays to String b in field being converted to datetime caught Illegal Argument Exception e get Message field discarded Pig Warning m Log return null Override public Map String Object bytes To Map byte b Resource Field Schema field Schema throws Exception if b null return null Map String Object map try Byte Array Input Stream bis new Byte Array Input Stream b Pushback Input Stream in new Pushback Input Stream bis map consume Map in field Schema catch Exception e Log Utils warn this Unable to interpret value Arrays to String b in field being converted to type map caught Parse Exception e get Message field discarded Pig Warning m Log return null return map Override public Tuple bytes To Tuple byte b Resource Field Schema field Schema throws Exception if b null return null Tuple t try Byte Array Input Stream bis new Byte Array Input Stream b Pushback Input Stream in new Pushback Input Stream bis t consume Tuple in field Schema catch Exception e Log Utils warn this Unable to interpret value Arrays to String b in field being converted to type tuple caught Parse Exception e get Message field discarded Pig Warning m Log return null return t Override public Big Integer bytes To Big Integer byte b throws Exception if b null b length return null return new Big Integer new String b Override public Big Decimal bytes To Big Decimal byte b throws Exception if b null b length return null return new Big Decimal new String b Override public byte to Bytes Data Bag bag throws Exception return bag to String get Bytes Override public byte to Bytes String s throws Exception return s get Bytes Override public byte to Bytes Double d throws Exception return d to String get Bytes Override public byte to Bytes Float f throws Exception return f to String get Bytes Override public byte to Bytes Integer i throws Exception return i to String get Bytes Override public byte to Bytes Long l throws Exception return l to String get Bytes Override public byte to Bytes Boolean b throws Exception return b to String get Bytes Override public byte to Bytes Date Time dt throws Exception return dt to String get Bytes Override public byte to Bytes Map String Object m throws Exception return Data Type map To String m get Bytes Override public byte to Bytes Tuple t throws Exception return t to String get Bytes Override public byte to Bytes Data Byte Array a throws Exception return a get Override public byte to Bytes Big Integer bi throws Exception return bi to String get Bytes Override public byte to Bytes Big Decimal bd throws Exception return bd to String get Bytes 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig newplan logical import java util Array List import java util Linked List import java util List import java util Set import org apache pig impl logical Layer Frontend Exception import org apache pig impl logical Layer schema Schema import org apache pig newplan Dependency Order Walker import org apache pig newplan Operator import org apache pig newplan logical expression Logical Expression Plan import org apache pig newplan logical expression Project Expression import org apache pig newplan logical relational Cogroup import org apache pig newplan logical relational Cross import org apache pig newplan logical relational Cube import org apache pig newplan logical relational Distinct import org apache pig newplan logical relational Filter import org apache pig newplan logical relational For Each import org apache pig newplan logical relational Generate import org apache pig newplan logical relational Inner Load import org apache pig newplan logical relational Join import org apache pig newplan logical relational Limit import org apache pig newplan logical relational Load import org apache pig newplan logical relational Native import org apache pig newplan logical relational Rank import org apache pig newplan logical relational Sort import org apache pig newplan logical relational Split import org apache pig newplan logical relational Split Output import org apache pig newplan logical relational Store import org apache pig newplan logical relational Stream import org apache pig newplan logical relational Union import org apache pig newplan logical relational Logical Plan import org apache pig newplan logical relational Logical Relational Nodes Visitor import org apache pig newplan logical relational Logical Relational Operator import org apache pig newplan logical relational Logical Schema import com google common collect Lists public class Util public static Logical Schema translate Schema Schema schema if schema null return null Logical Schema s new Logical Schema List Schema Field Schema ll schema get Fields for Schema Field Schema f ll Logical Schema Logical Field Schema f new Logical Schema Logical Field Schema f alias translate Schema f schema f type s add Field f return s public static Logical Schema Logical Field Schema translate Field Schema Schema Field Schema fs Logical Schema new Schema null if fs schema null new Schema translate Schema fs schema Logical Schema Logical Field Schema new Fs new Logical Schema Logical Field Schema fs alias new Schema fs type return new Fs This function translates the new Logical Schema into old Schema format required by Physical Operators param schema Logical Schema to be converted to Schema return Schema that is converted from Logical Schema throws Frontend Exception public static Schema translate Schema Logical Schema schema if schema null return null Schema s new Schema List Logical Schema Logical Field Schema ll schema get Fields for Logical Schema Logical Field Schema f ll Schema Field Schema f null try f new Schema Field Schema f alias translate Schema f schema f type f canonical Name Long f uid to String s add f catch Frontend Exception e return s If schema argument has fields where a bag does not contain a tuple schema it inserts a tuple schema It does so for all inner levels eg bag int bag int param sch return modified schema throws Frontend Exception public static Schema fix Schema Add Tuple In Bag Schema sch throws Frontend Exception Logical Schema log Sch translate Schema sch log Sch normalize return translate Schema log Sch public static Schema Field Schema translate Field Schema Logical Schema Logical Field Schema fs if fs null return null Schema new Schema null if fs schema null new Schema translate Schema fs schema Schema Field Schema new Fs null try new Fs new Schema Field Schema null new Schema fs type catch Frontend Exception e return new Fs public static For Each add For Each After Logical Plan plan Logical Relational Operator op int branch Set Integer columns To Drop throws Frontend Exception For Each foreach new For Each plan plan add foreach List Operator next plan get Successors op if next null Logical Relational Operator next Op Logical Relational Operator next get branch plan insert Between op foreach next Op foreach set Alias op get Alias else plan connect op foreach Logical Plan inner Plan new Logical Plan foreach set Inner Plan inner Plan Logical Schema schema op get Schema build foreach inner plan List Logical Expression Plan exps new Array List Logical Expression Plan Generate gen new Generate inner Plan exps new boolean schema size columns To Drop size inner Plan add gen for int i j i schema size i if columns To Drop contains i continue Inner Load inner Load new Inner Load inner Plan foreach i inner Plan add inner Load inner Plan connect inner Load gen Logical Expression Plan exp new Logical Expression Plan Project Expression prj new Project Expression exp j gen exp add prj exps add exp return foreach Returns a Linked List of operators contained within the physical plan which implement the supplied class in dependency order Returns an empty Linked List of no such operators exist param plan param op Class return a Linked List of operators contained within the plan which implement the supplied class empty if no such ops exist throws Frontend Exception public static extends Logical Relational Operator Linked List get Logical Relational Operators Logical Plan plan Class op Class throws Frontend Exception Op Finder finder new Op Finder plan op Class finder visit return finder get Found Ops private static class Op Finder extends Logical Relational Operator extends Logical Relational Nodes Visitor final Class op Class private Linked List found Ops Lists new Linked List public Op Finder Logical Plan plan Class op Class throws Frontend Exception super plan new Dependency Order Walker plan this op Class op Class public Linked List get Found Ops return found Ops Suppress Warnings unchecked private void visit Op Logical Relational Operator op if op Class is Assignable From op get Class found Ops add op public void visit Load load throws Frontend Exception visit Op load Override public void visit Filter filter throws Frontend Exception visit Op filter Override public void visit Store store throws Frontend Exception visit Op store Override public void visit Join join throws Frontend Exception visit Op join Override public void visit For Each foreach throws Frontend Exception visit Op foreach Override public void visit Generate gen throws Frontend Exception visit Op gen Override public void visit Inner Load load throws Frontend Exception visit Op load Override public void visit Cube cube throws Frontend Exception visit Op cube Override public void visit Cogroup lo Cogroup throws Frontend Exception visit Op lo Cogroup Override public void visit Split lo Split throws Frontend Exception visit Op lo Split Override public void visit Split Output lo Split Output throws Frontend Exception visit Op lo Split Output Override public void visit Union lo Union throws Frontend Exception visit Op lo Union Override public void visit Sort lo Sort throws Frontend Exception visit Op lo Sort Override public void visit Rank lo Rank throws Frontend Exception visit Op lo Rank Override public void visit Distinct lo Distinct throws Frontend Exception visit Op lo Distinct Override public void visit Limit lo Limit throws Frontend Exception visit Op lo Limit Override public void visit Cross lo Cross throws Frontend Exception visit Op lo Cross Override public void visit Stream lo Stream throws Frontend Exception visit Op lo Stream Override public void visit Native native throws Frontend Exception visit Op native 
Licensed to the Apache Software Foundation under one or more contributor license agreements See the file distributed with this work for additional information regarding copyright ownership The licenses this file to you under the Apache License Version the License you may not use this file except in compliance with the License You may obtain a copy of the License at http www apache org licenses Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an either express or implied See the License for the specific language governing permissions and limitations under the License package org apache pig backend hadoop accumulo import java io Buffered Output Stream import java io File import java io File Input Stream import java io File Output Stream import java io Exception import java io Input Stream import java net import java net Decoder import java text Message Format import java util Enumeration import java util Hash Map import java util Hash Set import java util Map import java util Set import java util jar Jar File import java util jar Jar Output Stream import java util jar Manifest import java util zip Zip Entry import java util zip Zip File import java util zip Zip Output Stream import org apache hadoop conf Configuration import org apache hadoop fs File System import org apache hadoop fs Path import org apache hadoop util Jar Finder import org apache hadoop util String Utils import org apache log j Logger import com google common base Preconditions public class Utils private static final Logger log Logger get Logger Utils class Thanks Base public static void add Dependency Jars Configuration conf Class classes throws Exception File System local Fs File System get Local conf Set String jars new Hash Set String Add jars that are already in the tmpjars variable jars add All conf get String Collection tmpjars add jars as we find them to a map of contents jar name so that we can avoid creating new jars for classes that have already been packaged Map String String packaged Classes new Hash Map String String Add jars containing the specified classes for Class clazz classes if clazz null continue Path path find Or Create Jar clazz local Fs packaged Classes if path null log warn Could not find jar for class clazz in order to ship it to the cluster continue if local Fs exists path log warn Could not validate jar file path for class clazz continue jars add path to String if jars is Empty return conf set tmpjars String Utils array To String jars to Array new String jars size If org apache hadoop util Jar Finder is available hadoop finds the Jar for a class or creates it if it does n t exist If the class is in a directory in the classpath it creates a Jar on the fly with the contents of the directory and returns the path to that Jar If a Jar is created it is created in the system temporary directory Otherwise returns an existing jar that contains a class of the same name Maintains a mapping from jar contents to the tmp jar created param my class the class to find param fs the File System with which to qualify the returned path param packaged Classes a map of class name to path return a jar file that contains the class throws Exception private static Path find Or Create Jar Class my class File System fs Map String String packaged Classes throws Exception attempt to locate an existing jar for the class String jar find Containing Jar my class packaged Classes if null jar jar is Empty jar Jar Finder get Jar my class update Map jar packaged Classes if null jar jar is Empty return null log debug String format For class s using jar s my class get Name jar return new Path jar make Qualified fs Add entries to code packaged Classes code corresponding to class files contained in code jar code param jar The jar who s content to list param packaged Classes map class jar private static void update Map String jar Map String String packaged Classes throws Exception if null jar jar is Empty return Zip File zip null try zip new Zip File jar for Enumeration extends Zip Entry iter zip entries iter has More Elements Zip Entry entry iter next Element if entry get Name ends With class packaged Classes put entry get Name jar finally if null zip zip close Find a jar that contains a class of the same name if any It will return a jar file even if that is not the first thing on the class path that has a class with the same name Looks first on the classpath and then in the code packaged Classes code map param my class the class to find return a jar file that contains the class or null throws Exception private static String find Containing Jar Class my class Map String String packaged Classes throws Exception Class Loader loader my class get Class Loader String class file my class get Name replace All class first search the classpath for Enumeration itr loader get Resources class file itr has More Elements url itr next Element if jar equals url get Protocol String to Return url get Path if to Return starts With file to Return to Return substring file length Decoder is a misnamed class since it actually decodes x www form urlencoded type rather than actual encoding which the file path has Therefore it would decode s to s which is incorrect spaces are actually either unencoded or encoded as Replace s first so that they are kept sacred during the decoding process to Return to Return replace All to Return Decoder decode to Return return to Return replace All now look in any jars we ve packaged using Jar Finder Returns null when no jar is found return packaged Classes get class file Returns the full path to the Jar containing the class It always return a param klass class return path to the Jar containing the class Suppress Warnings rawtypes public static String jar Finder Get Jar Class klass Preconditions check Not Null klass klass Class Loader loader klass get Class Loader if loader null String class file klass get Name replace All class try for Enumeration itr loader get Resources class file itr has More Elements url itr next Element String path url get Path if path starts With file path path substring file length path Decoder decode path if jar equals url get Protocol path Decoder decode path return path replace All else if file equals url get Protocol String klass Name klass get Name klass Name klass Name replace class path path substring path length klass Name length File base Dir new File path File test Dir new File System get Property test build dir target test dir test Dir test Dir get Absolute File if test Dir exists test Dir mkdirs File temp Jar File create Temp File hadoop test Dir temp Jar new File temp Jar get Absolute Path jar create Jar base Dir temp Jar return temp Jar get Absolute Path catch Exception e throw new Runtime Exception e return null private static void copy To Zip Stream Input Stream is Zip Entry entry Zip Output Stream zos throws Exception zos put Next Entry entry byte arr new byte int read is read arr while read zos write arr read read is read arr public static void jar Dir File dir String relative Path Zip Output Stream zos throws Exception Preconditions check Not Null relative Path relative Path Preconditions check Not Null zos zos by spec if there is a manifest it must be the first entry in the File manifest File new File dir Jar File Zip Entry manifest Entry new Zip Entry Jar File if manifest File exists zos put Next Entry manifest Entry new Manifest write new Buffered Output Stream zos zos close Entry else Input Stream is new File Input Stream manifest File try copy To Zip Stream is manifest Entry zos finally if is null is close if zos null zos close Entry zos close Entry zip Dir dir relative Path zos true zos close private static void zip Dir File dir String relative Path Zip Output Stream zos boolean start throws Exception String dir List dir list for String a Dir List dir List File f new File dir a Dir List if f is Hidden if f is Directory if start Zip Entry dir Entry new Zip Entry relative Path f get Name zos put Next Entry dir Entry zos close Entry String file Path f get Path File file new File file Path zip Dir file relative Path f get Name zos false else String path relative Path f get Name if path equals Jar File Zip Entry an Entry new Zip Entry path Input Stream is new File Input Stream f try copy To Zip Stream is an Entry zos finally if is null is close if zos null zos close Entry private static void create Jar File dir File jar File throws Exception Preconditions check Not Null dir dir Preconditions check Not Null jar File jar File File jar Dir jar File get Parent File if jar Dir exists if jar Dir mkdirs throw new Exception Message Format format could not create dir jar Dir Jar Output Stream zos new Jar Output Stream new File Output Stream jar File jar Dir dir zos 
