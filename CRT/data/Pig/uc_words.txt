[ PIG-3419 ] Pluggable Execution Engine < p > In an effort to adapt Pig to work using Apache Tez ( < a href= https : //issues.apache.org/jira/browse/TEZ class= external-link rel= nofollow > https : //issues.apache.org/jira/browse/TEZ < /a > ) , I made some changes to allow for a cleaner ExecutionEngine abstraction than existed before . The changes are not that major as Pig was already relatively abstracted out between the frontend and backend . The changes in the attached commit are essentially the barebones changes & # 8211 ; I tried to not change the structure of Pig s different components too much . I think it will be interesting to see in the future how we can refactor more areas of Pig to really honor this abstraction between the frontend and backend . < /p > < p > Some of the changes was to reinstate an ExecutionEngine interface to tie together the front end and backend , and making the changes in Pig to delegate to the EE when necessary , and creating an MRExecutionEngine that implements this interface . Other work included changing ExecType to cycle through the ExecutionEngines on the classpath and select the appropriate one ( this is done using Java ServiceLoader , exactly how MapReduce does for choosing the framework to use between local and distributed mode ) . Also I tried to make ScriptState , JobStats , and PigStats as abstract as possible in its current state . I think in the future some work will need to be done here to perhaps re-evaluate the usage of ScriptState and the responsibilities of the different statistics classes . I haven t touched the PPNL , but I think more abstraction is needed here , perhaps in a separate patch . < /p > 
[ PIG-1510 ] Add ` deepCopy ` for LogicalExpressions < p > It would be useful to have a way to ` deepCopy ` an expression . ` deepCopy ` will create a new object so that changes made to one object will not reflect in the copy . There are 2 reasons why we don t override clone. < /p > < ul > < li > It may be better to use ` deepCopy ` since the copy semantics are explicit ( since deepCopy may be expensive ) . < /li > < li > A second important reason for defining ` deepCopy ` as a separate routine is that it can be passed a plan as an argument which will be updated as the expression is copied ( through plan.add and plan.connect ) . < /li > < /ul > < p > The usage would look like the following : < /p > < div class= preformatted panel style= border-width : 1px ; > < div class= preformattedContent panelContent > < pre > LogicalExpressionPlan logicalPlan = new LogicalExpressionPlan ( ) ; LogicalExpression copyExpression = origExpression.deepCopy ( logicalPlan ) ; < /pre > < /div > < /div > < p > An immediate motivation for this would be for constructing the expressions that constitute the CNF form of an expression. < /p > 
[ PIG-4408 ] Merge join should support replicated join as a predecessor < p > Since a replicated join doesn t trigger a reduce or change the output ordering a merge join should work after it < /p > 
[ PIG-4458 ] Support UDFs in a FOREACH Before a Merge Join < p > Right now , the MapSideMergeValidator outright rejects any foreach that has a UDF in it : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-keyword > private < /span > < span class= code-object > boolean < /span > isAcceptableForEachOp ( Operator lo ) < span class= code-keyword > throws < /span > LogicalToPhysicalTranslatorException { < span class= code-keyword > if < /span > ( lo < span class= code-keyword > instanceof < /span > LOForEach ) { OperatorPlan innerPlan = ( ( LOForEach ) lo ) .getInnerPlan ( ) ; validateMapSideMerge ( innerPlan.getSinks ( ) , innerPlan ) ; < span class= code-keyword > return < /span > ! containsUDFs ( ( LOForEach ) lo ) ; } < span class= code-keyword > else < /span > { < span class= code-keyword > return < /span > < span class= code-keyword > false < /span > ; } } < /pre > < /div > < /div > < p > There is a TODO for this later on in that same class ( inside containsUDFs ) : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-comment > // TODO ( dvryaboy ) : in the < span class= code-keyword > future < /span > we could relax < span class= code-keyword > this < /span > rule by tracing what fields < /span > < span class= code-comment > // are being passed into the UDF , and only refusing < span class= code-keyword > if < /span > the UDF is working on the < /span > < span class= code-comment > // join key . Transforms of other fields should be ok. < /span > < /pre > < /div > < /div > < p > We should do the TODO and relax this requirement or just remove it altogether < /p > 
[ PIG-4570 ] Allow AvroStorage to use a class for the schema < p > When you are managing Avro Schemas in your source code and you have the Java classes to serialize/deserialize Avro , it would be easier to do something like : < /p > < blockquote > < p > store a into tmp/myuser using AvroStorage ( null , -schemaclass com.myco.MyUser ) < /p > < /blockquote > < p > Rather than passing as the first agurment , the entire Avro schema json in the pig script , or keeping a ( potentially outdated ) set of Avro schema files in HDFS for the -schemafile option. < /p > < p > If your classpath is already aware of how to serialize/deserialize the Avro data , it would be good to leverage that with a fully qualified classname. < /p > 
[ PIG-4673 ] Built In UDF - REPLACE_MULTI : For a given string , search and replace all occurrences of search keys with replacement values . < p > Lets say we have a string = A1B2C3D4 . Our objective is to replace A with 1 , B with 2 , C with 3 and D with 4 to derive 11223344 string . < /p > < p > Using existing REPLACE method < /p > < p > REPLACE ( REPLACE ( REPLACE ( REPLACE ( A1B2C3D4 , A , 1 ) , B , 2 ) , C , 3 ) , D , 4 ) < /p > < p > With proposed UDF : REPLACE_MULTI method < /p > < p > General Syntax : < /p > < p > REPLACE_MULTI ( sourceString , [ search1 # replacement1 , ... ] ) < /p > < p > REPLACE_MULTI ( A1B2C3D4 , [ A # 1 , B # 2 , C # 3 , D # 4 ] ) < /p > < p > Advantage : < /p > < p > 1 . Function calls are reduced . < br/ > 2 . Ease to code and better readable. < /p > < p > Let me know your thoughts/ inputs on having this UDF in Piggy Bank . Will take this up based on this. < /p > 
[ PIG-4796 ] Authenticate with Kerberos using a keytab file < p > When running in a Kerberos secured environment users are faced with the limitation that their jobs can not run longer than the ( remaining ) ticket lifetime of their Kerberos tickets . The environment I work in these tickets expire after 10 hours , thus limiting the maximum job duration to at most 10 hours ( which is a problem ) . < /p > < p > In the Hadoop tooling there is a feature where you can authenticate using a Kerberos keytab file ( essentially a file that contains the encrypted form of the kerberos principal and password ) . Using this the running application can request new tickets from the Kerberos server when the initial tickets expire. < /p > < p > In my Java/Hadoop applications I commonly include these two lines : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-object > System < /span > .setProperty ( < span class= code-quote > java.security.krb5.conf < /span > , < span class= code-quote > /etc/krb5.conf < /span > ) ; UserGroupInformation.loginUserFromKeytab ( < span class= code-quote > nbasjes @ XXXXXX.NET < /span > , < span class= code-quote > /home/nbasjes/.krb/nbasjes.keytab < /span > ) ; < /pre > < /div > < /div > < p > This way I have run an Apache Flink based application for more than 170 hours ( about a week ) on the kerberos secured Yarn cluster. < /p > < p > What I propose is to have a feature that I can set the relevant kerberos values in my pig script and from there be able to run a pig job for many days on the secured cluster. < /p > < p > Proposal how this can look in a pig script : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > SET java.security.krb5.conf /etc/krb5.conf SET job.security.krb5.principal nbasjes @ XXXXXX.NET SET job.security.krb5.keytab /home/nbasjes/.krb/nbasjes.keytab < /pre > < /div > < /div > < p > So iff all of these are set ( or at least the last two ) then the aforementioned UserGroupInformation.loginUserFromKeytab method is called before submitting the job to the cluster. < /p > 
[ PIG-506 ] Does pig need a NATIVE keyword ? < p > Assume a user had a job that broke easily into three pieces . Further assume that pieces one and three were easily expressible in pig , but that piece two needed to be written in map reduce for whatever reason ( performance , something that pig could not easily express , legacy job that was too important to change , etc. ) . Today the user would either have to use map reduce for the entire job or manually handle the stitching together of pig and map reduce jobs . What if instead pig provided a NATIVE keyword that would allow the script to pass off the data stream to the underlying system ( in this case map reduce ) . The semantics of NATIVE would vary by underlying system . In the map reduce case , we would assume that this indicated a collection of one or more fully contained map reduce jobs , so that pig would store the data , invoke the map reduce jobs , and then read the resulting data to continue . It might look something like this : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > A = load myfile ; X = load myotherfile ; B = group A by $ 0 ; C = foreach B generate group , myudf ( B ) ; D = < span class= code-keyword > native < /span > ( jar=mymr.jar , infile=frompig outfile=topig ) ; E = join D by $ 0 , X by $ 0 ; ... < /pre > < /div > < /div > < p > This differs from streaming in that it allows the user to insert an arbitrary amount of native processing , whereas streaming allows the insertion of one binary . It also differs in that , for streaming , data is piped directly into and out of the binary as part of the pig pipeline . Here the pipeline would be broken , data written to disk , and the native block invoked , then data read back from disk. < /p > < p > Another alternative is to say this is unnecessary because the user can do the coordination from java , using the PIgServer interface to run pig and calling the map reduce job explicitly . The advantages of the native keyword are that the user need not be worried about coordination between the jobs , pig will take care of it . Also the user can make use of existing java applications without being a java programmer. < /p > 
[ PIG-5085 ] Support FLATTEN of maps < p > I have come across users asking for this quite a few times . Don t see why we should not support it with FLATTEN instead of users having to write a UDF for that < /p > 
[ PIG-626 ] Statistics ( records read by each mapper and reducer ) < p > This uses the counters framework that hadoop has . Initially , I am just interested in finding out the number of records read by each mapper/reducer particularly for the last job in any script . A sample code to access the statistics for the last job : < /p > < p > String reducePlan = stats.getPigStats ( ) .get ( stats.getLastJobID ( ) ) .get ( PIG_STATS_REDUCE_PLAN ) ; < br/ > if ( reducePlan == null ) < /p > { System.out.println ( Records written : + stats.getPigStats ( ) .get ( stats.getLastJobID ( ) ) .get ( PIG_STATS_MAP_OUTPUT_RECORDS ) ) ; } < p > else < /p > { System.out.println ( Records written : + stats.getPigStats ( ) .get ( stats.getLastJobID ( ) ) .get ( PIG_STATS_REDUCE_OUTPUT_RECORDS ) ) ; } < p > The patch contains 7 test cases . These include tests PigStorage and BinStorage along with one for multiple MR jobs case. < /p > 
[ PIG-713 ] Autocompletion doesn t complete aliases < p > Autocompletion only knows about keywords , but in different contexts , it would be nice if it completed aliases where an alias is expected. < /p > 
[ PIG-922 ] Logical optimizer : push up project < p > This is a continuation work of < a href= https : //issues.apache.org/jira/browse/PIG-697 class= external-link rel= nofollow > PIG-697 < /a > . We need to add another rule to the logical optimizer : Push up project , ie , prune columns as early as possible. < /p > 
[ PIG-741 ] Add LIMIT as a statement that works in nested FOREACH < p > I d like to compute the top 10 results in each group. < /p > < p > The natural way to express this in Pig would be : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > A = load ... using PigStorage ( ) as ( date : < span class= code-object > int < /span > , count : < span class= code-object > int < /span > , url : chararray ) ; B = group A by ( date ) ; C = foreach B { D = order A by count desc ; E = limit D 10 ; generate FLATTEN ( E ) ; } ; dump C ; < /pre > < /div > < /div > < p > Yeah , I could write a UDF / PiggyBank function to take the top n results . But since LIMIT already exists as a statement , it seems like it should also work in the nested foreach context. < /p > < p > Example workaround code. < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > C = foreach B { D = order A by count desc ; E = util.TOP ( D , 10 ) ; generate FLATTEN ( E ) ; } ; dump C ; < /pre > < /div > < /div > 
[ PIG-760 ] Serialize schemas for PigStorage ( ) and other storage types . < p > I m finding PigStorage ( ) really convenient for storage and data interchange because it compresses well and imports into Excel and other analysis environments well. < /p > < p > However , it is a pain when it comes to maintenance because the columns are in fixed locations and I d like to add columns in some cases. < /p > < p > It would be great if load PigStorage ( ) could read a default schema from a .schema file stored with the data and if store PigStorage ( ) could store a .schema file with the data. < /p > < p > I have tested this out and both Hadoop HDFS and Pig in -exectype local mode will ignore a file called .schema in a directory of part files. < /p > < p > So , for example , if I have a chain of Pig scripts I execute such as : < /p > < p > A = load data-1 using PigStorage ( ) as ( a : int , b : int ) ; < /p > < p > store A into data-2 using PigStorage ( ) ; < /p > < p > B = load data-2 using PigStorage ( ) ; < /p > < p > describe B ; < /p > < p > describe B should output something like < /p > { a : int , b : int } 
[ PIG-893 ] support cast of chararray to other simple types < p > Pig should support casting of chararray to integer , long , float , double , bytearray . If the conversion fails for reasons such as overflow , cast should return null and log a warning. < /p > 
[ PIG-895 ] Default parallel for Pig < p > For hadoop 20 , if user don t specify the number of reducers , hadoop will use 1 reducer as the default value . It is different from previous of hadoop , in which default reducer number is usually good . 1 reducer is not what user want for sure . Although user can use parallel keyword to specify number of reducers for each statement , it is wordy . We need a convenient way for users to express a desired number of reducers . Here is my propose : < /p > < p > 1 . Add one property default_parallel to Pig . User can set default_parallel in script . Eg : < br/ > set default_parallel 10 ; < /p > < p > 2 . default_parallel is a hint to Pig . Pig is free to optimize the number of reducers ( unlike parallel keyword ) . Currently , since we do not have a mechanism to determine the optimal number of reducers , default_parallel will be always granted , unless it is override by parallel keyword. < /p > < p > 3 . If user put multiple default_parallel inside script , the last entry will be taken. < /p > 
[ PIG-928 ] UDFs in scripting languages < p > It should be possible to write UDFs in scripting languages such as python , ruby , etc . This frees users from needing to compile Java , generate a jar , etc . It also opens Pig to programmers who prefer scripting languages over Java. < /p > 
[ PIG-979 ] Acummulator Interface for UDFs < p > Add an accumulator interface for UDFs that would allow them to take a set number of records at a time instead of the entire bag. < /p > 
[ PIG-984 ] PERFORMANCE : Implement a map-side group operator to speed up processing of ordered data < p > The general group by operation in Pig needs both mappers and reducers ( the aggregation is done in reducers ) . This incurs disk writes/reads between mappers and reducers. < /p > < p > However , in the cases where the input data has the following properties < /p > < p > 1 . The records with the same key are grouped together ( such as the data is sorted by the keys ) . < br/ > 2 . The records with the same key are in the same mapper input. < /p > < p > the group by operation can be performed in the mappers only and thus remove the overhead of disk writes/reads. < /p > < p > Alan proposed adding a hint to the group by clause like this one : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > A = load input using SomeLoader ( ... ) ; B = group A by $ 0 using < span class= code-quote > mapside < /span > ; C = foreach B generate ... < /pre > < /div > < /div > < p > The proposed addition of using mapside to group will be a mapside group operator that collects all records for a given key into a buffer . When it sees a key change it will emit the key and bag for records it had buffered . It will assume that all keys for a given record are collected together and thus there is not need to buffer across keys . < /p > < p > It is expected that SomeLoader will be implemented by data systems such as Zebra to ensure the data emitted by the loader satisfies the above properties ( 1 ) and ( 2 ) . < /p > < p > It will be the responsibility of the user ( or the loader ) to guarantee these properties ( 1 ) & amp ; ( 2 ) before invoking the mapside hint for the group by clause . The Pig runtime can t check for the errors in the input data. < /p > < p > For the group by clauses with mapside hint , Pig Latin will only support group by columns ( including * ) , not group by expressions nor group all . < /p > 
[ PIG-2651 ] Provide a much easier to use accumulator interface [ PIG-2066 ] Accumulators should be able to early-terminate < p > This introduces a new interface , IteratingAccumulatorEvalFunc ( that name is NOT final ... ) . The cool thing about this patch is that it is built purely on top of the existing Accumulator code ( well , it uses < a href= https : //issues.apache.org/jira/browse/PIG-2066 title= Accumulators should be able to early-terminate class= issue-link data-issue-key= PIG-2066 > < del > PIG-2066 < /del > < /a > , but it could easily work without it ) . That is to say , it s an easier way to write accumulators without having to fork the Pig codebase. < /p > < p > The downside is that the only way I am able to provide such a clean interface is by using a second thread . I need to explore any potential performance implications , but given that most of the easy to use Pig stuff has performance implications , I think as long as we measure and and document them , it s worth the much more usable interface . Plus I don t think it will be too bad as one thread does the heavy lifting , while another just ferries values in between . SUM could now be written as : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-keyword > public < /span > class SUM < span class= code-keyword > extends < /span > IteratingAccumulatorEvalFunc & lt ; < span class= code-object > Long < /span > & gt ; { < span class= code-keyword > public < /span > < span class= code-object > Long < /span > exec ( Iterator & lt ; Tuple & gt ; it ) < span class= code-keyword > throws < /span > IOException { < span class= code-object > long < /span > sum = 0 ; < span class= code-keyword > while < /span > ( it.hasNext ( ) ) { sum += ( < span class= code-object > Long < /span > ) it.next ( ) .get ( 0 ) ; } < span class= code-keyword > return < /span > sum ; } } < /pre > < /div > < /div > < p > Besides performance tests , I need to figure out how to properly test this sort of thing . I particularly welcome advice on that front. < /p > < p > Accumulators are currently forced to process the whole bag ; getValue ( ) is called at the very end. < /p > < p > Early termination is a handy feature to be able to use ( for IsEmpty , for example ) . < /p > < p > We can add this as a new interface extending Accumulator. < /p > 
[ PIG-997 ] [ zebra ] Sorted Table Support by Zebra < p > This new feature is for Zebra to support sorted data in storage . As a storage library , Zebra will not sort the data by itself . But it will support creation and use of sorted data either through PIG or through map/reduce tasks that use Zebra as storage format. < /p > < p > The sorted table keeps the data in a totally sorted manner across all TFiles created by potentially all mappers or reducers. < /p > < p > For sorted data creation through PIG s STORE operator , if the input data is sorted through ORDER BY , the new Zebra table will be marked as sorted on the sorted columns ; < /p > < p > For sorted data creation though Map/Reduce tasks , three new static methods of the BasicTableOutput class will be provided to allow or help the user to achieve the goal . setSortInfo allows the user to specify the sorted columns of the input tuple to be stored ; getSortKeyGenerator and getSortKey help the user to generate the key acceptable by Zebra as a sorted key based upon the schema , sorted columns and the input tuple. < /p > < p > For sorted data read through PIG s LOAD operator , pass string sorted as an extra argument to the TableLoader constructor to ask for sorted table to be loaded ; < /p > < p > For sorted data read through Map/Reduce tasks , a new static method of TableInputFormat class , requireSortedTable , can be called to ask for a sorted table to be read . Additionally , an overloaded version of the new method can be called to ask for a sorted table on specified sort columns and comparator. < /p > < p > For this release , sorted table only supported sorting in ascending order , not in descending order . In addition , the sort keys must be of simple types not complex types such as RECORD , COLLECTION and MAP . < /p > < p > Multiple-key sorting is supported . But the ordering of the multiple sort keys is significant with the first sort column being the primary sort key , the second being the secondary sort key , etc. < /p > < p > In this release , the sort keys are stored along with the sort columns where the keys were originally created from , resulting in some data storage redundancy. < /p > 
[ PIG-1916 ] Nested cross < p > It is useful to have cross inside foreach nested statement . One typical use case for nested foreach is after cogroup two relations , we want to flatten the records of the same key , and do some processing . This is naturally to be achieved by cross . Eg : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > C = cogroup user by uid , session by uid ; D = foreach C { crossed = cross user , session ; -- To flatten two input bags filtered = filter crossed by user : :region == session : :region ; result = foreach crossed generate processSession ( user : :age , user : :gender , session : :ip ) ; -- Nested foreach Jira : PIG-1631 generate result ; } < /pre > < /div > < /div > < p > If we don t have cross , user have to write a UDF process the bag user , session . It is much harder than a UDF process flattened tuples . This is especially true when we have nested foreach statement ( < a href= https : //issues.apache.org/jira/browse/PIG-1631 title= Support to 2 level nested foreach class= issue-link data-issue-key= PIG-1631 > < del > PIG-1631 < /del > < /a > ) . < /p > < p > This is a candidate project for Google summer of code 2011 . More information about the program can be found at < a href= http : //wiki.apache.org/pig/GSoc2011 class= external-link rel= nofollow > http : //wiki.apache.org/pig/GSoc2011 < /a > < /p > 
[ PIG-282 ] Custom Partitioner < p > By adding custom partitioner we can give control over which output partition a key ( /value ) goes to . We can add keywords to language e.g . < /p > < p > PARTITION BY UDF ( ... ) < /p > < p > or a similar syntax . UDF returns a number between 0 and n-1 where n is number of output partitions. < /p > 
[ PIG-2125 ] Make Pig work with hadoop .NEXT < p > We need to make Pig work with hadoop .NEXT , the svn branch currently is : < a href= https : //svn.apache.org/repos/asf/hadoop/common/branches/MR-279 class= external-link rel= nofollow > https : //svn.apache.org/repos/asf/hadoop/common/branches/MR-279 < /a > < /p > 
[ PIG-1025 ] Should be able to set job priority through Pig Latin < p > Currently users can set the job name through Pig Latin by saying < /p > < p > set job.name my job name < /p > < p > The ability to set the priority would also be nice , and the patch should be small . The goal is to be able to say < /p > < p > set job.priority high < /p > < p > and throw a JobCreationException in the JobControlCompiler if the priority is not one of the allowed string values from the o.a.h.mapred.JobPriority enum : very_low , low , normal , high , very_high . Case insensitivity makes this a little nicer. < /p > 
[ PIG-1035 ] support for skewed outer join < p > Similarly to skewed inner join , skewed outer join will help to scale in the presense of join keys that don t fit into memory < /p > 
[ PIG-1069 ] [ zebra ] Order Preserving Sorted Table Union < p > The output schema will adopt a schema union semantics , namely , if an output column only appears in one component table , the result rows will have the values of the column if the rows are from that component table and null otherwise ; on the other hand , if an output column appears in multiple component tables , the types of the column in all the component tables must be identical . Otherwise , an exception will be thrown . The result rows will have the values of the column if the rows are from the component tables that have the column themselves , or null if otherwise . < /p > < p > The order preserving sort-unioned results could be further indexed by the component tables if the projection contains column ( s ) named source_table . If so specified , the component table index will be output at the position ( s ) as specified in the projection list . If the underlying table is not a union of sorted tables , use of the special column name in projection will cause an exception thrown . < /p > < p > If an attempt is made to create a table of a column named source_table , an excpetion will be thrown as the name is reserved by zebra for the virtual name . < /p > 
[ PIG-1077 ] [ Zebra ] to support record ( row ) -based file split in Zebra s TableInputFormat < p > TFile currently supports split by record sequence number ( see Jira < a href= https : //issues.apache.org/jira/browse/HADOOP-6218 title= Split TFile by Record Sequence Number class= issue-link data-issue-key= HADOOP-6218 > < del > HADOOP-6218 < /del > < /a > ) . We want to utilize this to provide record ( row ) -based input split support in Zebra. < br/ > One prominent benefit is that : in cases where we have very large data files , we can create much more fine-grained input splits than before where we can only create one big split for one big file. < /p > < p > In more detail , the new row-based getSplits ( ) works by default ( user does not specify no . of splits to be generated ) as follows : < br/ > 1 ) Select the biggest column group in terms of data size , split all of its TFiles according to hdfs block size ( 64 MB or 128 MB ) and get a list of physical byte offsets as the output per TFile . For example , let us assume for the 1st TFile we get offset1 , offset2 , ... , offset10 ; < br/ > 2 ) Invoke TFile.getRecordNumNear ( long offset ) to get the RecordNum of a key-value pair near a byte offset . For the example above , say we get recordNum1 , recordNum2 , ... , recordNum10 ; < br/ > 3 ) Stitch < span class= error > & # 91 ; 0 , recordNum1 & # 93 ; < /span > , < span class= error > & # 91 ; recordNum1+1 , recordNum2 & # 93 ; < /span > , ... , < span class= error > & # 91 ; recordNum9+1 , recordNum10 & # 93 ; < /span > , < span class= error > & # 91 ; recordNum10+1 , lastRecordNum & # 93 ; < /span > splits of all column groups , respectively to form 11 record-based input splits for the 1st TFile . < br/ > 4 ) For each input split , we need to create a TFile scanner through : TFile.createScannerByRecordNum ( long beginRecNum , long endRecNum ) . < /p > < p > Note : conversion from byte offset to record number will be done by each mapper , rather than being done at the job initialization phase . This is due to performance concern since the conversion incurs some TFile reading overhead. < /p > 
[ PIG-2353 ] RANK function like in SQL < p > Implement a function that given a ( sorted ) bag adds to each tuple a unique , increasing identifier without gaps , like what RANK does for SQL. < /p > < p > This is a candidate project for Google summer of code 2012 . More information about the program can be found at < a href= https : //cwiki.apache.org/confluence/display/PIG/GSoc2012 class= external-link rel= nofollow > https : //cwiki.apache.org/confluence/display/PIG/GSoc2012 < /a > < /p > < p > Functionality implemented so far , is available at < a href= https : //reviews.apache.org/r/5523/diff/ # index_header class= external-link rel= nofollow > https : //reviews.apache.org/r/5523/diff/ # index_header < /a > < /p > 
[ PIG-1085 ] Pass JobConf and UDF specific configuration information to UDFs < p > Users have long asked for a way to get the JobConf structure in their UDFs . It would also be nice to have a way to pass properties between the front end and back end so that UDFs can store state during parse time and use it at runtime. < /p > < p > This patch does part of what is proposed in < a href= https : //issues.apache.org/jira/browse/PIG-602 title= Pass global configurations to UDF class= issue-link data-issue-key= PIG-602 > < del > PIG-602 < /del > < /a > , but not all of it . It does not provide a way to give user specified configuration files to UDFs . So I will mark 602 as depending on this bug , but it isn t a duplicate. < /p > 
[ PIG-1354 ] UDFs for dynamic invocation of simple Java methods < p > The need to create wrapper UDFs for simple Java functions creates unnecessary work for Pig users , slows down the development process , and produces a lot of trivial classes . We can use Java s reflection to allow invoking a number of methods on the fly , dynamically , by creating a generic UDF to accomplish this. < /p > 
[ PIG-987 ] [ zebra ] Zebra Column Group Access Control < p > Access Control : when processes try to read from the column groups , Zebra should be able to handle allowed vs. disallowed user/application accesses . The security is eventuallt granted by corresponding HDFS security of the data stored. < /p > < p > Expected behavior when column group permissions are set : < /p > < p > When user selects only columns that they do not have permissions to access , Zebra should return error with message Error # : Permission denied for accessing column & lt ; column name or names & gt ; < /p > < p > Access control applies to an entire column group , so all columns in a column group have same permissions . < /p > 
[ PIG-1404 ] PigUnit - Pig script testing simplified . < p > The goal is to provide a simple xUnit framework that enables our Pig scripts to be easily : < /p > < ul class= alternate type= square > < li > unit tested < /li > < li > regression tested < /li > < li > quickly prototyped < /li > < /ul > < p > No cluster set up is required. < /p > < p > For example : < /p > < p > TestCase < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > @ Test < span class= code-keyword > public < /span > void testTop3Queries ( ) { < span class= code-object > String < /span > [ ] args = { < span class= code-quote > n=3 < /span > , } ; test = < span class= code-keyword > new < /span > PigTest ( < span class= code-quote > top_queries.pig < /span > , args ) ; < span class= code-object > String < /span > [ ] input = { < span class= code-quote > yahoo\t10 < /span > , < span class= code-quote > twitter\t7 < /span > , < span class= code-quote > facebook\t10 < /span > , < span class= code-quote > yahoo\t15 < /span > , < span class= code-quote > facebook\t5 < /span > , .... } ; < span class= code-object > String < /span > [ ] output = { < span class= code-quote > ( yahoo,25L ) < /span > , < span class= code-quote > ( facebook,15L ) < /span > , < span class= code-quote > ( twitter,7L ) < /span > , } ; test.assertOutput ( < span class= code-quote > data < /span > , input , < span class= code-quote > queries_limit < /span > , output ) ; } < /pre > < /div > < /div > < p > top_queries.pig < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > data = LOAD $ input AS ( query : CHARARRAY , count : INT ) ; ... queries_sum = FOREACH queries_group GENERATE group AS query , SUM ( queries.count ) AS count ; ... queries_limit = LIMIT queries_ordered $ n ; STORE queries_limit INTO $ output ; < /pre > < /div > < /div > < p > They are 3 modes : < /p > < ul > < li > LOCAL ( if pigunit.exectype.local properties is present ) < /li > < li > MAPREDUCE ( use the cluster specified in the classpath , same as HADOOP_CONF_DIR ) < ul > < li > automatic mini cluster ( is the default and the HADOOP_CONF_DIR to have in the class path will be : ~/pigtest/conf ) < /li > < li > pointing to an existing cluster ( if pigunit.exectype.cluster properties is present ) < /li > < /ul > < /li > < /ul > < p > For now , it would be nice to see how this idea could be integrated in Piggybank and if PigParser/PigServer could improve their interfaces in order to make PigUnit simple. < /p > < p > Other components based on PigUnit could be built later : < /p > < ul class= alternate type= square > < li > standalone MiniCluster < /li > < li > notion of workspaces for each test < /li > < li > standalone utility that reads test configuration and generates a test report ... < /li > < /ul > < p > It is a first prototype , open to suggestions and can definitely take advantage of feedbacks. < /p > < p > How to test , in pig_trunk : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > Apply patch $ pig_trunk ant compile-test $ pig_trunk ant $ pig_trunk/contrib/piggybank/java ant test -Dtest.timeout=999999 < /pre > < /div > < /div > < p > ( it takes 15 min in MAPREDUCE minicluster , tests will need to be split in the future between unit and integration ) < /p > < p > Many examples are in : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > contrib/piggybank/java/src/test/java/org/apache/pig/piggybank/test/pigunit/TestPigTest.java < /pre > < /div > < /div > < p > When used as a standalone , do not forget commons-lang-2.4.jar and the HADOOP_CONF_DIR to your cluster in your CLASSPATH. < /p > 
[ PIG-1427 ] Monitor and kill runaway UDFs < p > As a safety measure , it is sometimes useful to monitor UDFs as they execute . It is often preferable to return null or some other default value instead of timing out a runaway evaluation and killing a job . We have in the past seen complex regular expressions lead to job failures due to just half a dozen ( out of millions ) particularly obnoxious strings. < /p > < p > It would be great to give Pig users a lightweight way of enabling UDF monitoring. < /p > 
[ PIG-1429 ] Add Boolean Data Type to Pig < p > Pig needs a Boolean data type . Pig-1097 is dependent on doing this . < /p > < p > I volunteer . Is there anything beyond the work in src/org/apache/pig/data/ plus unit tests to make this work ? < /p > < p > This is a candidate project for Google summer of code 2011 . More information about the program can be found at < a href= http : //wiki.apache.org/pig/GSoc2011 class= external-link rel= nofollow > http : //wiki.apache.org/pig/GSoc2011 < /a > < /p > 
[ PIG-1461 ] support union operation that merges based on column names < p > When the data has schema , it often makes sense to union on column names in schema rather than the position of the columns . < br/ > The behavior of existing union operator should remain backward compatible . < /p > < p > This feature can be supported using either a new operator or extending union to support using clause . I am thinking of having a new operator called either unionschema or merge . Does anybody have any other suggestions for the syntax ? < /p > < p > example - < /p > < p > L1 = load x as ( a , b ) ; < br/ > L2 = load y as ( b , c ) ; < br/ > U = unionschema L1 , L2 ; < /p > < p > describe U ; < br/ > U : < /p > { a : bytearray , b : byetarray , c : bytearray } 
[ PIG-1479 ] Embed Pig in scripting languages < p > It should be possible to embed Pig calls in a scripting language and let functions defined in the same script available as UDFs. < br/ > This is a spin off of < a href= https : //issues.apache.org/jira/browse/PIG-928 class= external-link rel= nofollow > https : //issues.apache.org/jira/browse/PIG-928 < /a > which lets users define UDFs in scripting languages. < /p > 
[ PIG-1631 ] Support to 2 level nested foreach < p > What I would like to do is generate certain metrics for every listing impression in the context of a page like clicks on the page etc . So , I first group by to get clicks and impression together . Now , I would want to iterate through the mini-table ( one per serve-id ) and compute metrics . Since nested foreach within foreach is not supported I ended up writing a UDF that took both the bags and computed the metric . It would have been elegant to keep the logic of iterating over the records outside in the PIG script . < /p > < p > Here is some pseudocode of how I would have liked to write it : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > -- Let us say in our page context there was click on rank 2 < span class= code-keyword > for < /span > which there were 3 ads A1 = LOAD ... AS ( page_id , rank ) ; -- clicks . A2 = Load ... AS ( page_id , rank ) ; -- impressions B = COGROUP A1 by ( page_id ) , A2 by ( page_id ) ; -- Let us say B contains the following schema -- ( group , { ( A1 ... ) } { ( A2 ... ) } ) -- Each record would be in B would be : -- page_id_1 , { ( page_id_1 , 2 ) } { ( page_id_1 , 1 ) ( page_id_1 , 2 ) ( page_id_1 , 3 ) ) } C = FOREACH B GENERATE { D = FLATTEN ( A1 ) , FLATTEN ( A2 ) ; -- This wont work in current pig as well . Basically , I would like a mini-table which represents an entire serve . FOREACH D GENERATE page_id_1 , A2 : rank , SOMEUDF ( A1 : rank , A2 : :rank ) ; -- This UDF returns a value ( like v1 , v2 , v3 depending on A1 : :rank and A2 : :rank ) } ; # output # page_id , 1 , v1 # page_id , 2 , v2 # page_id , 3 , v3 DUMP C ; < /pre > < /div > < /div > < p > P.S : I understand that I could have alternatively , flattened the fields of B and then done a GROUP on page_id and then iterated through the records calling SOMEUDF appropriately but that would be 2 map-reduce operations AFAIK . < /p > < p > This is a candidate project for Google summer of code 2011 . More information about the program can be found at < a href= http : //wiki.apache.org/pig/GSoc2011 class= external-link rel= nofollow > http : //wiki.apache.org/pig/GSoc2011 < /a > < /p > 
[ PIG-4925 ] Support for passing the bloom filter to the Bloom UDF < p > Currently the Bloom Filter from BuildBloom has to be stored to HDFS to be able to be used in Bloom UDF . Most of the time the bloom filter is not reused and so have to be deleted after the end of the script . The load/store also forces multiple DAGs . If it was passed as a scalar , then it would be simpler and more efficient. < /p > 
[ PIG-1693 ] support project-range expression . ( was : There needs to be a way in foreach to indicate and all the rest of the fields ) < p > A common use case we see in Pig is people have many columns in their data and they only want to operate on a few of them . Consider for example if before storing data with ten columns , the user wants to perform a cast on one column : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > ... Z = foreach Y generate ( < span class= code-object > int < /span > ) firstcol , secondcol , thridcol , forthcol , fifthcol , sixthcol , seventhcol , eigthcol , ninethcol , tenthcol ; store Z into output ; < /pre > < /div > < /div > < p > Obviously this only gets worse as the user has more columns . Ideally the above could be transformed to something like : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > ... Z = foreach Y generate ( < span class= code-object > int < /span > ) firstcol , < span class= code-quote > and all the < span class= code-keyword > rest < /span > < /span > ; store Z into output < /pre > < /div > < /div > 
[ PIG-1752 ] UDFs should be able to indicate files to load in the distributed cache < p > Currently there is no way for a UDF to load a file into the distributed cache. < /p > 
[ PIG-1758 ] Deep cast of complex type < p > Pig does not handle deep cast from bag - & gt ; bag , tuple - & gt ; tuple . Eg , the following script does not produce desired result : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > a = load 1.txt as ( a0 : bag { t : tuple ( i0 : < span class= code-object > double < /span > ) } ) ; b = foreach a generate ( bag { tuple ( < span class= code-object > int < /span > ) } ) a0 ; dump b ; < /pre > < /div > < /div > < p > The result tuple still contain int inside tuple of bag . < /p > < p > < a href= https : //issues.apache.org/jira/browse/PIG-613 title= Casting complex type ( tuple/bag/map ) does not take effect class= issue-link data-issue-key= PIG-613 > < del > PIG-613 < /del > < /a > fix the case we cast bytearray < del > & gt ; bag/tuple , we take complex type including inner types , but bag < /del > & gt ; bag , tuple- & gt ; tuple is still not effective. < /p > 
[ PIG-1782 ] Add ability to load data by column family in HBaseStorage < p > It would be nice to load all columns in the column family by using short hand syntax like : < /p > < div class= preformatted panel style= border-width : 1px ; > < div class= preformattedContent panelContent > < pre > CpuMetrics = load hbase : //SystemMetrics USING org.apache.pig.backend.hadoop.hbase.HBaseStorage ( cpu : , -loadKey ) ; < /pre > < /div > < /div > < p > Assuming there are columns cpu : sys.0 , cpu : sys.1 , cpu : user.0 , cpu : user.1 , in cpu column family. < /p > < p > CpuMetrics would contain something like : < /p > < div class= preformatted panel style= border-width : 1px ; > < div class= preformattedContent panelContent > < pre > ( rowKey , cpu : sys.0 , cpu : sys.1 , cpu : user.0 , cpu : user.1 ) < /pre > < /div > < /div > 
[ PIG-1793 ] Add macro expansion to Pig Latin < p > As production Pig scripts grow longer and longer , Pig Latin has a need to integrate standard programming techniques of separation and code sharing offered by functions and modules . A proposal of adding macro expansion to Pig Latin is posted here : < a href= http : //wiki.apache.org/pig/TuringCompletePig class= external-link rel= nofollow > http : //wiki.apache.org/pig/TuringCompletePig < /a > < /p > < p > Below is a brief summary of the proposed syntax ( and examples ) : < /p > < ul > < li > Macro Definition < /li > < /ul > < p > The existing DEFINE keyword will be expanded to allow definitions of Pig macros . < /p > < p > < b > Syntax < /b > < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > define & lt ; name & gt ; ( & lt ; params & gt ; ) returns & lt ; aliases & gt ; { & lt ; Pig Latin fragment & gt ; } ; < /pre > < /div > < /div > < p > < b > Example < /b > < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > define my_macro ( A , sortkey ) returns C { B = filter $ A by my_filter ( * ) ; $ C = order B by $ sortkey ; } < /pre > < /div > < /div > < ul > < li > Macro Expansion < /li > < /ul > < p > < b > Syntax < /b > < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > & lt ; aliases & gt ; = & lt ; macro name & gt ; ( & lt ; params & gt ; ) ; < /pre > < /div > < /div > < p > < b > Example : < /b > Use above macro in a Pig script : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > X = load foo as ( user , address , phone ) ; Y = my_macro ( X , user ) ; store Y into bar ; < /pre > < /div > < /div > < p > This script is expanded into the following Pig Latin statements : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > X = load foo as ( user , address , phone ) ; macro_my_macro_B_1 = filter X by my_filter ( * ) ; Y = order macro_my_macro_B_1 by user ; store Y into bar ; < /pre > < /div > < /div > < p > < b > Notes < /b > < /p > < p > 1 . Any alias in the macro which isn t visible from outside will be prefixed with macro name and suffixed with instance id to avoid namespace collision . < br/ > 2 . Macro expansion is not a complete replacement for function calls . Recursive expansions are not supported . < /p > < ul > < li > Macro Import < /li > < /ul > < p > The new IMPORT keyword can be used to add macros defined in another Pig Latin file. < /p > < p > < b > Syntax < /b > < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-keyword > import < /span > & lt ; Pig Latin file name & gt ; ; < /pre > < /div > < /div > < p > < b > Example < /b > < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-keyword > import < /span > my_macro.pig ; < /pre > < /div > < /div > < p > < b > Note : < /b > All macro names are in the global namespace . < /p > 
[ PIG-1794 ] Javascript support for Pig embedding and UDFs in scripting languages < p > The attached patch proposes a javascript implementation for Pig embedding and UDFs in scripting languages. < br/ > It is similar to the Jython implementation and uses Rhino provided in the JDK. < br/ > some differences : < /p > < ul class= alternate type= square > < li > output schema is provided by : & lt ; functionName & gt ; .outSchema= & lt ; schema & gt ; as javascript does not have annotations or decorators but functions are first class objects < /li > < li > tuples are converted to objects using the input schema ( the other way around using the output schema ) < /li > < /ul > < p > The attached patch is not final yet . In particular it lacks unit tests. < br/ > See test/org/apache/pig/test/data/tc.js for the transitive closure example < /p > < p > See the following JIRAs for more context : < br/ > < a href= https : //issues.apache.org/jira/browse/PIG-928 class= external-link rel= nofollow > https : //issues.apache.org/jira/browse/PIG-928 < /a > < br/ > < a href= https : //issues.apache.org/jira/browse/PIG-1479 class= external-link rel= nofollow > https : //issues.apache.org/jira/browse/PIG-1479 < /a > < /p > 
[ PIG-1876 ] Typed map for Pig < p > Currently Pig map type is untyped , which means map value is always of bytearray ( ie . unknown ) type . In < a href= https : //issues.apache.org/jira/browse/PIG-1277 title= Pig should give error message when cogroup on tuple keys of different inner type class= issue-link data-issue-key= PIG-1277 > < del > PIG-1277 < /del > < /a > , we allow unknown type to be a shuffle key , which somewhat relieve the problem . However , typed map is still beneficial in that : < /p > < p > 1 . User can make semantic use of the map value type . Currently , user need to explicitly cast map value , which is ugly < br/ > 2 . Though < a href= https : //issues.apache.org/jira/browse/PIG-1277 title= Pig should give error message when cogroup on tuple keys of different inner type class= issue-link data-issue-key= PIG-1277 > < del > PIG-1277 < /del > < /a > allow unknown type be a shuffle key , the performance suffers . We don t have a raw comparator for the unknown type , instead , we need to instantiate the value object and invoke its comparator < /p > < p > Here is proposed syntax for typed map : < br/ > map < span class= error > & # 91 ; type & # 93 ; < /span > < /p > < p > Typed map can be used in place of untyped map could occur . For example : < br/ > a = load 1.txt as ( map < span class= error > & # 91 ; int & # 93 ; < /span > ) ; < br/ > b = foreach a generate ( map < span class= error > & # 91 ; ( i : int ) & # 93 ; < /span > ) a0 ; - - Map value is tuple < br/ > b = stream a through ` cat ` as ( m : map [ < /p > { ( i : int , j : chararray ) } < p > ] ) ; - - Map value is bag < /p > < p > MapLookup a typed map will result datatype of map value. < br/ > a = load 1.txt as ( map < span class= error > & # 91 ; int & # 93 ; < /span > ) ; < br/ > b = foreach a generate $ 0 # key ; < /p > < p > Schema for b : < br/ > b : < /p > { int } < p > The behavior of untyped map will remain the same. < /p > 
[ PIG-1881 ] Need a special interface for Penny ( Inspector Gadget ) < p > The proposed Penny tool needs access to Pig s new logical plan in order to inject code into the the dataflow . Once it has modified the plan it needs to then be able to hand back that modified plan and have Pig execute it. < /p > < p > As we don t want to open this functionality up to general users , the proposal is to do this by subclasses PigServer with a new class that is marked as LimitedPrivate for Penny only . This class will provide calls to parse a Pig Latin script and return a logical plan , and one to take a logical plan and execute it. < /p > 
[ PIG-1891 ] Enable StoreFunc to make intelligent decision based on job success or failure < p > We are in the process of using PIG for various data processing and component integration . Here is where we feel pig storage funcs lack : < /p > < p > They are not aware if the over all job has succeeded . This creates a problem for storage funcs which needs to upload results into another system : < /p > < p > DB , FTP , another file system etc. < /p > < p > I looked at the DBStorage in the piggybank ( < a href= http : //svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java ? view=markup class= external-link rel= nofollow > http : //svn.apache.org/viewvc/pig/trunk/contrib/piggybank/java/src/main/java/org/apache/pig/piggybank/storage/DBStorage.java ? view=markup < /a > ) and what I see is essentially a mechanism which for each task does the following : < /p > < p > 1 . Creates a recordwriter ( in this case open connection to db ) < br/ > 2 . Open transaction. < br/ > 3 . Writes records into a batch < br/ > 4 . Executes commit or rollback depending if the task was successful. < /p > < p > While this aproach works great on a task level , it does not work at all on a job level . < /p > < p > If certain tasks will succeed but over job will fail , partial records are going to get uploaded into the DB. < /p > < p > Any ideas on the workaround ? < /p > < p > Our current workaround is fairly ugly : We created a java wrapper that launches pig jobs and then uploads to DB s once pig s job is successful . While the approach works , it s not really integrated into pig. < /p > 
[ PIG-833 ] Storage access layer < p > A layer is needed to provide a high level data access abstraction and a tabular view of data in Hadoop , and could free Pig users from implementing their own data storage/retrieval code . This layer should also include a columnar storage format in order to provide fast data projection , CPU/space-efficient data serialization , and a schema language to manage physical storage metadata . Eventually it could also support predicate pushdown for further performance improvement . Initially , this layer could be a contrib project in Pig and become a hadoop subproject later on. < /p > 
[ PIG-1959 ] Penny : a framework for workflow instrumentation < p > Penny is a framework for instrumenting Pig workflows . It rewrites scripts to insert monitoring points , aka agents , and provides a communication framework for triggering and collecting events from these agents. < /p > 
[ PIG-2293 ] Pig should support a more efficient merge join against data sources that natively support point lookups or where the join is against large , sparse tables . < p > The existing PIG merge join has the following limitations : < br/ > 1 . It assumes the right side of the table must be accessed sequentially - record by record. < br/ > 2 . It does not perform well against large , sparse tables. < /p > < p > The current implementation of the merge join introduced the interface IndexableLoadFunc . This LoadFunc < br/ > supports the ability to seekNear a given key ( before reading the next record ) . < br/ > The merge join physical operator only calls seekNear for the first key in each split ( effectively eliminating splits < br/ > where the first and subsequent keys will not be found ) . Subsequent joins are found by reading sequentially through < br/ > the records on the right table looking for matches from the left table. < /p > < p > While this method works well for dense join tables - it performs poorly against large sparse tables or data sources that support < br/ > point lookups natively ( HBase for example ) . < /p > < p > The proposed enhancement is to add a new join type - merge-sparse to PIG latin . When specified in the PIG script , this join type < br/ > will cause the merge join operator to call seekNear on each and every key ( rather than just the first in each split ) . < /p > 
[ PIG-2328 ] Add builtin UDFs for building and using bloom filters < p > Bloom filters are a common way to do select a limited set of records before moving data for a join or other heavy weight operation . Pig should add UDFs to support building and using bloom filters. < /p > 
[ PIG-2332 ] JsonLoader/JsonStorage < p > A JsonLoader/JsonStorage implementation for Pig . This is based on Alan s implementation in the book ( < a href= http : //ofps.oreilly.com/titles/9781449302641/load_and_store_funcs.html class= external-link rel= nofollow > http : //ofps.oreilly.com/titles/9781449302641/load_and_store_funcs.html < /a > ) . I made some minor changes : < br/ > 1 . Drop the jackson feature requires 1.01+ . Since Hadoop 203+ bundles jackson 1.01 , newer feature fails when running on Hadoop 203+. < br/ > 2 . Using Json format for schema . This borrows Dmitry s schema implememtation in PigStorage. < br/ > 3 . Some bug fixes. < /p > 
[ PIG-2338 ] Need signature for EvalFunc < p > In LoadFunc/StoreFunc , we generate a unique signature for each LoadFunc/StoreFunc , so user can use it as key to retrieve properties in UDFContext . We need a similar mechanism for EvalFunc. < /p > 
[ PIG-2359 ] Support more efficient Tuples when schemas are known < p > Pig Tuples have significant overhead due to the fact that all the fields are Objects. < br/ > When a Tuple only contains primitive fields ( ints , longs , etc ) , it s possible to avoid this overhead , which would result in significant memory savings. < /p > 
[ PIG-2456 ] Pig should have a pigrc to specify default script cache < p > There should be a way to specify default statements in pig . This is helpful when multiple users are using pig in interactive mode. < /p > 
[ PIG-2482 ] Integrate HCat DDL command into Pig < p > We would like to run hcat DDL command inside Pig script or Grunt . We can use a similar approach as fs or sh . < /p > < p > Grunt & gt ; hcat create table ..... < /p > < p > Similar to fs and sh , we don t plan to add Java API in PigServer for it. < /p > 
[ PIG-2317 ] Ruby/Jruby UDFs < p > It should be possible to write UDFs in Ruby . These UDFs will be registered in the same way as python and javascript UDFs. < /p > 
[ PIG-2525 ] Support pluggable PigProgressNotifcationListeners on the command line < p > It would be convenient to be able to implement < tt > PigProcessNotifcationListener < /tt > and to wire it into a pig script . This jira is to support setting a listener and its constructor args on the command , perhaps like this : < /p > < p > < tt > noformat < /tt > < br/ > pig -j MyListener -J foo , bar , bat -f my_script.pig < br/ > < tt > noformat < /tt > < /p > < p > where < tt > MyListener < /tt > takes a single string in the constructor which will get passed < tt > foo , bar , bat < /tt > . < /p > 
[ PIG-2547 ] Easier UDFs : Convenient EvalFunc super-classes < p > We ve got a few abstract extensions of EvalFunc that make life easier . If people are interested we can push said classes into Pig. < /p > < p > There are 3 classes , each extending the next . Class naming is all TBD. < /p > < ul > < li > < tt > TypedOutputEvalFunc & lt ; OUT & gt ; < /tt > - Implements < tt > public Schema outputSchema ( Schema input ) < /tt > based on the generic type of the subclass . Provides common helper validation functions which increment counters for good and bad Tuple data passed . Useful where the input to be worked on is a tuple of size N or greater. < /li > < li > < tt > PrimitiveEvalFunc & lt ; IN , OUT & gt ; < /tt > - Same as above with helper validation allowing the ability it subclass and just implement < tt > public OUT exec ( IN input ) < /tt > , where IN and OUT are primitives . Useful when the input is a single primitive in position 0 of a tuple. < /li > < li > < tt > FunctionWrapperEvalFunc < /tt > - Wraps a Guava Function implementation ( < a href= http : //guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/base/Function.html class= external-link rel= nofollow > http : //guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/base/Function.html < /a > ) and allows UDFs to be used in Pig scripts like so , where < tt > MyFunction < /tt > is a class that implements < tt > Function < /tt > : < /li > < /ul > < div class= preformatted panel style= border-width : 1px ; > < div class= preformattedContent panelContent > < pre > DEFINE myUdf org.apache.pig.FunctionWrapperEvalFunc ( MyFunction ) < /pre > < /div > < /div > 
[ PIG-2548 ] Support for providing parameters to python script < p > If I have an embedded pig script in python , there is no way to get user passed parameters in the python script. < br/ > Though < a href= https : //issues.apache.org/jira/browse/PIG-2165 title= Need a way to deal with params and param_file in embedded pig in python class= issue-link data-issue-key= PIG-2165 > < del > PIG-2165 < /del > < /a > adds the capability of reading these params in the pig script , from python it is still not possible. < br/ > It would be nice to have this feature if I have some sort of post processing happening in my python scrip based on the params. < /p > 
[ PIG-2579 ] Support for multiple input schemas in AvroStorage < p > This is a barebones patch for AvroStorage which enables support of multiple input schemas . The assumption is that the input consists of avro files having different schemas that can be unioned , e.g. , flat records . < /p > < p > A simple illustrative example is attached ( avro_storage_union_schema_test.tar.gz ) : run create_avro1.pig , followed by create_avro2.pig , followed by read_avro.pig. < /p > 
[ PIG-2650 ] Convenience mock Loader and Storer to simplify unit testing of Pig scripts < p > A test would look as follows : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > PigServer pigServer = < span class= code-keyword > new < /span > PigServer ( ExecType.LOCAL ) ; TupleFactory tf = TupleFactory.getInstance ( ) ; Data data = Storage.resetData ( pigServer.getPigContext ( ) ) ; data.set ( < span class= code-quote > foo < /span > , Arrays.asList ( tf.newTuple ( < span class= code-quote > a < /span > ) , tf.newTuple ( < span class= code-quote > b < /span > ) , tf.newTuple ( < span class= code-quote > c < /span > ) ) ) ; pigServer.registerQuery ( < span class= code-quote > A = LOAD foo USING mock.Storage ( ) ; < /span > ) ; < span class= code-comment > // some complex script to test < /span > pigServer.registerQuery ( < span class= code-quote > STORE A INTO bar USING mock.Storage ( ) ; < /span > ) ; Iterator & lt ; Tuple & gt ; out = data.get ( < span class= code-quote > bar < /span > ) .iterator ( ) ; assertEquals ( < span class= code-quote > a < /span > , out.next ( ) .get ( 0 ) ) ; assertEquals ( < span class= code-quote > b < /span > , out.next ( ) .get ( 0 ) ) ; assertEquals ( < span class= code-quote > c < /span > , out.next ( ) .get ( 0 ) ) ; < /pre > < /div > < /div > 
[ PIG-2660 ] PPNL should get notified of plan before it gets executed < p > The < tt > PigProgressNotificationListner < /tt > should get notified of the plan ( < tt > MROperPlan < /tt > ) before it gets executed . This allows listeners to inspect the plan and have an idea what to expect in the execution flow . Proposal is to add the following method to the PPNL interface , which is marked as evolving : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > < span class= code-keyword > public < /span > void initialPlanNotification ( MROperPlan plan ) ; < /pre > < /div > < /div > 
[ PIG-283 ] Allow to set arbitrary jobconf key-value pairs inside pig program < p > It would be useful to be able to set arbitrary JobConf key-value pairs inside a pig program ( e.g . in front of a COGROUP statement ) . < br/ > I wonder whether the simplest way to add this feature is by expanding the set command functionality. < /p > 
[ PIG-2855 ] Provide a method to measure time spent in UDFs < p > When debugging slow jobs , it is often useful to know whether time is being spent in UDFs , and in which UDFs . This is easy to measure from within the framework , we should let users optionally track these metrics. < /p > 
[ PIG-2857 ] Add a -tagPath option to PigStorage < p > We recently added a -tagSource option to PigStorage , which allows us to add filenames from which records come to the returned tuples. < /p > < p > Often , users want the whole path , not just the source file . I propose we add a -tagPath option to do this. < /p > 
[ PIG-2900 ] Streaming should provide conf settings in the environment < p > Hadoop Streaming converts jobconf properties into environment variables ; Pig streaming does not . This is a useful feature that Pig streaming should provide. < /p > 
[ PIG-2994 ] Grunt shortcuts < p > This feature is aimed at providing shortcuts for frequently used commands like illustrate , dump , explain , describe , quit , help etc . This feature is inspired from postgres ( psql ) shortcuts . I tried implementing a simple shortcut for quitting the grunt shell using \q with very minimal changes . I think this feature will help save many keystrokes for users . If this feature looks useful I can submit the current patch for review and go ahead with implementing the following shortcuts < /p > < p > \i & lt ; alias & gt ; - illustrate < br/ > \e & lt ; alias & gt ; - explain < br/ > \de & lt ; alias & gt ; - describe < br/ > \du & lt ; alias & gt ; - dump < br/ > \h - help < /p > < p > This will also be useful to view information about tables/statistics stored in HCatalog similar to the way psql does . < br/ > \dt & lt ; alias & gt ; - display table < br/ > \dm - display metadata < br/ > etc .. < /p > < p > except \t , \r and \n delimiters we should be able to use all other characters as shortcuts . < br/ > Please let me know your thoughts. < /p > 
[ PIG-3065 ] pig output format/committer should support recovery for hadoop 0.23 < p > In hadoop 0.23 the output committer can optionally support recovery to handle < br/ > the application master getting restarted ( failing some # of attempts ) . If its possible the pig outputformat/committer should support recovery. < /p > 
[ PIG-3075 ] Allow AvroStorage STORE Operations To Use Schema Specified By URI < p > I ve attached a patch that makes AvroStorage accept a schema_uri flag when storing an alias . The schema used to store the contents of the alias is the contents of the file the URI refers to ( which must exist ) . This fits somewhere between the schema flag - specifying the schema as an inline string - and the same flag - specifying the schema as it appears in a data file. < /p > 
[ PIG-3086 ] Allow A Prefix To Be Added To URIs In PigUnit Tests < p > When running PigUnit tests that use the local file system it d be useful to re-write absolute paths in the pig script under test so you can jail the data the test uses to a known folder. < /p > 
[ PIG-3090 ] Introduce a syntax to be able to easily refer to the previously defined relation < p > Sometimes I feel like swimming with ANTLRs . This particular feature isn t too hard to add ... and supports syntax like this : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > a = load thing as ( x : < span class= code-object > int < /span > ) ; b = foreach @ generate x ; c = foreach @ generate x ; d = foreach @ generate x ; < /pre > < /div > < /div > < p > I have a patch , though I need to make sure it doesn t change anything ( it shouldn t ) and I need to add tests. < /p > 
[ PIG-4360 ] HBaseStorage should support setting the timestamp field < p > Currently the timestamp is always set to the current time in milliseconds . There should be an option to override that . I propose a configuration setting that uses the second field as the timestamp column. < /p > 
[ PIG-3211 ] Allow default Load/Store funcs to be configurable < p > PigStorage is used by default when a Load/StoreFunc is not specified . It would be useful to make this configurable. < /p > 
[ PIG-3247 ] Piggybank functions to mimic OVER clause in SQL < p > In order to test Hive I have written some UDFs to mimic the behavior of SQL s OVER clause . I thought they would be useful to share. < /p > 
[ PIG-3294 ] Allow Pig use Hive UDFs < p > It would be nice if Pig provide some interoperability with Hive . We can wrap Hive UDF in Pig so we can use Hive UDF in Pig. < /p > < p > This is a candidate project for Google summer of code 2013 . More information about the program can be found at < a href= https : //cwiki.apache.org/confluence/display/PIG/GSoc2013 class= external-link rel= nofollow > https : //cwiki.apache.org/confluence/display/PIG/GSoc2013 < /a > < /p > 
[ PIG-3367 ] Add assert keyword ( operator ) in pig < p > Assert operator can be used for data validation . With assert you can write script as following- < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > a = load something as ( a0 : < span class= code-object > int < /span > , a1 : < span class= code-object > int < /span > ) ; < span class= code-keyword > assert < /span > a by a0 & gt ; 0 , a cant be negative < span class= code-keyword > for < /span > reasons ; < /pre > < /div > < /div > < p > This script will fail if assert is violated. < /p > 
[ PIG-3390 ] Make pig working with HBase 0.95 < p > The HBase 0.95 changed API in incompatible way . Following APIs that < tt > HBaseStorage < /tt > in Pig uses are no longer available : < /p > < ul > < li > < tt > Mutation.setWriteToWAL ( Boolean ) < /tt > < /li > < li > < tt > Scan.write ( DataOutput ) < /tt > < /li > < /ul > < p > Also in addition the HBase is no longer available as one monolithic archive with entire functionality , but was broken down into smaller pieces such as < tt > hbase-client < /tt > , < tt > hbase-server < /tt > , ... < /p > 
[ PIG-3430 ] Add xml format for explaining MapReduce Plan . < p > At Mortar we needed an easy way to store/parse a script s map reduce plan . We added an xml output format for the MapReduce plan to make this easier . We also added a flag to keep track of if each store or load was from the original script ( and associated with an alias ) or if its a temporary store/load generated by Pig. < /p > 
[ PIG-1904 ] Default split destination < p > split statement is better to have a default destination , eg : < /p > < div class= code panel style= border-width : 1px ; > < div class= codeContent panelContent > < pre class= code-java > SPLIT A INTO X IF f1 & lt ; 7 , Y IF f2==5 , Z IF ( f3 & lt ; 6 OR f3 & gt ; 6 ) , OTHER otherwise ; -- OTHERS has all tuples with f1 & gt ; =7 & amp ; & amp ; f2 ! =5 & amp ; & amp ; f3==6 < /pre > < /div > < /div > < p > This is a candidate project for Google summer of code 2011 . More information about the program can be found at < a href= http : //wiki.apache.org/pig/GSoc2011 class= external-link rel= nofollow > http : //wiki.apache.org/pig/GSoc2011 < /a > < /p > 
[ PIG-3463 ] Pig should use hadoop local mode for small jobs < p > Pig should use hadoop local mode for small jobs - few mappers , few reducers and few mb of data. < /p > 
[ PIG-3573 ] Provide StoreFunc and LoadFunc for Accumulo < p > Accumulo has some code to allow reading and writing from it through Pig . I ve been working on making it more robust and would like to try to get it included into Pig ( to avoid the necessity to bundle additional jars ) . < /p > < p > Some info on what currently exists < a href= http : //people.apache.org/~elserj/accumulo-pig/ class= external-link rel= nofollow > http : //people.apache.org/~elserj/accumulo-pig/ < /a > , and the current code < a href= https : //git-wip-us.apache.org/repos/asf ? p=accumulo-pig.git class= external-link rel= nofollow > https : //git-wip-us.apache.org/repos/asf ? p=accumulo-pig.git < /a > < /p > < p > 1 . Need to translate Maven build into Ant+Ivy < br/ > 2 . Need to figure out how to support Accumulo 1.4 and 1.5 builds ( differences in dependencies and APIs ) < /p > 
[ PIG-3765 ] Ability to disable Pig commands and operators < p > This is an admin feature providing ability to blacklist or/and whitelist certain commands and operations . Pig exposes a few of these that could be not very safe in a multitenant environment . For example , sh invokes shell commands , set allows users to change non-final configs . While these are tremendously useful in general , having an ability to disable would make Pig a safer platform . The goal is to allow administrators to be able to have more control over user scripts . Default behaviour would still be the same - no filters applied on commands and operators. < /p > 
[ PIG-4128 ] New logical optimizer rule : ConstantCalculator < p > Pig used to have a LogicExpressionSimplifier to simplify expression which also calculates constant expression . The optimizer rule is buggy and we disable it by default in < a href= https : //issues.apache.org/jira/browse/PIG-2316 title= Incorrect results for FILTER * * * BY ( * * * OR * * * ) with FilterLogicExpressionSimplifier optimizer turned on class= issue-link data-issue-key= PIG-2316 > < del > PIG-2316 < /del > < /a > . < /p > < p > However , we do need this feature especially in partition/predicate push down , since both does not deal with complex constant expression , we d like to replace the expression with constant before the actual push down . Yes , user may manually do the calculation and rewrite the query , but even rewrite is sometimes not possible . Consider the case user want to push a datetime predicate , user have to write a ToDate udf since Pig does not have datetime constant. < /p > < p > In this Jira , I provide a new rule : ConstantCalculator , which is much simpler and much less error prone , to replace LogicExpressionSimplifier. < /p > 
[ PIG-4141 ] Ship UDF/LoadFunc/StoreFunc dependent jar automatically < p > When user use AvroStorage/JsonStorage/OrcStorage , they need to register dependent jars manually . It would be much convenient if we can provide a mechanism for UDF/LoadFunc/StoreFunc to claim the dependency and ship jars automatically. < /p > 
[ PIG-4370 ] HBaseStorage should support delete markers < p > We have a case where writing deletes to HBase would be very useful in Pig . There is a precedent for delete operations - < a href= https : //issues.apache.org/jira/browse/CASSANDRA-5867 class= external-link rel= nofollow > https : //issues.apache.org/jira/browse/CASSANDRA-5867 < /a > < br/ > so I think it s a valid use case for Pig especially considering that in HBase a delete is really a write of a tombstone marker < /p > 
