Pluggable Execution Engine p effort adapt work using Apache Tez https nofollow https made changes allow cleaner ExecutionEngine abstraction existed changes major already relatively abstracted frontend backend changes attached commit essentially barebones changes 8211 tried change structure different components much think interesting see future refactor areas really honor abstraction frontend backend p changes reinstate ExecutionEngine interface tie together front end backend making changes delegate EE necessary creating MRExecutionEngine implements interface work included changing ExecType cycle ExecutionEngines classpath select appropriate one done using Java ServiceLoader exactly MapReduce choosing framework use local distributed mode Also tried make ScriptState JobStats PigStats abstract possible current state think future work need done perhaps usage ScriptState responsibilities different statistics classes touched PPNL think abstraction needed perhaps separate patch
Add deepCopy LogicalExpressions p would useful way deepCopy expression deepCopy create new object changes made one object reflect copy 2 reasons override ul li may better use deepCopy since copy semantics explicit since deepCopy may expensive li second important reason defining deepCopy separate routine passed plan argument updated expression copied p usage would look like following div preformatted panel 1px div preformattedContent panelContent pre LogicalExpressionPlan logicalPlan new LogicalExpressionPlan LogicalExpression copyExpression logicalPlan p immediate motivation would constructing expressions constitute CNF form
Merge join support replicated join predecessor p Since replicated join trigger reduce change output ordering merge join work
Support UDFs FOREACH Merge Join p Right MapSideMergeValidator outright rejects foreach UDF div code panel 1px div codeContent panelContent pre span private span boolean isAcceptableForEachOp Operator lo span throws LogicalToPhysicalTranslatorException span lo span instanceof LOForEach OperatorPlan innerPlan LOForEach lo validateMapSideMerge innerPlan span return containsUDFs LOForEach lo span else span return span false p TODO later class inside containsUDFs div code panel 1px div codeContent panelContent pre span TODO dvryaboy span future could relax span rule tracing fields span passed UDF refusing span UDF working span join key Transforms fields p TODO relax requirement remove altogether
Allow AvroStorage use class schema p managing Avro Schemas source code Java classes Avro would easier something like blockquote p store using AvroStorage null p Rather passing first agurment entire Avro schema json script keeping potentially outdated set Avro schema files HDFS p classpath already aware Avro data would good leverage fully qualified
Built UDF given string search replace occurrences search keys replacement values p Lets say string A1B2C3D4 objective replace 1 B 2 C 3 4 derive 11223344 string p Using existing REPLACE method p REPLACE REPLACE REPLACE REPLACE A1B2C3D4 1 B 2 C 3 4 p proposed UDF method p General Syntax p sourceString search1 replacement1 p A1B2C3D4 1 B 2 C 3 4 p Advantage p 1 Function calls reduced 2 Ease code better p Let know inputs UDF Piggy Bank take based
Authenticate Kerberos using keytab file p running Kerberos secured environment users faced limitation jobs run longer remaining ticket lifetime Kerberos tickets environment work tickets expire 10 hours thus limiting maximum job duration 10 hours problem p Hadoop tooling feature authenticate using Kerberos keytab file essentially file contains encrypted form kerberos principal password Using running application request new tickets Kerberos server initial tickets p applications commonly include two lines div code panel 1px div codeContent panelContent pre span System span span span nbasjes span p way run Apache Flink based application 170 hours week kerberos secured Yarn p propose feature set relevant kerberos values script able run job many days secured p Proposal look script div code panel 1px div codeContent panelContent pre SET SET nbasjes SET p iff set least last two aforementioned method called submitting job
need NATIVE keyword p Assume user job broke easily three pieces assume pieces one three easily expressible piece two needed written map reduce whatever reason performance something could easily express legacy job important change Today user would either use map reduce entire job manually handle stitching together map reduce jobs instead provided NATIVE keyword would allow script pass data stream underlying system case map reduce semantics NATIVE would vary underlying system map reduce case would assume indicated collection one fully contained map reduce jobs would store data invoke map reduce jobs read resulting data continue might look something like div code panel 1px div codeContent panelContent pre load myfile X load myotherfile B group 0 C foreach B generate group myudf B span native E join 0 X 0 p differs streaming allows user insert arbitrary amount native processing whereas streaming allows insertion one binary also differs streaming data piped directly binary part pipeline pipeline would broken data written disk native block invoked data read back p Another alternative say unnecessary user coordination java using PIgServer interface run calling map reduce job explicitly advantages native keyword user need worried coordination jobs take care Also user make use existing java applications without java
Support FLATTEN maps p come across users asking quite times see support FLATTEN instead users write UDF
Statistics records read mapper reducer p uses counters framework hadoop Initially interested finding number records read particularly last job script sample code access statistics last job p String reducePlan reducePlan null Records written p else Records written p patch contains 7 test cases include tests PigStorage BinStorage along one multiple MR jobs
Autocompletion complete aliases p Autocompletion knows keywords different contexts would nice completed aliases alias
Logical optimizer push project p continuation work https nofollow need add another rule logical optimizer Push project ie prune columns early
Add LIMIT statement works nested FOREACH p like compute top 10 results p natural way express would div code panel 1px div codeContent panelContent pre load using PigStorage date span int count span int url chararray B group date C foreach B order count desc E limit 10 generate FLATTEN E dump C p Yeah could write UDF PiggyBank function take top n results since LIMIT already exists statement seems like also work nested foreach p Example workaround div code panel 1px div codeContent panelContent pre C foreach B order count desc E 10 generate FLATTEN E dump C
Serialize schemas PigStorage storage types p finding PigStorage really convenient storage data interchange compresses well imports Excel analysis environments p However pain comes maintenance columns fixed locations like add columns p would great load PigStorage could read default schema file stored data store PigStorage could store file p tested Hadoop HDFS local mode ignore file called directory part p example chain scripts execute p load using PigStorage int b int p store using PigStorage p B load using PigStorage p describe B p describe B output something like int b int
support cast chararray simple types p support casting chararray integer long float double bytearray conversion fails reasons overflow cast return null log
Default parallel p hadoop 20 user specify number reducers hadoop use 1 reducer default value different previous hadoop default reducer number usually good 1 reducer user want sure Although user use parallel keyword specify number reducers statement wordy need convenient way users express desired number reducers propose p 1 Add one property User set script Eg set 10 p 2 hint free optimize number reducers unlike parallel keyword Currently since mechanism determine optimal number reducers always granted unless override parallel p 3 user put multiple inside script last entry
UDFs scripting languages p possible write UDFs scripting languages python ruby etc frees users needing compile Java generate jar etc also opens programmers prefer scripting languages
Acummulator Interface UDFs p Add accumulator interface UDFs would allow take set number records time instead entire
PERFORMANCE Implement group operator speed processing ordered data p general group operation needs mappers reducers aggregation done reducers incurs disk mappers p However cases input data following properties p 1 records key grouped together data sorted keys 2 records key mapper p group operation performed mappers thus remove overhead disk p Alan proposed adding hint group clause like one div code panel 1px div codeContent panelContent pre load input using SomeLoader B group 0 using span mapside C foreach B generate p proposed addition using mapside group mapside group operator collects records given key buffer sees key change emit key bag records buffered assume keys given record collected together thus need buffer across keys p expected SomeLoader implemented data systems Zebra ensure data emitted loader satisfies properties 1 2 p responsibility user loader guarantee properties 1 amp 2 invoking mapside hint group clause runtime check errors input p group clauses mapside hint Latin support group columns including group expressions group
Provide much easier use accumulator interface Accumulators able p introduces new interface IteratingAccumulatorEvalFunc name final cool thing patch built purely top existing Accumulator code well uses https Accumulators able del could easily work without say easier way write accumulators without fork p downside way able provide clean interface using second thread need explore potential performance implications given easy use stuff performance implications think long measure document worth much usable interface Plus think bad one thread heavy lifting another ferries values SUM could written div code panel 1px div codeContent panelContent pre span public class SUM span extends IteratingAccumulatorEvalFunc lt span Long gt span public span Long exec Iterator lt Tuple gt span throws IOException span long sum 0 span sum span Long 0 span return sum p Besides performance tests need figure properly test sort thing particularly welcome advice p Accumulators currently forced process whole bag getValue called p Early termination handy feature able use IsEmpty example p add new interface extending
zebra Sorted Table Support Zebra p new feature Zebra support sorted data storage storage library Zebra sort data support creation use sorted data either PIG tasks use Zebra storage p sorted table keeps data totally sorted manner across TFiles created potentially mappers p sorted data creation PIG STORE operator input data sorted ORDER new Zebra table marked sorted sorted columns p sorted data creation though tasks three new static methods BasicTableOutput class provided allow help user achieve goal setSortInfo allows user specify sorted columns input tuple stored getSortKeyGenerator getSortKey help user generate key acceptable Zebra sorted key based upon schema sorted columns input p sorted data read PIG LOAD operator pass string sorted extra argument TableLoader constructor ask sorted table loaded p sorted data read tasks new static method TableInputFormat class requireSortedTable called ask sorted table read Additionally overloaded version new method called ask sorted table specified sort columns p release sorted table supported sorting ascending order descending order addition sort keys must simple types complex types RECORD COLLECTION MAP p sorting supported ordering multiple sort keys significant first sort column primary sort key second secondary sort key p release sort keys stored along sort columns keys originally created resulting data storage
Nested cross p useful cross inside foreach nested statement One typical use case nested foreach cogroup two relations want flatten records key processing naturally achieved cross Eg div code panel 1px div codeContent panelContent pre C cogroup user uid session uid foreach C crossed cross user session flatten two input bags filtered filter crossed user session result foreach crossed generate processSession user user session Nested foreach Jira generate result p cross user write UDF process bag user session much harder UDF process flattened tuples especially true nested foreach statement https Support 2 level nested foreach del p candidate project Google summer code 2011 information program found http nofollow http
Custom Partitioner p adding custom partitioner give control output partition key goes add keywords language p PARTITION UDF p similar syntax UDF returns number 0 n number output
Make work hadoop p need make work hadoop svn branch currently https nofollow https
able set job priority Latin p Currently users set job name Latin saying p set job name p ability set priority would also nice patch small goal able say p set high p throw JobCreationException JobControlCompiler priority one allowed string values enum low normal high Case insensitivity makes little
support skewed outer join p Similarly skewed inner join skewed outer join help scale presense join keys fit memory
zebra Order Preserving Sorted Table Union p output schema adopt schema union semantics namely output column appears one component table result rows values column rows component table null otherwise hand output column appears multiple component tables types column component tables must identical Otherwise exception thrown result rows values column rows component tables column null otherwise p order preserving results could indexed component tables projection contains column named specified component table index output position specified projection list underlying table union sorted tables use special column name projection cause exception thrown p attempt made create table column named excpetion thrown name reserved zebra virtual name
Zebra support record row file split Zebra TableInputFormat p TFile currently supports split record sequence number see Jira https Split TFile Record Sequence Number del want utilize provide record row input split support One prominent benefit cases large data files create much input splits create one big split one big p detail new getSplits works default user specify splits generated follows 1 Select biggest column group terms data size split TFiles according hdfs block size 64 MB 128 MB get list physical byte offsets output per TFile example let us assume 1st TFile get offset1 offset2 offset10 2 Invoke long offset get RecordNum pair near byte offset example say get recordNum1 recordNum2 recordNum10 3 Stitch span error 91 0 recordNum1 93 span error 91 recordNum2 93 span error 91 recordNum10 93 span error 91 lastRecordNum 93 splits column groups respectively form 11 input splits 1st TFile 4 input split need create TFile scanner long beginRecNum long endRecNum p Note conversion byte offset record number done mapper rather done job initialization phase due performance concern since conversion incurs TFile reading
RANK function like SQL p Implement function given sorted bag adds tuple unique increasing identifier without gaps like RANK p candidate project Google summer code 2012 information program found https nofollow https p Functionality implemented far available https nofollow https
Pass JobConf UDF specific configuration information UDFs p Users long asked way get JobConf structure UDFs would also nice way pass properties front end back end UDFs store state parse time use p patch part proposed https Pass global configurations UDF del provide way give user specified configuration files UDFs mark 602 depending bug
UDFs dynamic invocation simple Java methods p need create wrapper UDFs simple Java functions creates unnecessary work users slows development process produces lot trivial classes use Java reflection allow invoking number methods fly dynamically creating generic UDF accomplish
zebra Zebra Column Group Access Control p Access Control processes try read column groups Zebra able handle allowed disallowed accesses security eventuallt granted corresponding HDFS security data p Expected behavior column group permissions set p user selects columns permissions access Zebra return error message Error Permission denied accessing column lt column name names gt p Access control applies entire column group columns column group permissions
PigUnit script testing simplified p goal provide simple xUnit framework enables scripts easily ul alternate square li unit tested li regression tested li quickly prototyped p cluster set p example p TestCase div code panel 1px div codeContent panelContent pre Test span public void testTop3Queries span String args span test span new PigTest span args span String input span span span span span span String output span span span span data input span output p div code panel 1px div codeContent panelContent pre data LOAD input query CHARARRAY count INT FOREACH GENERATE group query SUM count LIMIT n STORE output p 3 modes ul li LOCAL properties present li MAPREDUCE use cluster specified classpath ul li automatic mini cluster default class path li pointing existing cluster properties present p would nice see idea could integrated Piggybank could improve interfaces order make PigUnit p components based PigUnit could built later ul alternate square li standalone MiniCluster li notion workspaces test li standalone utility reads test configuration generates test report p first prototype open suggestions definitely take advantage p test div code panel 1px div codeContent panelContent pre Apply patch ant ant ant test p takes 15 min MAPREDUCE minicluster tests need split future unit integration p Many examples div code panel 1px div codeContent panelContent pre p used standalone forget cluster
Monitor kill runaway UDFs p safety measure sometimes useful monitor UDFs execute often preferable return null default value instead timing runaway evaluation killing job past seen complex regular expressions lead job failures due half dozen millions particularly obnoxious p would great give users lightweight way enabling UDF
Add Boolean Data Type p needs Boolean data type dependent p volunteer anything beyond work plus unit tests make work p candidate project Google summer code 2011 information program found http nofollow http
support union operation merges based column names p data schema often makes sense union column names schema rather position columns behavior existing union operator remain backward compatible p feature supported using either new operator extending union support using clause thinking new operator called either unionschema merge anybody suggestions syntax p example p L1 load x b L2 load b c U unionschema L1 L2 p describe U U bytearray b byetarray c bytearray
Embed scripting languages p possible embed calls scripting language let functions defined script available spin https nofollow https lets users define UDFs scripting
Support 2 level nested foreach p would like generate certain metrics every listing impression context page like clicks page etc first group get clicks impression together would want iterate one per compute metrics Since nested foreach within foreach supported ended writing UDF took bags computed metric would elegant keep logic iterating records outside PIG script p pseudocode would liked write div code panel 1px div codeContent panelContent pre Let us say page context click rank 2 span 3 ads A1 LOAD rank clicks A2 Load rank impressions B COGROUP A1 A2 Let us say B contains following schema group A1 A2 record would B would 2 1 2 3 C FOREACH B GENERATE FLATTEN A1 FLATTEN A2 wont work current well Basically would like represents entire serve FOREACH GENERATE A2 rank SOMEUDF A1 rank A2 UDF returns value like v1 v2 v3 depending A1 A2 output 1 v1 2 v2 3 v3 DUMP C p understand could alternatively flattened fields B done GROUP iterated records calling SOMEUDF appropriately would 2 operations AFAIK p candidate project Google summer code 2011 information program found http nofollow http
Support passing bloom filter Bloom UDF p Currently Bloom Filter BuildBloom stored HDFS able used Bloom UDF time bloom filter reused deleted end script also forces multiple DAGs passed scalar would simpler
support expression needs way foreach indicate rest fields p common use case see people many columns data want operate Consider example storing data ten columns user wants perform cast one column div code panel 1px div codeContent panelContent pre Z foreach generate span int firstcol secondcol thridcol forthcol fifthcol sixthcol seventhcol eigthcol ninethcol tenthcol store Z output p Obviously gets worse user columns Ideally could transformed something like div code panel 1px div codeContent panelContent pre Z foreach generate span int firstcol span span rest store Z output
UDFs able indicate files load distributed cache p Currently way UDF load file distributed
Deep cast complex type p handle deep cast bag gt bag tuple gt tuple Eg following script produce desired result div code panel 1px div codeContent panelContent pre load a0 bag tuple i0 span double b foreach generate bag tuple span int a0 dump b p result tuple still contain int inside tuple bag p https Casting complex type take effect del fix case cast bytearray del gt take complex type including inner types bag gt bag gt tuple still
Add ability load data column family HBaseStorage p would nice load columns column family using short hand syntax like div preformatted panel 1px div preformattedContent panelContent pre CpuMetrics load hbase USING cpu p Assuming columns cpu cpu cpu cpu cpu column p CpuMetrics would contain something like div preformatted panel 1px div preformattedContent panelContent pre rowKey cpu cpu cpu cpu
Add macro expansion Latin p production scripts grow longer longer Latin need integrate standard programming techniques separation code sharing offered functions modules proposal adding macro expansion Latin posted http nofollow http p brief summary proposed syntax examples ul li Macro Definition p existing DEFINE keyword expanded allow definitions macros p b Syntax div code panel 1px div codeContent panelContent pre define lt name gt lt params gt returns lt aliases gt lt Latin fragment gt p b Example div code panel 1px div codeContent panelContent pre define sortkey returns C B filter C order B sortkey ul li Macro Expansion p b Syntax div code panel 1px div codeContent panelContent pre lt aliases gt lt macro name gt lt params gt p b Example Use macro script div code panel 1px div codeContent panelContent pre X load foo user address phone X user store bar p script expanded following Latin statements div code panel 1px div codeContent panelContent pre X load foo user address phone filter X order user store bar p b Notes p 1 alias macro visible outside prefixed macro name suffixed instance id avoid namespace collision 2 Macro expansion complete replacement function calls Recursive expansions supported ul li Macro Import p new IMPORT keyword used add macros defined another Latin p b Syntax div code panel 1px div codeContent panelContent pre span import lt Latin file name gt p b Example div code panel 1px div codeContent panelContent pre span import p b Note macro names global namespace
Javascript support embedding UDFs scripting languages p attached patch proposes javascript implementation embedding UDFs scripting similar Jython implementation uses Rhino provided differences ul alternate square li output schema provided lt functionName gt lt schema gt javascript annotations decorators functions first class objects li tuples converted objects using input schema way around using output schema p attached patch final yet particular lacks unit See transitive closure example p See following JIRAs context https nofollow https https nofollow https
Typed map p Currently map type untyped means map value always bytearray ie unknown type https give error message cogroup tuple keys different inner type del allow unknown type shuffle key somewhat relieve problem However typed map still beneficial p 1 User make semantic use map value type Currently user need explicitly cast map value ugly 2 Though https give error message cogroup tuple keys different inner type del allow unknown type shuffle key performance suffers raw comparator unknown type instead need instantiate value object invoke comparator p proposed syntax typed map map span error 91 type 93 p Typed map used place untyped map could occur example load map span error 91 int 93 b foreach generate map span error 91 int 93 a0 Map value tuple b stream cat map int j chararray p Map value bag p MapLookup typed map result datatype map load map span error 91 int 93 b foreach generate 0 key p Schema b b int p behavior untyped map remain
Need special interface Penny Inspector Gadget p proposed Penny tool needs access new logical plan order inject code dataflow modified plan needs able hand back modified plan execute p want open functionality general users proposal subclasses PigServer new class marked LimitedPrivate Penny class provide calls parse Latin script return logical plan one take logical plan execute
Enable StoreFunc make intelligent decision based job success failure p process using PIG various data processing component integration feel storage funcs lack p aware job succeeded creates problem storage funcs needs upload results another system p DB FTP another file system p looked DBStorage piggybank http nofollow http see essentially mechanism task following p 1 Creates recordwriter case open connection db 2 Open 3 Writes records batch 4 Executes commit rollback depending task p aproach works great task level work job level p certain tasks succeed job fail partial records going get uploaded p ideas workaround p current workaround fairly ugly created java wrapper launches jobs uploads DB job successful approach works really integrated
Storage access layer p layer needed provide high level data access abstraction tabular view data Hadoop could free users implementing data code layer also include columnar storage format order provide fast data projection data serialization schema language manage physical storage metadata Eventually could also support predicate pushdown performance improvement Initially layer could contrib project become hadoop subproject later
Penny framework workflow instrumentation p Penny framework instrumenting workflows rewrites scripts insert monitoring points aka agents provides communication framework triggering collecting events
support efficient merge join data sources natively support point lookups join large sparse tables p existing PIG merge join following limitations 1 assumes right side table must accessed sequentially record 2 perform well large sparse p current implementation merge join introduced interface IndexableLoadFunc LoadFunc supports ability seekNear given key reading next record merge join physical operator calls seekNear first key split effectively eliminating splits first subsequent keys found Subsequent joins found reading sequentially records right table looking matches left p method works well dense join tables performs poorly large sparse tables data sources support point lookups natively HBase example p proposed enhancement add new join type PIG latin specified PIG script join type cause merge join operator call seekNear every key rather first split
Add builtin UDFs building using bloom filters p Bloom filters common way select limited set records moving data join heavy weight operation add UDFs support building using bloom
p implementation based Alan implementation book http nofollow http made minor changes 1 Drop jackson feature requires Since Hadoop bundles jackson newer feature fails running Hadoop 2 Using Json format schema borrows Dmitry schema implememtation 3 bug
Need signature EvalFunc p generate unique signature user use key retrieve properties UDFContext need similar mechanism
Support efficient Tuples schemas known p Tuples significant overhead due fact fields Tuple contains primitive fields ints longs etc possible avoid overhead would result significant memory
pigrc specify default script cache p way specify default statements helpful multiple users using interactive
Integrate HCat DDL command p would like run hcat DDL command inside script Grunt use similar approach fs sh p Grunt gt hcat create table p Similar fs sh plan add Java API PigServer
UDFs p possible write UDFs Ruby UDFs registered way python javascript
Support pluggable PigProgressNotifcationListeners command line p would convenient able implement tt PigProcessNotifcationListener wire script jira support setting listener constructor args command perhaps like p tt noformat MyListener foo bar bat tt noformat p tt MyListener takes single string constructor get passed tt foo bar bat
Easier UDFs Convenient EvalFunc p got abstract extensions EvalFunc make life easier people interested push said classes p 3 classes extending next Class naming ul li tt TypedOutputEvalFunc lt gt Implements tt public Schema outputSchema Schema input based generic type subclass Provides common helper validation functions increment counters good bad Tuple data passed Useful input worked tuple size N li tt PrimitiveEvalFunc lt gt helper validation allowing ability subclass implement tt public exec input primitives Useful input single primitive position 0 li tt FunctionWrapperEvalFunc Wraps Guava Function implementation http nofollow http allows UDFs used scripts like tt MyFunction class implements tt Function div preformatted panel 1px div preformattedContent panelContent pre DEFINE myUdf MyFunction
Support providing parameters python script p embedded script python way get user passed parameters python Though https Need way deal params embedded python del adds capability reading params script python still would nice feature sort post processing happening python scrip based
Support multiple input schemas AvroStorage p barebones patch AvroStorage enables support multiple input schemas assumption input consists avro files different schemas unioned flat records p simple illustrative example attached run followed followed
Convenience mock Loader Storer simplify unit testing scripts p test would look follows div code panel 1px div codeContent panelContent pre PigServer pigServer span new PigServer TupleFactory tf Data data span foo span span b span c span LOAD foo USING span complex script test span STORE bar USING Iterator lt Tuple gt span bar assertEquals span 0 assertEquals span b 0 assertEquals span c 0
PPNL get notified plan gets executed p tt PigProgressNotificationListner get notified plan tt MROperPlan gets executed allows listeners inspect plan idea expect execution flow Proposal add following method PPNL interface marked evolving div code panel 1px div codeContent panelContent pre span public void initialPlanNotification MROperPlan plan
Allow set arbitrary jobconf pairs inside program p would useful able set arbitrary JobConf pairs inside program front COGROUP statement wonder whether simplest way add feature expanding set command
Provide method measure time spent UDFs p debugging slow jobs often useful know whether time spent UDFs UDFs easy measure within framework let users optionally track
Add option PigStorage p recently added option PigStorage allows us add filenames records come returned p Often users want whole path source file propose add option
Streaming provide conf settings environment p Hadoop Streaming converts jobconf properties environment variables streaming useful feature streaming
Grunt shortcuts p feature aimed providing shortcuts frequently used commands like illustrate dump explain describe quit help etc feature inspired postgres psql shortcuts tried implementing simple shortcut quitting grunt shell using minimal changes think feature help save many keystrokes users feature looks useful submit current patch review go ahead implementing following shortcuts p lt alias gt illustrate lt alias gt explain lt alias gt describe lt alias gt dump help p also useful view information stored HCatalog similar way psql lt alias gt display table display metadata etc p except delimiters able use characters shortcuts Please let know
output support recovery hadoop p hadoop output committer optionally support recovery handle application master getting restarted failing attempts possible support
Allow AvroStorage STORE Operations Use Schema Specified URI p attached patch makes AvroStorage accept flag storing alias schema used store contents alias contents file URI refers must exist fits somewhere schema flag specifying schema inline string flag specifying schema appears data
Allow Prefix Added URIs PigUnit Tests p running PigUnit tests use local file system useful absolute paths script test jail data test uses known
Introduce syntax able easily refer previously defined relation p Sometimes feel like swimming ANTLRs particular feature hard add supports syntax like div code panel 1px div codeContent panelContent pre load thing x span int b foreach generate x c foreach generate x foreach generate x p patch though need make sure change anything need add
HBaseStorage support setting timestamp field p Currently timestamp always set current time milliseconds option override propose configuration setting uses second field timestamp
Allow default funcs configurable p PigStorage used default specified would useful make
Piggybank functions mimic clause SQL p order test Hive written UDFs mimic behavior SQL clause thought would useful
Allow use Hive UDFs p would nice provide interoperability Hive wrap Hive UDF use Hive UDF p candidate project Google summer code 2013 information program found https nofollow https
Add assert keyword operator p Assert operator used data validation assert write script div code panel 1px div codeContent panelContent pre load something a0 span int a1 span int span assert a0 gt 0 cant negative span reasons p script fail assert
Make working HBase p HBase changed API incompatible way Following APIs tt HBaseStorage uses longer available ul li tt Boolean li tt DataOutput p Also addition HBase longer available one monolithic archive entire functionality broken smaller pieces tt tt
Add xml format explaining MapReduce Plan p Mortar needed easy way script map reduce plan added xml output format MapReduce plan make easier also added flag keep track store load original script associated alias temporary generated
Default split destination p split statement better default destination eg div code panel 1px div codeContent panelContent pre SPLIT X f1 lt 7 Z f3 lt 6 f3 gt 6 otherwise OTHERS tuples f1 gt amp amp f2 amp amp p candidate project Google summer code 2011 information program found http nofollow http
use hadoop local mode small jobs p use hadoop local mode small jobs mappers reducers mb
Provide StoreFunc LoadFunc Accumulo p Accumulo code allow reading writing working making robust would like try get included avoid necessity bundle additional jars p info currently exists http nofollow http current code https nofollow https p 1 Need translate Maven build 2 Need figure support Accumulo builds differences dependencies APIs
Ability disable commands operators p admin feature providing ability blacklist whitelist certain commands operations exposes could safe multitenant environment example sh invokes shell commands set allows users change configs tremendously useful general ability disable would make safer platform goal allow administrators able control user scripts Default behaviour would still filters applied commands
New logical optimizer rule ConstantCalculator p used LogicExpressionSimplifier simplify expression also calculates constant expression optimizer rule buggy disable default https Incorrect results FILTER FilterLogicExpressionSimplifier optimizer turned del p However need feature especially push since deal complex constant expression like replace expression constant actual push Yes user may manually calculation rewrite query even rewrite sometimes possible Consider case user want push datetime predicate user write ToDate udf since datetime p Jira provide new rule ConstantCalculator much simpler much less error prone replace
Ship dependent jar automatically p user use need register dependent jars manually would much convenient provide mechanism claim dependency ship jars
HBaseStorage support delete markers p case writing deletes HBase would useful precedent delete operations https nofollow https think valid use case especially considering HBase delete really write tombstone marker
